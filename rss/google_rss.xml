<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-08-30T12:27:30.866-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="MOOC"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="accessibility"></category><category term="optimization"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Android"></category><category term="Awards"></category><category term="ICML"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Responsible AI"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Maps"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="RAI-HCT Highlights"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1273&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8352967842060136321&lt;/id>;&lt;发布>;2023-08-29T12:57:00.000-07:00&lt;/发布>;&lt;更新>;2023-08- 29T12:57:11.011-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器人&quot;>; &lt;/category>;&lt;title type=&quot;text&quot;>;SayTap：四足运动的语言&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;作者：研究科学家 Yujin Tang 和 Wenhao Yu , 谷歌&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5Kd OrmhySUpQqM01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s320/SayTap%20hero.jpg&quot;样式=“显示：无；” />; &lt;p>; 人类和四足机器人之间简单有效的交互为创造智能且有能力的辅助机器人铺平了道路，创造了一个技术以超乎我们想象的方式改善我们生活的未来。这种人机交互系统的关键是使四足机器人能够响应自然语言指令。 &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;大型语言模型&lt;/a>; (LLM) 的最新发展证明了执行&lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;高层规划&lt;/a>;的潜力。然而，对于法学硕士来说，理解低级命令（例如关节角度目标或电机扭矩）仍然是一个挑战，特别是对于本质上不稳定的有腿机器人，需要高频控制信号。因此，大多数&lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;现有&lt;/a>;&lt;a href=&quot;https://code-as-policies.github.io/&quot;>;工作&lt;/a>; 假设为法学硕士提供高级 API 来规定机器人行为，这从本质上限制了系统的表达能力。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://saytap.github.io&quot;>;SayTap：语言到四足运动&lt;/a>;”中，我们提出一种使用脚部接触模式（指四足智能体在移动时将脚放在地面上的顺序和方式）作为桥梁的方法，以自然语言和输出低电平的运动控制器来桥接人类命令。级别命令。这产生了交互式四足机器人系统，该系统允许用户灵活地制定不同的运动行为（例如，用户可以使用简单的语言要求机器人行走、跑步、跳跃或进行其他动作）。我们贡献了 LLM 提示设计、奖励函数以及将 SayTap 控制器暴露给接触模式的可行分布的方法。我们证明 SayTap 是一个能够实现多种运动模式的控制器，这些模式可以转移到真实的机器人硬件上。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;SayTap 方法&lt;/h2>; &lt;p>; SayTap 方法使用接触模式模板，该模板是4 X &lt;i>;T&lt;/i>; 0 和 1 矩阵，其中 0 代表代理的脚在空中，1 代表脚在地面上。从上到下，矩阵中的每一行给出左前 (FL)、右前 (FR)、左后 (RL) 和右后 (RR) 脚的脚接触模式。 SayTap 的控制频率为 50 Hz，因此每个 0 或 1 持续 0.02 秒。在这项工作中，所需的脚接触模式由大小为 L&lt;sub>;w&lt;/sub>; 和形状为 4 X L&lt;sub>;w&lt;/sub>; 的循环滑动窗口定义。滑动窗口从接触模式模板中提取四个脚接地标志，这些标志指示脚是在地面上还是在 &lt;i>;t&lt;/i>; + 1 和 &lt;i>;t&lt;/i>; + L 之间的空中&lt;子>;w&lt;/子>;。下图概述了 SayTap 方法。 &lt;/p>; &lt;p>; SayTap 引入了这些所需的脚部接触模式，作为自然语言用户命令和运动控制器之间的新接口。运动控制器用于完成主要任务（例如，遵循指定的速度）并在指定的时间将机器人的脚放在地面上，使得实现的脚接触模式尽可能接近期望的接触模式。为了实现这一点，除了机器人的本体感觉数据（例如，关节位置和速度）和任务相关输入（例如，用户指定的速度命令）之外，运动控制器还将每个时间步所需的脚部接触模式作为其输入。 。我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_reinforcement_learning&quot;>;深度强化学习&lt;/a>;来训练运动控制器并将其表示为深度神经网络。在控制器训练期间，随机生成器对所需的脚部接触模式进行采样，然后优化策略以输出低级机器人动作，以实现所需的脚部接触模式。然后在测试时，法学硕士将用户命令转换为脚部接触模式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2​​aLrGlJ9CTOIkfI-G8jVZrYE083 _U84CCOq2pGuewb4szwCvoRc8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/s935/image13.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;935&quot; height=&quot;297&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2​​aLrGlJ9CTOIkfI-G8jVZrYE083_U84CCOq2pGuewb4szwCvoRc 8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/w640-h297/image13.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap 方法概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;br />; &lt;video autoplay=&quot; “循环=””静音=””playsinline=””样式==左边缘：10%；右边距：10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/saytap_blog.mp4&quot; type= &quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left” ： 汽车;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap 使用脚接触模式（例如，每只脚的 0 和 1 序列插图（其中 0 表示脚在空中，1 表示脚在地面）作为桥接自然语言用户命令和低级控制命令的接口。使用基于强化学习的运动控制器，经过训练可实现所需的接触模式，SayTap 允许四足机器人接受简单直接的指令（例如，“慢慢向前小跑。”）以及模糊的用户命令（例如，“好消息，我们这个周末要去野餐！”）并做出反应&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 我们证明，当给出正确设计的提示时，法学硕士能够准确地将用户命令映射到指定格式的足部接触模式模板中，即使在命令是非结构化或模糊的情况下。在训练中，我们使用随机模式生成器来生成具有各种模式长度 &lt;i>;T&lt;/i>; 的接触模式模板，基于循环内的脚与地面的接触比给定的步态类型&lt;i>;G&lt;/i>;，以便运动控制器能够学习广泛的运动分布，从而获得更好的泛化能力。有关更多详细信息，请参阅&lt;a href=&quot;https://arxiv.org/abs/2306.07580&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 一个简单的提示，仅包含三个上下文示例常见的脚部接触模式，法学硕士可以将各种人类命令准确地转换为接触模式，甚至可以推广到那些没有明确指定机器人应如何反应的指令。 &lt;/p>; &lt;p>; SayTap 提示简洁明了，由四个部分组成： (1) 一般说明，描述 LLM 应完成的任务； (2) 步态定义，提醒法学硕士有关四足步态的基本知识以及它们如何与情绪相关； (3)输出格式定义； (4) 为法学硕士提供在上下文中学习的机会的示例。我们还指定了五种速度，允许机器人向前或向后、快速或缓慢移动或保持静止。 &lt;/p>; &lt;span style=&quot;font-size:small;&quot;>; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px;white-space: pre-wrap;&quot;>;&lt; font color=&quot;#0000ff&quot;>;一般说明块&lt;/font>; 您是狗脚接触模式专家。您的工作是根据输入给出速度和脚接触模式。无论输入是什么，您始终会以正确的格式给出输出。 &lt;font color=&quot;#0000ff&quot;>;步态定义块&lt;/font>; 下面是关于步态的描述： 1. 小跑是两条对角相对的腿同时着地的步态。 2. 踱步是身体左右两侧的腿同时着地的步态。 3. 跳跃是两前/后腿同时着地的步态。它具有较长的暂停阶段，例如，在至少 25% 的周期长度内，所有脚都离开地面。这样的步态也给人一种幸福的感觉。 &lt;font color=&quot;#0000ff&quot;>;输出格式定义块&lt;/font>; 以下是描述速度和脚接触模式的规则： 1. 您应该首先输出速度，然后输出脚接触模式。 2. 有五种速度可供选择：[-1.0、-0.5、0.0、0.5、1.0]。 3. 一个图案有 4 条线，每条线代表一条腿的脚接触图案。 4. 每行都有一个标签。 “FL”是左前腿，“FR”是右前腿，“RL”是左后腿，“RR”是右后腿。 5. 每行中，“0”代表脚在空中，“1”代表脚在地上。 &lt;font color=&quot;#0000ff&quot;>;示例块&lt;/font>; 输入：Trot Slow 输出：0.5 FL: 11111111111111111000000000 FR: 00000000011111111111111111 RL: 00000000011111111111111111 RR: 11111111111111111000000000 输入: 绑定到位 输出: 0.0 FL: 11111111111100000000000000 FR: 11111111111100000000000000 RL: 00000011111111111100000000 RR: 00000011111111111100000000 输入: 快退 输出: -1.0 FL: 11111111100001111111110000 FR: 0000111111111000011 1111111 RL: 11111111100001111111110000 RR: 00001111111110000111111111 输入: &lt;/pre>;&lt;/span>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0 &quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;SayTap 提示 LLM。蓝色文字用于说明，不输入LLM。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;遵循简单直接的命令&lt;/h3>; &lt;p>; 我们在下面的视频中演示了 SayTap 系统可以成功执行命令直接且清晰的任务。尽管三个上下文示例中未涵盖某些命令，但我们能够引导 LLM 通过“步态定义块”（请参阅​​上面提示中的第二个块）表达其预训练阶段的内部知识提示。 &lt;/p>; &lt;br>; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;源src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/ source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;遵循非结构化或模糊命令&lt;/h3>; &lt;p >; 但更有趣的是 SayTap 处理非结构化和模糊指令的能力。只需稍加提示即可将某些步态与一般情绪印象联系起来，机器人在听到令人兴奋的信息（例如“我们要去野餐！”）时就会上下跳跃。此外，它还准确地呈现场景（例如，当被告知地面非常热时，它的脚几乎不接触地面，快速移动）。 &lt;/p>; &lt;br>; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/ source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和未来工作&lt;/h2>; &lt;p>;我们推出 SayTap，这是一种四足机器人交互系统，允许用户灵活地制定不同的运动行为。 SayTap 引入了所需的脚接触模式作为自然语言和低级控制器之间的新接口。这个新界面简单灵活，此外，它允许机器人遵循直接指令和没有明确说明机器人应如何反应的命令。 &lt;/p>; &lt;p>; 未来工作的一个有趣方向是测试暗示特定感觉的命令是否允许法学硕士输出所需的步态。在上面结果部分显示的步态定义块中，我们提供了一个将快乐心情与跳跃步态联系起来的句子。我们相信，提供更多信息可以增强法学硕士的解释（例如，隐含的感受）。在我们的评估中，快乐的感觉和跳跃步态之间的联系使机器人在遵循模糊的人类命令时表现得生动。未来工作的另一个有趣的方向是引入多模式输入，例如视频和音频。理论上，从这些信号转换而来的脚部接触模式仍然适用于我们的管道，并将解锁更多有趣的用例。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;Yujin Tang、Wenhao Yu、Jie Tan、Heiga Zen、Aleksandra Faust 和 Tatsuya Harada 进行了这项研究。这项工作是在团队在 Google Research 期间构思和执行的，并将在 Google DeepMind 继续进行。作者衷心感谢张廷南、Linda Luu、Kuang-Huei Lee、Vincent Vanhoucke 和 Douglas Eck 在实验中提供的宝贵讨论和技术支持。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/内容>;&lt;link href=&quot;http://blog.research.google/feeds/8352967842060136321/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08 /saytap-language-to-quadrupedal.html&quot; rel=&quot;alternate&quot; title=&quot;SayTap：语言到四足运动&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>; http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com /g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height= “72”网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQq M01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s72-c/SayTap%20hero.jpg&quot; width=&quot;72 &quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签： Blogger.com，1999：Blog-847492631452026626.POST-781660063956467509 &lt;/ID>; &lt;/ID>; &lt;/id>; &lt;Ebtumed>; 2023-08-28-28T09：59：009：00.00.003-07：00：003-07：00：00 07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;RO-ViT：使用视觉转换器进行开放词汇对象检测的区域感知预训练&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者Dahun Kim 和 Wei Cheng Kuo，研究科学家，Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa 98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVuIj5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s1100/RO-VIT -hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 检测视觉世界中的物体的能力对于计算机视觉和机器智能至关重要，从而实现自适应自主代理和多功能购物系统等应用。然而，现代物体检测器受到训练数据的手动注释的限制，导致词汇量明显小于现实中遇到的大量物体。为了克服这个问题，出现了&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;开放词汇检测任务&lt;/a>; (OVD)，它利用图像-文本对进行训练并合并新的类别名称在测试时将它们与图像内容相关联。通过将类别视为文本嵌入，开放词汇检测器可以预测各种看不见的对象。各种技术，例如&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;图像文本预训练&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot; >;知识蒸馏&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2112.09106&quot;>;伪标签&lt;/a>;和冻结模型，通常采用&lt;a href=&quot;https://en. wikipedia.org/wiki/Convolutional_neural_network&quot;>;卷积神经网络&lt;/a>;（CNN）主干已经被提出。随着&lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;>;视觉转换器&lt;/a>; (ViT) 的日益普及，这一点非常重要探索它们构建熟练的开放词汇检测器的潜力。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 现有方法假设可以使用预训练的&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;视觉语言模型&lt;/a>;（VLM），并专注于从这些模型中进行微调或&lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;蒸馏&lt;/a>;，以解决图像之间的差异级预训练和对象级微调。然而，由于 VLM 主要是为分类和检索等图像级任务而设计的，因此它们在预训练阶段没有充分利用对象或区域的概念。因此，如果我们将局部性信息构建到图像文本预训练中，这可能有利于开放词汇检测。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2305.07011&quot;>;RO-ViT：使用视觉变压器进行开放词汇对象检测的区域感知预训练&lt;/a>;”中，在 &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>; 上提出，我们引入了一种简单的方法，以区域感知的方式预训练视觉变换器，以改进开放词汇检测。在视觉转换器中，位置嵌入被添加到图像块中，以对图像中每个块的空间位置信息进行编码。标准预训练通常使用全图像位置嵌入，这不能很好地推广到检测任务。因此，我们提出了一种新的位置嵌入方案，称为“裁剪位置嵌入”，它可以更好地与检测微调中区域裁剪的使用保持一致。此外，我们将 &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function#Neural_networks&quot;>;softmax 交叉熵损失&lt;/a>; 替换为 &lt;a href=&quot;https://arxiv.org/ abs/1708.02002&quot;>;对比图像文本学习中的焦点损失&lt;/a>;，使我们能够从更具挑战性和信息量更大的示例中学习。最后，我们利用新对象提议的最新进展来增强开放词汇检测微调，这是由于观察到现有方法经常由于对前景类别的过度拟合而在提议阶段错过新对象。我们还在&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm/rovit&quot;>;此处&lt;/a>;发布了代码。 &lt;/p>; &lt;br />; &lt;h2>;区域感知图像文本预训练&lt;/h2>; &lt;p>; 现有的 VLM 经过训练，可以将整个图像与文本描述进行匹配。然而，我们观察到现有对比预训练方法和开放词汇检测中使用位置嵌入的方式之间存在不匹配。位置嵌入对于转换器很重要，因为它们提供了集合中每个元素来自何处的信息。此信息通常对下游识别和定位任务有用。预训练方法通常在训练期间应用全图像位置嵌入，并对下游任务（例如零样本识别）使用相同的位置嵌入。然而，识别发生在开放词汇检测微调的区域级别，这需要全图像位置嵌入泛化到他们在预训练期间从未见过的区域。 &lt;/p>; &lt;p>; 为了解决这个问题，我们提出了&lt;em>;裁剪位置嵌入&lt;/em>;（CPE）。通过 CPE，我们将位置嵌入从预训练的典型图像尺寸（例如 224x224 像素）上采样到检测任务的典型图像尺寸（例如 1024x1024 像素）。然后我们随机裁剪一个区域并调整其大小，并在预训练期间将其用作图像级位置嵌入。作物的位置、比例和纵横比是随机采样的。直观上，这导致模型将图像本身视为完整图像，而不是一些较大的未知图像的区域裁剪。这更好地匹配下游检测用例，其中识别发生在区域级别而不是图像级别。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZ qTJq4heQruWCeaQsM5uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s952 /RO-ViT-img6.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;952 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZqTJq4heQruWCeaQsM5 uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s16000/RO-ViT-img6.jpg&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;对于预训练，我们提出&lt;em>;裁剪位置嵌入&lt;/em>;&amp;nbsp; （CPE）随机裁剪位置嵌入区域并调整其大小，而不是使用整个图像位置嵌入（PE）。此外，我们使用焦点损失而不是常见的softmax交叉熵损失来进行对比学习。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们还发现从困难的例子中学习是有益的焦点损失。与 softmax 交叉熵损失相比，焦点损失可以更好地控制样本的权重。我们在图像到文本和文本到图像的损失中采用焦点损失并将其替换为 softmax 交叉熵损失。 CPE 和焦点损失都没有引入额外的参数并且计算成本最小。 &lt;/p>; &lt;br />; &lt;h2>;开放词汇检测器微调&lt;/h2>; &lt;p>;开放词汇检测器使用“基本”类别的检测标签进行训练，但需要检测“测试时的“基础”和“新颖”（未标记）类别。尽管主干特征是根据大量开放词汇数据预先训练的，但添加的检测器层（颈部和头部）是使用下游检测数据集进行新训练的。现有的方法经常在对象提议阶段错过新颖/未标记的对象，因为提议倾向于将它们分类为背景。为了解决这个问题，我们利用新颖的对象提议方法的最新进展，并采用基于本地化质量的对象性（即&lt;a href=&quot;https://arxiv.org/abs/2108.06753&quot;>;centerness&lt;/a>;分数）而不是物体或非物体的二元分类得分，它与检测得分相结合。在训练过程中，我们将每个检测到的区域的检测分数计算为该区域嵌入之间的&lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;余弦相似度&lt;/a>;（通过&lt;a href计算） =&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks&quot;>;RoI-Align&lt;/a>; 操作）和基本类别的文本嵌入。在测试时，我们附加新颖类别的文本嵌入，并且现在通过基本类别和新颖类别的联合来计算检测分数。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw 7MCZE0IsOhdkt8ny6ipDfFi4BxNOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s682/RO-ViT -img4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;682&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw7MCZE0IsOhdkt8ny6ipDfFi4Bx NOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s16000/RO-ViT-img4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过用检测器头替换全局平均池化，将预训练的 ViT 主干转移到下游开放词汇检测。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks#:~:text=new%20method%20叫-,ROIAlign,-%2C%20which%20can%20represent&quot; style=&quot;text- align: left;&quot;>;RoI-Align&lt;/a>;&amp;nbsp;嵌入与缓存的类别嵌入匹配以获得VLM分数，该分数与检测分数组合成开放词汇检测分数。&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们评估 RO-ViT &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; 开放词汇检测基准。在系统级别，我们的最佳模型在稀有类别上实现了 33.6 个框&lt;a href=&quot;https://blog.paperspace.com/mean-average- precision/&quot;>;平均精度&lt;/a>;（&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;AP&lt;sub>;r&lt;/sub>;&lt;/a>;) 和 32.1 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;掩码AP&lt;sub>;r&lt;/sub>;&lt;/a>;，其性能优于现有的基于 ViT 的最佳方法 OWL-ViT 8.0 AP&lt;sub>;r&lt;/sub>; 和最佳的基于 CNN 的方法 ViLD-Ens 5.8 掩模 AP&lt;sub>;r&lt;/sub>;。它还超过了许多其他基于知识蒸馏、预训练或弱监督联合训练的方法的性能。&lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr -caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel-0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQH rll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height =“426”data-original-width =“1200”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel- 0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQHrll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s16000/image2.png &quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RO-ViT 的性能优于当前状态艺术 (SOTA)&lt;a href=&quot;https://arxiv.org/abs/2205.06230&quot; style=&quot;text-align: left;&quot;>;基于 ViT&lt;/a>; 和&lt;a href=&quot; https://arxiv.org/abs/2104.13921&quot; style=&quot;text-align: left;&quot;>;基于 CNN 的 LVIS 开放词汇检测基准的方法。我们在稀有类别 (AP&lt;sub>;r&lt;/sub>;) 上显示掩模 AP，但基于 SOTA ViT 的 (OwL-ViT) 则显示框 AP。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/表>; &lt;p>; 除了通过开放词汇检测评估区域级表示之外，我们还通过 &lt;a href=&quot;https://arxiv.org/abs 评估 RO-ViT 在图像文本检索中的图像级表示/1504.00325&quot;>;MS-COCO&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/1505.04870&quot;>;Flickr30K&lt;/a>; 基准测试。我们的具有 303M ViT 的模型在 MS COCO 上优于具有 1B ViT 的最先进的 CoCa 模型，并且在 Flickr30K 上处于同等水平。这表明我们的预训练方法不仅提高了区域级表示，还提高了检索的全局图像级表示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKV rwp3_wXzf8KGqaY50rK3fLcWtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s1692/image5.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;940&quot; data-original-width=&quot;1692&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKVrwp3_wXzf8KGqaY50rK3fLc WtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;我们在 MS COCO 和 Flickr30K 基准测试上展示了零样本图像文本检索，并与双编码器方法进行了比较。我们报告图像到文本 (I2T) 和文本到图像 (T2I) 检索任务的recall@1（top-1 召回率）。 RO-ViT 的性能优于最先进的&lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot; style=&quot;text-align: left;&quot;>;CoCa&lt;/a>; &amp;nbsp;相同的主干。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFfI7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT- 03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s1476/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;362&quot; 数据-original-width=&quot;1476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFf I7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT-03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td >;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LVIS 上的 RO-ViT 开放词汇检测。为了清楚起见，我们仅显示新颖的类别。 RO-ViT 检测到许多在检测训练期间从未见过的新类别：“fishbowl”、“sombrero”、“persimmon”、“gargoyle”。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;位置嵌入的可视化&lt;/h2>; &lt;p>; 我们将 RO-ViT 学习到的位置嵌入可视化并与基线进行比较。每个图块是一个补丁与所有其他补丁的位置嵌入之间的余弦相似度。例如，左上角的图块（以红色标记）可视化了位置（行=1，列=1）的位置嵌入与 2D 中所有其他位置的位置嵌入之间的相似性。补丁的亮度表示不同位置的学习位置嵌入的接近程度。 RO-ViT 在不同的斑块位置形成更明显的簇，在中心斑块周围显示对称的全局模式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwS twANDQkmi6f2w62aXNhkEu3daexyXV5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s1686/RO-ViT-img1.jpg&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1686&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwStwANDQkmi6f2w62aXNHkEu3daexyX V5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s16000/RO-ViT-img1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;每个图块显示补丁的位置嵌入（在指示的行列位置）与所有其他补丁的位置嵌入之间的余弦相似度。使用 ViT-B/16 主干。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了 RO-ViT，一种对比图像文本预训练框架，以弥合图像级预训练和开放词汇检测微调之间的差距。我们的方法简单、可扩展，并且易于应用于任何对比骨干网，计算开销最小，并且不增加参数。 RO-ViT 在 LVIS 开放词汇检测基准和图像文本检索基准上实现了最先进的水平，表明学习的表示不仅在区域级别有益，而且在图像级别也非常有效。我们希望这项研究能够帮助从图像文本预训练的角度进行开放词汇检测的研究，这对区域级和图像级任务都有好处。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;Dahun Kim、Anelia Angelova 和 Wei Cheng Kuo 进行了这项工作，目前在 Google DeepMind 工作。我们要感谢 Google Research 的同事提供的建议和有益的讨论。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/781660063956467509/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html#comment-form&quot; rel=&quot;replies &quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;edit&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;RO-ViT：区域感知预训练使用视觉转换器进行开放词汇对象检测训练” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/ uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1 .blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVu Ij5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s72-c/RO-ViT-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss /&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-3964459643854458124&lt;/id>; &lt;发布>;2023-08-25T10:38:00.003-07:00&lt;/发布>;&lt;更新>;2023-08-25T10:46:59.205-07:00&lt;/更新>;&lt;category schema=&quot;http://www .blogger.com/atom/ns#&quot; term=&quot;机器感知&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT 亮点&quot;>; &lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 研究的负责任的人工智能：感知公平性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部感知公平性团队联合负责人 Susanna Ricco 和 Utsav Prabhu&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MOHdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJ e8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1-inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s320/hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google 的&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;负责任的人工智能研究&lt;/a>;建立在具有不同背景和专业知识的团队之间的协作基础上，研究人员和产品开发人员之间，并最终与整个社区之间。感知公平团队将计算机视觉和机器学习 (ML) 公平性方面的深厚主题专业知识与构建感知系统的研究人员直接联系相结合，推动 Google 及其他领域产品的进步。我们正在共同努力，在 &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google 人工智能原则&lt;/a>;的指导下，从头开始有意设计具有包容性的系统。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf -_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkypL7MqBpdeqIJm 9H5sani/s948/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;214&quot; data-original-width=&quot;948 “高度=“144”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf-_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ 7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkYpL7MqBpdeqIJm9H5sani/w640-h144/image1.gif&quot;宽度=“640”/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;感知公平研究涵盖先进多模式模型的设计、开发和部署，包括最新的基础和生成模型为 Google 产品提供支持的模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 我们团队的使命是推进多模式 ML 系统的公平性和包容性前沿，特别是与&lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;>;基础&lt;/a>;模型和&lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;生成人工智能&lt;/a >;。这涵盖了核心技术组件，包括分类、本地化、字幕、检索、视觉问答、文本到图像或文本到视频生成以及生成图像和视频编辑。我们相信，公平和包容可以而且应该成为这些应用程序的首要性能目标。我们的研究重点是解锁新颖的分析和缓解措施，使我们能够在整个开发周期中主动设计这些目标。我们回答核心问题，例如：我们如何使用机器学习负责任地、忠实地模拟人类对人口、文化和社会身份的看法，以促进公平和包容？我们可以测量哪些类型的系统偏差（例如，在某些肤色的人的图像上表现不佳）以及如何使用这些指标来设计更好的算法？如何构建更具包容性的算法和系统，并在发生故障时快速反应？ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;衡量媒体中人物的代表性&lt;/h2>; &lt;p>; 可以编辑、策划或创建图像或视频可以影响任何接触其输出的人，塑造或强化世界各地观众的信念。减少代表性伤害（例如强化刻板印象或诋毁或消除人群）的研究需要对内容和&lt;a href=&quot;https://ai.googleblog.com/2023/07/using-societal -context-knowledge-to.html&quot;>;社会背景&lt;/a>;。它取决于不同的观察者如何看待自己、他们的社区或他人的代表方式。关于哪些社会类别应该使用计算工具进行研究以及如何负责任地进行研究，该领域存在相当多的争论。我们的研究重点是致力于寻求可扩展的解决方案，这些解决方案以社会学和社会心理学为基础，与人类的感知相一致，拥抱问题的主观本质，并实现细致入微的测量和缓解。一个例子是我们对&lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;人类感知和肤色注释差异的研究使用&lt;a href=&quot;skintone.google&quot;>;僧侣肤色等级&lt;/a>;的图像&lt;/a>;。 &lt;/p>; &lt;p>; 我们的工具还用于研究大规模内容集合中的表示。通过我们的社交探索媒体理解 (MUSE) 项目，我们与学术研究人员、非营利组织和主要消费品牌合作，了解主流媒体和广告内容的模式。我们于 2017 年首次发表了这项研究成果，其中一项共同撰写的研究分析了&lt;a href=&quot;https://about.google/intl/ALL_au/main/gender-equality-films/&quot;>;好莱坞电影中的性别平等&lt;/a >;。从那时起，我们增加了分析的规模和深度。 2019 年，我们发布了基于&lt;a href=&quot;https://www.thinkwithgoogle.com/feature/diversity-inclusion/&quot;>;超过 270 万条 YouTube 广告&lt;/a>;的调查结果。在&lt;a href=&quot;https://blog.google/technology/ai/using-ai-to-study-12-years-of-representation-in-tv/&quot;>;最新研究&lt;/a>;中，我们研究了在超过十二年的美国流行电视节目中，感知性别表现、感知年龄和肤色的交叉表现。这些研究为内容创作者和广告商提供了见解，并进一步为我们自己的研究提供信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFuL3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPt S8ONz02Nrgn4If7YOukbw0txEfy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/s800/image3.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;800&quot; height=&quot;360&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFul3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPtS8ONz02Nrgn4If7YOukbw0tx Efy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/w640-h360/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;计算信号的图示（非实际数据），可以大规模分析以揭示媒体集合中的代表性模式。 [视频集/Getty Images]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;展望未来，我们正在扩展我们关注的机器学习公平性概念以及涉及的领域它们得到负责任的应用。除了逼真的人物图像之外，我们正在努力开发工具，以在插图、人形角色的抽象描述，甚至根本没有人的图像中对社区和文化的表现进行建模。最后，我们不仅需要推理描绘的是谁，还需要推理他们是如何被描绘的——通过周围的图像内容、随附的文本和更广泛的文化背景传达了什么样的叙事。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;分析感知系统的偏差属性&lt;/h2>; &lt;p>;构建高级机器学习系统非常复杂，多个利益相关者告知决定产品行为的各种标准。历史上，总体质量是使用测试数据集的汇总统计数据（例如总体准确性）作为用户体验的代理来定义和测量的。但并非所有用户都以相同的方式体验产品。 &lt;br />; &lt;/p>; &lt;p>; 感知公平性能够对汇总统计之外的细微系统行为进行实际测量，并使这些指标成为系统质量的核心，直接通知产品行为和发布决策。这通常比看起来要困难得多。将复杂的偏见问题（例如，交叉亚组之间的表现差异或刻板印象强化实例）提炼为少量指标而不丢失重要的细微差别是极具挑战性的。另一个挑战是平衡公平性指标和其他产品指标（例如用户满意度、准确性、延迟）之间的相互作用，尽管它们是兼容的，但通常被认为是冲突的。研究人员通常将他们的工作描述为优化“准确性-公平性”权衡，而实际上广泛的用户满意度与满足公平性和包容性目标是一致的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu 2yPn_wK23Z8PdYkeEmycLRJ0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s640/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu2yPn_wK23Z8PdYkeEmycLRJ 0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们构建并发布了&lt;a href=&quot;https://storage.googleapis.com/openimages/web/extended.html#miap&quot;>;MIAP 数据集&lt;/a>;作为&lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;>;开放图像&lt;/a>;的一部分，利用我们对社交相关概念的感知和检测的研究复杂系统中的偏见行为，以创建进一步推进计算机视觉中 ML 公平性研究的资源。原始照片来源 - 左：&lt;a href=&quot;https://www.flickr.com/photos/boston_public_library/8242010414/&quot;>;波士顿公共图书馆&lt;/a>;；中间：&lt;a href=&quot;https://www.flickr.com/photos/jenrobinson/20183915655/&quot;>;jen robinson&lt;/a>;；右：&lt;a href=&quot;https://www.flickr.com/photos/mrgarin/2484859086/&quot;>;加林·丰斯&lt;/a>;；所有使用均已获得 &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;>;CC-BY 2.0 许可证&lt;/a>;许可。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt; /table>; &lt;br />; &lt;p>; 为此，我们的团队专注于两个广泛的研究方向。首先，民主化对易于理解且广泛适用的公平分析工具的访问，让合作伙伴组织将其采用到产品工作流程中，并通知整个公司的领导层解释结果。这项工作包括制定广泛的基准、&lt;a href=&quot;https://ai.googleblog.com/2021/06/a-step-toward-more-inclusive-people.html&quot;>;整理广泛使用的高质量测试数据集&lt;/a>; 以及以切片分析和反事实测试等技术为中心的工具——通常建立在前面描述的核心表示信号工作的基础上。其次，&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3461702.3462557&quot;>;推进公平分析的新颖方法&lt;/a>; - 包括与&lt;a href=&quot;https://ai 合作.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;可能带来突破性发现的产品努力&lt;/a>;或&lt;a href=&quot;https://services.google.com/ fh/files/blogs/bsr-google-cr-api-hria-executive-summary.pdf&quot;>;告知启动策略&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;负责任地推进人工智能&lt;/h2>; &lt;p>;我们的工作并不止于分析模型行为。相反，我们将此作为与产品团队的其他研究人员和工程师合作确定算法改进的起点。在过去的一年里，我们推出了升级的组件，为 Google 相册中的搜索和&lt;a href=&quot;https://blog.google/products/photos/google-photos-memories-view/&quot;>;回忆&lt;/a>;功能提供支持，通过添加层来防止错误在系统中级联，从而实现更一致的性能并大幅提高稳健性。我们正在努力改进 Google 图片中的排名算法，以实现多样化。我们更新了可能强化历史刻板印象的算法，负责任地使用额外信号，以便&lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;每个人都更有可能看到他们自己反映在搜索结果中并找到他们正在寻找的内容&lt;/a>;。 &lt;/p>; &lt;p>; 这项工作自然地延续到了生成人工智能的世界，其中 &lt;a href=&quot;https://blog.google/technology/research/how-ai-creates-photorealistic-images-from-text /&quot;>;模型可以根据图像和文本提示创建图像或视频集合&lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning -in.html&quot;>;可以回答有关图像和视频的问题&lt;/a>;。我们对这些技术&lt;a href=&quot;https://blog.google/products/shopping/ai-virtual-try-on-google-shopping/&quot;>;为用户提供新体验&lt;/a的潜力感到兴奋>; 并作为进一步我们自己研究的工具。为了实现这一目标，我们正在与研究和负责任的人工智能社区合作，开发减轻故障模式的护栏。我们正在利用我们的工具来理解表示，以支持可与人类反馈相结合的可扩展基准，并投资于从预训练到部署的研究，以引导模型生成更高质量、更具包容性和更可控的输出。我们希望这些模型能够激励人们，产生多样化的输出，在不依赖比喻或刻板印象的情况下翻译概念，并在提示的反事实变化中提供一致的行为和响应。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;机遇和正在进行的工作&lt;/h2>; &lt;p>; 尽管经过十多年的专注工作，该领域感知公平技术似乎仍然是一个新兴且快速发展的领域，充满了突破性技术的机会。我们继续看到在跨学科奖学金支持下贡献技术进步的机会。我们可以在图像中测量的内容与人类身份和表达的基本方面之间的差距很大 - 缩小这一差距将需要越来越复杂的媒体分析解决方案。表明真实代表性的数据指标，位于适当的背景下并考虑多种观点，对我们来说仍然是一个公开的挑战。我们是否能够可靠地识别对细微刻板印象的描述，不断更新它们以反映不断变化的社会，并辨别它们可能令人反感的情况？由人类反馈驱动的算法进步指明了一条充满希望的前进道路。 &lt;/p>; &lt;p>; 最近对现代大型模型开发背景下的人工智能安全和道德的关注激发了衡量系统偏差的新思维方式。我们正在探索使用这些模型的多种途径 - 以及基于概念的可解释性方法、因果推理方法和尖端用户体验研究的最新发展 - 来量化和最大程度地减少不良偏见行为。我们期待着应对未来的挑战并开发适合每个人的技术。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢 Perception 的每一位成员公平团队以及我们所有的合作者。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3964459643854458124/comments/default&quot; rel=&quot;replies&quot; title =&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html#comment-form &quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124&quot; rel=&quot;edit “ type =“application/atom+xml”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124”rel =“self”type =“application/atom+xml” &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Google 研究的负责任的人工智能：感知公平&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com &lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded .gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MO HdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJe8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1 -inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr ：总计>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6350776943545232437&lt;/id>;&lt;发布>;2023-08-24T15:10:00.000-07:00&lt;/已发布>;&lt;更新>;2023-08-24T15:10:57.268-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;量子计算&quot;>;&lt; /category>;&lt;title type=&quot;text&quot;>;如何将嘈杂的量子处理器与经典计算机进行比较&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Sergio Boixo 和 Vadim 发布Smelyanskiy，Google 量子 AI 团队首席科学家&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qs EcLuEDPn​​WZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s320/hero.jpg&quot;样式= “显示：无；” />; &lt;p>; 一台全面的纠错量子计算机将能够解决一些经典计算机无法解决的问题，但建造这样的设备是一项艰巨的任务。我们为实现完全错误所取得的&lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;里程碑&lt;/a>;感到自豪-修正了量子计算机，但是大型计算机还需要几年的时间。与此同时，我们正在使用当前的噪声量子处理器作为量子实验的灵活平台。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 与纠错量子&lt;em>;计算机&lt;/em>;相比，目前在噪声量子&lt;em>;处理器&lt;/em>;中进行的实验在噪声降低量子态之前，仅限于几千个量子操作或门。 2019 年，我们在量子处理器&lt;a href= “https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;并首次表明&lt;/a>;它的性能优于最先进的经典超级计算。 &lt;/p>; &lt;p>; 虽然它们尚未达到超越经典的能力，但我们也使用我们的处理器来观察新的物理现象，例如&lt;a href=&quot;https://www.nature.com/articles/s41586 -021-04257-w&quot;>;时间晶体&lt;/a>;和&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;马约拉纳边缘模式&lt;/a>;，并制作了新的实验发现，例如相互作用光子的强大&lt;a href=&quot;https://ai.googleblog.com/2022/12/formation-of-robust-bound-states-of.html&quot;>;束缚态&lt;/a>;以及 Floquet 演化的马约拉纳边缘模式的&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;噪声弹性&lt;/a>;。 &lt;/p>; &lt;p>; 我们预计，即使在这种中间的、嘈杂的状态下，我们也会找到量子处理器的应用，其中有用的量子实验可以比经典超级计算机上的计算速度快得多——我们称之为“计算应用” “量子处理器。目前还没有人展示出这样一种超越经典的计算应用。因此，当我们的目标是实现这一里程碑时，问题是：将在此类量子处理器上运行的量子实验与经典应用程序的计算成本进行比较的最佳方法是什么？ &lt;/p>; &lt;p>; 我们已经知道如何将纠错量子算法与经典算法进行比较。在这种情况下，&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_complexity&quot;>;计算复杂度&lt;/a>;领域告诉我们，我们可以比较它们各自的计算成本——即完成任务所需的操作。但对于我们目前的实验性量子处理器来说，情况还没有那么明确。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;噪声量子处理实验的有效量子体积、保真度和计算成本&lt;/a>;”中，我们提供了一个框架为了测量量子实验的计算成本，引入了实验的“有效量子体积”，即有助于测量结果的量子操作或门的数量。我们应用这个框架来评估最近三个实验的计算成本：我们的&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;随机电路采样&lt;/a>; &lt;a href=&quot;https://arxiv .org/abs/2304.11119&quot;>;实验&lt;/a>;，我们的&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>;实验测量被称为“失序相关器”的量”（OTOC）&lt;/a>;，以及&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;最近关于 Floquet 进化的实验&lt;/a>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Ising_model&quot;>;伊辛模型&lt;/a>;。我们对 OTOC 感到特别兴奋，因为它们提供了一种直接的方法来通过实验测量电路的有效量子体积（一系列量子门或操作），这本身对于经典计算机来说是一项计算上困难的任务，要精确估计。 OTOC 在&lt;a href=&quot;https://en.wikipedia.org/wiki/Nuclear_Magnetic_resonance&quot;>;核磁共振&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/ Electron_paraMagnetic_resonance&quot;>;电子自旋共振光谱&lt;/a>;。因此，我们相信 OTOC 实验是量子处理器首次计算应用的有希望的候选者。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd 6VsOOMzDaexufiJs_8oyo1G37h2lr4Y1Y28zD7lZ3OIB_EL_LSY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/s1280/image3.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;1280&quot; height=&quot;297&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd6VsOOMzDaexufiJs_8oyo1G 37h2lr4Y1Y28zD7lZ3OIB_EL_LsY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/w640-h297/image3.png&quot;宽度=“640”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;计算成本和最近一些量子实验的影响图。虽然有些（例如，&lt;a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;QC-QMC 2022&lt;/a>;）产生了很大影响，而另一些（例如，&lt; a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;RCS 2023&lt;/a>;）的计算成本很高，但还没有一个既有用又困难，值得考虑“计算应用程序”。我们假设我们未来的 OTOC 实验可能是第一个突破这个门槛的实验。文中引用了绘制的其他实验。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div >; &lt;h2>;随机电路采样：评估噪声电路的计算成本&lt;/h2>; &lt;p>; 当谈到在噪声量子处理器上运行量子电路时，有两个相互竞争的考虑因素。一方面，我们的目标是做一些经典难以实现的事情。计算成本（在经典计算机上完成任务所需的操作数量）取决于量子电路的&lt;em>;有效量子体积&lt;/em>;：体积越大，计算成本越高，量子越多处理器的性能可以超越经典处理器。 &lt;/p>; &lt;p>; 但另一方面，在嘈杂的处理器上，每个量子门都会给计算带来错误。操作越多，误差就越大，并且量子电路在测量感兴趣的量时的保真度就越低。考虑到这一点，我们可能更喜欢更简单、有效体积更小的电路，但这些电路很容易被经典计算机模拟。我们希望最大化这些相互竞争的考虑因素的平衡，称为“计算资源”，如下所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1w OEPS4TwK55cMxJmg19mQD7ZCtuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/s973/image1.png&quot; 样式 = &quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;653&quot; data-original-width=&quot;973&quot; height=&quot;430&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1wOEPS4TwK55cMxJmg19mQD7Z Ctuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/w640-h430/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;量子电路中量子体积和噪声之间的权衡图，以称为“计算资源”的量捕获。对于有噪声的量子电路，这最初会随着计算成本的增加而增加，但最终，噪声会超出电路并导致其减少。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; 我们可以通过一个简单的 &lt;a href=&quot;https://ai.googleblog.com/2019&quot; 看到这些相互竞争的考虑因素如何发挥作用/10/quantum-supremacy-using-programmable.html&quot;>;量子处理器的“hello world”程序&lt;/a>;，称为随机电路采样（RCS），这是量子处理器超越经典计算机的首次演示。任何一个门的任何错误都可能使这个实验失败。不可避免的是，这是一个很难以高保真度实现的实验，因此它也可以作为系统保真度的基准。但它也对应于量子处理器可实现的最高已知计算成本。我们最近报告了迄今为止进行的&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;最强大的 RCS&lt;/a>; 实验，测得的实验保真度较低，为 1.7x10&lt;sup>;-3&lt;/ support>;，理论计算成本高达 ~10&lt;sup>;23&lt;/sup>;。这些量子电路有 700 个二量子位门。我们估计这个实验需要约 47 年的时间才能在世界上最大的超级计算机上进行模拟。虽然这检查了计算应用程序所需的两个框之一——它的性能优于传统的超级计算机——但它本身并不是一个特别有用的应用程序。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;OTOC 和 Floquet 演化：局部可观测量的有效量子体积&lt;/h2>; &lt;p>; 有量子&lt;a href=&quot;https://en.wikipedia.org/wiki/Many-body_problem&quot;>;多体物理学&lt;/a>;中存在许多经典的棘手问题，因此在我们的量子上运行其中一些实验处理器潜力巨大。我们通常认为这些实验与 RCS 实验有些不同。我们通常关心的是更具体的局部物理可观测值，而不是在实验结束时测量所有量子位的量子态。由于并非电路中的每个操作都必然影响可观测量，因此局部可观测量的有效量子体积可能小于运行实验所需的完整电路的有效量子体积。 &lt;/p>; &lt;p>; 我们可以通过应用&lt;a href=&quot;https://en.wikipedia.org/wiki/Special_relativity&quot;>;相对论&lt;/a>;中的光锥概念来理解这一点，它决定了哪些事件时空之间可以存在因果关系：某些事件不可能相互影响，因为信息需要时间在它们之间传播。我们说两个这样的事件都在它们各自的光锥之外。在量子实验中，我们用一种叫做“蝴蝶锥”的东西取代了光锥，其中锥体的生长由蝴蝶速度决定，即信息在整个系统中传播的速度。 （这个速度的特征是测量 OTOC，稍后讨论。）局部可观测量的有效量子体积本质上是蝴蝶锥体的体积，仅包括与可观测量有因果关系的量子运算。因此，信息在系统中传播得越快，有效体积就越大，因此经典模拟就越困难。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY -WpDN34wUfig-rtH1NC7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/s1392/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1392&quot; data-original-width=&quot;1092&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY-WpDN34wUfig-rtH1NC 7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/w502-h640/image2.png&quot; width=&quot;502&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;描述门的有效体积 V&lt;sub>;eff&lt;/sub>;局部可观测量 B。称为有效面积 A&lt;sub>;eff&lt;/sub>; 的相关量由平面和圆锥体的横截面表示。底座的周长对应于以蝴蝶速度v&lt;sub>;B&lt;/sub>;移动的信息传播的前端。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>;我们将此框架应用于最近的&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;实验&lt;/a>;，实现所谓的 Floquet Ising 模型（一种物理模型）与时间晶体和马约拉纳实验有关。根据该实验的数据，可以直接估计最大电路的有效保真度为 0.37。由于测得的门错误率为 ~1%，因此估计的有效体积为 ~100。这比光锥小得多，光锥包括 127 个量子位上的 2000 个门。所以，本次实验的蝴蝶速度相当小。事实上，我们认为，使用获得比实验更高精度的数值模拟，有效体积仅覆盖约 28 个量子位，而不是 127 个。这种小的有效体积也已通过 OTOC 技术&lt;a href=&quot;https://arxiv.org/abs/2306.17839&quot;>;证实&lt;/a>;。尽管这是一个深度电路，但估计的计算成本为 5x10&lt;sup>;11&lt;/sup>;，几乎比最近的 RCS 实验少了一万亿倍。相应地，该实验可以在单个 A100 GPU 上&lt;a href=&quot;https://arxiv.org/abs/2306.15970&quot;>;模拟&lt;/a>;每个数据点不到一秒的时间。因此，虽然这确实是一个有用的应用程序，但它并没有满足计算应用程序的第二个要求：大大优于经典模拟。 &lt;/p>; &lt;p>; OTOC 的信息置乱实验是计算应用的一个有前途的途径。 OTOC 可以告诉我们有关系统的重要物理信息，例如蝶形速度，这对于精确测量电路的有效量子体积至关重要。使用快速纠缠门的 OTOC 实验为首次超越经典的量子处理器计算应用演示提供了潜在的途径。事实上，在 2021 年的&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>;实验&lt;/a>;中，我们实现了 F&lt;sub>;eff &lt;/sub>; 的有效保真度~ 0.06，实验信噪比约为 1，对应于 ~250 个门的有效体积和 2x10&lt;sup>;12&lt;/sup>; 的计算成本。 &lt;/p>; &lt;p>; 虽然这些早期的 OTOC 实验不够复杂，无法超越经典模拟，但 OTOC 实验成为计算应用首次演示的良好候选者有其深刻的物理原因。近期量子处理器可以实现的大多数有趣的量子现象很难经典地模拟，这与探索许多量子能级的量子电路相对应。这种演化通常是混乱的，并且标准时间顺序相关器（TOC）在这种情况下很快衰减到纯随机平均值。没有留下任何实验信号。 &lt;a href=&quot;https://arxiv.org/abs/2101.08870&quot;>;OTOC 测量&lt;/a>;不会发生这种情况，它允许我们随意增加复杂性，仅受每个门的错误的限制。我们预计，将错误率降低一半将使计算成本加倍，从而将该实验推向超越经典的阶段。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;使用我们开发的有效量子体积框架，我们确定我们的 RCS 和 OTOC 实验以及最近的 Floquet 进化实验的计算成本。虽然这些都无法满足计算应用的要求，但我们预计，随着错误率的提高，OTOC 实验将成为量子处理器的第一个超越经典的有用应用。 &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6350776943545232437/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html&quot; rel=&quot;alternate&quot; title=&quot;如何将嘈杂的量子处理器与经典计算机进行比较&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2 qsEcLuEDPn​​WZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s72 -c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-3589347607411181063&lt;/id>;&lt;已发布>;2023-08-24T12:33:00.002-07:00&lt;/已发布>;&lt;已更新>;2023-08-24T12:33:37.720-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;教授语言模型进行算法推理&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;发布者：MILA 研究生 Hattie Zhou、Google 研究科学家 Hanie Sedghi&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n _pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm- xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s320/hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 大型语言模型 (LLM)，例如 &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>; 和&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; 近年来取得了令人瞩目的进步，由&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;扩大模型&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;>;训练数据大小驱动&lt;/a>;。尽管如此，一个长期存在的争论是法学硕士是否可以进行象征性推理（即根据逻辑规则操纵符号）。例如，法学硕士能够在数字较小时执行简单的算术运算，但在数字较大时则难以执行。这表明法学硕士尚未学习执行这些算术运算所需的基本规则。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 虽然神经网络具有强大的&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper .pdf&quot;>;模式匹配功能&lt;/a>;，它们很容易过度拟合数据中的虚假统计模式。当训练数据庞大且多样化且评估呈分布时，这并不妨碍良好的性能。然而，对于需要基于规则的推理（例如加法）的任务，法学硕士很难应对分布外泛化，因为训练数据中的虚假相关性通常比真正的基于规则的解决方案更容易利用。因此，尽管各种自然语言处理任务取得了重大进展，但加法等简单算术任务的性能仍然是一个挑战。即使在 &lt;a href=&quot;https://arxiv.org/abs/2103.03874 上对 &lt;a href=&quot;https://openai.com/research/gpt-4&quot;>;GPT-4&lt;/a>; 进行了适度改进&quot;>;MATH&lt;/a>;数据集，&lt;a href=&quot;https://arxiv.org/abs/2303.12712&quot;>;错误仍然很大程度上是由于算术和计算错误&lt;/a>;。因此，一个重要的问题是法学硕士是否能够进行算法推理，这涉及通过应用一组定义算法的抽象规则来解决任务。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;通过情境学习教授算法推理&lt;/a>;”中，我们描述了一种利用&lt;a href =&quot;https://arxiv.org/abs/2005.14165&quot;>;情境学习&lt;/a>;，以实现法学硕士的算法推理能力。上下文学习是指模型在模型上下文中看到一些任务示例后执行任务的能力。使用提示将任务指定给模型，无需更新权重。我们还提出了一种新颖的算法提示技术，使通用语言模型能够对比所见更难的算术问题实现强大的&lt;a href=&quot;https://arxiv.org/abs/2207.04901&quot;>;泛化&lt;/a>;在提示中。最后，我们证明模型可以通过适当选择提示策略，在分布外的示例上可靠地执行算法。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNy QMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q -uy_Xt6i7VkZCGiKqA97AGFnlB/s1200/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;1200 “ height =“89”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfV dwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/w640-h89/image1.gif&quot;宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过提供算法提示，我们可以教授通过情境学习对算术规则进行建模。在此示例中，LLM（单词预测器）在提示一个简单的加法问题（例如，267+197）时输出正确答案，但在询问具有较长数字的类似加法问题时会失败。然而，当更困难的问题附加了添加的算法提示（单词预测器下方显示带有白色 +&lt;strong>; &lt;/strong>; 的蓝色框）时，模型能够正确回答。此外，该模型还能够通过组合一系列加法计算来模拟乘法算法（&lt;strong>;X&lt;/strong>;）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;将算法作为一种技能进行教学&lt;/h2>; &lt;p>; 为了将算法作为一种技能来教授模型，我们开发算法提示，它建立在其他基本原理增强方法的基础上（例如，&lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpad&lt;/a>;和&lt;a href=&quot;https://ai. googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;思想链&lt;/a>;）。算法提示从法学硕士中提取算法推理能力，与其他提示方法相比有两个显着区别：（1）它通过输出算法解决方案所需的步骤来解决任务，（2）它以足够的细节解释每个算法步骤，因此法学硕士不允许有任何误解。 &lt;/p>; &lt;p>; 为了获得算法提示的直觉，让我们考虑两个数字相加的任务。在便笺式提示中，我们从右到左处理每个数字，并在每一步跟踪进位值（即，如果当前数字大于 9，我们将在下一个数字上加 1）。然而，在只看到几个进位值的例子后，进位规则就很模糊了。我们发现，包含显式方程来描述进位规则有助于模型关注相关细节并更准确地解释提示。我们利用这种洞察力开发了两个数字加法的算法提示，其中我们为每个计算步骤提供了明确的方程，并以明确的格式描述了各种索引操作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260v McydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mlLTkDzy14YMJ32U5mkQT2Y85drKXIVr/s1584 /image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1250&quot; data-original-width=&quot;1584&quot; height=&quot; 505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60h wF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/w640-h505/image4.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;各种添加提示策略说明。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 仅使用三个答案长度最多为 5 位数字的加法提示示例，我们评估了最多 19 位数字的加法性能。准确性是通过在整个答案范围内均匀采样的 2,000 个示例来衡量的。如下所示，使用算法提示可以在比提示中看到的时间更长的时间内保持问题的高精度，这表明该模型确实通过执行与输入无关的算法来解决任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZ jl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s1600/image3.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;548&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8 YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align :center;&quot;>;测试不同提示方式下长度递增的加法题的准确性。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;利用算法技能作为工具使用&lt;/h2>; &lt;p>;为了评估模型是否可以在更广泛的推理过程中利用算法推理，我们使用小学数学应用题来评估性能（&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot; >;GSM8k&lt;/a>;）。我们特别尝试用算法解决方案替换 GSM8k 的加法计算。 &lt;/p>; &lt;p>; 受上下文长度限制和不同算法之间可能存在的干扰的推动，我们探索了一种策略，其中不同提示的模型相互交互以解决复杂的任务。在 GSM8k 的背景下，我们有一个专门使用&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;思想链提示&lt;/a>;进行非正式数学推理的模型，以及第二个模型专门研究使用&lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;算法提示&lt;/a>;进行加法。非正式数学推理模型被提示输出专门的标记，以便调用加法提示模型来执行算术步骤。我们提取标记之间的查询，将它们发送到加法模型并将答案返回到第一个模型，之后第一个模型继续其输出。我们使用 GSM8k (GSM8k-Hard) 中的一个难题来评估我们的方法，其中我们随机选择 50 个仅加法问题并增加问题中的数值。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2 Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/s1268/An%20example%20from%20the %20GSM8k-Hard%20dataset.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;330&quot; data-original-width=&quot;1268 “高度=“167”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358p LdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/w640-h167/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; 宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;来自 GSM8k-Hard 数据集的示例。思想链提示用括号增强，以指示何时应执行算法调用。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们发现使用单独的上下文和模型与专门的提示是解决GSM8k-Hard的有效方法。下面，我们观察到使用算法调用加法的模型的性能是思想链基线的 2.3 倍。最后，该策略提供了一个通过情境学习促进专门从事不同技能的法学硕士之间的互动来解决复杂任务的示例。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C 7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/s716/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;192&quot; data-original-width=&quot;716&quot; height=&quot;108&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-Dr H0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/w400-h108/image2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;有或没有算法调用的 GSM8k-Hard 上的思想链 (CoT) 性能。&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了一个利用&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;情境学习&lt;/a>;和新颖的算法提示技术来解锁法学硕士的算法推理能力。我们的结果表明，通过提供更详细的解释，可能可以将更长的上下文转化为更好的推理性能。因此，这些发现表明能够使用或以其他方式模拟长上下文并产生更多信息丰富的基本原理作为有前途的研究方向。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢我们的合著者 Behnam Neyshabur、Azade Nova、Hugo Larochelle 和 Aaron Courville 对本文做出的宝贵贡献以及对博客的良好反馈。我们感谢 Tom Small 创建本文中的动画。这项工作是 Hattie Zhou 在 Google Research 实习期间完成的。&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3589347607411181063/comments/default &quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to- Reason.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /3589347607411181063&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;self&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html&quot; rel=&quot;alternate&quot; title=&quot;教学算法推理的语言模型” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_C g8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-6641308922483151364&lt;/id>;&lt;发布>;2023-08-22T11：47：00.004- 07:00&lt;/发布>;&lt;更新>;2023-08-24T10:34:33.888-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=深度学习&quot;>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“机器学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”。 blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;机器人技能综合奖励语言&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class= &quot;byline-author&quot;>;发布者：Google 研究科学家 Wenhao Yu 和 Fei Xia&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d- RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTneyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw /s320/Language%20to%20reward.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 使最终用户能够以交互方式教导机器人执行新颖的任务，这是机器人成功集成到现实世界应用程序中的关键能力。例如，用户可能想要教机器狗表演新技巧，或者教机械手机器人如何根据用户偏好整理午餐盒。 &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;大型语言模型&lt;/a>; (LLM) 的最新进展对大量互联网数据进行的培训显示出实现这一目标的有希望的道路。事实上，研究人员已经从&lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;分步规划中探索了利用 LLM 进行机器人技术的多种方法&lt;/a>; 和&lt;a href=&quot;https://interactive-language.github.io/&quot;>;以目标为导向的对话&lt;/a>;到&lt;a href=&quot;https://ai.googleblog.com/2022/ 11/robots-that-write-their-own-code.html&quot;>;机器人代码编写代理&lt;/a>;。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 虽然这些方法赋予了组合概括的新模式，但它们侧重于使用语言将来自 &lt;a href=&quot;https:// 的新行为链接在一起。 sites.research.google/palm-saycan&quot;>;现有的控制原语库&lt;/a>;，要么是手动设计的，要么是先验学习的。尽管拥有有关机器人运动的内部知识，但由于相关训练数据的可用性有限，法学硕士很难直接输出低级机器人命令。因此，这些方法的表达受到可用原语的广度的瓶颈，其设计通常需要广泛的专业知识或大量数据收集。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2306.08647&quot;>;机器人技能综合奖励语言&lt;/a>;”中，我们提出了一种使用户能够教授机器人的方法通过自然语言输入进行新颖的动作。为此，我们利用奖励函数作为接口，弥合语言和低级机器人动作之间的差距。我们认为，鉴于奖励函数在语义、模块化和可解释性方面的丰富性，它们为此类任务提供了理想的接口。它们还通过黑盒优化或强化学习 (RL) 提供与低级策略的直接连接。我们开发了一种语言到奖励系统，利用 LLM 将自然语言用户指令翻译为奖励指定代码，然后应用 &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>;找到最大化生成的奖励函数的最佳低级机器人动作。我们使用四足机器人和灵巧机械手机器人模拟各种机器人控制任务，展示了我们的语言奖励系统。我们进一步在物理机器人操纵器上验证我们的方法。 &lt;/p>; &lt;p>; 语言到奖励系统由两个核心组件组成：(1) 奖励翻译器，以及 (2) 运动控制器&lt;em>;。奖励翻译器将用户的自然语言指令映射到以 &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;Python 代码表示的奖励函数&lt;/a>;。运动控制器使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;后退地平线优化&lt;/a>;来优化给定的奖励函数，以找到最佳的低级机器人动作，例如数量应施加到每个机器人电机的扭矩。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65 Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s720/image5.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUq G3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;由于预训练数据集中缺乏数据，LLM 无法直接生成低级机器人动作。我们建议使用奖励函数来弥合语言和低级机器人动作之间的差距，并根据自然语言指令实现新颖的复杂机器人动作。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;奖励翻译器：将用户指令翻译为奖励函数&lt;/h2>; &lt;p>; 奖励翻译器模块的构建目标是将自然语言用户指令映射到奖励函数。奖励调整是高度特定领域的，需要专业知识，因此当我们发现在通用语言数据集上训练的法学硕士无法直接为特定硬件生成奖励函数时，我们并不感到惊讶。为了解决这个问题，我们应用了法学硕士的&lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;情境学习&lt;/a>;能力。此外，我们将奖励翻译器分为两个子模块：&lt;em>;动作描述符&lt;/em>;和&lt;em>;奖励编码器&lt;/em>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;运动描述符&lt;/h3>; &lt;p>; 首先，我们设计一个运动描述符来解释来自用户并将其扩展为遵循预定义模板的所需机器人运动的自然语言描述。该运动描述符将潜在不明确或模糊的用户指令转化为更具体和描述性的机器人运动，使奖励编码任务更加稳定。而且，用户通过动作描述字段与系统进行交互，因此与直接显示奖励函数相比，这也为用户提供了更易于解释的界面。 &lt;/p>; &lt;p>; 为了创建运动描述符，我们使用 LLM 将用户输入转换为所需机器人运动的详细描述。我们设计了&lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_motion_descriptor.txt&quot;>;提示&lt;/a>;，指导法学硕士输出包含适量详细信息的动作描述和格式。通过将模糊的用户指令转换为更详细的描述，我们能够使用我们的系统更可靠地生成奖励函数。这个想法也可以更广泛地应用于机器人任务之外，并且与 &lt;a href=&quot;https://innermonologue.github.io/&quot;>;Inner-Monologue&lt;/a>; 和 &lt;a href=&quot;https:/ /ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;思维链提示&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Reward Coder &lt;/h3>; &lt;p>; 在第二阶段，我们使用与 Motion 相同的 LLM奖励编码器的描述符，它将生成的运动描述转换为奖励函数。奖励函数使用 Python 代码表示，以受益于法学硕士关于奖励、编码和代码结构的知识。 &lt;/p>; &lt;p>; 理想情况下，我们希望使用 LLM 直接生成奖励函数 &lt;i>;R&lt;/i>; (&lt;i>;s&lt;/i>;, &lt;i>;t&lt;/i>;)将机器人状态 &lt;i>;s&lt;/i>; 和时间 &lt;i>;t&lt;/i>; 映射为标量奖励值。然而，从头开始生成正确的奖励函数对于法学硕士来说仍然是一个具有挑战性的问题，纠正错误需要用户理解生成的代码以提供正确的反馈。因此，我们预先定义了一组常用于感兴趣的机器人的奖励项，并允许法学硕士组合不同的奖励项来制定最终的奖励函数。为了实现这一目标，我们设计了一个&lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_reward_coder.txt&quot;>;提示&lt;/a>;来指定奖励条款并指导LLM为任务生成正确的奖励函数。&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; &lt;/p>;&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellpacing=&quot;0&quot;class=&quot;tr-caption -container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JX GD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s1293/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “556”数据原始宽度=“1293”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXU od3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s16000/image4.jpg&quot;/>;&lt;/a>;&lt;/td >;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;奖励翻译器的内部结构，其任务是将用户输入映射到奖励函数。&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;运动控制器：将奖励功能转换为机器人动作&lt;/h2>; &lt;p>; 运动控制器采用奖励转换器生成的奖励函数，并合成一个控制器，将机器人观察映射到低级机器人动作。为此，我们将控制器综合问题表述为马尔可夫决策过程（MDP），可以使用不同的策略来解决，包括强化学习、&lt;a href=&quot;https://en.wikipedia.org/wiki/Trajectory_optimization&quot;>;离线轨迹优化&lt;/a>;或&lt;a href=&quot;https://en.wikipedia.org/wiki/ Model_predictive_control&quot;>;模型预测控制&lt;/a>; (MPC)。具体来说，我们使用基于 &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; (MJPC) 的开源实现。 &lt;/p>; &lt;p>; MJPC 展示了多种行为的交互式创建，例如腿运动、抓取和手指步态，同时支持多种规划算法，例如 &lt;a href=&quot;https://homes.cs. Washington.edu/~todorov/papers/TodorovACC05.pdf&quot;>;迭代线性-二次-高斯&lt;/a>; (iLQG) 和&lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;预测采样&lt;/一个>;。更重要的是，MJPC 中的频繁重新规划增强了其对系统不确定性的鲁棒性，并在与 LLM 结合时实现了交互式运动合成和校正系统。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;示例&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;机器狗&lt;/h3>; &lt;p>; 在第一个示例中，我们将语言奖励系统应用于模拟四足机器人，并教它执行各种技能。对于每项技能，用户将向系统提供简洁的指令，然后系统将使用奖励函数作为中间接口来合成机器人运动。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_quadruped_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;灵巧的操纵器&lt;/h3>; &lt;p>;然后我们将语言应用到-为灵巧的操纵机器人执行各种操纵任务提供奖励系统。灵巧的机械臂有27个自由度，控制起来非常有挑战性。其中许多任务需要超出掌握能力的操作技能，这使得预先设计的基元很难工作。我们还提供了一个示例，用户可以交互地指示机器人将苹果放入抽屉内。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_manipulation_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;在真实机器人上进行验证&lt;/h2>; &lt;p>;我们还验证了语言 - to-reward 方法使用现实世界的操纵机器人来执行诸如拾取物体和打开抽屉等任务。为了在运动控制器中执行优化，我们使用基准标记系统 &lt;a href=&quot;https://en.wikipedia.org/wiki/ARTag&quot;>;AprilTag&lt;/a>; 和 &lt;a href=&quot;https:/ /ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot;>;F-VLM&lt;/a>;，一种开放词汇对象检测工具，用于识别表格的位置和被操纵的物体。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;yes&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/real/l2r_demo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 在这项工作中，我们描述了一种新的范例用于通过奖励函数将法学硕士与机器人连接起来，由低级模型预测控制工具 MuJoCo MPC 提供支持。使用奖励函数作为接口使 LLM 能够在语义丰富的空间中工作，发挥 LLM 的优势，同时确保最终控制器的表达能力。为了进一步提高系统的性能，我们建议使用结构化运动描述模板来更好地从法学硕士中提取有关机器人运动的内部知识。我们在两个模拟机器人平台和一个真实机器人上演示了我们提出的系统，用于运动和操纵任务。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢我们的合著者 Nimrod Gileadi、Chuyuan Fu、Sean Kirmani、Kuang-Huei Lee、Montse Gonzalez Arenas、Hao-Tien Lewis Jiang、Tom Erez、Leonard Hasenclever、Brian Ichter、Ted Shaw、Peng Xu、Andy Zeng、Tingnan Zhang、Nicolas Heess、Dorsa Sadigh、感谢 Jie Tan 和 Yuval Tassa 在该项目各个方面的帮助和支持。我们还要感谢 Ken Caluwaerts、Kristian Hartikainen、Steven Bohez、Carolina Parada、Marc Toussaint 以及 Google DeepMind 团队的反馈和贡献。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/em>;内容>;&lt;link href=&quot;http://blog.research.google/feeds/6641308922483151364/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google /2023/08/language-to-rewards-for-robotic-skill.html&quot; rel=&quot;alternate&quot; title=&quot;机器人技能合成奖励语言&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>; Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot; http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt; /author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G _NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s72-c/Language%20to%20reward.jpg “ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-6990000750711940626&lt;/id>;&lt;发布>;2023-08-21T00:33:00.002-07:00&lt;/发布>;&lt;更新>;2023-08-21T00 :35:57.579-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Interspeech&quot;>;&lt;/ category>;&lt;title type=&quot;text&quot;>;Google 在 Interspeech 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 项目经理 Catherine Armato&lt;/span>; &lt; img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3 OeZJVLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s1075/Interspeech2023-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 本周，第 24 届国际语音交流协会年会&lt;/a>;（INTERSPEECH 2023）在爱尔兰都柏林举行，代表世界上最广泛的口语理解和处理研究和技术会议之一。语音相关研究领域的专家齐聚一堂，参加口头报告和海报会议，并在全球范围内建立合作。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 我们很高兴成为&lt;a href=&quot;https://interspeech2023.org/sponsorship-exhibition/sponsors/&quot;>;白金赞助商&lt;/ INTERSPEECH 2023 的 a>;，我们将在其中展示 20 多篇研究出版物并支持许多研讨会和特别会议。我们欢迎现场与会者莅临 Google Research 展位，与我们的研究人员会面，参与问答和演示我们的一些最新语音技术，这有助于提高可访问性，为数十亿用户的沟通提供便利。此外，我们鼓励在线与会者访问我们&lt;a href=&quot;http://topia.io/google-research&quot;>;Topia 虚拟展位&lt;/a>;，在这里您可以获得有关研究和机会的最新信息在谷歌。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter 帐户，了解 Google 展位活动（例如演示和问答环节）。您还可以在下面详细了解 INTERSPEECH 2023 上展示的 Google 研究（Google 隶属关系以&lt;strong>;粗体&lt;/strong>;显示）。 &lt;/p>; &lt;br />; &lt;h2>;董事会和组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; ISCA 董事会、技术委员会主席：&lt;strong>;&lt;em>;Bhuvana Ramabhadran &lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; 区域主席包括：&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;语音和音频信号分析：&lt;strong>;&lt;em>;Richard Rose&lt;/em>; &lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;语音合成和口语生成：&lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp ;特殊区域：&lt;strong>;&lt;em>;Tara Sainath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;卫星事件&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/competition2023.html&quot;>;2023 年 VoxCeleb 演讲者识别挑战赛&lt;/a>; (VoxSRC-23)&lt; br />; 主办方包括：&lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ssw2023.org/&quot;>;ISCA 语音合成研讨会&lt;/ a>; (SSW12)&lt;br />; 演讲者包括：&lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;主题演讲 – ISCA 奖章获得者&lt;/ h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/narayanan23_interspeech.pdf&quot;>;桥梁语音科学和技术 — 现在与未来&lt;/a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Shrikanth Narayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;调查谈话&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; AI时代的语音压缩&lt;br />;演讲者：&lt;strong>;&lt;em>;Jan Skoglund&lt;/em>;&lt;br />;&lt;/ strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;特别会议文件&lt;​​/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www .isca-speech.org/archive/pdfs/interspeech_2023/rose23_interspeech.pdf&quot;>;用于在重叠语音上微调 ASR 模型的级联编码器&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Richard Rose&lt;/b>;， &lt;b>;奥斯卡·张&lt;/b>;，&lt;b>;奥利维尔·西奥汉&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs /interspeech_2023/erdogan23_interspeech.pdf&quot;>;TokenSplit：使用离散语音表示进行直接、精炼和转录条件语音分离和识别&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Hakan Erdogan&lt;/b>;，&lt;b >;Scott Wisdom&lt;/b>;、Xuankai Chang*、&lt;b>;Zalán Borsos&lt;/b>;、&lt;b>;Marco Tagliasacchi&lt;/b>;、&lt;b>;Neil Zeghidour&lt;/b>;、&lt;b>;John R. Hershey&lt; /b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href =&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/liang23d_interspeech.pdf&quot;>;DeePMOS：深度后验平均意见-言语得分&lt;/a>;&lt;br />; &lt;em>;梁新宇， Fredrik Cumlin、&lt;strong>; Christian Schüldt&lt;/strong>;、Saikat Chatterjee&lt;strong>;&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca- peech.org/archive/pdfs/interspeech_2023/baskar23_interspeech.pdf&quot;>;O-1：使用 Oracle 进行自我训练和 1-最佳假设&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Murali Karthick Baskar&lt;/b>; 、&lt;b>;安德鲁·罗森伯格&lt;/b>;、&lt;b>;布瓦纳·拉马巴德兰&lt;/b>;、&lt;b>;卡蒂克·奥德卡西&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot; https://www.isca-speech.org/archive/pdfs/interspeech_2023/huo23b_interspeech.pdf&quot;>;利用特征融合方法重新研究语音基础模型的高效迁移学习&lt;/a>;&lt;br />; &lt;em>;&lt; b>;Zhouyuan Huor&lt;/b>;、&lt;b>;Khe Chai Sim&lt;/b>;、&lt;b>;Dongseong Hwang&lt;/b>;、&lt;b>;Tsendsuren Munkhdalai&lt;/b>;、&lt;b>;Tara N. Sainath&lt;/b>; >;,&lt;b>;佩德罗·莫雷诺&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/camp23_interspeech .pdf&quot;>;MOS 与 AB：使用集群标准误差可靠地评估文本转语音系统&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Joshua Camp&lt;/b>;、&lt;b>;Tom Kenter&lt;/b >;、&lt;b>;列夫·芬克尔斯坦&lt;/b>;、&lt;b>;罗布·克拉克&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech .org/archive/pdfs/interspeech_2023/gong23c_interspeech.pdf&quot;>;LanSER：语言模型支持的语音情感识别&lt;/a>;&lt;br />; &lt;em>;Taesik Kong，&lt;strong>; Josh Belanich&lt;/strong>;，&lt;strong>; Krishna Somandepalli&lt;/strong>;、&lt;strong>;Arsha Nagrani&lt;/strong>;、&lt;strong>;Brian Eoff&lt;/strong>;、&lt;strong>;Brendan Jou&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23fa_interspeech.pdf&quot;>;基于一致性的流媒体 ASR 的模块化域适应&lt;/a>;&lt;br />; &lt;em>;&lt; b>;李秋佳&lt;/b>;、&lt;b>;李波&lt;/b>;、&lt;b>;Dongseong Hwang&lt;/b>;、&lt;b>;Tara N. Sainath&lt;/b>;、&lt;b>;Pedro M.Mengibar&lt;/b>; b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/panchapagesan23_interspeech.pdf&quot;>;关于训练神经残余声学回声抑制器改进的 ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Sankaran Panchapagesan&lt;/b>;、&lt;b>;Turaj Zakizadeh Shabestary&lt;/b>;、&lt;b>;Arun Narayanan&lt;/b>;&lt;br />;&lt;/em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/eisenstein23_interspeech.pdf&quot;>;MD3：对话的多方言数据集&lt;/a>;&lt; br />; &lt;em>;&lt;b>;雅各布·爱森斯坦&lt;/b>;、&lt;b>;Vinodkumar Prabhakaran&lt;/b>;、&lt;b>;克拉拉·里维拉&lt;/b>;、Dorottya Demszky、Devyani Sharma&lt;br />;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/wu23e_interspeech.pdf&quot;>;双模式 NAM：端到端的有效 Top-K 上下文注入结束 ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Zelin Wu&lt;/b>;、&lt;b>;Tsendsuren Munkhdalai&lt;/b>;、&lt;b>;Pat Rondon&lt;/b>;、&lt;b>;Golan Pundak&lt;/b>; b>;、&lt;b>;Khe Chai Sim&lt;/b>;、&lt;b>;Christopher Li&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca -speech.org/archive/pdfs/interspeech_2023/blau23_interspeech.pdf&quot;>;使用文本注入来改进语音中个人标识符的识别&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yochai Blau&lt;/b>;，&lt;b >;Rohan Agrawal&lt;/b>;、&lt;b>;Lior Madmony&lt;/b>;、&lt;b>;Gary Wang&lt;/b>;、&lt;b>;Andrew Rosenberg&lt;/b>;、&lt;b>;陈哲怀&lt;/b>;、&lt;b >;佐里克·格赫曼&lt;/b>;、&lt;b>;格纳迪·别廖兹金&lt;/b>;、&lt;b>;帕里萨·哈加尼&lt;/b>;、&lt;b>;布瓦纳·拉玛巴德兰&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23j_interspeech.pdf&quot;>;如何估计预训练语音模型的模型可迁移性？&lt;/a>;&lt;br />; &lt;em>;Zih-Ching Chen、Chao-Han Huck Yang*、&lt;strong>;李波&lt;/strong>;、&lt;strong>;张宇&lt;/strong>;、&lt;strong>;陈南新&lt;/strong>;、&lt;strong>;Shuo- yiin Chang&lt;/strong>;、&lt;strong>;Rohit Prabhavalkar、&lt;/strong>;李鸿毅、&lt;strong>;Tara N. Sainath&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/peyser23_interspeech.pdf&quot;>;在不对齐的情况下改进联合语音文本表示&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Cal Peyser &lt;/b>;、&lt;b>;钟猛&lt;/b>;、&lt;b>;胡可&lt;/b>;、&lt;b>;Rohit Prabhavalkar&lt;/b>;、&lt;b>;安德鲁·罗森伯格&lt;/b>;、&lt;b>;Tara N .Sainath&lt;/b>;、Michael Picheny、Kyunghyun Cho &lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/ bijwadia23_interspeech.pdf&quot;>;语音模型中大写和轮流预测的文本注入&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shaan Bijwadia&lt;/b>;，&lt;b>;Shuo-yiin Chang&lt;/b>;， &lt;b>;王蔚然&lt;/b>;、&lt;b>;孟忠&lt;/b>;、&lt;b>;张浩&lt;/b>;、&lt;b>;Tara N. Sainath&lt;/b>;&lt;br />;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23_interspeech.pdf&quot;>;用于设备上语音到语音转换的流式 Parrotron&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;、&lt;b>;Fadi Biadsy&lt;/b>;、&lt;b>;张霞&lt;/b>;、&lt;b>;蒋立阳&lt;/b>;、&lt;b>;凤凰草地鹨&lt;/b>;、&lt;b>;Shivani Agrawal&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/ pdfs/interspeech_2023/huang23b_interspeech.pdf&quot;>;使用双向语言模型的语义分割改进了长格式 ASR&lt;/a>;&lt;br />; &lt;strong>;W. Ronny Huang&lt;/strong>;、&lt;strong>;张浩&lt;/strong>;、&lt;strong>;Shankar Kumar&lt;/strong>;、&lt;strong>;张硕仪&lt;/strong>;、&lt;strong>;Tara N. Sainath&lt;br />; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/taguchi23_interspeech.pdf&quot;>;通用自动语音转录为国际音标&lt;/ a>;&lt;br />; &lt;em>;田口千寻、酒井佑介、&lt;strong>;Parisa Haghani&lt;/strong>;、姜大卫&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /www.isca-speech.org/archive/pdfs/interspeech_2023/hu23c_interspeech.pdf&quot;>;用于流式多语言 ASR 的混合专家一致性&lt;/a>;&lt;br />; &lt;em>;&lt;b>;胡柯&lt;/b>; , &lt;b>;李波&lt;/b>;,&lt;b>;Tara N. Sainath&lt;/b>;,&lt;b>;张宇&lt;/b>;,&lt;b>;Francoise Beaufays&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23c_interspeech.pdf&quot;>;手机实时频谱图反演&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;、&lt;b>;Marco Tagliasacchi&lt;/b>;、&lt;b>;李云鹏&lt;/b>;、&lt;b>;蒋立阳&lt;/b>;、&lt;b>;张霞&lt;/ b>;、&lt;b>;Fadi Biadsy&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/ rybakov23b_interspeech.pdf&quot;>;自动语音识别的 2 位一致性量化&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;、&lt;b>;Phoenix Meadowlark&lt;/b>;、&lt;b>;Shaojin Ding &lt;/b>;、&lt;b>;邱大卫&lt;/b>;、&lt;b>;李健&lt;/b>;、&lt;b>;林大卫&lt;/b>;、&lt;b>;何彦章&lt;/b>;&lt;br />;&lt;/ em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/koizumi23_interspeech.pdf&quot;>;LibriTTS-R：恢复的多扬声器文本到-语音语料库&lt;/a>;&lt;br />; &lt;em>;&lt;b>;小泉佑马&lt;/b>;、&lt;b>;平贺禅&lt;/b>;、&lt;b>;成田茂树&lt;/b>;、&lt;b>;丁一帆&lt;/ b>;、矢田部航平、&lt;b>;盛冈伸之&lt;/b>;、&lt;b>;Michiel Bacchiani&lt;/b>;、&lt;b>;张宇&lt;/b>;、&lt;b>;韩伟&lt;/b>;、&lt;b>;Ankur Bapna&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/yu23_interspeech.pdf&quot;>;PronScribe：语音和文本的高精度多模态音素转录&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yang Yu&lt;/b>;、Matthew Perez*、&lt;b>;Ankur Bapna&lt;/b>;、&lt;b>;Fadi Haik&lt; /b>;、&lt;b>;Siamak Tazari&lt;/b>;、&lt;b>;张宇&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca -speech.org/archive/pdfs/interspeech_2023/vashishth23_interspeech.pdf&quot;>;用于语言识别的标签感知语音表示学习&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shikhar Vashishth&lt;/b>;、&lt;b>;Shikhar Bharadwaj &lt;/b>;、&lt;b>;Sriram Ganapathy&lt;/b>;、&lt;b>;安库尔·巴普纳&lt;/b>;、&lt;b>;马敏&lt;/b>;、&lt;b>;韩伟&lt;/b>;、&lt;b>;维拉·阿克塞尔罗德&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p >; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>;&lt;/content>; &lt;link href=&quot;http://blog.research.google/feeds/6990000750711940626/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://blog.research.google/2023/08/google-at-interspeech-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/google -at-interspeech-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at Interspeech 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http:// www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005 #thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJ VLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s72-c/Interspeech2023-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http: //search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog- 8474926331452026626.post-362086873015740792&lt;/id>;&lt;发布>;2023-08-18T11:28:00.004-07:00&lt;/发布>;&lt;更新>;2023-08-18T11:30:51.167-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“计算机视觉”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模态学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;使用大语言模型进行自主视觉信息搜索&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：学生研究员 Ziniu Hu 和 Google 研究感知团队研究科学家 Alireza Fathi &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUF zF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s2500/AVIS.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 在适应大型语言模型 (LLM) 以适应任务的多模式输入方面已经取得了巨大进展，包括 &lt;a href=&quot;https://huggingface.co/docs/transformers/main/tasks/image_captioning#:~ :text=Image%20captioning%20is%20the%20task,by%20describing%20images%20to%20them。&quot;>;图像字幕&lt;/a>;，&lt;a href=&quot;https://huggingface.co/tasks/visual-question -answering&quot;>;视觉问答 (VQA)&lt;/a>; 和&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;开放词汇识别&lt;/a>;。尽管取得了这些成就，当前最先进的视觉语言模型（VLM）在视觉信息搜索数据集上的表现还不够，例如 &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a >; 和 &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>;，需要外部知识来回答问题。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe -vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R 9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-宽度=“1999”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpm MLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;需要外部知识来回答问题的视觉信息搜索查询示例。图像取自 OK-VQA 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs//2306.08129&quot;>; AVIS：使用大型语言模型进行自主视觉信息搜索&lt;/a>;”，我们介绍了一种新颖的方法，该方法在视觉信息搜索任务上取得了最先进的结果。我们的方法将法学硕士与三种类型的工具集成：（i）用于从图像中提取视觉信息的计算机视觉工具，（ii）用于检索开放世界知识和事实的网络搜索工具，以及（iii）用于收集相关信息的图像搜索工具来自与视觉上相似的图像相关的元数据。 AVIS 采用法学硕士支持的规划器在每个步骤中选择工具和查询。它还使用 LLM 支持的推理机来分析工具输出并提取关键信息。工作记忆组件在整个过程中保留信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoS​​FcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s1999/image6.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1758&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3i Fnyn7oy8bB81TCbh_D-I3unIwgxswoS​​FcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;AVIS 生成的工作流程示例，用于回答具有挑战性的视觉信息搜索问题。输入图像取自 Infoseek 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2 >;与之前的工作比较&lt;/h2>; &lt;p>;最近的研究（例如，&lt;a href=&quot;https://arxiv.org/abs/2304.09842&quot;>;Chameleon&lt;/a>;、&lt;a href=&quot;https:// viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>; 和 &lt;a href=&quot;https://multimodal-react.github.io/&quot;>;MM-ReAct&lt;/a>;）探索向法学硕士添加工具以实现多模式输入。这些系统遵循两个阶段的过程：规划（将问题分解为结构化程序或指令）和执行（使用工具收集信息）。尽管在基本任务中取得了成功，但这种方法在复杂的现实场景中常常会出现问题。 &lt;/p>; &lt;p>; 人们对应用法学硕士作为自主代理的兴趣也激增（例如，&lt;a href=&quot;https://openai.com/research/webgpt&quot;>;WebGPT&lt;/a>; 和 &lt;a href=&quot;https://react-lm.github.io/&quot;>;ReAct&lt;/a>;）。这些代理与环境交互，根据实时反馈进行调整并实现目标。然而，这些方法并没有限制每个阶段可以调用的工具，导致搜索空间巨大。因此，即使是当今最先进的法学硕士也可能陷入无限循环或传播错误。 AVIS 通过指导法学硕士使用来解决这个问题，并受到用户研究中人类决策的影响。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;通过用户研究为 LLM 决策提供依据&lt;/h2>; &lt;p>; 中的许多视觉问题数据集，例如 &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; 和 &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a >; 即使对人类来说也是一个挑战，通常需要各种工具和 API 的帮助。下面显示了 OK-VQA 数据集中的一个示例问题。我们进行了一项用户研究，以了解使用外部工具时人类的决策。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP 8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s1999 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;889&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWx b9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们进行了一项用户研究，以了解人类在使用外部工具时的决策。图像取自 OK-VQA 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 用户配备了与我们的方法相同的工具集，包括 &lt;a href=&quot;https ://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PALI&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022 /04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; 和&lt;a href=&quot;https://en.wikipedia.org/wiki/Search_engine&quot;>;网络搜索&lt;/a >;。他们接收输入图像、问题、检测到的对象作物以及链接到图像搜索结果的按钮。这些按钮提供了有关检测到的对象作物的各种信息，例如知识图实体、相似的图像标题、相关产品标题和相同的图像标题。 &lt;/p>; &lt;p>; 我们记录用户操作和输出，并通过两种关键方式将其用作我们系统的指南。首先，我们通过分析用户做出的决策顺序构建一个转换图（如下所示）。该图定义了不同的状态并限制每个状态下可用的操作集。例如，在启动状态，系统只能采取以下三种操作之一：PALI 字幕、PALI VQA 或对象检测。其次，我们使用人类决策的例子来指导我们的规划者和推理者与相关的上下文实例，以提高我们系统的性能和有效性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-I CB-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/s1146/image7.png “ style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;845&quot; height=&quot;640&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr 1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/w472-h640/image7.png“宽度=“472”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS 转换图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot; line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;总体框架&lt;/h2>; &lt;p>;我们的方法采用动态决策策略，旨在响应视觉信息寻求查询。我们的系统具有三个主要组件。首先，我们有一个规划器来确定后续操作，包括适当的 API 调用及其需要处理的查询。其次，我们有一个&lt;em>;工作内存&lt;/em>;，它保留有关从 API 执行获得的结果的信息。最后，我们有一个推理器，其作用是处理 API 调用的输出。它确定所获得的信息是否足以产生最终响应，或者是否需要额外的数据检索。 &lt;/p>; &lt;p>; 每次需要决定使用哪种工具以及向其发送什么查询时，规划器都会执行一系列步骤。根据当前状态，规划器提供一系列潜在的后续行动。潜在的行动空间可能太大，使得搜索空间变得棘手。为了解决这个问题，规划器参考转换图来消除不相关的动作。规划器还排除之前已经采取并存储在工作记忆中的动作。 &lt;/p>; &lt;p>; 接下来，规划器收集一组相关的上下文示例，这些示例是根据人类之前在用户研究期间做出的决策组合而成的。通过这些示例以及保存从过去的工具交互中收集的数据的工作记忆，规划者可以制定提示。然后，提示被发送到法学硕士，法学硕士返回结构化答案，确定下一个要激活的工具以及要向其分派的查询。这种设计允许在整个过程中多次调用规划器，从而促进动态决策，逐渐导致回答输入查询。 &lt;/p>; &lt;p>; 我们使用推理器来分析工具执行的输出，提取有用的信息并决定工具输出属于哪个类别：提供信息、非信息或最终答案。我们的方法利用法学硕士以及适当的提示和上下文示例来执行推理。如果推理机断定它已准备好提供答案，它将输出最终响应，从而结束任务。如果它确定工具输出没有信息，它将返回给规划器以根据当前状态选择另一个操作。如果它发现工具输出有用，它将修改状态并将控制权转移回规划器以在新状态下做出新决策。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQf WpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/s1999/image2.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1469&quot; data-original-width=&quot;1999&quot; height= “470”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSur SEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/w640-h470/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt; /td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS 采用动态决策策略来响应视觉信息寻求查询。&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们评估 AVIS &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; 和 &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; 数据集。如下所示，甚至是强大的视觉语言模型，例如 &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog。 com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;，在 Infoseek 上微调时无法产生高精度。我们的方法 (AVIS) 在没有微调的情况下，对该数据集的未见实体分割达到了 50.7% 的准确率。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2E ikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s1200 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHj qYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Infoseek 数据集上的 AVIS 视觉问答结果。与之前基于 &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; 的基线相比，AVIS 实现了更高的准确率、&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; 和 &lt;a href=&quot;https:// arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们在 OK-VQA 数据集上的结果如下所示。 AVIS 通过少量上下文示例实现了 60.2% 的准确率，高于之前的大多数作品。与在 OK-VQA 上微调的 PALI 模型相比，AVIS 的准确度较低，但相当。与 Infoseek 相比，AVIS 优于微调后的 PALI，这种差异是由于 OK-VQA 中的大多数问答示例依赖于常识知识而不是细粒度知识。因此，PaLI 能够将此类通用知识编码在模型参数中，并且不需要外部知识。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d 5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s1200/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a 6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s16000/image1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;A-OKVQA 上的视觉问答结果。与之前使用少样本或零样本学习的作品相比，AVIS 实现了更高的准确度，包括 &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single- Visual-language-model&quot;>;火烈鸟&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;巴利语&lt;/ a>; 和 &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>;。 AVIS 还比之前在 OK-VQA 数据集上进行微调的大多数作品实现了更高的准确度，包括 &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language- pre.html&quot;>;REVEAL&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2206.01201&quot;>;ReVIVE&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/ 2112.08614&quot;>;KAT&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>;，并获得接近微调&lt;a href=&quot;https的结果://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了一种新颖的方法，使法学硕士能够使用各种用于回答知识密集型视觉问题的工具。我们的方法以从用户研究中收集的人类决策数据为基础，采用了一个结构化框架，该框架使用法学硕士支持的规划器来动态决定工具选择和查询形成。由 LLM 驱动的推理机的任务是从所选工具的输出中处理和提取关键信息。我们的方法迭代地使用规划器和推理器来利用不同的工具，直到收集到回答视觉问题所需的所有必要信息。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Ziniu Hu、Ahmet Iscen 进行、孙晨、张凯伟、孙一舟、David A. Ross、Cordelia Schmid 和 Alireza Fathi。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/ feeds/362086873015740792/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous -visual-information-seeking.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/ 8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel =&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous-visual-information-seeking.html&quot; rel=&quot;alternate&quot; 标题=&quot;使用大语言模型进行自主视觉信息搜索&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/ uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1 .blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqY oAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s72-c/AVIS.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:总计>;0&lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-4779594044050650231&lt;/id>;&lt;发布>;2023-08-17T11:08:00.000 -07:00&lt;/已发布>;&lt;更新>;2023-08-17T11:08:24.346-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”term= &quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;神经网络剪枝组合优化&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Athena 团队研究科学家 Hussein Hazimeh 和麻省理工学院研究生 Riade Benbaki&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDN jcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s320/CHITA%20hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 现代神经网络在各种应用中都取得了令人印象深刻的性能，例如 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model /&quot;>;语言、数学推理&lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;愿景&lt;/a>; 。然而，这些网络通常使用需要大量计算资源的大型架构。这使得向用户提供此类模型变得不切实际，特别是在可穿戴设备和智能手机等资源有限的环境中。降低预训练网络的推理成本的&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;广泛使用的方法&lt;/a>;是通过删除一些网络来修剪它们它们的权重，不会显着影响效用。在标准神经网络中，每个权重定义两个神经元之间的连接。因此，在权重被修剪后，输入将通过较小的连接集传播，因此需要较少的计算资源。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw -bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBRYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEg D-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s1600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width= “1600”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FonAvjG2Hw J4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;原始网络与修剪后的网络。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;剪枝方法可以应用于网络训练过程的不同阶段：训练后、训练期间或训练之前（即权重初始化后立即）。在这篇文章中，我们重点关注训练后设置：给定一个预先训练的网络，我们如何确定应该修剪哪些权重？一种流行的方法是&lt;a href=&quot;https://jmlr.org/papers/volume22/21-0366/21-0366.pdf&quot;>;幅度剪枝&lt;/a>;，它删除幅度最小的权重。虽然有效，但该方法没有直接考虑删除权重对网络性能的影响。另一个流行的范例是&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;基于优化的剪枝&lt;/a>;，它根据权重的删除对损失函数的影响程度来删除权重。尽管在概念上很有吸引力，但大多数现有的基于优化的方法似乎都面临着性能和计算要求之间的严重权衡。进行粗略近似的方法（例如，假设对角线&lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;>;Hessian_matrix&quot;>;Hessian 矩阵&lt;/a>;）可以很好地扩展，但性能相对较低。另一方面，虽然进行较少近似的方法往往表现更好，但它们的可扩展性似乎要差得多。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2302.14623&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>;”中，介绍于 &lt;a href=&quot; https://icml.cc/Conferences/2023/&quot;>;ICML 2023&lt;/a>;，我们描述了如何开发一种基于优化的方法来大规模修剪预训练的神经网络。 CHITA（代表“组合 Hessian 迭代阈值算法”）在可扩展性和性能权衡方面优于现有的剪枝方法，并且它通过利用多个领域的进步来做到这一点，包括 &lt;a href=&quot;https://en. wikipedia.org/wiki/High-Dimensional_statistics&quot;>;高维统计&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;组合优化&lt;/a>;和神经网络修剪。例如，CHITA 的速度比最先进的修剪方法快 20 倍到 1000 倍 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>;，并将准确性提高 10 以上% 在许多设置中。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;贡献概述&lt;/h2>; &lt;p>; CHITA 相对于流行方法有两项显着的技术改进：&lt; /p>; &lt;ul>; &lt;li>;&lt;strong>;二阶信息的有效使用&lt;/strong>;：使用二阶信息的修剪方法（即，与&lt;a href=&quot;https://en.wikipedia. org/wiki/Second_derivative&quot;>;二阶导数&lt;/a>;）在许多设置中实现了最先进的技术。在文献中，此信息通常通过计算 Hessian 矩阵或其逆矩阵来使用，这种操作很难扩展，因为 Hessian 大小与权重数量成二次方。通过仔细的重新表述，CHITA 使用二阶信息，而无需显式计算或存储 Hessian 矩阵，从而实现更高的可扩展性。 &lt;/li>;&lt;li>;&lt;strong>;组合优化&lt;/strong>;：流行的基于优化的方法使用一种简单的优化技术，单独修剪权重，即，在决定修剪某个权重时，它们不考虑是否其他权重已被修剪。这可能会导致重要权重的修剪，因为当其他权重被修剪时，孤立地被认为不重要的权重可能会变得重要。 CHITA 通过使用更先进的组合优化算法来避免这个问题，该算法考虑了修剪一个权重如何影响其他权重。 &lt;/li>; &lt;/ul>; &lt;p>; 在下面的部分中，我们讨论 CHITA 的剪枝公式和算法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;计算友好的剪枝公式&lt;/h2>; &lt;p>; 有许多可能的剪枝候选者，其中通过仅保留原始网络权重的子集来获得。令&lt;em>;k&lt;/em>;为用户指定的参数，表示要保留的权重数量。剪枝可以自然地表述为一个&lt;a href=&quot;https://arxiv.org/abs/1803.01454&quot;>;最佳子集选择&lt;/a>;（BSS）问题：在所有可能的剪枝候选者（即权重子集）中仅保留 &lt;em>;k&lt;/em>; 个权重，选择损失最小的候选者。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithv XGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s1600/image5.gif&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY 1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;作为 BSS 问题的剪枝：在具有相同权重总数的所有可能的剪枝候选者中，最佳候选者被定义为损失最小的候选者。该图显示了四个候选者，但这个数字通常要大得多。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;解决原始损失函数上的剪枝BSS问题通常是通过计算来解决的棘手的。因此，与之前的工作类似，例如 &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf&quot;>;OBD&lt;/a>; 和 &lt;a href=&quot; https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&quot;>;OBS&lt;/a>;，我们用 &lt;a href=&quot; 来近似损失https://en.wikipedia.org/wiki/Quadratic_form&quot;>;二次函数&lt;/a>;，使用二阶&lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor_series&quot;>;泰勒级数&lt; /a>;，其中 Hessian 矩阵是根据经验 &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;>;Fisher 信息矩阵&lt;/a>; 进行估计的。虽然梯度通常可以有效地计算，但由于其庞大的规模，计算和存储 Hessian 矩阵的成本过高。在文献中，通常通过对 Hessian 矩阵（例如，对角矩阵）和算法（例如，单独剪枝权重）做出限制性假设来应对这一挑战。 &lt;/p>; &lt;p>; CHITA 使用修剪问题的有效重新表述（使用二次损失的 BSS），避免显式计算 Hessian 矩阵，同时仍然使用该矩阵中的所有信息。这是通过利用经验费希尔信息矩阵的低&lt;a href=&quot;https://en.wikipedia.org/wiki/Rank_(linear_algebra)&quot;>;秩&lt;/a>;结构来实现的。这种重新表述可以被视为稀疏&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;>;线性回归&lt;/a>;问题，其中每个回归系数对应于神经网络中的特定权重。获得此回归问题的解决方案后，设置为零的系数将对应于应修剪的权重。我们的回归数据矩阵是 (&lt;em>;n&lt;/em>; x &lt;em>;p&lt;/em>;)，其中 &lt;em>;n&lt;/em>; 是批次（子样本）大小，&lt;em>;p&lt;/em>; em>; 是原始网络中的权重数量。通常&lt;em>;n&lt;/em>; &lt;&lt; &lt;em>;p&lt;/em>;，因此使用此数据矩阵进行存储和操作比使用 (&lt;em>;p&lt;/em>; x &lt;em>;p&lt;/em>;) Hessian 操作的常见修剪方法更具可扩展性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdK vBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s1601/image3.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUf ysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;CHITA 将二次损失近似重新表述为线性回归 (LR) 问题，这需要昂贵的 Hessian 矩阵。 LR 的数据矩阵在 &lt;em>;p&lt;/em>; 中呈线性，这使得重构比原始二次近似更具可扩展性。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;可扩展的优化算法&lt;/h2>; &lt;p>; CHITA 在以下稀疏性约束下将剪枝减少为线性回归问题：最多 &lt; em>;k&lt;/em>; 个回归系数可以不为零。为了获得此问题的解决方案，我们考虑对著名的&lt;a href=&quot;https://arxiv.org/pdf/0805.0510.pdf&quot;>;迭代硬阈值&lt;/a>; (IHT) 算法进行修改。 IHT 执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;>;梯度下降&lt;/a>;，每次更新后执行以下后处理步骤：Top-&lt;em >;k&lt;/em>;（即，具有最大幅度的&lt;em>;k&lt;/em>;系数）设置为零。 IHT 通常会为问题提供良好的解决方案，并且它会迭代探索不同的剪枝候选对象并联合优化权重。 &lt;/p>; &lt;p>; 由于问题的规模，具有恒定&lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;>;学习率&lt;/a>;的标准 IHT 可能会非常慢收敛。为了更快地收敛，我们开发了一种新的&lt;a href=&quot;https://en.wikipedia.org/wiki/Line_search&quot;>;线搜索&lt;/a>;方法，该方法利用问题结构来找到合适的学习率，即导致损失足够大的减少。我们还采用了多种计算方案来提高 CHITA 的效率和二阶近似的质量，从而产生了一个改进的版本，我们称之为 CHITA++。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>; 我们将 CHITA 的运行时间和准确性与几种状态进行比较使用不同架构的最先进的修剪方法，包括 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/1704.04861 &quot;>;移动网络&lt;/a>;。 &lt;/p>; &lt;p>; &lt;strong>;运行时间&lt;/strong>;：CHITA 比执行联合优化的同类方法更具可扩展性（而不是单独修剪权重）。例如，CHITA 在剪枝 ResNet 时的加速比可以达到 1000 倍以上。 &lt;/p>; &lt;p>; &lt;strong>;后剪枝精度&lt;/strong>;：&lt;strong>; &lt;/strong>;下面，我们比较了 CHITA 和 CHITA++ 与幅度剪枝（MP）的性能，&lt;a href=&quot;https: //arxiv.org/abs/2004.14340&quot;>;Woodfisher&lt;/a>; (WF) 和&lt;a href=&quot;https://arxiv.org/abs/2203.04466&quot;>;组合脑外科医生&lt;/a>; (CBS)，用于修剪 70% 的模型权重。总体而言，我们看到 CHITA 和 CHITA++ 取得了良好的改进。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1 aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz- KwvpQVIJKDEnkg_/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlk Fi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a >;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ResNet20 上各种方法的后剪枝精度。报告修剪 70% 模型权重的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-标题容器&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_ AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img边框=“0”数据原始高度=“742”数据原始宽度=“1200”高度=“396”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA -5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-thHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-CP-w2OUUDsPUNMBhGCTkSBFjV6ODFY60 K-VCFnGENDN4LtfA/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MobileNet 上各种方法的后剪枝精度。报告修剪 70% 模型权重的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 接下来，我们报告修剪更大网络的结果：ResNet50（在此网络上，一些ResNet20 图中列出的方法无法扩展）。这里我们与幅度剪枝和&lt;a href=&quot;https://arxiv.org/abs/2107.03356&quot;>;M-FAC&lt;/a>;进行比较。下图显示，CHITA 在各种稀疏度水平下实现了更好的测试精度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU 5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/s1050/image6.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;784&quot; data-original-width=&quot;1050&quot; height=&quot;478&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6J MfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/w640-h478/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;测试使用不同方法获得的修剪网络的准确性。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论、局限性和未来工作&lt;/h2>; &lt;p>; 我们提出了 CHITA，一种基于优化的方法用于修剪预先训练的神经网络。 CHITA 通过有效地使用二阶信息并借鉴组合优化和高维统计的思想，提供可扩展性和有竞争力的性能。 &lt;/p>; &lt;p>; CHITA 专为&lt;em>;非结构化修剪&lt;/em>;而设计，可以去除任何重量。理论上，非结构化剪枝可以显着降低计算要求。然而，在实践中实现这些减少需要支持稀疏计算的特殊软件（可能还有硬件）。相比之下，结构化修剪会删除神经元等整个结构，可能会提供在通用软件和硬件上更容易实现的改进。将 CHITA 扩展到结构化修剪将会很有趣。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是 Google 之间研究合作的一部分和麻省理工学院。感谢 Rahul Mazumder、Natalia Ponomareva、Wenyu Chen、Xiang Men、Zhe Zhu 和 Sergei Vassivitskii 在准备这篇文章和论文时提供的帮助。还要感谢 John Guilyard 在本文中创建图形。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4779594044050650231/comments/default&quot; rel= &quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html#comment -form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel= “编辑”类型=“application/atom+xml”/>;&lt;link href=“http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231”rel=“self”type=“application/atom” +xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html&quot; rel=&quot;alternate&quot; title=&quot;组合优化神经网络剪枝&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS 2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K /s72-c/CHITA%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr ：总计>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8689679451447865270&lt;/id>;&lt;发布>;2023-08-15T12:59:00.001-07:00&lt;/已发布>;&lt;更新>;2023-08-15T13:00:54.887-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt; /类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“推荐系统”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom” /ns#&quot; term=&quot;社交网络&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;研究：社会意识暂时因果解码器推荐系统&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;发布者：Google 研究工程师 Eltayeb Ahmed 和高级研究科学家 Subhrajit Roy&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt -PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx 8AO1_/s837/study-hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 阅读对于年轻学生有很多好处，例如&lt;a href=&quot;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/284286/reading_for_pleasure。 pdf&quot;>;更好的语言和生活技能&lt;/a>;，并且快乐阅读已被证明与&lt;a href=&quot;https://files.eric.ed.gov/fulltext/ED541404.pdf&quot;>;学业成功相关&lt; /a>;.此外，学生们还表示，阅读&lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/1361454042000312284&quot;>;情绪健康得到改善&lt;/a>;，并且&lt;a href=&quot;https:// eric.ed.gov/?id=ED496343&quot;>;更好的常识和对其他文化的更好的理解&lt;/a>;。由于线上和线下有大量的阅读材料，找到适合年龄的、相关的和引人入胜的内容可能是一项具有挑战性的任务，但帮助学生做到这一点是让他们参与阅读的必要步骤。向学生提供相关阅读材料的有效推荐有助于学生持续阅读，而这正是机器学习 (ML) 可以提供帮助的地方。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 机器学习已广泛应用于构建&lt;a href=&quot;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592 -5&quot;>;推荐系统&lt;/a>;，适用于各种类型的数字内容，从视频到书籍再到电子商务项目。推荐系统在一系列数字平台上使用，以帮助向用户展示相关且有吸引力的内容。在这些系统中，机器学习模型经过训练，可以根据用户偏好、用户参与度和推荐项目单独向每个用户推荐项目。这些数据为模型提供了强大的学习信号，使其能够推荐可能感兴趣的项目，从而改善用户体验。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2306.07946&quot;>;研究：社交意识暂时因果解码器推荐系统&lt;/a>;”中，我们提出了一个有声读物的内容推荐系统在教育环境中考虑阅读的社会性质。我们与 &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 合作开发了 STUDY 算法，这是一个教育非营利组织，旨在促进阅读障碍学生的阅读，通过学校向学生提供有声读物广泛的订阅计划。利用 Learning Ally 图书馆中的各种有声读物，我们的目标是帮助学生找到合适的内容，以帮助提高他们的阅读体验和参与度。由于一个人的同龄人当前正在阅读的内容会对他们感兴趣的阅读内容产生重大影响，因此我们共同处理同一教室中学生的阅读参与历史。这使得我们的模型能够从有关学生本地社交群体（在本例中为他们的教室）当前趋势的实时信息中受益。 &lt;/p>; &lt;br />; &lt;h2>;数据&lt;/h2>; &lt;p>; &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 拥有一个大型数字图书馆，其中包含针对以下人群的精选有声读物：学生，使其非常适合构建社会推荐模型，以帮助提高学生的学习成果。我们收到了两年的匿名有声读物消费数据。数据中的所有学生、学校和分组都是匿名的，仅通过随机生成的 ID 进行识别，无法通过 Google 追溯到真实实体。此外，所有潜在可识别元数据仅以聚合形式共享，以保护学生和机构不被重新识别。这些数据包括学生与有声读物互动的带时间戳的记录。对于每次交互，我们都有一个匿名的学生 ID（包括学生的年级和匿名的学校 ID）、有声读物标识符和日期。虽然许多学校将单一年级的学生分布在多个教室中，但我们利用此元数据做出简化的假设，即同一所学校和同一年级的所有学生都在同一教室。虽然这为建立更好的社交推荐模型提供了基础，但值得注意的是，这并不能让我们重新识别个人、班级群体或学校。 &lt;/p>; &lt;br />; &lt;h2>;STUDY 算法&lt;/h2>; &lt;p>; 我们将推荐问题描述为&lt;a href=&quot;https://arxiv.org/abs/2104.10584&quot;>;点击率&lt;/a>; 预测问题，我们对用户与每个特定项目交互的条件概率进行建模，条件是 1）用户和项目特征以及 2）当前用户的项目交互历史序列。 &lt;a href=&quot;https://arxiv.org/abs/1905.06874&quot;>;之前的工作&lt;/a>;建议&lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer&lt;基于 /a>; 的模型是 Google Research 开发的一种广泛使用的模型类，非常适合对这个问题进行建模。当单独处理每个用户时，这将成为一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;自回归序列建模问题&lt;/a>;。我们使用这个概念框架来建模我们的数据，然后扩展这个框架来创建研究方法。 &lt;/p>; &lt;p>; 虽然这种点击率预测方法可以对单个用户过去和未来的项目偏好之间的依赖关系进行建模，并且可以在训练时学习用户之间的相似性模式，但它无法在推理时对不同用户之间的依赖关系进行建模时间。为了认识阅读的社会性并弥补这一缺陷，我们开发了 STUDY 模型，该模型将每个学生阅读的多个书籍序列连接成一个序列，该序列收集单个教室中多个学生的数据。 &lt;/p>; &lt;p>; 然而，如果要通过 Transformer 建模，这种数据表示需要仔细研究。在 Transformer 中，注意力掩码是控制哪些输入可用于通知哪些输出的预测的矩阵。使用序列中的所有先验标记来通知输出预测的模式导致传统上在因果解码器中发现的上三角注意矩阵。然而，由于输入 STUDY 模型的序列不是按时间排序的，即使它的每个组成子序列都是标准的&lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;>;因果解码器&lt;/a>;不再适合这个序列。当尝试预测每个标记时，模型不允许关注序列中位于其之前的每个标记；其中一些令牌可能具有较晚的时间戳，并且包含在部署时不可用的信息。 &lt;br />; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tq Rd-ZWDXOCRpHAy0FZE9sV0xB8reyJ-- Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/s1787/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1651&quot; data-original-width = =第1787章pHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/w400-h370/image1.png&quot;宽度=&quot; 400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在此图中，我们显示了通常用于因果解码器。每列代表一个输出，每列代表一个输出。特定位置处的矩阵条目的值为 1（显示为蓝色）表示模型在预测相应列的输出时可以观察到该行的输入，而值为 0（显示为白色）表示相反的情况.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; STUDY 模型建立在因果变换器的基础上，通过将三角矩阵注意掩码替换为灵活的注意掩码，其值基于时间戳，以允许关注不同的子序列。与常规 Transformer 相比，常规 Transformer 不允许跨不同子序列进行关注，并且在序列内具有三角矩阵掩码，而 STUDY 在序列内维护因果三角关注矩阵，并且跨序列具有灵活的值，其值取决于时间戳。因此，序列中任何输出点的预测都是由过去相对于当前时间点发生的所有输入点通知的，无论它们是出现在序列中当前输入之前还是之后。这种因果约束很重要，因为如果不在训练时强制执行，模型可能会学习使用未来的信息进行预测，而这对于现实世界的部署来说是不可用的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6Zm UAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s1104/image3.jpg&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFHANO2eJdeFMIWt7K 58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;在（a）中，我们展示了一个具有因果注意力的顺序自回归变压器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQf vFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s1035/Study.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1035&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB 8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s16000/Study.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;在 (a) 中，我们展示了一个具有因果注意力的顺序自回归转换器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; !--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt; tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2 _wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/s1104/image3.png&quot;样式= “左边距：自动；右边距：自动；”&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;548&quot; height=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9Fd caJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/w318-h640/image3.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>; &lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在 (a) 中，我们展示了一个具有因果注意力的顺序自回归转换器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>; 我们使用 Learning Ally 数据集来训练 STUDY 模型以及多个基线以进行比较。我们实现了一个自回归点击率转换器解码器，我们将其称为“个体”，一个&lt;em>;k&lt;/em>;最近邻基线（KNN），以及一个可比较的社交基线，社交注意力记忆网络（SAMN） 。我们使用第一学年的数据进行训练，并使用第二学年的数据进行验证和测试。 &lt;/p>; &lt;p>; 我们通过测量用户实际交互的下一个项目在模型的前 &lt;em>;n&lt;/em>; 个推荐中的时间百分比来评估这些模型，即，hits@&lt;em>;n， &lt;/em>;对于&lt;em>;n&lt;/em>;的不同值。除了在整个测试集上评估模型之外，我们还报告了模型在测试集的两个子集上的得分，这两个子集比整个数据集更具挑战性。我们观察到，学生通常会在多个会话中与有声读物进行交互，因此简单地推荐用户最近读过的一本书将是一个强有力的简单推荐。因此，第一个测试子集（我们称之为“非延续”）是指当学生与与之前交互不同的书籍进行交互时，我们仅查看每个模型在推荐上的表现。我们还观察到学生会重温他们过去读过的书籍，因此通过将针对每个学生的推荐仅限于他们过去读过的书籍，可以在测试集上取得出色的表现。尽管向学生推荐旧的收藏夹可能很有价值，但推荐系统的大部分价值来自于向用户展示新的和未知的内容。为了衡量这一点，我们在学生第一次与标题交互的测试集子集上评估模型。我们将这个评估子集命名为“小说”。 &lt;/p>; &lt;p>; 我们发现，在我们评估的几乎每个切片中，STUDY 都优于所有其他测试模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr 7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/s1526/Study-img2.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1486&quot; data-original-width=&quot;1526&quot; height=&quot;390&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx 0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/w400-h390/Study-img2.png&quot;宽度=“400”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在此图中，我们比较了 Study、Individual、KNN 和 SAMN 四种模型的性能。我们用 hist@5 来衡量性能，即模型在模型的前 5 个推荐中建议用户阅读的下一个标题的可能性有多大。我们在整个测试集（全部）以及新颖的和非连续的分割上评估模型。我们发现 STUDY 在所有分组中始终优于其他三个模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;适当分组的重要性&lt;/h2>; &lt;p>; STUDY 算法的核心是将用户组织成组，并在模型的单次前向传递中对同一组中的多个用户进行联合推理。我们进行了一项消融研究，研究了实际分组对模型性能的重要性。在我们提出的模型中，我们将同一年级和学校的所有学生分组在一起。然后，我们对由同一年级和学区的所有学生定义的组进行实验，并将所有学生放入一个组中，并在每次前向传递中使用随机子集。我们还将这些模型与个体模型进行比较以供参考。 &lt;/p>; &lt;p>; 我们发现使用更加本地化的分组更为有效，学校和年级分组的效果优于地区和年级分组。这支持了这样的假设：学习模型之所以成功，是因为阅读等活动的社会性质——人们的阅读选择可能与周围人的阅读选择相关。这两种模型都优于其他两种模型（单组和个人），其中不使用年级水平对学生进行分组。这表明来自具有相似阅读水平和兴趣的用户的数据有利于性能。 &lt;/p>; &lt;br />; &lt;h2>;未来的工作&lt;/h2>; &lt;p>; 这项工作仅限于为假设社交关系同质的用户群进行推荐建模。将来，对关系不均匀的用户群体进行建模将是有益的，即，存在明显不同类型的关系或已知不同关系的相对强度或影响。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作涉及由研究人员、软件工程师和教育主题专家组成的多学科团队的协作努力。我们感谢我们的合著者：来自 Google 的 Diana Mincu、Lauren Harrell 和 Katherine Heller。我们还要感谢 Learning Ally 的同事 Jeff Ho、Akshat Shah、Erin Walker 和 Tyler Bastian，以及 Google 的合作者 Marc Repnyek、Aki Estrella、Fernando Diaz、Scott Sanner、Emily Salkey 和 Lev Proleev。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8689679451447865270/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/study-socially-aware-temporally-causal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; 类型=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;链接 href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog .research.google/2023/08/study-socially-aware-temporally-causal.html&quot; rel=&quot;alternate&quot; title=&quot;研究：社会意识时间因果解码器推荐系统&quot; type=&quot;text/html&quot;/>;&lt;author >;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16 “rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>;&lt;/gd :image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqa JM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s72-c/study-hero.png “ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-3240052415427459327&lt;/id>;&lt;发布>;2023-08-09T11:32:00.001-07:00&lt;/发布>;&lt;更新>;2023-08-09T12 :26:53.831-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言理解&quot;>;&lt;/category>;&lt;title type=&quot;text &quot;>;文档理解方面的进展&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Athena 团队 Google 研究部软件工程师 Sandeep Tata&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85z UbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s320/Document%20understanding%20hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 过去几年，能够自动处理复杂业务文档并将其转换为结构化对象的系统取得了快速进展。一个可以从收据、保险报价等文档中&lt;a href=&quot;https://ai.googleblog.com/2020/06/extracting-structured-data-from.html&quot;>;自动提取数据&lt;/a>;的系统和财务报表，通过避免容易出错的手动工作，有可能显着提高业务工作流程的效率。基于 &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; 架构的最新模型已经展示了&lt;a href=&quot; https://ai.googleblog.com/2022/04/formnet-beyond-sequential-modeling-for.html&quot;>;准确度显着提升&lt;/a>;。更大的模型，例如 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>;，也被用来进一步简化这些业务工作流程。然而，学术文献中使用的数据集未能捕捉到现实世界用例中遇到的挑战。因此，学术基准报告了很强的模型准确性，但这些相同的模型在用于复杂的现实应用程序时表现不佳。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;VRDU：视觉丰富文档理解的基准&lt; /a>;”，在 &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>; 上发布，我们宣布发布新的&lt;a href=&quot;https://research.google /resources/datasets/visually-rich-document-understanding/&quot;>;视觉丰富文档理解&lt;/a>; (VRDU) 数据集旨在弥补这一差距，帮助研究人员更好地跟踪文档理解任务的进展。根据经常使用文档理解模型的现实文档类型，我们列出了良好的文档理解基准的五个要求。然后，我们描述了研究界当前使用的大多数数据集为何无法满足其中一项或多项要求，而 VRDU 却满足了所有这些要求。我们很高兴地宣布公开发布 VRDU &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;数据集&lt;/a>;和&lt;a href=&quot;https ://github.com/google-research-datasets/vrdu&quot;>;评估代码&lt;/a>;，采用&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;知识共享许可&lt;/a>;一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;基准要求&lt;/h2>; &lt;p>; 首先，我们比较了最先进的模型准确性（例如，使用 &lt;a href=&quot;https://arxiv.org/abs/2203.08411&quot;>;FormNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2012.14740&quot;>;LayoutLMv2&lt;/ a>;）关于现实世界用例与学术基准（例如，&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;、&lt;a href=&quot;https://openreview.a>;） net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;）。我们观察到，最先进的模型与学术基准结果不符，并且在现实世界中的准确性要低得多。接下来，我们将经常使用文档理解模型的典型数据集与学术基准进行比较，并确定了五个数据集要求，使数据集能够更好地捕获现实应用程序的复杂性：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;丰富模式：&lt;/strong>;在实践中，我们看到了各种用于结构化提取的丰富模式。实体具有不同的数据类型（数字、字符串、日期等），这些数据类型可能是必需的、可选的或在单个文档中重复，甚至可能是嵌套的。像（标题、问题、答案）这样的简单平面模式的提取任务并不能反映实践中遇到的典型问题。 &lt;/li>;&lt;li>;&lt;strong>;布局丰富的文档：&lt;/strong>;文档应该具有复杂的布局元素。实际设置中的挑战来自以下事实：文档可能包含表格、键值对、在单列和双列布局之间切换、不同部分具有不同的字体大小、包括带有标题甚至脚注的图片。将此与大多数文档以句子、段落和带有节标题的章节组织的数据集进行对比 - 这些文档通常是 &lt;a href=&quot;https://arxiv.org/abs 上经典自然语言处理文献的焦点/2007.14062&quot;>;长&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2004.08483&quot;>;输入&lt;/a>;。 &lt;/li>;&lt;li>;&lt;strong>;多样化模板：&lt;/strong>;基准测试应包括不同的结构布局或模板。对于大容量模型来说，通过记忆结构从特定模板中提取数据是微不足道的。然而，在实践中，人们需要能够推广到新的模板/布局，这是基准测试中的训练-测试分割应该衡量的一种能力。 &lt;/li>;&lt;li>;&lt;strong>;高质量 OCR&lt;/strong>;：文档应具有高质量&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;光学字符识别&lt;/a>; (OCR) 结果。我们此基准测试的目标是专注于 VRDU 任务本身，并排除 OCR 引擎选择带来的可变性。 &lt;/li>;&lt;li>;&lt;strong>;令牌级注释&lt;/strong>;：文档应包含可以映射回相应输入文本的真实注释，以便每个令牌都可以注释为相应实体的一部分。这与简单地提供要为实体提取的值的文本形成对比。这是生成干净的训练数据的关键，我们不必担心与给定值的偶然匹配。例如，在某些收据中，如果税额为零，“税前总计”字段可能具有与“总计”字段相同的值。具有令牌级别注释会阻止我们生成训练数据，其中匹配值的两个实例都被标记为“total”字段的真实值，从而产生嘈杂的示例。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn 5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s1600 /Benchmark%20GIF.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6Ai WX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s16000/Benchmark%20GIF.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;VRDU 数据集和任务&lt;/h2>; &lt;p>; VRDU 数据集是两个公开可用数据集的组合，&lt;a href=&quot;https://efile.40%;&quot;>; fara.gov/ords/fara/f?p=1235:10&quot;>;注册表&lt;/a>;和&lt;a href=&quot;https://publicfiles.fcc.gov/&quot;>;广告购买表单&lt;/a>;。这些数据集提供了代表现实世界用例的示例，并满足上述五个基准要求。 &lt;/p>; &lt;p>; Ad-buy Forms 数据集包含 641 个包含政治广告详细信息的文档。每份文件都是由电视台和竞选团队签署的发票或收据。文档使用表格、多列和键值对来记录广告信息，例如产品名称、播放日期、总价、发布日期和时间。 &lt;/p>; &lt;p>; 登记表数据集包含 1,915 个文档，其中包含有关外国代理人在美国政府登记的信息。每份文件都记录了涉及需要公开披露的活动的外国代理人的基本信息。内容包括注册人名称、相关部门地址、活动目的等详细信息。 &lt;/p>; &lt;p>; 我们从公众&lt;a href=&quot;https://www.fcc.gov/&quot;>;联邦通信委员会&lt;/a>; (FCC) 和&lt;a href=&quot; https://www.justice.gov/nsd-fara&quot;>;外国代理人登记法&lt;/a>; (FARA) 网站，并使用 &lt;a href=&quot;https://cloud.google.com/ 将图像转换为文本&quot;>;Google Cloud 的&lt;/a>; &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;OCR&lt;/a>;。我们丢弃了少量几页长的文档，并且处理在两分钟内没有完成。这也使我们能够避免发送很长的文档进行手动注释——对于单个文档来说，这项任务可能需要一个多小时。然后，我们为具有文档标记任务经验的注释者团队定义了模式和相应的标记指令。 &lt;/p>; &lt;p>; 还向注释者提供了一些我们自己标记的示例标记文档。该任务要求注释者检查每个文档，围绕每个文档架构中实体的每次出现绘制边界框，并将该边界框与目标实体相关联。第一轮标注后，一组专家被指派对结果进行审核。修正后的结果包含在已发布的 VRDU 数据集中。有关标记协议和每个数据集架构的更多详细信息，请参阅&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG 2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s1600/Chart.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i7 2av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s16000/Chart.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;现有学术基准 (&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;、&lt;a href=&quot;https://openreview.net/ pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;、&lt;a href=&quot;https:// /arxiv.org/abs/2105.05796&quot;>;Kleister-NDA&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-Charity&lt;/a>;、&lt;a href=&quot;https ://wandb.ai/stacey/deepform_v1/reports/DeepForm-Understand-Structured-Documents-at-Scale--VmlldzoyODQ3Njg&quot;>;DeepForm&lt;/a>;）未达到我们为项目确定的五项要求中的一项或多项良好的文档理解基准。 VRDU 满足了所有这些要求。请参阅我们的&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;，了解每个数据集的背景以及它们如何未能满足一项或多项要求的讨论。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们构建了四个不同的模型训练集，分别有 10、50、100 和 200 个样本。然后，我们使用三个任务（如下所述）评估 VRDU 数据集：(1) 单模板学习、(2) 混合模板学习和 (3) 看不见的模板学习。对于每项任务，我们在测试集中包含 300 个文档。我们使用测试集上的 &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1 分数&lt;/a>; 来评估模型。 &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;单模板学习&lt;/em>; (STL)：这是最简单的场景，其中训练、测试和验证集仅包含单个模板。这个简单的任务旨在评估模型处理固定模板的能力。当然，我们期望此任务的 F1 分数非常高（0.90+）。 &lt;/li>;&lt;li>;&lt;em>;混合模板学习&lt;/em>;（MTL）：此任务与大多数相关论文使用的任务类似：训练集、测试集和验证集都包含属于同一集的文档模板。我们从数据集中随机抽取文档并构建分割，以确保每个模板的分布在采样过程中不会改变。 &lt;/li>;&lt;li>;&lt;em>;未见模板学习&lt;/em>;（UTL）：这是最具挑战性的设置，我们评估模型是否可以泛化到未见模板。例如，在注册表数据集中，我们使用三个模板中的两个来训练模型，并使用其余一个来测试模型。训练、测试和验证集中的文档来自不相交的模板集。据我们所知，以前的基准测试和数据集没有明确提供这样的任务，旨在评估模型泛化到训练期间未见过的模板的能力。 &lt;/li>; &lt;/ul>; &lt;p>; 目标是能够评估模型的数据效率。在我们的&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;中，我们比较了使用 STL、MTL 和 UTL 任务的两个最新模型，并提出了三个观察结果。首先，与其他基准测试不同，VRDU 具有挑战性，并表明模型有很大的改进空间。其次，我们表明，即使是最先进的模型，few-shot 性能也低得惊人，即使是最好的模型，其 F1 分数也低于 0.60。第三，我们表明模型很难处理结构化的重复字段，并且在这些字段上的表现特别差。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们发布了新的&lt;a href=&quot;https:// Research.google/resources/datasets/visually-rich-document-understanding/&quot;>;视觉丰富文档理解&lt;/a>; (VRDU) 数据集，可帮助研究人员更好地跟踪文档理解任务的进度。我们描述了为什么 VRDU 更好地反映了该领域的实际挑战。我们还提出了实验，表明 VRDU 任务具有挑战性，并且与文献中通常使用的数据集（典型的 F1 分数为 0.90+）相比，最近的模型有很大的改进空间。我们希望 VRDU 数据集和评估代码的发布有助于研究团队推进文档理解的最新技术。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;非常感谢王子龙、周一超、魏Wei 和 Chen-Yu Lee 与 Sandeep Tata 共同撰写了这篇论文。感谢 Marc Najork、Riham Mansour 以及 Google Research 和 Cloud AI 团队的众多合作伙伴提供了宝贵的见解。感谢 John Guilyard 创建本文中的动画。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3240052415427459327/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/advances-in-document-understanding.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/08/advances-in-document-understanding.html&quot; rel=&quot;alternate&quot; title=&quot;文档理解方面的进展&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google人工智能&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http ://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/作者>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV 8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s72-c/Document%20understanding%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt; id>;标签：blogger.com，1999：blog-8474926331452026626.post-595171581401765238&lt;/id>;&lt;发布>;2023-08-08T14:02:00.000-07:00&lt;/发布>;&lt;更新>;2023-08-08T14： 02:34.659-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/ category>;&lt;title type=&quot;text&quot;>;AdaTape：具有自适应计算和动态读写的基础模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：薛福兆、研究实习生和研究科学家 Mostafa Dehghani，Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIq JhdQrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudIEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s2000/adatape.png&quot; style=&quot;显示：无；” />; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;>;自适应计算&lt;/a>;是指机器学习系统根据环境变化调整其行为的能力。虽然传统的神经网络具有固定的功能和计算能力，即它们花费相同数量的 FLOP 来处理不同的输入，但具有自适应和动态计算的模型会根据输入的复杂性来调整其专用于处理每个输入的计算预算。输入。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 神经网络中的自适应计算具有吸引力有两个关键原因。首先，引入适应性的机制提供了归纳偏差，可以在解决一些具有挑战性的任务中发挥关键作用。例如，为不同的输入启用不同数量的计算步骤对于解决需要不同深度的建模层次结构的算术问题至关重要。其次，它使从业者能够通过动态计算提供的更大灵活性来调整推理成本，因为可以调整这些模型以花费更多的 FLOP 来处理新输入。 &lt;/p>; &lt;p>; 神经网络可以通过对各种输入使用不同的函数或计算预算来实现自适应。深度神经网络可以被认为是一个基于输入及其参数输出结果的函数。为了实现自适应函数类型，根据输入选择性地激活参数子集，这一过程称为条件计算。在 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;mixture-of-experts&lt;/a>; 的研究中探索了基于函数类型的自适应性，其中每个输入的稀疏激活参数样本是通过路由确定的。自适应计算的另一个研究领域涉及动态计算预算。与标准神经网络（例如 &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;）不同，&lt;a href= &quot;https://arxiv.org/abs/2005.14165&quot;>;GPT-3&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling -to.html&quot;>;PaLM&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;ViT&lt;/a>;其计算预算对于不同的样本是固定的，&lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;最近的研究&lt;/a>;表明，自适应计算预算可以提高 Transformer 不足的任务的性能。其中许多工作通过使用动态深度来分配计算预算来实现自适应性。例如，提出了自适应计算时间（ACT）算法来为递归神经网络提供自适应计算预算。 &lt;a href=&quot;https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html&quot;>;通用转换器&lt;/a>;将 ACT 算法扩展到转换器，使计算预算取决于用于每个输入示例或标记的转换器层数。最近的研究，例如 &lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;PonderNet&lt;/a>;，在改进动态停止机制的同时也遵循类似的方法。 &lt;/p>; &lt;p>; 在论文“&lt;a href=&quot;https://arxiv.org/abs/2301.13195&quot;>;具有弹性输入序列的自适应计算&lt;/a>;”中，我们介绍了一种利用自适应计算的新模型，称为&lt;em>;AdaTape&lt;/em>;。该模型是基于 Transformer 的架构，使用一组动态令牌来创建弹性输入序列，与以前的作品相比，提供了关于适应性的独特视角。 AdaTape 使用自适应磁带读取机制来根据输入的复杂性确定添加到每个输入的不同数量的磁带标记。 AdaTape 实现起来非常简单，提供了一个有效的旋钮来在需要时提高准确性，而且与&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;其他自适应基线&lt;/a相比也更加高效>; 因为它直接将自适应性注入到输入序列中，而不是模型深度中。最后，Adatape 在标准任务（例如图像分类）以及算法任务上提供更好的性能，同时保持良好的质量和成本权衡。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;具有弹性输入序列的自适应计算转换器&lt;/h2>; &lt;p>; AdaTape 使用两种自适应函数类型和动态计算预算。具体来说，对于标记化后的一批输入序列（例如，来自视觉变换器中的图像的非重叠补丁的线性投影），AdaTape 使用表示每个输入的向量来动态选择可变大小的磁带标记序列。 &lt;/p>; &lt;p>; AdaTape 使用称为“磁带库”的令牌库来存储通过自适应磁带读取机制与模型交互的所有候选磁带令牌。我们探索了两种不同的创建磁带库的方法：输入驱动的磁带库和可学习的磁带库。 &lt;/p>; &lt;p>; 输入驱动库的总体思想是从输入中提取一组令牌，同时采用与原始模型令牌生成器不同的方法将原始输入映射到输入标记的序列。这使得能够动态地按需访问来自使用不同观点（例如，不同图像分辨率或不同抽象级别）获得的输入的信息。 &lt;/p>; &lt;p>; 在某些情况下，不同抽象级别的标记化是不可能的，因此输入驱动的磁带库是不可行的，例如当很难进一步拆分 &lt;a href=&quot; 中的每个节点时https://arxiv.org/abs/2012.09699&quot;>;图形转换器&lt;/a>;。为了解决这个问题，AdaTape 提供了一种更通用的方法，通过使用一组可训练向量作为磁带令牌来生成磁带库。这种方法被称为可学习库，可以被视为嵌入层，其中模型可以根据输入示例的复杂性动态检索标记。可学习的银行使 AdaTape 能够生成更灵活的磁带库，使其能够根据每个输入示例的复杂性动态调整其计算预算，例如，更复杂的示例从磁带库中检索更多的令牌，这使得模型不会只使用存储在银行中的知识，但由于输入现在更大，所以还要花费更多的 FLOP 来处理它。 &lt;/p>; &lt;p>; 最后，选定的磁带标记将附加到原始输入并馈送到以下转换器层。对于每个转换器层，在所有输入和磁带标记上使用相同的多头注意力。然而，使用了两种不同的前馈网络（FFN）：一种用于来自原始输入的所有令牌，另一种用于所有磁带令牌。通过对输入和磁带令牌使用单独的前馈网络，我们观察到质量稍好一些。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfda EU1-w8kv7USzuc-VBwGumvHxfccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s1786/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;871&quot; data-original-width=&quot;1786&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfdaEU1-w8kv7USzuc-VBwGumvHxf ccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AdaTape 概述。对于不同的样本，我们从磁带库中挑选不同数量的不同标记。磁带库可以由输入驱动，例如通过提取一些额外的细粒度信息，或者它可以是一组可训练向量。自适应磁带读取用于针对不同的输入递归地选择不同长度的磁带令牌序列。然后将这些标记简单地附加到输入并馈送到变压器编码器。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;AdaTape 提供有用的归纳偏差&lt;/h2>; &lt;p>; 我们在奇偶校验上评估 AdaTape，这对于标准 Transformer 来说是一项非常具有挑战性的任务，以研究 AdaTape 中归纳偏差的影响。对于奇偶校验任务，给定序列 1、0 和 -1，模型必须预测序列中 1 数量的偶数或奇数。 Parity 是最简单的非计数器自由或&lt;a href=&quot;https://arxiv.org/abs/2009.11264&quot;>;周期性正则语言&lt;/a>;，但也许令人惊讶的是，标准 Transformer 无法解决该任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3 EoYw4UhzcKK0fGIscOJSv36zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/s866/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;432&quot; data-original-width=&quot;866&quot; height=&quot;319&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3EoYw4UhzcKK0fGIscOJSv36 zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/w640-h319/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;对奇偶校验任务的评估。标准 Transformer 和 Universal Transformer 无法执行此任务，两者都显示出随机猜测基线水平的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;尽管进行了短期评估，对于简单的序列，标准 Transformer 和通用 Transformer 都无法执行奇偶校验任务，因为它们无法在模型中维护计数器。然而，AdaTape 的性能优于所有基线，因为它在其输入选择机制中融入了轻量级循环，提供了归纳偏差，可以实现计数器的隐式维护，这在标准 Transformer 中是不可能的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;图像分类评估&lt;/h2>; &lt;p>; 我们还在图像分类任务上评估 AdaTape。为此，我们从头开始在 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet-1K&lt;/a>; 上训练 AdaTape。下图显示了 AdaTape 和基线方法的准确性，包括 &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>; 和 Universal Transformer ViT（UViT 和 U2T）与它们的速度（以每秒由每个代码处理的图像数量来衡量）。在质量和成本权衡方面，AdaTape 的性能比替代自适应变压器基线要好得多。就效率而言，较大的 AdaTape 模型（就参数数量而言）比较小的基线更快。这些结果与&lt;a href=&quot;https://arxiv.org/abs/2110.12894&quot;>;之前的工作&lt;/a>;的发现一致，该发现表明自适应模型深度架构不太适合许多加速器，例如热塑性聚氨酯。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7Z Ih6Rujxnd-H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H -Hxzek9ziXZsw5adSEMZU/s1473/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;657&quot; data-original-width=&quot;1473 “高度=“285”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7ZIh6Rujxnd- H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H-Hxzek9ziXZsw5adSEMZU/w640-h285/image2.png&quot; 宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们通过从头开始在 ImageNet 上进行训练来评估 AdaTape 。对于&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>;，我们不仅从论文中报告了他们的结果，还通过从头开始训练重新实​​现了A-ViT，即，A-ViT（我们的）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A AdaTape 行为的研究&lt;/h2>; &lt;p>; 除了奇偶校验任务和 ImageNet-1K 上的性能之外，我们还评估了 AdaTape 在 &lt;a href=&quot;https:// /arxiv.org/abs/1707.02968&quot;>;JFT-300M&lt;/a>;验证集。为了更好地理解模型的行为，我们将输入驱动银行上的代币选择结果可视化为热图，其中较浅的颜色意味着该位置被更频繁地选择。热图显示 AdaTape 更频繁地选择中心补丁。这与我们的先验知识相一致，因为中心补丁通常包含更多信息，尤其是在具有自然图像的数据集的背景下，其中主要对象位于图像的中间。这一结果凸显了 AdaTape 的智能性，因为它可以有效地识别信息更丰富的补丁并确定优先级，以提高其性能。 &lt;/p>; &lt;p>;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgk R2VcDoKTicWQ_hufXwrlAcPMoeYqcrCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/s839/ image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;839&quot; height=&quot;270 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgkR2VcDoKTicWQ_hufXwrlAcPMoeYq crCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/w640-h270/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr >;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们可视化 AdaTape-B/32（左）和 AdaTape-B/16（右）的磁带令牌选择热图。颜色越热/越浅，意味着该位置的补丁被更频繁地选择。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; AdaTape 的特点是自适应磁带读取机制生成的弹性序列长度。这还引入了一种新的感应偏置，使 AdaTape 能够解决对标准变压器和现有自适应变压器都具有挑战性的任务。通过对图像识别基准进行全面的实验，我们证明了当计算保持恒定时，AdaTape 优于标准转换器和自适应架构转换器。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者之一，Mostafa Dehghani ，现供职于 Google DeepMind。 &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/595171581401765238/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html&quot; rel=&quot;alternate&quot; title=&quot;AdaTape：具有自适应计算和动态读写的基础模型&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIqJhd Qrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudiEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s72- c/adatape.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/条目>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-4436946873570093049&lt;/id>;&lt;已发布>;2023-08-03T11:24:00.001-07:00&lt;/已发布>;&lt;已更新>; 2023-08-03T11:24:40.465-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“AI”>;&lt;/类别>;&lt;类别方案=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term= &quot;健康&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;多模式医疗人工智能&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：健康人工智能主管 Greg Corrado 、Google 研究部和 Google 研究部工程与研究副总裁 Yossi Matias&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjm G9lDTBWdy5kH_bR3-tqN8KzdhgmrLL_N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3 /s1600/medpalm.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 医学本质上是一门多模式学科。在提供护理时，临床医生通常会解读各种模式的数据，包括医学图像、临床记录、实验室测试、电子健康记录、基因组学等。在过去十年左右的时间里，人工智能系统在特定模式下的特定任务上取得了专家级的性能——一些人工智能系统&lt;a href=&quot;https://www.nature.com/articles/s41591 -019-0447-x&quot;>;处理 CT 扫描&lt;/a>;，而其他人&lt;a href=&quot;https://jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;分析高倍病理切片&lt;/a>;，还有一些&lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/NEJMc2112090&quot;>;寻找罕见的遗传变异&lt;/a>;。这些系统的输入往往是复杂的数据，例如图像，它们通常提供结构化输出，无论是离散等级的形式还是&lt;a href=&quot;https://www.nature.com/articles/s41591-018- 0107-6&quot;>;密集图像分割掩模。&lt;/a>;同时，大型语言模型 (LLM) 的容量和功能具有&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;变得如此先进&lt;/a>;，以至于他们通过用简单的语言进行解释和回应，表现出了对医学知识的理解和专业知识。但是，我们如何将这些功能结合在一起来构建可以利用所有这些来源的信息的医疗人工智能系统？ &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在今天的博文中，我们概述了为法学硕士带来多模式能力的一系列方法，并分享了关于构建多模式医学法学硕士的可处理性的一些令人兴奋的结果，正如最近的三篇研究论文所述。这些论文反过来概述了如何将&lt;em>;从头&lt;/em>;模式引入法学硕士，如何将最先进的医学成像基础模型移植到对话式法学硕士上，以及建立法学硕士的第一步真正的多模式医疗人工智能系统。如果成功成熟，多模式医学法学硕士可能会成为涵盖专业医学、医学研究和消费者应用的新辅助技术的基础。与我们之前的工作一样，我们强调需要与医学界和医疗保健生态系统合作仔细评估这些技术。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;一系列方法&lt;/h2>; &lt;p>; 最近提出了几种构建多模式法学硕士的方法月 [&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;1&lt;/a>;，&lt;a href=&quot;https:// /www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;2&lt;/a>;，&lt;a href=&quot;https://ai.googleblog.com/2023 /03/palm-e-embodied-multimodal-language.html&quot;>;3&lt;/a>;]，毫无疑问，新方法将在一段时间内继续出现。为了了解为医疗人工智能系统带来新模式的机会，我们将考虑三种广泛定义的方法：工具使用、模型移植和通才系统。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ ZKRMY82JiKLD26G2skhiJEEh8Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s1600/image1.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ZKRMY82JiKLD26G2skhiJEEh8 Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;工具使用&lt;/h2>; &lt;p>; 在 &lt;em >;工具使用方法，一个中心医学法学硕士将各种模式的数据分析外包给一组针对这些任务独立优化的软件子系统：工具。工具使用的常见助记示例是教法学硕士使用计算器而不是自己做算术。在医疗领域，面对胸部 X 光检查的医学法学硕士可以将该图像转发到放射学人工智能系统并整合该响应。这可以通过子系统提供的应用程序编程接口（API）来完成，或者更奇特的是，两个具有不同专业的医疗人工智能系统进行对话来完成。 &lt;/p>; &lt;p>; 这种方法有一些重要的好处。它允许子系统之间实现最大的灵活性和独立性，使卫生系统能够根据经过验证的子系统性能特征在技术提供商之间混合和匹配产品。此外，子系统之间的人类可读通信通道最大限度地提高了可审核性和可调试性。也就是说，在独立子系统之间实现正确的通信可能很棘手，这会缩小信息传输范围，或暴露通信错误和信息丢失的风险。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;模型嫁接&lt;/h2>; &lt;p>; 一种更综合的方法是采用专门用于每个相关领域，并对其进行调整以直接插入法学硕士 - 将视觉模型&lt;em>;嫁接&lt;/em>;到核心推理代理上。与由法学硕士确定所使用的具体工具的工具使用相反，在模型移植中，研究人员可以在开发过程中选择使用、完善或开发特定模型。在谷歌研究中心最近的两篇论文中，我们表明这实际上是可行的。神经法学硕士通常通过首先将单词映射到&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings&quot;>;向量嵌入空间&lt;/一个>;。这两篇论文都基于将数据从新模态映射到法学硕士已经熟悉的输入词嵌入空间的想法。第一篇论文“&lt;a href=&quot;https://arxiv.org/abs/2307.09018&quot;>;基于个人特定数据的健康多模式法学硕士&lt;/a>;”表明&lt;a href= “https://www.ukbiobank.ac.uk/&quot;>;英国生物银行&lt;/a>;如果我们首先训练神经网络分类器来解释&lt;a href=&quot;https://www.mayoclinic.org/tests -procedures/spirometry/about/pac-20385201&quot;>;呼吸图&lt;/a>;（一种用于评估呼吸能力的方式），然后调整该网络的输出作为 LLM 的输入。 &lt;/p>; &lt;p>; 第二篇论文，“&lt;a href=&quot;https://arxiv.org/abs/2308.01317&quot;>;ELIXR：通过大语言模型和放射学的结合实现通用 X 射线人工智能系统视觉编码器&lt;/a>;”，采用相同的策略，但将其应用于放射学中的全尺寸图像编码器模型。从&lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;了解胸部 X 光检查的基础模型&lt;/a>;开始，已展示为了成为在此模式中构建各种分类器的良好基础，本文描述了训练一个轻量级的医疗信息适配器，该适配器将基础模型的顶层输出重新表示为一系列标记LLM 的输入嵌入空间。尽管对视觉编码器和语言模型都没有进行微调，但生成的系统显示了未经训练的功能，包括&lt;a href=&quot;https://arxiv.org/abs/2210.10163&quot;>;语义搜索&lt;/a>;和&lt;a href=&quot;https://www.nature.com/articles/sdata2018251&quot;>;视觉问答&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFM aMMzTSUu4fif7t9SVsr9MduEcISlA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s1600/image3.gif&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFMaMMzTSUu4fif7t9SVsr9MduEc ISLA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;我们嫁接模型的方法是通过训练医疗信息适配器来将现有或改进的图像编码器的输出映射为法学硕士可理解的形式。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 模型嫁接有很多优点。它使用相对适度的计算资源来训练适配器层，但允许法学硕士在每个数据域中现有的高度优化和验证的模型上构建。将问题模块化为编码器、适配器和 LLM 组件还可以方便在开发和部署此类系统时对各个软件组件进行测试和调试。相应的缺点是，专业编码器和 LLM 之间的通信不再是人类可读的（作为一系列高维向量），并且嫁接过程不仅需要为每个特定领域的编码器构建一个新的适配器，还需要为每个每个编码器的&lt;em>;修订&lt;/em>;。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;通才系统&lt;/h2>; &lt;p>; 多模式医疗人工智能最根本的方法是构建一个集成的系统，完全通才的系统本身就能够吸收所有来源的信息。在该领域的第三篇论文“&lt;a href=&quot;https://arxiv.org/abs/2307.14334&quot;>;迈向通才生物医学人工智能&lt;/a>;”中，我们没有为每种数据模态使用单独的编码器和适配器，以 &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;PaLM-E&lt;/a>; 为基础构建，PaLM-E 是最近发布的多模式模型单个 LLM (&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;) 和 &lt; a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;单视觉编码器 (ViT)&lt;/a>;。在此设置中，LLM 文本编码器涵盖文本和表格数据模式，但现在所有其他数据都被视为图像并馈送到视觉编码器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42 Hepv4DNsEWebRmtorN05xdpkJ2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s1600/image2.gif&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42Hepv4DNsEWebRmtorN05xdpkJ 2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s16000/image2.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Med-PaLM M 是一个大型多模态生成模型，可以使用相同的模型权重灵活地编码和解释生物医学数据，包括临床语言、成像和基因组学。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们通过在论文中描述的医学数据集上微调整套模型参数，将 PaLM-E 专门应用于医学领域。由此产生的通用医疗人工智能系统是 &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM&lt;/a>; 的多模式版本，我们称之为 Med-PaLM M。灵活的多模式序列到序列的架构使我们能够在一次交互中交织各种类型的多模态生物医学信息。据我们所知，这是单个统一模型的首次演示，该模型可以解释多模式生物医学数据并在所有任务中使用相同的模型权重集来处理各种任务（论文中有详细评估）。 &lt;/p>; &lt;p>; 这种多模态通才系统方法是我们描述的方法中最雄心勃勃、同时也是最优雅的方法。原则上，这种直接方法最大限度地提高了模式之间的灵活性和信息传输。由于没有 API 来维持兼容性，也没有适配器层的激增，通用方法可以说是最简单的设计。但同样的优雅也是其一些缺点的根源。计算成本通常更高，并且由于单一视觉编码器服务于多种模式，领域专业化或系统可调试性可能会受到影响。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;多模式医疗人工智能的现实&lt;/h2>; &lt;p>;为了在医学中充分利用人工智能，我们需要将经过预测人工智能训练的专家系统的优势与通过生成人工智能实现的灵活性结合起来。哪种方法（或方法组合）在该领域最有用取决于许多尚未评估的因素。通才模型的灵活性和简单性是否比模型嫁接或工具使用的模块化更有价值？哪种方法可以为特定的实际用例提供最高质量的结果？支持医学研究或医学教育与增强医疗实践的首选方法是否不同？回答这些问题需要持续进行严格的实证研究，并与医疗保健提供者、医疗机构、政府实体和医疗保健行业合作伙伴继续进行广泛的直接合作。我们期待着共同寻找答案。 &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4436946873570093049/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/multimodal-medical-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google /2023/08/multimodal-medical-ai.html&quot; rel=&quot;alternate&quot; title=&quot;多模式医疗 AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http ://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/ g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot; 72“网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjmG9lDTBWdy5kH_bR3-tqN8KzdhgmrLL _N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3/s72-c/medpalm.png“宽度=“72”xmlns：媒体=“http ://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：博客-8474926331452026626.post-5980448298988153366&lt;/id>;&lt;发布>;2023-07-26T09:33:00.003-07:00&lt;/发布>;&lt;更新>;2023-08-01T08:31:25.216-07:00&lt;/更新>; &lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;寻找无源域的通用方法适应&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Eleni Triantafillou 和学生研究员 Malik Boudiaf &lt;/span>; &lt;img src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4epC-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgy M4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/s1500/notela.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 深度学习最近在广泛的问题和应用中取得了巨大进展，但模型在部署在未知的领域或分布中时常常会出现不可预测的失败。 &lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;无源域适应&lt;/a>; (SFDA) 是一个研究领域，旨在设计适应预训练模型的方法（在一个“源域”）到一个新的“目标域”，仅使用后者的未标记数据。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 设计深度模型的适应方法是一个重要的研究领域。虽然模型和训练数据集规模的不断扩大是其成功的关键因素，但这种趋势的负面后果是训练此类模型的计算成本越来越高，在某些情况下使得大型模型训练&lt;a href=&quot;https:// spectrum.ieee.org/deep-learning-computational-cost&quot;>;不太容易访问&lt;/a>;并且不必要&lt;a href=&quot;https://penntoday.upenn.edu/news/hidden-costs-ai-impending-energy- and-resource-strain&quot;>;增加碳足迹&lt;/a>;。缓解这一问题的一个途径是通过设计技术来利用和重用已经训练好的模型来处理新任务或推广到新领域。事实上，在&lt;a href=&quot;https://arxiv.org/abs/1911.02685&quot;>;迁移学习&lt;/a>;的框架下，对模型适应新任务进行了广泛的研究。 &lt;/p>; &lt;p>; SFDA is a particularly practical area of this research because several real-world applications where adaptation is desired suffer from the unavailability of labeled examples from the target domain. In fact, SFDA is enjoying increasing attention [&lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;2&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2110.04202&quot;>;3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;4&lt;/a>;]. However, albeit motivated by ambitious goals, most SFDA research is grounded in a very narrow framework, considering simple &lt;a href=&quot;https://arxiv.org/abs/2108.13624&quot;>;distribution shifts&lt;/a>; in image classification tasks. &lt;/p>; &lt;p>; In a significant departure from that trend, we turn our attention to the field of bioacoustics, where naturally-occurring distribution shifts are ubiquitous, often characterized by insufficient target labeled data, and represent an obstacle for practitioners. Studying SFDA in this application can, therefore, not only inform the academic community about the generalizability of existing methods and identify open research directions, but can also directly benefit practitioners in the field and aid in addressing one of the biggest challenges of our century: biodiversity preservation. &lt;/p>; &lt;p>; In this post, we announce “&lt;a href=&quot;https://arxiv.org/abs/2302.06658&quot;>;In Search for a Generalizable Method for Source-Free Domain Adaptation&lt;/a>;”, appearing at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023. &lt;/a>;We show that state-of-the-art SFDA methods can underperform or even collapse when confronted with realistic distribution shifts in bioacoustics. Furthermore, existing methods perform differently relative to each other than observed in vision benchmarks, and surprisingly, sometimes perform worse than no adaptation at all. We also propose NOTELA, a new simple method that outperforms existing methods on these shifts while exhibiting strong performance on a range of vision datasets. Overall, we conclude that evaluating SFDA methods (only) on the commonly-used datasets and distribution shifts leaves us with a myopic view of their relative performance and generalizability. To live up to their promise, SFDA methods need to be tested on a wider range of distribution shifts, and we advocate for considering naturally-occurring ones that can benefit high-impact applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Distribution shifts in bioacoustics&lt;/h2>; &lt;p>; Naturally-occurring distribution shifts are ubiquitous in bioacoustics. The largest labeled dataset for bird songs is &lt;a href=&quot;https://www.researchgate.net/publication/280978332_The_Xeno-canto_collection_and_its_relation_to_sound_recognition_and_classification&quot;>;Xeno-Canto&lt;/a>; (XC), a collection of user-contributed recordings of wild birds from across the world. Recordings in XC are “focal”: they target an individual captured in natural conditions, where the song of the identified bird is at the foreground. For continuous monitoring and tracking purposes, though, practitioners are often more interested in identifying birds in &lt;em>;passive recordings &lt;/em>;(“soundscapes”), obtained through omnidirectional microphones. This is a well-documented problem that &lt;a href=&quot;https://www.researchgate.net/publication/344893963_Overview_of_BirdCLEF_2020_Bird_Sound_Recognition_in_Complex_Acoustic_Environments&quot;>;recent&lt;/a>; &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1574954121000273&quot;>;work&lt;/a>; shows is very challenging. Inspired by this realistic application, we study SFDA in bioacoustics using a bird species classifier that was pre-trained on XC as the source model, and several “soundscapes” coming from different geographical locations — &lt;a href=&quot;https://doi.org/10.5281/zenodo.7050013&quot;>;Sierra Nevada&lt;/a>; (S. Nevada); &lt;a href=&quot;https://doi.org/10.1002/ecy.3329&quot;>;Powdermill&lt;/a>; Nature Reserve, Pennsylvania, USA; &lt;a href=&quot;https://doi.org/10.5281/zenodo.7078498&quot;>;Hawai&#39;i&lt;/a>;; Caples Watershed, California, USA; &lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;Sapsucker Woods&lt;/a>;, New York, USA (SSW); and &lt;a href=&quot;https://www.google.com/url?q=https://zenodo.org/record/7525349%23.ZB8z_-xudhE&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1688498539746392&amp;amp;usg=AOvVaw07CsIKIE-dcNyMKFT-n_JT&quot;>;Colombia&lt;/a>; —&amp;nbsp;as our target domains. &lt;/p>; &lt;p>; This shift from the focalized to the passive domain is substantial: the recordings in the latter often feature much lower signal-to-noise ratio, several birds vocalizing at once, and significant distractors and environmental noise, like rain or wind. In addition, different soundscapes originate from different geographical locations, inducing extreme label shifts since a very small portion of the species in XC will appear in a given location. Moreover, as is common in real-world data, both the source and target domains are significantly class imbalanced, because some species are significantly more common than others. In addition, we consider a &lt;em>;multi-label &lt;/em>;classification problem since there may be several birds identified within each recording, a significant departure from the standard single-label image classification scenario where SFDA is typically studied. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeGafX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQOzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s1378/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;1378&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeGafX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQOzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the &quot;focal → soundscapes&quot; shift. In the focalized domain, recordings are typically composed of a single bird vocalization in the foreground, captured with high signal-to-noise ratio (SNR), though there may be other birds vocalizing in the background.&lt;strong>; &lt;/strong>;On the other hand, soundscapes contain recordings from omnidirectional microphones and can be composed of multiple birds vocalizing simultaneously, as well as environmental noises from insects, rain, cars, planes, etc.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Audio files&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;Focal domain&lt;em>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audio controls=&quot;controls&quot; src=&quot;https://storage.googleapis.com/chirp-public-bucket/notela-blog-post/XC417991%20-%20Yellow-throated%20Vireo%20-%20Vireo%20flavifrons.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;Soundscape domain&lt;em>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audio controls=&quot;controls&quot; src=&quot;https://storage.googleapis.com/chirp-public-bucket/notela-blog-post/yetvir-soundscape.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/em>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Spectogram images&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR4VQQSIFLm6u1t0NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/s936/left.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR4VQQSIFLm6u1t0NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/w640-h546/left.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iyjBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTCmlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/s936/right.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iyjBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTCmlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/w640-h546/right.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the distribution shift from the focal domain (&lt;b>;left&lt;/b>;) to the soundscape domain (&lt;b>;right&lt;/b>;), in terms of the audio files (&lt;b>;top&lt;/b>;) and spectrogram images (&lt;b>;bottom&lt;/b>;) of a representative recording from each dataset. Note that in the second audio clip, the bird song is very faint; a common property in soundscape recordings where bird calls aren&#39;t at the “foreground”. Credits: &lt;b>;Left:&lt;/b>; XC &lt;a href=&quot;https://xeno-canto.org/417991&quot;>;recording&lt;/a>; by Sue Riffe (&lt;a href=&quot;https://creativecommons.org/licenses/by-nc-sa/4.0/&quot;>;CC-BY-NC license&lt;/a>;). &lt;b>;Right:&lt;/b>; Excerpt from a recording made available by Kahl, Charif, &amp;amp; Klinck. (2022) &quot;A collection of fully-annotated soundscape recordings from the Northeastern United States&quot; [&lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;link&lt;/a>;] from the SSW soundscape dataset (&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;CC-BY license&lt;/a>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;State-of-the-art SFDA models perform poorly on bioacoustics shifts&lt;/h2>; &lt;p>; As a starting point, we benchmark six state-of-the-art SFDA methods on our bioacoustics benchmark, and compare them to the &lt;em>;non-adapted&lt;/em>; baseline (the source model). Our findings are surprising: without exception, existing methods are unable to consistently outperform the source model on all target domains. In fact, they often underperform it significantly. &lt;/p>; &lt;p>; As an example, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>;, a recent method, aims to make models produce confident predictions for each example by reducing the uncertainty of the model&#39;s output probabilities. While Tent performs well in various tasks, it doesn&#39;t work effectively for our bioacoustics task. In the single-label scenario, minimizing entropy forces the model to choose a single class for each example confidently. However, in our multi-label scenario, there&#39;s no such constraint that any class should be selected as being present. Combined with significant distribution shifts, this can cause the model to collapse, leading to zero probabilities for all classes. Other benchmarked methods like &lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;, &lt;a href=&quot;https://openreview.net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;DUST&lt;/a>; and &lt;a href=&quot;https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;Pseudo-Labelling&lt;/a>;, which are strong baselines for standard SFDA benchmarks, also struggle with this bioacoustics task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj38druynuW9zh26no15rOJCj1aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s1434/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;730&quot; data-original-width=&quot;1434&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj38druynuW9zh26no15rOJCj1aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Evolution of the test &lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Mean_average_precision&quot;>;mean average precision&lt;/a>; (mAP), a standard metric for multilabel classification, throughout the adaptation procedure on the six soundscape datasets. We benchmark our proposed NOTELA and Dropout Student (see below), as well as &lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;, &lt;a href=&quot;https://openreview.net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;DUST&lt;/a>; and &lt;a href=&quot;https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;Pseudo-Labelling&lt;/a>;. Aside from NOTELA, all other methods fail to consistently improve the source model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Introducing NOisy student TEacher with Laplacian Adjustment (NOTELA)&lt;/h2>; &lt;p>; Nonetheless, a surprisingly positive result stands out: the less celebrated &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;>;Noisy Student&lt;/a>; principle appears promising. This unsupervised approach encourages the model to reconstruct its own predictions on some target dataset, but under the application of random noise. While noise may be introduced through various channels, we strive for simplicity and use &lt;a href=&quot;https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9&quot;>;model dropout&lt;/a>; as the only noise source: we therefore refer to this approach as&lt;em>; Dropout Student (DS)&lt;/em>;. In a nutshell, it encourages the model to limit the influence of individual neurons (or filters) when making predictions on a specific target dataset. &lt;/p>; &lt;p>; DS, while effective, faces a model collapse issue on various target domains. We hypothesize this happens because the source model initially lacks confidence in those target domains. We propose improving DS stability by using the feature space directly as an auxiliary source of truth. NOTELA does this by encouraging similar pseudo-labels for nearby points in the feature space, inspired by &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&#39;s method&lt;/a>; and Laplacian &lt;a href=&quot;https://www.researchgate.net/publication/220319905_Manifold_Regularization_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples&quot;>;regularization&lt;/a>;. This simple approach is visualized below, and consistently and significantly outperforms the source model in both audio and visual tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4VZgRaN-w1jovOwJ4gEXoupksTEpvb5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s1870/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;124&quot; data-original-width=&quot;1870&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4VZgRaN-w1jovOwJ4gEXoupksTEpvb5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9QnJb5JxwLFwbmJJN_3jmlGB9-CTug92969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s1080/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9QnJb5JxwLFwbmJJN_3jmlGB9-CTug92969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;NOTELA in action. The audio recordings are forwarded through the full model to obtain a first set of predictions, which are then refined through Laplacian regularization, a form of post-processing based on clustering nearby points. Finally, the refined predictions are used as targets for the &lt;em>;noisy model &lt;/em>;to reconstruct.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The standard artificial image classification benchmarks have inadvertently limited our understanding of the true generalizability and robustness of SFDA methods. We advocate for broadening the scope and adopt a new assessment framework that incorporates naturally-occurring distribution shifts from bioacoustics. We also hope that NOTELA serves as a robust baseline to facilitate research in that direction. NOTELA&#39;s strong performance perhaps points to two factors that can lead to developing more generalizable models: first, developing methods with an eye towards harder problems and second, favoring simple modeling principles. However, there is still future work to be done to pinpoint and comprehend existing methods&#39; failure modes on harder problems. We believe that our research represents a significant step in this direction, serving as a foundation for designing SFDA methods with greater generalizability. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;One of the authors of this post, Eleni Triantafillou, is now at Google DeepMind. We are posting this blog post on behalf of the authors of the NOTELA paper: Malik Boudiaf, Tom Denton, Bart van Merriënboer, Vincent Dumoulin*, Eleni Triantafillou* (where * denotes equal contribution). We thank our co-authors for the hard work on this paper and the rest of the Perch team for their support and feedback.&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;Note that in this audio clip, the bird song is very faint; a common property in soundscape recordings where bird calls aren&#39;t at the “foreground”.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5980448298988153366/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/in-search-of-generalizable-method-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/in-search-of-generalizable-method-for.html&quot; rel=&quot;alternate&quot; title=&quot;In search of a generalizable method for source-free domain adaptation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4epC-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgyM4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/s72-c/notela.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2519516457542613363&lt;/id>;&lt;published>;2023-07-23T14:13:00.002-07:00&lt;/published>;&lt;updated>;2023-08-07T10:08:27.713-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ICML 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Cat Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4nvXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s320/Google-ICML-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Groups across Google actively pursue research in the field of machine learning (ML), ranging from theory and application. We build ML systems to solve deep scientific and engineering challenges in areas of language, music, visual processing, algorithm development, and more. We aim to build a more collaborative ecosystem with the broader ML research community through open-sourcing tools and datasets, publishing our work, and actively participating in conferences. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Google is proud to be a &lt;a href=&quot;https://icml.cc/virtual/2023/sponsor_list&quot;>;Diamond Sponsor&lt;/a>; of the 40th &lt;a href=&quot;https://icml.cc/virtual/2023/index.html&quot;>;International Conference on Machine Learning&lt;/a>; (ICML 2023), a premier annual conference, which is being held this week in Honolulu, Hawaii. As a leader in ML research, Google has a strong presence at this year&#39;s conference with over 120 accepted papers and active involvement in a number of workshops and tutorials. Google is also proud to be a Platinum Sponsor for both the &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; and &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;Women in Machine Learning&lt;/a>; workshops. We look forward to sharing some of our extensive ML research and expanding our partnership with the broader ML research community. &lt;/p>; &lt;p>; Registered for ICML 2023? We hope you&#39;ll visit the Google booth to learn more about the exciting work, creativity, and fun that goes into solving a portion of the field&#39;s most interesting challenges.访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter 帐户，了解 Google 展位活动（例如演示和问答环节）。 See &lt;a href=&quot;http://www.deepmind.com/blog/google-deepmind-research-at-icml-2023&quot;>;Google DeepMind&#39;s blog&lt;/a>; to learn about their technical participation at ICML 2023. &lt;/p>; &lt;p>; Take a look below to learn more about the Google research being presented at ICML 2023 (Google affiliations in &lt;strong>;bold&lt;/strong>;).&lt;/p>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Board Members include: &lt;strong>;&lt;em>;Corinna Cortes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>; &lt;br>; Tutorial Chairs include: &lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Lhyy8H75KA&quot;>;Scaling Vision Transformers to 22 Billion Parameters&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Josip Djolonga&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Basil Mustafa&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Piotr Padlewski&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Heek&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Justin Gilmer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andreas Steiner&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Robert Geirhos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ibrahim Alabdulmohsin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lucas Beyer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Tschannen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiao Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Carlos Riquelme&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthias Minderer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joan Puigcerver,&lt;/em>; &lt;em>;Utku Evci&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Manoj Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gamaleldin F. Elsayed&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fisher Yu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Avital Oliver&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fantine Huot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mark Patrick Collier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alexey Gritsenko&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vighnesh Birodkar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Cristina Vasconcelos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Mensink&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alexander Kolesnikov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Filip Pavetić&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dustin Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mario Lučić&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiaohua Zhai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel Keysers&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremiah Harmsen&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Neil Houlsby&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=C9NEblP8vS&quot;>;Fast Inference from Transformers via Speculative Decoding&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yaniv Leviathan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matan Kalman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yossi Matias&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=bUFUaawOTk&quot;>;Best of Both Worlds Policy Optimization&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;, &lt;em>;Chen-Yu Wei&lt;/em>;, &lt;strong>;&lt;em>;Julian Zimmert&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=9PJ2V6qvQL&quot;>;Inflow, Outflow, and Reciprocity in Machine Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Walid Krichene&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;Transformers Learn In-Context by Gradient Descent&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Eyvind Niklasson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ettore Randazzo&lt;/em>;&lt;/strong>;, &lt;em>;João Sacramento&lt;/em>;,&lt;strong>; &lt;em>;Alexander Mordvintsev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andrey Zhmoginov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Max Vladymyrov&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=EfhmBBrXY2&quot;>;Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Luke Vilnis&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;, &lt;em>;Patrick Murray*&lt;/em>;, &lt;em>;Alexandre Passos*&lt;/em>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ayBKRjGDEI&quot;>;Differentially Private Hierarchical Clustering with Provable Approximation Guarantees&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/differentially-private-clustering-for.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Jacob Imola&lt;/em>;*,&lt;strong>; &lt;em>;Alessandro Epasto&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mohammad Mahdian&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vincent Cohen-Addad&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ZVxT2ToHR5&quot;>;Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;H. Brendan McMahan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Keith Rush&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=1UaGAhLAsL&quot;>;Random Classification Noise Does Not Defeat All Convex Potential Boosters Irrespective of Model Choice&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Richard Nock&lt;/em>;&lt;/strong>;, &lt;em>;Robert Williamson&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=qw8zAw6mzJ&quot;>;Simplex Random Features&lt;/a>; &lt;br>; &lt;em>;Isaac Reid&lt;/em>;,&lt;strong>; &lt;em>;Krzysztof Choromanski&lt;/em>;&lt;/strong>;, &lt;em>;Valerii Likhosherstov&lt;/em>;, &lt;em>;Adrian Weller&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=bF1LVbP493&quot;>;Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mandar Joshi&lt;/em>;&lt;/strong>;, &lt;em>;Iulia Turc&lt;/em>;,&lt;strong>; &lt;em>;Hexiang Hu&lt;/em>;&lt;/strong>;, &lt;em>;Fangyu Liu&lt;/em>;,&lt;strong>; &lt;em>;Julian Eisenschlos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Shaw&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Wei Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kristina Toutanova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=eIQIcUKs0T&quot;>;Mu&lt;sup>;2&lt;/sup>;SLAM: Multitask, Multilingual Speech and Language Models&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yong Cheng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yu Zhang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Melvin Johnson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wolfgang Macherey&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ankur Bapna&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=5h42xM0pwn&quot;>;Robust Budget Pacing with a Single Sample&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Santiago Balseiro&lt;/em>;&lt;/strong>;, &lt;em>;Rachitesh Kumar&lt;/em>;*,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Balasubramanian Sivan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Di Wang&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=0bR5JuxaoN&amp;amp;name=pdf&quot;>;A Statistical Perspective on Retrieval-Based Models&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Soumya Basu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;, &lt;em>;Manzil Zaheer&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=XjTcC4EA4P&quot;>;Approximately Optimal Core Shapes for Tensor Decompositions&lt;/a>; &lt;br>; &lt;em>;Mehrdad Ghadiri&lt;/em>;,&lt;strong>; &lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Fu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=rWGp9FbS0Q&amp;amp;name=pdf&quot;>;Efficient List-Decodable Regression Using Batches&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Abhimanyu Das&lt;/em>;&lt;/strong>;, &lt;em>;Ayush Jain&lt;/em>;*,&lt;strong>; &lt;em>;Weihao Kong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rajat Sen&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=SpFIO5Mdso&amp;amp;name=pdf&quot;>;Efficient Training of Language Models Using Few-Shot Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Sashank J. Reddi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sobhan Miryoosefi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Stefani Karp&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shankar Krishnan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Seungyeon Kim&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sanjiv Kumar&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=Bj76bauv1Q&amp;amp;name=pdf&quot;>;Fully Dynamic Submodular Maximization Over Matroids&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;, &lt;em>;Federico Fusco&lt;/em>;, &lt;strong>;&lt;em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Morteza Zadimoghaddam&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=VlEAJkmlMs&amp;amp;name=pdf&quot;>;GFlowNet-EM for Learning Compositional Latent Variable Models&lt;/a>; &lt;br>; &lt;em>;Edward J Hu&lt;/em>;, &lt;em>;Nikolay Malkin&lt;/em>;, &lt;em>;Moksh Jain&lt;/em>;, &lt;strong>;&lt;em>;Katie Everett&lt;/em>;&lt;/strong>;, &lt;em>;Alexandros Graikos&lt;/em>;, &lt;em>;Yoshua Bengio&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rB0VaD44FZ&quot;>;Improved Online Learning Algorithms for CTR Prediction in Ad Auctions&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Zhe Feng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>;, &lt;em>;Zixin Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sfdKdeczaw&amp;amp;name=pdf&quot;>;Large Language Models Struggle to Learn Long-Tail Knowledge&lt;/a>; &lt;br>; &lt;em>;Nikhil Kandpal&lt;/em>;, &lt;em>;Haikang Deng&lt;/em>;,&lt;strong>; &lt;em>;Adam Roberts&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Eric Wallace&lt;/em>;&lt;/strong>;, &lt;em>;Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=UdiUd99I81&quot;>;Multi-channel Autobidding with Budget and ROI Constraints&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yuan Deng&lt;/em>;&lt;/strong>;, &lt;em>;Negin Golrezaei&lt;/em>;, &lt;em>;Patrick Jaillet&lt;/em>;, &lt;em>;Jason Cheuk Nam Liang&lt;/em>;, &lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZMvv6laV5b&amp;amp;name=pdf&quot;>;Multi-layer Neural Networks as Trainable Ladders of Hilbert Spaces&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Zhengdao Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfkSyUJyqg&quot;>;On User-Level Private Convex Optimization&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Badih Ghazi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pritish Kamath&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ravi Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghu Meka&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pasin Manurangsi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Chiyuan Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zAgouWgI7b&amp;amp;name=pdf&quot;>;PAC Generalization via Invariant Representations&lt;/a>; &lt;br>; &lt;em>;Advait U Parulekar&lt;/em>;,&lt;strong>; &lt;em>;Karthikeyan Shanmugam&lt;/em>;&lt;/strong>;, &lt;em>;Sanjay Shakkottai&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0rZvMIfECW&amp;amp;name=pdf&quot;>;Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice&lt;/a>; &lt;br>; &lt;em>;Toshinori Kitamura&lt;/em>;, &lt;em>;Tadashi Kozuno&lt;/em>;, &lt;em>;Yunhao Tang&lt;/em>;, &lt;strong>;&lt;em>;Nino Vieillard&lt;/em>;&lt;/strong>;, &lt;em>;Michal Valko&lt;/em>;, &lt;em>;Wenhao Yang&lt;/em>;,&lt;strong>; &lt;em>;Jincheng Mei&lt;/em>;&lt;/strong>;, &lt;em>;Pierre Menard&lt;/em>;, &lt;em>;Mohammad Gheshlaghi Azar&lt;/em>;, &lt;em>;Remi Munos&lt;/em>;,&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;,&lt;em>;Csaba Szepesvari&lt;/em>;, &lt;em>;Wataru Kumagai&lt;/em>;, &lt;em>;Yutaka Matsuo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mrykt39VUw&amp;amp;name=pdf&quot;>;Speeding Up Bellman Ford via Minimum Violation Permutations&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;, &lt;em>;Ola Svensson&lt;/em>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LxodbQa62n&amp;amp;name=pdf&quot;>;Statistical Indistinguishability of Learning Algorithms&lt;/a>; &lt;br>; &lt;em>;Alkis Kalavasis&lt;/em>;,&lt;strong>; &lt;em>;Amin Karbasi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shay Moran&lt;/em>;&lt;/strong>;, &lt;em>;Grigoris Velegkas&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=G5vKSJVhJL&quot;>;Test-Time Adaptation with Slot-Centric Models&lt;/a>; &lt;br>; &lt;em>;Mihir Prabhudesai&lt;/em>;, &lt;em>;Anirudh Goyal&lt;/em>;,&lt;strong>; &lt;em>;Sujoy Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Gaurav Aggarwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;, &lt;em>;Deepak Pathak&lt;/em>;, &lt;em>;Katerina Fragkiadaki>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=2WEMW6rGgG&quot;>;Algorithms for Bounding Contribution for Histogram Estimation Under User-Level Privacy&lt;/a>; &lt;br>; &lt;em>;Yuhan Liu&lt;/em>;*, &lt;strong>;&lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Wennan Zhu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Marco Gruteser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=SgeIqUvo4w&amp;amp;name=pdf&quot;>;Bandit Online Linear Optimization with Hints and Queries&lt;/a>; &lt;br>; &lt;em>;Aditya Bhaskara&lt;/em>;, &lt;em>;Ashok Cutkosky&lt;/em>;,&lt;strong>; &lt;em>;Ravi Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Manish Purohit&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=wagsJnR5GO&amp;amp;name=pdf&quot;>;CLUTR: Curriculum Learning via Unsupervised Task Representation Learning&lt;/a>; &lt;br>; &lt;em>;Abdus Salam Azad&lt;/em>;,&lt;strong>; &lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;, &lt;em>;Jasper Emhoff&lt;/em>;, &lt;em>;Nathaniel Alexis&lt;/em>;,&lt;strong>; &lt;em>;Aleksandra Faust&lt;/em>;&lt;/strong>;, &lt;em>;Pieter Abbeel&lt;/em>;, &lt;em>;Ion Stoica&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=R3WrLjtzG8&amp;amp;name=pdf&quot;>;CSP: Self-Supervised Contrastive Spatial Pre-training for Geospatial-Visual Representations&lt;/a>; &lt;br>; &lt;em>;Gengchen Mai&lt;/em>;, &lt;strong>;&lt;em>;Ni Lao&lt;/em>;&lt;/strong>;, &lt;em>;Yutong He&lt;/em>;, &lt;em>;Jiaming Song&lt;/em>;, &lt;em>;Stefano Ermon&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=vd5JYAml0A&amp;amp;name=pdf&quot;>;Ewald-Based Long-Range Message Passing for Molecular Graphs&lt;/a>; &lt;br>; &lt;em>;Arthur Kosmala&lt;/em>;,&lt;strong>; &lt;em>;Johannes Gasteiger&lt;/em>;&lt;/strong>;, &lt;em>;Nicholas Gao&lt;/em>;, &lt;em>;Stephan Günnemann&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Iey50XHA3g&amp;amp;name=pdf&quot;>;Fast (1+ε)-Approximation Algorithms for Binary Matrix Factorization&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;, &lt;em>;Maximilian Vötsch&lt;/em>;, &lt;strong>;&lt;em>;David Woodruff&lt;/em>;&lt;/strong>;, &lt;em>;Samson Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=b9opfVNw6O&amp;amp;name=pdf&quot;>;Federated Linear Contextual Bandits with User-Level Differential Privacy&lt;/a>; &lt;br>; &lt;em>;Ruiquan Huang&lt;/em>;, &lt;em>;Huanyu Zhang&lt;/em>;, &lt;em>;Luca Melis&lt;/em>;, &lt;em>;Milan Shen&lt;/em>;,&lt;strong>; &lt;em>;Meisam Hejazinia&lt;/em>;&lt;/strong>;, &lt;em>;Jing Yang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=m21SgZnBWZ&amp;amp;name=pdf&quot;>;Investigating the Role of Model-Based Learning in Exploration and Transfer&lt;/a>; &lt;br>; &lt;em>;Jacob C Walker&lt;/em>;, &lt;em>;Eszter Vértes&lt;/em>;, &lt;em>;Yazhe Li&lt;/em>;,&lt;strong>; &lt;em>;Gabriel Dulac-Arnold&lt;/em>;&lt;/strong>;, &lt;em>;Ankesh Anand&lt;/em>;, &lt;em>;Theophane Weber&lt;/em>;,&lt;em>; Jessica B Hamrick&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=K1sJiHvy02&quot;>;Label Differential Privacy and Private Training Data Release&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Robert Busa-Fekete&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andres Munoz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Umar Syed&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Q4QFG5Fe4O&amp;amp;name=pdf&quot;>;Lifelong Language Pretraining with Distribution-Specialized Experts&lt;/a>; &lt;br>; &lt;em>;Wuyang Chen&lt;/em>;*, &lt;strong>;&lt;em>;Yanqi Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Nan Du&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yanping Huang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;James Laudon&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Zhifeng Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Claire Cui&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=06djx2x2Rf&amp;amp;name=pdf&quot;>;Multi-User Reinforcement Learning with Low Rank Rewards&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Dheeraj Mysore Nagaraj&lt;/em>;&lt;/strong>;, &lt;em>;Suhas S Kowshik&lt;/em>;, &lt;strong>;&lt;em>;Naman Agarwal&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Praneeth Netrapalli&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Prateek Jain&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DwOUndjwiV&amp;amp;name=pdf&quot;>;Multi-View Masked World Models for Visual Robotic Manipulation&lt;/a>; &lt;br>; &lt;em>;Younggyo Seo&lt;/em>;, &lt;em>;Junsu Kim&lt;/em>;, &lt;em>;Stephen James&lt;/em>;,&lt;strong>; &lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;, &lt;em>;Jinwoo Shin&lt;/em>;, &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VTpHpqM3Cf&amp;amp;name=pdf&quot;>;PaLM-E: An Embodied Multimodal Language Model&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Danny Driess&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Fei Xia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Corey Lynch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Ichter&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;Ayzaan Wahid&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan Tompson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Quan Vuong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tianhe Yu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wenlong Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yevgen Chebotar&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pierre Sermanet&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Duckworth&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vincent Vanhoucke&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Karol Hausman&lt;/em>;&lt;/strong>;, &lt;em>;Marc Toussaint&lt;/em>;,&lt;strong>; &lt;em>;Klaus Greff&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Igor Mordatch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pete Florence&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=y8qAZhWbNs&quot;>;Private Federated Learning with Autotuned Compression&lt;/a>; &lt;br>; &lt;em>;Enayat Ullah&lt;/em>;*, &lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=7WdMBofQFx&amp;amp;name=pdf&quot;>;Refined Regret for Adversarial MDPs with Linear Function Approximation&lt;/a>; &lt;br>; &lt;em>;Yan Dai&lt;/em>;, &lt;em>;Haipeng Luo&lt;/em>;, &lt;em>;Chen-Yu Wei&lt;/em>;, &lt;strong>;&lt;em>;Julian Zimmert&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ccwSdYv1GI&amp;amp;name=pdf&quot;>;Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory&lt;/a>; &lt;br>; &lt;em>;Justin Cui&lt;/em>;,&lt;em>; Ruoche Wan&lt;/em>;, &lt;strong>;&lt;em>;Si Si&lt;/em>;&lt;/strong>;, &lt;em>;Cho-Jui Hsieh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=X7jMTrwuCz&amp;amp;name=pdf&quot;>;SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance&lt;/a>; &lt;br>; &lt;em>;Amit Attia&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6EVUnWGBMU&quot;>;The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation&lt;/a>; &lt;br>; &lt;em>;Mark Rowland&lt;/em>;, &lt;em>;Yunhao Tang&lt;/em>;, &lt;em>;Clare Lyle&lt;/em>;, &lt;em>;Rémi Munos&lt;/em>;, &lt;strong>;&lt;em>;Marc G. Bellemare&lt;/em>;&lt;/strong>;, &lt;em>;Will Dabney&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=iMHNLJRSVz&amp;amp;name=pdf&quot;>;Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features&lt;/a>; &lt;br>; &lt;em>;Chieh Hubert Lin&lt;/em>;, &lt;em>;Hung-Yu Tseng&lt;/em>;, &lt;em>;Hsin-Ying Lee&lt;/em>;, &lt;em>;Maneesh Kumar Singh&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4UStsbnfVT&quot;>;User-Level Private Stochastic Convex Optimization with Optimal Rates&lt;/a>; &lt;br>; &lt;em>;Raef Bassily&lt;/em>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6MU5xdrO7t&amp;amp;name=pdf&quot;>;A Simple Zero-Shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models&lt;/a>; &lt;br>; &lt;em>;James Urquhart Allingham&lt;/em>;*, &lt;strong>;&lt;em>;Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael W Dusenberry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiuye Gu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yin Cui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dustin Tran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremiah Zhe Liu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mXv2aVqUGG&amp;amp;name=pdf&quot;>;Can Large Language Models Reason About Program Invariants?&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kexin Pei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Bieber&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kensen Shi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Charles Sutton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pengcheng Yin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=pWeQdceMHL&amp;amp;name=pdf&quot;>;Concurrent Shuffle Differential Privacy Under Continual Observation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jay Tenenbaum&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Haim Kaplan&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Uri Stemmer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Xqedp0Iu1S&amp;amp;name=pdf&quot;>;Constant Matters: Fine-Grained Error Bound on Differentially Private Continual Observation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Hendrik Fichtenberger&lt;/em>;&lt;/strong>;, &lt;em>;Monika Henzinger&lt;/em>;, &lt;em>;Jalaj Upadhyay&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=NfCA622s8O&amp;amp;name=pdf&quot;>;Cross-Entropy Loss Functions: Theoretical Analysis and Applications&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;, &lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;, &lt;em>;Yutao Zhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=5UZYtGEPTt&amp;amp;name=pdf&quot;>;Efficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation&lt;/a>; &lt;br>; &lt;em>;Orin Levy&lt;/em>;, &lt;strong>;&lt;em>;Alon Cohen&lt;/em>;&lt;/strong>;,&lt;em>; Asaf Cassel&lt;/em>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KrsaROSs8b&amp;amp;name=pdf&quot;>;Fairness in Streaming Submodular Maximization Over a Matroid Constraint&lt;/a>; &lt;br>; &lt;em>;Marwa El Halabi&lt;/em>;, &lt;em>;Federico Fusco&lt;/em>;, &lt;strong>;&lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jakab Tardos&lt;/em>;&lt;/strong>;, &lt;em>;Jakub Tarnawski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZX4uS605XV&amp;amp;name=pdf&quot;>;The Flan Collection: Designing Data and Methods for Effective Instruction Tuning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Shayne Longpre&lt;/em>;, &lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tu Vu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Albert Webson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hyung Won Chung&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Quoc V Le&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Barret Zoph&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jason Wei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adam Roberts&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=rzN05i4GOE&amp;amp;name=pdf&quot;>;Graph Reinforcement Learning for Network Control via Bi-level Optimization&lt;/a>; &lt;br>; &lt;em>;Daniele Gammelli&lt;/em>;,&lt;strong>; &lt;em>;James Harrison&lt;/em>;&lt;/strong>;, &lt;em>;Kaidi Yang&lt;/em>;, &lt;em>;Marco Pavone&lt;/em>;, &lt;em>;Filipe Rodrigues&lt;/em>;, &lt;em>;Francisco C. Pereira&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Fgn23Fsmtv&amp;amp;name=pdf&quot;>;Learning-Augmented Private Algorithms for Multiple Quantile Release&lt;/a>; &lt;br>; &lt;em>;Mikhail Khodak&lt;/em>;*, &lt;strong>;&lt;em>;Kareem Amin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Travis Dick&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=uSHBQdWmuC&amp;amp;name=pdf&quot;>;LegendreTron: Uprising Proper Multiclass Loss Learning&lt;/a>; &lt;br>; &lt;em>;Kevin H Lam&lt;/em>;, &lt;strong>;&lt;em>;Christian Walder&lt;/em>;&lt;/strong>;, &lt;em>;Spiridon Penev&lt;/em>;,&lt;strong>; &lt;em>;Richard Nock&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VnGIZsmxDG&amp;amp;name=pdf&quot;>;Measuring the Impact of Programming Language Distribution&lt;/a>; &lt;br>; &lt;em>;Gabriel Orlanski&lt;/em>;*, &lt;strong>;&lt;em>;Kefan Xiao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeffrey Hui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Howland&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Malmaud&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jacob Austin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rishabh Singh&lt;/em>;&lt;/strong>;, &lt;em>;Michele Catasta&lt;/em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=f69OtekDi4&quot;>;Multi-task Differential Privacy Under Distribution Skew&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Walid Krichene&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shuang Song&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Li Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=hi9UssZdHR&quot;>;Muse: Text-to-Image Generation via Masked Generative Transformers&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Huiwen Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Han Zhang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jarred Barber&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;AJ Maschinot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;José Lezama&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lu Jiang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kevin Murphy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;William T. Freeman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Rubinstein&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuanzhen Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dilip Krishnan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;On the Convergence of Federated Averaging with Cyclic Client Participation&lt;/a>; &lt;br>; &lt;em>;Yae Jee Cho&lt;/em>;, &lt;em>;Pranay Sharma&lt;/em>;, &lt;em>;Gauri Joshi&lt;/em>;, &lt;strong>;&lt;em>;Zheng Xu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tong Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=GimajxXNc0&amp;amp;name=pdf&quot;>;Optimal Stochastic Non-smooth Non-convex Optimization Through Online-to-Non-convex Conversion&lt;/a>; &lt;br>; &lt;em>;Ashok Cutkosky&lt;/em>;, &lt;strong>;&lt;em>;Harsh Mehta&lt;/em>;&lt;/strong>;, &lt;em>;Francesco Orabona&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=4SHQv4cp3I&amp;amp;name=pdf&quot;>;Out-of-Domain Robustness via Targeted Augmentations&lt;/a>; &lt;br>; &lt;em>;Irena Gao&lt;/em>;, &lt;em>;Shiori Sagawa&lt;/em>;,&lt;strong>; &lt;em>;Pang Wei Koh&lt;/em>;&lt;/strong>;, &lt;em>;Tatsunori Hashimoto&lt;/em>;,&lt;em>; Percy Liang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=b6Hxt4Jw10&quot;>;Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models&lt;/a>; &lt;br>; &lt;em>;Jamil Arbas&lt;/em>;, &lt;em>;Hassan Ashtiani&lt;/em>;,&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=nlUAvrMbUZ&amp;amp;name=pdf&quot;>;Pre-computed Memory or On-the-Fly Encoding? A Hybrid Approach to Retrieval Augmentation Makes the Most of Your Compute&lt;/a>; &lt;br>; &lt;em>;Michiel de Jong&lt;/em>;, &lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nicholas FitzGerald&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fei Sha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;William W. Cohen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1FldU7JzGh&amp;amp;name=pdf&quot;>;Scalable Adaptive Computation for Iterative Generation&lt;/a>; &lt;br>; &lt;em>;Allan Jabri&lt;/em>;*, &lt;strong>;&lt;em>;David J. Fleet&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ting Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HiKPaeowPB&quot;>;Scaling Spherical CNNs&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Carlos Esteves&lt;/em>;&lt;/strong>;, &lt;em>;Jean-Jacques Slotine&lt;/em>;, &lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0O7b2Y198V&amp;amp;name=pdf&quot;>;STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition&lt;/a>; &lt;br>; &lt;em>;Yucheng Lu&lt;/em>;, &lt;strong>;&lt;em>;Shivani Agrawal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Suvinay Subramanian&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Oleg Rybakov&lt;/em>;&lt;/strong>;, &lt;em>;Christopher De Sa&lt;/em>;, &lt;em>;Amir Yazdanbakhsh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LZt1HIEoAf&amp;amp;name=pdf&quot;>;Stratified Adversarial Robustness with Rejection&lt;/a>; &lt;br>; &lt;em>;Jiefeng Chen&lt;/em>;, &lt;em>;Jayaram Raghuram&lt;/em>;, &lt;em>;Jihye Choi&lt;/em>;,&lt;strong>; &lt;em>;Xi Wu&lt;/em>;&lt;/strong>;, &lt;em>;Yingyu Liang&lt;/em>;, &lt;em>;Somesh Jha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=CnHxxjqkMi&amp;amp;name=pdf&quot;>;When Does Privileged information Explain Away Label Noise?&lt;/a>; &lt;br>; &lt;em>;Guillermo Ortiz-Jimenez&lt;/em>;*,&lt;strong>; &lt;em>;Mark Collier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anant Nawalgaria&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alexander D&#39;Amour&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jesse Berent&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Effrosyni Kokiopoulou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2bGTacOn8v&amp;amp;name=pdf&quot;>;Adaptive Computation with Elastic Input Sequence&lt;/a>; &lt;br>; &lt;em>;Fuzhao Xue&lt;/em>;*, &lt;strong>;&lt;em>;Valerii Likhosherstov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;, &lt;em>;Yang You&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Pbaiy3fRCt&amp;amp;name=pdf&quot;>;Can Neural Network Memorization Be Localized?&lt;/a>; &lt;br>; &lt;em>;Pratyush Maini&lt;/em>;,&lt;strong>; &lt;em>;Michael C. Mozer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>;, &lt;em>;Zachary C. Lipton&lt;/em>;, &lt;em>;J. Zico Kolter&lt;/em>;,&lt;strong>; &lt;em>;Chiyuan Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Ct2N6RWZpQ&amp;amp;name=pdf&quot;>;Controllability-Aware Unsupervised Skill Discovery&lt;/a>; &lt;br>; &lt;em>;Seohong Park&lt;/em>;,&lt;strong>; &lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;, &lt;em>;Youngwoon Lee&lt;/em>;, &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2Mbo7IEtZW&amp;amp;name=pdf&quot;>;Efficient Learning of Mesh-Based Physical Simulation with Bi-Stride Multi-Scale Graph Neural Network&lt;/a>; &lt;br>; &lt;em>;Yadi Cao&lt;/em>;,&lt;strong>; &lt;em>;Menglei Chai&lt;/em>;&lt;/strong>;, &lt;em>;Minchen Li&lt;/em>;, &lt;em>;Chenfanfu Jiang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zN4oRCrlnM&amp;amp;name=pdf&quot;>;Federated Heavy Hitter Recovery Under Linear Sketching&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Adria Gascon&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Peter Kairouz&lt;/strong>;&lt;/em>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;Graph Generative Model for Benchmarking Graph Neural Networks&lt;/a>; &lt;br>; &lt;em>;Minji Yoon&lt;/em>;, &lt;em>;Yue Wu&lt;/em>;, &lt;strong>;&lt;em>;John Palowitch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;em>;Russ Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=IFhGrPAn8f&amp;amp;name=pdf&quot;>;H-Consistency Bounds for Pairwise Misranking Loss Surrogates&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;, &lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;, &lt;em>;Yutao Zhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DF6ypWrepg&amp;amp;name=pdf&quot;>;Improved Regret for Efficient Online Reinforcement Learning with Linear Function Approximation&lt;/a>; &lt;br>; &lt;em>;Uri Sherman&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZXeTCRZJp9&amp;amp;name=pdf&quot;>;Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames&lt;/a>; &lt;br>; &lt;em>;Ondrej Biza&lt;/em>;*,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gamaleldin Fathy Elsayed&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=a35tteW8if&quot;>;Multi-task Off-Policy Learning from Bandit Feedback&lt;/a>; &lt;br>; &lt;em>;Joey Hong&lt;/em>;, &lt;em>;Branislav Kveton&lt;/em>;, &lt;em>;Manzil Zaheer&lt;/em>;, &lt;em>;Sumeet Katariya&lt;/em>;,&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=yv8GUQREda&amp;amp;name=pdf&quot;>;Optimal No-Regret Learning for One-Sided Lipschitz Functions&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Guru Guruganesh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jon Schneider&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Joshua Ruizhi Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=AwxfYvdPZV&amp;amp;name=pdf&quot;>;Policy Mirror Ascent for Efficient and Independent Learning in Mean Field Games&lt;/a>; &lt;br>; &lt;em>;Batuhan Yardim&lt;/em>;, &lt;em>;Semih Cayci&lt;/em>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;, &lt;em>;Niao He&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ILMHlUn4k6&amp;amp;name=pdf&quot;>;Regret Minimization and Convergence to Equilibria in General-Sum Markov Games&lt;/a>; &lt;br>; &lt;em>;Liad Erez&lt;/em>;, &lt;em>;Tal Lancewicki&lt;/em>;, &lt;em>;Uri Sherman&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=skDVsmXjPR&amp;amp;name=pdf&quot;>;Reinforcement Learning Can Be More Efficient with Multiple Rewards&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rdOuTlTUMX&quot;>;Reinforcement Learning with History-Dependent Dynamic Contexts&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Guy Tennenholtz&lt;/em>;&lt;/strong>;, &lt;em>;Nadav Merlis&lt;/em>;, &lt;strong>;&lt;em>;Lior Shani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Martin Mladenov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Craig Boutlier&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sdhcjMzhHN&amp;amp;name=pdf&quot;>;User-Defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems&lt;/a>; &lt;br>; &lt;em>;Marc Anton Finzi&lt;/em>;*,&lt;strong>; &lt;em>;Anudhyan Boral&lt;/em>;&lt;/strong>;, &lt;em>;Andrew Gordon Wilson&lt;/em>;,&lt;strong>; &lt;em>;Fei Sha&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Leonardo Zepeda-Nunez&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LDBIVZCnLl&amp;amp;name=pdf&quot;>;Discrete Key-Value Bottleneck&lt;/a>; &lt;br>; &lt;em>;Frederik Träuble&lt;/em>;, &lt;em>;Anirudh Goyal&lt;/em>;, &lt;em>;Nasim Rahaman&lt;/em>;,&lt;strong>; &lt;em>;Michael Curtis Mozer&lt;/em>;&lt;/strong>;, &lt;em>;Kenji Kawaguchi&lt;/em>;, &lt;em>;Yoshua Bengio&lt;/em>;, &lt;em>;Bernhard Schölkopf&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nVO6YTca8O&quot;>;DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm&lt;/a>; &lt;br>; &lt;em>;Lisang Ding&lt;/em>;, &lt;em>;Kexin Jin&lt;/em>;,&lt;strong>; &lt;em>;Bicheng Ying&lt;/em>;&lt;/strong>;, &lt;em>;Kun Yuan&lt;/em>;, &lt;em>;Wotao Yin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3Ge74dgjjU&amp;amp;name=pdf&quot;>;Exphormer: Sparse Transformers for Graphs&lt;/a>; &lt;br>; &lt;em>;Hamed Shirzad&lt;/em>;, &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Balaji Venkatachalam&lt;/em>;&lt;/strong>;, &lt;em>;Danica J. Sutherland&lt;/em>;,&lt;strong>; &lt;em>;Ali Kemal Sinop&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=dolp65Z6re&amp;amp;name=pdf&quot;>;Fast, Differentiable and Sparse Top-k: A Convex Analysis Perspective&lt;/a>; &lt;br>; &lt;em>;Michael Eli Sander&lt;/em>;*, &lt;strong>;&lt;em>;Joan Puigcerver&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Josip Djolonga&lt;/em>;&lt;/strong>;, &lt;em>;Gabriel Peyré&lt;/em>;,&lt;strong>; &lt;em>;Mathieu Blondel&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=priTMs7n6e&quot;>;Improved Policy Evaluation for Randomized Trials of Algorithmic Resource Allocation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Aditya Mate&lt;/em>;&lt;/strong>;, &lt;em>;Bryan Wilder&lt;/em>;,&lt;strong>; &lt;em>;Aparna Taneja&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Milind Tambe&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Yh9sFZQk7Y&amp;amp;name=pdf&quot;>;In Search for a Generalizable Method for Source Free Domain Adaptation&lt;/a>; &lt;br>; &lt;em>;Malik Boudiaf&lt;/em>;*, &lt;strong>;&lt;em>;Tom Denton&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bart van Merrienboer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vincent Dumoulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Eleni Triantafillou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mSofpvUxCL&amp;amp;name=pdf&quot;>;Learning Rate Schedules in the Presence of Distribution Shift&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adel Javanmard&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pratik Worah&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=slM2r4bRD1&amp;amp;name=pdf&quot;>;Not All Semantics Are Created Equal: Contrastive Self-Supervised Learning with Automatic Temperature Individualization&lt;/a>; &lt;br>; &lt;em>;Zi-Hao Qiu&lt;/em>;, &lt;em>;Quanqi Hu&lt;/em>;, &lt;em>;Zhuoning Yuan&lt;/em>;,&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;Lijun Zhang&lt;/em>;, &lt;em>;Tianbao Yang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=EUQsBO975P&amp;amp;name=pdf&quot;>;On the Relationship Between Explanation and Prediction: A Causal View&lt;/a>; &lt;br>; &lt;em>;Amir-Hossein Karimi&lt;/em>;*, &lt;em>;Krikamol Muandet&lt;/em>;,&lt;strong>; &lt;em>;Simon Kornblith&lt;/em>;&lt;/strong>;, &lt;em>;Bernhard Schölkopf&lt;/em>;,&lt;strong>; &lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=qorOnDor89&amp;amp;name=pdf&quot;>;On the Role of Attention in Prompt-Tuning&lt;/a>; &lt;br>; &lt;em>;Samet Oymak&lt;/em>;,&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;, &lt;em>;Mahdi Soltanolkotabi&lt;/em>;, &lt;em>;Christos Thrampoulidis&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2jvwyTm6Pk&amp;amp;name=pdf&quot;>;PLay: Parametrically Conditioned Layout Generation Using Latent Diffusion&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Chin-Yi Cheng&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yang Li&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=631FTQB0UB&amp;amp;name=pdf&quot;>;The Power of Learned Locally Linear Models for Nonlinear Policy Optimization&lt;/a>; &lt;br>; &lt;em>;Daniel Pfrommer&lt;/em>;, &lt;em>;Max Simchowitz&lt;/em>;, &lt;em>;Tyler Westenbroek&lt;/em>;, &lt;em>;Nikolai Matni&lt;/em>;,&lt;strong>; &lt;em>;Stephen Tu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=BDYIci7bVs&amp;amp;name=pdf&quot;>;Relevant Walk Search for Explaining Graph Neural Networks&lt;/a>; &lt;br>; &lt;em>;Ping Xiong&lt;/em>;, &lt;em>;Thomas Schnake&lt;/em>;, &lt;em>;Michael Gastegger&lt;/em>;, &lt;em>;Grégoire Montavon&lt;/em>;,&lt;strong>; &lt;em>;Klaus Robert Muller&lt;/em>;&lt;/strong>;,&lt;em>;Shinichi Nakajima&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RX70NHEPE0&amp;amp;name=pdf&quot;>;Repository-Level Prompt Generation for Large Language Models of Code&lt;/a>; &lt;br>; &lt;em>;Disha Shrivastava&lt;/em>;,&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel Tarlow&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=r3M5cBtpYq&amp;amp;name=pdf&quot;>;Robust and Private Stochastic Linear Bandits&lt;/a>; &lt;br>; &lt;em>;Vasileios Charisopoulos&lt;/em>;*, &lt;strong>;&lt;em>;Hossein Esfandiari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6l9YG3wHA9&amp;amp;name=pdf&quot;>;Simple Diffusion: End-to-End Diffusion for High Resolution Images&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Emiel Hoogeboom&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Heek&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tim Salimans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=cw6Zb0sEiT&amp;amp;name=pdf&quot;>;Tied-Augment: Controlling Representation Similarity Improves Data Augmentation&lt;/a>; &lt;br>; &lt;em>;Emirhan Kurtulus&lt;/em>;, &lt;strong>;&lt;em>;Zichao Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yann Dauphin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ekin D. Cubuk&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1d3O0b1rbL&amp;amp;name=pdf&quot;>;Why Is Public Pre-Training Necessary for Private Model Training?&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Arun Ganesh&lt;/em>;&lt;/strong>;, &lt;em>;Mahdi Haghifam&lt;/em>;*, &lt;strong>;&lt;em>;Milad Nasr&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Steinke&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Om Thakkar&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Abhradeep Guha Thakurta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lun Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XXC601YWgq&amp;amp;name=pdf&quot;>;A Connection Between One-Step RL and Critic Regularization in Reinforcement Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Benjamin Eysenbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;, &lt;em>;Ruslan Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;Beyond Uniform Lipschitz Condition in Differentially Private Optimization&lt;/a>; &lt;br>; &lt;em>;Rudrajit Das&lt;/em>;*, &lt;strong>;&lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tong Zhang&lt;/em>;&lt;/strong>;, &lt;em>;Sujay Sanghavi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Y5jGkbZ0W3&amp;amp;name=pdf&quot;>;Efficient Graph Field Integrators Meet Point Clouds&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Krzysztof Choromanski&lt;/em>;&lt;/strong>;, &lt;em>;Arijit Sehanobish&lt;/em>;, &lt;em>;Han Lin&lt;/em>;, &lt;em>;Yunfan Zhao&lt;/em>;, &lt;em>;Eli Berger,&lt;/em>; &lt;em>;Tetiana Parshakova&lt;/em>;, &lt;em>;Alvin Pan&lt;/em>;, &lt;em>;David Watkins&lt;/em>;, &lt;em>;Tianyi Zhang&lt;/em>;, &lt;em>;Valerii Likhosherstov&lt;/em>;, &lt;em>;Somnath Basu Roy Chowdhury&lt;/em>;,&lt;strong>; &lt;em>;Avinava Dubey&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Deepali Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tamas Sarlos&lt;/em>;&lt;/strong>;, &lt;em>;Snigdha Chaturvedi&lt;/em>;, &lt;em>;Adrian Weller&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RAeN6s9RZV&amp;amp;name=pdf&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>; &lt;br>; &lt;em>;Riade Benbaki&lt;/em>;, &lt;em>;Wenyu Chen&lt;/em>;, &lt;em>;Xiang Meng&lt;/em>;, &lt;strong>;&lt;em>;Hussein Hazimeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhe Zhao&lt;/em>;&lt;/strong>;, &lt;em>;Rahul Mazumder&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2M7lwN0DTp&amp;amp;name=pdf&quot;>;Jump-Start Reinforcement Learning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Ikechukwu Uchendu&lt;/em>;*, &lt;strong>;&lt;em>;Ted Xiao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yao Lu&lt;/em>;&lt;/strong>;, &lt;em>;Banghua Zhu&lt;/em>;, &lt;em>;Mengyuan Yan&lt;/em>;, &lt;em>;Joséphine Simon&lt;/em>;, &lt;em>;Matthew Bennice&lt;/em>;, &lt;em>;Chuyuan Fu&lt;/em>;, &lt;em>;Cong Ma&lt;/em>;, &lt;em>;Jiantao Jiao&lt;/em>;,&lt;strong>; &lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Karol Hausman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=WPjMrOi1KE&amp;amp;name=pdf&quot;>;Learning in POMDPs is Sample-Efficient with Hindsight Observability&lt;/a>; &lt;br>; &lt;em>;Jonathan Lee&lt;/em>;,&lt;strong>; &lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;, &lt;em>;Tong Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=K0InBsKODr&amp;amp;name=pdf&quot;>;Low-Variance Gradient Estimation in Unrolled Computation Graphs with ES-Single&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Vicol&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Qh0Gbq3lkh&amp;amp;name=pdf&quot;>;Masked Trajectory Models for Prediction, Representation, and Control&lt;/a>; &lt;br>; &lt;em>;Philipp Wu&lt;/em>;, &lt;em>;Arjun Majumdar&lt;/em>;, &lt;em>;Kevin Stone&lt;/em>;, &lt;em>;Yixin Lin&lt;/em>;,&lt;strong>; &lt;em>;Igor Mordatch&lt;/em>;&lt;/strong>;, &lt;em>;Pieter Abbeel&lt;/em>;, &lt;em>;Aravind Rajeswaran&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DnTVBs6zbz&amp;amp;name=pdf&quot;>;Overcoming Simplicity Bias in Deep Networks Using a Feature Sieve&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Rishabh Tiwari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pradeep Shenoy&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KKaTURYcKG&amp;amp;name=pdf&quot;>;Pairwise Ranking Losses of Click-Through Rates Prediction for Welfare Maximization in Ad Auctions&lt;/a>; &lt;br>; &lt;em>;Boxiang Lyu&lt;/em>;,&lt;strong>; &lt;em>;Zhe Feng&lt;/em>;&lt;/strong>;, &lt;em>;Zachary Robertson&lt;/em>;,&lt;strong>; &lt;em>;Sanmi Koyejo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=UTtYSDO1MK&amp;amp;name=pdf&quot;>;Predictive Flows for Faster Ford-Fulkerson&lt;/a>; &lt;br>; &lt;em>;Sami Davies&lt;/em>;, &lt;em>;Benjamin Moseley&lt;/em>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuyan Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=SVCYSBgFIr&amp;amp;name=pdf&quot;>;Scaling Laws for Multilingual Neural Machine Translation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Patrick Fernandes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Behrooz Ghorbani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=msZrQQAlBA&amp;amp;name=pdf&quot;>;Sequential Monte Carlo Learning for Time Series Structure Discovery&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Feras Saad&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Patton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthew Douglas Hoffman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rif A. Saurous&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vikash Mansinghka&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XqyXhjVRxR&amp;amp;name=pdf&quot;>;Stochastic Gradient Succeeds for Bandits&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jincheng Mei&lt;/em>;&lt;/strong>;, &lt;em>;Zixin Zhong&lt;/em>;,&lt;strong>; &lt;em>;Bo Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;, &lt;em>;Csaba Szepesvari&lt;/em>;,&lt;strong>; &lt;em>;Dale Schuurmans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nm4NwFfp7a&quot;>;Subset-Based Instance Optimality in Private Estimation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Travis Dick&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Kulesza&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zvCSNsoyKW&amp;amp;name=pdf&quot;>;The Unreasonable Effectiveness of Few-Shot Learning for Machine Translation&lt;/a>; &lt;br>; &lt;em>;Xavier Garcia&lt;/em>;, &lt;em>;Yamini Bansal&lt;/em>;,&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;George Foster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Maxim Krikun&lt;/em>;&lt;/strong>;, &lt;em>;Melvin Johnson&lt;/em>;, &lt;em>;Orhan Firat&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21552&quot;>;Self-Supervised Learning in Vision: from Research Advances to Best Practices&lt;/a>; &lt;br>; &lt;em>;Xinlei Chen&lt;/em>;, &lt;em>;Ishan Misra&lt;/em>;, &lt;em>;Randall Balestriero&lt;/em>;, &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;, &lt;em>;Christoph Feichtenhofer&lt;/em>;, &lt;em>;Mark Ibrahim&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21560&quot;>;How to DP-fy ML: A Practical Tutorial to Machine Learning with Differential Privacy&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21558&quot;>;Recent Advances in the Generalization Theory of Neural Networks&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Tengyu Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Damian&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EXPO Day workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;Graph Neural Networks in Tensorflow: A Practical Guide&lt;/a>; &lt;br>; Workshop Organizers include: &lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan Halcrow&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google sponsored affinity workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; (LAXAI) &lt;br>; Platinum Sponsor &lt;br>; Keynote Speaker:&lt;em>; &lt;strong>;Monica Ribero&lt;/strong>;&lt;/em>; &lt;br>; Panelist: &lt;strong>;&lt;em>;Yao Qin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;Women in Machine Learning&lt;/a>; (WiML) &lt;br>; Platinum Sponsor &lt;br>; Panelists:&lt;strong>;&lt;em>; Yao Qin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://fl-icml2023.github.io/&quot;>;Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/imlh2023/home?authuser=1&quot;>;Interpretable Machine Learning in Healthcare&lt;/a>; (IMLH) &lt;br>; Organizer: &lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://klr-icml2023.github.io/&quot;>;Knowledge and Logical Reasoning in the Era of Data-Driven Learning&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Beliz Günel&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mfpl-icml-2023&quot;>;The Many Facets of Preference-Based Learning&lt;/a>; (MFPL) &lt;br>; Organizer: &lt;strong>;&lt;em>;Robert Busa-Fekete&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://syns-ml.github.io/2023/&quot;>;The Synergy of Scientific and Machine Learning Modelling&lt;/a>; (SynS &amp;amp; ML) &lt;br>; Speaker: &lt;strong>;&lt;em>;Sercan Arik&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://tomworkshop.github.io/&quot;>;Theory of Mind in Communicating Agents&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Pei Zhou&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/aihci/home&quot;>;Artificial Intelligence &amp;amp; Human Computer Interaction&lt;/a>; &lt;br>; Organizer:&lt;em>; &lt;strong>;Yang Li&lt;/strong>;&lt;/em>;,&lt;strong>; &lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://dmlr.ai/&quot;>;Data-Centric Machine Learning Research&lt;/a>; (DMLR) &lt;br>; Organizer: &lt;strong>;&lt;em>;Alicia Parrish&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Peter Mattson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Isabelle Guyon&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://neuralcompression.github.io/workshop23&quot;>;Neural Compression: from Information Theory to Applications&lt;/a>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Johannes Ballé&lt;/em>;&lt;/strong>; &lt;br>; Panelist: &lt;strong>;&lt;em>;George Toderici&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/teach-icml-23/home&quot;>;Neural Conversational AI Workshop - What&#39;s Left to TEACH (Trustworthy, Enhanced, Adaptable, Capable and Human-centric) Chatbots?&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/scis-workshop-23&quot;>;Spurious Correlations, Invariance and Stability&lt;/a>; (SCIS) &lt;br>; Organizer: &lt;strong>;&lt;em>;Amir Feder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>; &lt;br>; Title: Unsupervised Graph Embedding @ Google (&lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;paper&lt;/a>;, &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;EXPO workshop&lt;/a>;) &lt;br>; Tuesday, July 25th at 10:30 AM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;br>; Title: Federated Learning of Gboard Language Models with Differential Privacy (&lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;paper 1&lt;/a>;, &lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;paper 2&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot;>;blog post&lt;/a>;) &lt;br>; Tuesday, July 25th at 3:30 PM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;br>; Title: Self-supervised scene understanding (&lt;a href=&quot;https://arxiv.org/abs/2302.04973&quot;>;paper 1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2203.11194&quot;>;paper 2&lt;/a>;) &lt;br>; Wednesday, July 26th at 10:30 AM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Max Vladymyrov&lt;/em>;&lt;/strong>; &lt;br>; Title: Transformers learn in-context by gradient descent (&lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;paper&lt;/a>;) &lt;br>; Wednesday, July 26th at 3:30 PM HST &lt;/p>; &lt;/div>; &lt;br>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2519516457542613363/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-icml-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-icml-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICML 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4nvXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s72-c/Google-ICML-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4421751509192561388&lt;/id>;&lt;published>;2023-07-20T09:22:00.002-07:00&lt;/published>;&lt;updated>;2023-07-20T15:31:26.737-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Using societal context knowledge to foster the responsible application of AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Donald Martin, Jr., Technical Program Manager, Head of Societal Context Understanding Tools and Solutions (SCOUTS), Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s1100/scouts.png&quot; style=&quot;display: none;&quot; />; &lt;p>; AI-related products and technologies are constructed and deployed in a &lt;em>;societal context&lt;/em>;: that is, a dynamic and complex collection of social, cultural, historical, political and economic circumstances. Because societal contexts by nature are dynamic, complex, non-linear, contested, subjective, and highly qualitative, they are challenging to translate into the quantitative representations, methods, and practices that dominate standard machine learning (ML) approaches and responsible AI product development practices. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The first phase of AI product development is &lt;em>;problem understanding&lt;/em>;, and this phase has tremendous influence over how problems (eg, increasing cancer screening availability and accuracy) are formulated for ML systems to solve as well many other downstream decisions, such as dataset and ML architecture choice. When the societal context in which a product will operate is not articulated well enough to result in robust problem understanding, the resulting ML solutions can be fragile and even propagate unfair biases. &lt;/p>; &lt;p>; When AI product developers lack access to the knowledge and tools necessary to effectively understand and consider societal context during development, they tend to abstract it away. This abstraction leaves them with a shallow, quantitative understanding of the problems they seek to solve, while product users and society stakeholders — who are proximate to these problems and embedded in related societal contexts — tend to have a deep qualitative understanding of those same problems. This qualitative–quantitative divergence in ways of understanding complex problems that separates product users and society from developers is what we call the &lt;em>;problem understanding chasm&lt;/em>;. &lt;/p>; &lt;p>; This chasm has repercussions in the real world: for example, it was the root cause of &lt;a href=&quot;https://www.science.org/doi/10.1126/science.aax2342&quot;>;racial bias discovered by a widely used healthcare algorithm&lt;/a>; intended to solve the problem of choosing patients with the most complex healthcare needs for special programs. Incomplete understanding of the societal context in which the algorithm would operate led system designers to form incorrect and oversimplified causal theories about what the key problem factors were. Critical socio-structural factors, including lack of access to healthcare, lack of trust in the health care system, and underdiagnosis due to human bias,&lt;em>; &lt;/em>;were left out while spending on healthcare was highlighted as a predictor of complex health need. &lt;/p>; &lt;p>; To bridge the problem understanding chasm responsibly, AI product developers need tools that put community-validated and structured knowledge of societal context about complex societal problems at their fingertips — starting with problem understanding, but also throughout the product development lifecycle. To that end, &lt;a href=&quot;https://sites.research.google/scouts/&quot;>;Societal Context Understanding Tools and Solutions&lt;/a>; (SCOUTS) — part of the &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; (RAI-HCT) team within Google Research — is a dedicated research team focused on the mission to “empower people with the scalable, trustworthy societal context knowledge required to realize responsible, robust AI and solve the world&#39;s most complex societal problems.” SCOUTS is motivated by the significant challenge of articulating societal context, and it conducts innovative foundational and applied research to produce structured societal context knowledge and to integrate it into all phases of the AI-related product development lifecycle. Last year we &lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;announced&lt;/a>; that &lt;a href=&quot;https://jigsaw.google.com/&quot;>;Jigsaw&lt;/a>;, Google&#39;s incubator for building technology that explores solutions to threats to open societies, leveraged our structured societal context knowledge approach during the data preparation and evaluation phases of model development to scale bias mitigation for their widely used &lt;a href=&quot;https://perspectiveapi.com/&quot;>;Perspective API&lt;/a>; toxicity classifier. Going forward SCOUTS&#39; research agenda focuses on the problem understanding phase of AI-related product development with the goal of bridging the problem understanding chasm. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Bridging the AI problem understanding chasm&lt;/h2>; &lt;p>; Bridging the AI problem understanding chasm requires two key ingredients: 1) a reference frame for organizing structured societal context knowledge and 2) participatory, non-extractive methods to elicit community expertise about complex problems and represent it as structured knowledge. SCOUTS has published innovative research in both areas.&lt;/p>; &lt;br>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 0%; margin-right: 0%;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://sites.research.google/scouts/videos/scouts_pull_intro.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;div style=&quot;line-height: 80%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the problem understanding chasm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;A societal context reference frame&lt;/h3>; &lt;p>; An essential ingredient for producing structured knowledge is a taxonomy for creating the structure to organize it. SCOUTS collaborated with other RAI-HCT teams (&lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;TasC&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/03/responsible-ai-at-google-research.html&quot;>;Impact Lab&lt;/a>;), &lt;a href=&quot;https://www.deepmind.com/&quot;>;Google DeepMind&lt;/a>;, and external system dynamics experts to &lt;a href=&quot;https://arxiv.org/abs/2006.09663&quot;>;develop a taxonomic reference frame&lt;/a>; for societal context. To contend with the complex, dynamic, and adaptive nature of societal context, we leverage &lt;a href=&quot;https://en.wikipedia.org/wiki/Complex_adaptive_system&quot;>;complex adaptive systems&lt;/a>; (CAS) theory to propose a high-level taxonomic model for organizing societal context knowledge. The model pinpoints three key elements of societal context and the dynamic feedback loops that bind them together&lt;strong>;: &lt;/strong>;agents, precepts, and artifacts. &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Agents&lt;/em>;: These can be individuals or institutions. &lt;/li>;&lt;li>;&lt;em>;Precepts&lt;/em>;: The preconceptions — including beliefs, values, stereotypes and biases — that constrain and drive the behavior of agents. An example of a basic precept is that “all basketball players are over 6 feet tall.” That limiting assumption can lead to failures in identifying basketball players of smaller stature. &lt;/li>;&lt;li>;&lt;em>;Artifacts&lt;/em>;: Agent behaviors produce many kinds of artifacts, including language, data, technologies, societal problems and products. &lt;/li>; &lt;/ul>; &lt;p>; The relationships between these entities are dynamic and complex. Our work hypothesizes that precepts are the most critical element of societal context and we highlight &lt;em>;the problems people perceive&lt;/em>; and &lt;em>;the causal theories they hold about why those problems exist&lt;/em>; as particularly influential precepts that are core to understanding societal context. For example, in the case of racial bias in a medical algorithm described earlier, the causal theory precept held by designers was that&lt;em>; complex health problems would cause healthcare expenditures to go up for all populations&lt;/em>;. That incorrect precept directly led to the choice of healthcare spending as the proxy variable for the model to predict complex healthcare need, which in turn led to the model being biased against Black patients who, due to societal factors such as lack of access to healthcare and underdiagnosis due to bias on average, do not always spend more on healthcare when they have complex healthcare needs. A key open question is how can we ethically and equitably elicit causal theories from the people and communities who are most proximate to problems of inequity and transform them into useful structured knowledge? &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s1600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrative version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s1600/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Taxonomic version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Working with communities to foster the responsible application of AI to healthcare&lt;/h3>; &lt;p>; Since its inception, SCOUTS has worked&lt;a href=&quot;https://accelerate.withgoogle.com/stories/exploring-systems-dynamics-inclusive-ml-and-societal-impact-meet-googlers-donald-martin-and-jamaal-sebastian-barnes&quot;>; to build capacity&lt;/a>; in historically marginalized communities to articulate the broader societal context of the complex problems that matter to them using a practice called community based system dynamics (CBSD). &lt;a href=&quot;https://en.wikipedia.org/wiki/System_dynamics&quot;>;System dynamics&lt;/a>; (SD) is a methodology for articulating causal theories about complex problems, both &lt;em>;qualitatively&lt;strong>; &lt;/strong>;&lt;/em>;as causal loop and &lt;a href=&quot;https://online.visual-paradigm.com/knowledge/business-design/what-is-stock-and-flow-diagram/&quot;>;stock and flow diagrams&lt;/a>; (CLDs and SFDs, respectively) and &lt;em>;quantitatively&lt;/em>; as simulation models. The inherent support of visual qualitative tools, quantitative methods, and collaborative model building makes it an ideal ingredient for bridging the problem understanding chasm. CBSD is a &lt;a href=&quot;https://medium.com/people-ai-research/qa-donald-martin-on-community-based-system-dynamics-and-machine-learning-3fa21e42c680&quot;>;community-based, participatory variant of SD&lt;/a>; specifically focused on building capacity within communities to collaboratively describe and model the problems they face as causal theories, directly without intermediaries. With CBSD we&#39;ve witnessed community groups learn the basics and begin drawing CLDs within 2 hours. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s1147/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;863&quot; data-original-width=&quot;1147&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community members learning system dynamics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There is a huge potential for AI to &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9955430/#:~:text=AI%20algorithms%20can%20analyze%20medical,diseases%20more%20accurately%20and%20quickly.&quot;>;improve medical diagnosis&lt;/a>;. But the safety, equity, and reliability of AI-related health diagnostic algorithms depends on diverse and balanced training datasets. An open challenge in the health diagnostic space is the dearth of training sample data from historically marginalized groups. SCOUTS collaborated with the &lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community and CBSD experts to produce &lt;a href=&quot;https://arxiv.org/abs/2305.13485&quot;>;qualitative and quantitative causal theories&lt;/a>; for the data gap problem. The theories include critical factors that make up the broader societal context surrounding health diagnostics, including cultural memory of death and trust in medical care. &lt;/p>; &lt;p>; The figure below depicts the causal theory generated during the collaboration described above as a CLD. It hypothesizes that trust in medical care influences all parts of this complex system and is the key lever for increasing screening, which in turn generates data to overcome the data diversity gap. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s1024/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;835&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s1072/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;561&quot; data-original-width=&quot;1072&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Causal loop diagram of the health diagnostics data gap&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; These community-sourced causal theories are a first step to bridge the problem understanding chasm with trustworthy societal context knowledge. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; As discussed in this blog, the problem understanding chasm is a critical open challenge in responsible AI. SCOUTS conducts exploratory and applied research in collaboration with other teams within Google Research, external community, and academic partners across multiple disciplines to make meaningful progress solving it. Going forward our work will focus on three key elements, guided by our &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;: &lt;/p>; &lt;ol>; &lt;li>;Increase awareness and understanding of the problem understanding chasm and its implications through talks, publications, and training. &lt;/li>;&lt;li>;Conduct foundational and applied research for representing and integrating societal context knowledge into AI product development tools and workflows, from conception to monitoring, evaluation and adaptation. &lt;/li>;&lt;li>;Apply community-based causal modeling methods to the AI health equity domain to realize impact and build society&#39;s and Google&#39;s capability to produce and leverage global-scale societal context knowledge to realize responsible AI. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s960/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SCOUTS flywheel for bridging the problem understanding chasm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;Thank you to John Guilyard for graphics development, everyone in SCOUTS, and all of our collaborators and sponsors.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4421751509192561388/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/using-societal-context-knowledge-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/using-societal-context-knowledge-to.html&quot; rel=&quot;alternate&quot; title=&quot;Using societal context knowledge to foster the responsible application of AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s72-c/scouts.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8123471024413959829&lt;/id>;&lt;published>;2023-07-18T13:15:00.000-07:00&lt;/published>;&lt;updated>;2023-07-18T13:15:15.633-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Self-Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SimPer: Simple self-supervised learning of periodic targets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Daniel McDuff, Staff Research Scientist, and Yuzhe Yang, Student Researcher, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s320/SimPer%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; Learning from periodic data (signals that repeat, such as a heart beat or the daily temperature changes on Earth&#39;s surface) is crucial for many real-world applications, from &lt;a href=&quot;https://cloud.google.com/blog/topics/sustainability/weather-prediction-with-ai&quot;>;monitoring weather systems&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/take-pulse-health-and-wellness-your-phone/&quot;>;detecting vital signs&lt;/a>;. For example, in the environmental remote sensing domain, periodic learning is often needed to enable nowcasting of environmental changes, such as &lt;a href=&quot;https://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html&quot;>;precipitation patterns or land surface temperature&lt;/a>;. In the health domain, learning from video measurement has shown to extract (quasi-)periodic vital signs such as &lt;a href=&quot;https://www.ahajournals.org/doi/full/10.1161/JAHA.118.008585&quot;>;atrial fibrillation&lt;/a>; and &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9298820&quot;>;sleep apnea episodes&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Approaches like &lt;a href=&quot;https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html&quot;>;RepNet&lt;/a>; highlight the importance of these types of tasks, and present a solution that recognizes repetitive activities within a single video. However, these are supervised approaches that require a significant amount of data to capture repetitive activities, all labeled to indicate the number of times an action was repeated. Labeling such data is often challenging and resource-intensive, requiring researchers to manually capture gold-standard temporal measurements that are synchronized with the modality of interest (eg, video or satellite imagery). &lt;/p>; &lt;p>; Alternatively, self-supervised learning (SSL) methods (eg, &lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&quot;>;SimCLR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;), which leverage a large amount of unlabeled data to learn representations that capture periodic or quasi-periodic temporal dynamics, have demonstrated success in &lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j.html&quot;>;solving classification tasks&lt;/a>;. However, they overlook the intrinsic periodicity (ie, the ability to identify if a frame is part of a periodic process) in data and fail to learn robust representations that capture periodic or frequency attributes. This is because periodic learning exhibits characteristics that are distinct from prevailing learning tasks. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s1338/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1338&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Feature similarity is different in the context of periodic representations as compared to static features (eg, images). For example, videos that are offset by short time delays or are reversed should be similar to the original sample, whereas videos that have been upsampled or downsampled by a factor &lt;em>;x&lt;/em>; should be different from the original sample by a factor of &lt;em>;x&lt;/em>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To address these challenges, in “&lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;SimPer: Simple Self-Supervised Learning of Periodic Targets&lt;/a>;”, published at the eleventh &lt;a href=&quot;https://iclr.cc/&quot;>;International Conference on Learning Representations&lt;/a>; (ICLR 2023), we introduced a self-supervised contrastive framework for learning periodic information in data. Specifically, SimPer leverages the temporal properties of periodic targets using &lt;em>;temporal self-contrastive learning&lt;/em>;, where positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the &lt;em>;same&lt;/em>; input instance. We propose &lt;em>;periodic feature similarity&lt;/em>; that explicitly defines how to measure similarity in the context of periodic learning. Moreover, we design a generalized contrastive loss&lt;em>; &lt;/em>;that extends the classic &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE loss&lt;/a>; to a soft regression variant that enables contrasting over continuous labels (frequency). Next, we demonstrate that SimPer effectively learns period feature representations compared to state-of-the-art SSL methods, highlighting its intriguing properties including better data efficiency, robustness to spurious correlations, and generalization to distribution shifts. Finally, we are excited to release the &lt;a href=&quot;https://github.com/YyzHarry/SimPer&quot;>;SimPer code repo&lt;/a>; with the research community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The SimPer framework&lt;/h2>; &lt;p>; SimPer introduces a temporal self-contrastive learning framework. Positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the same input instance. For temporal video examples, periodicity-invariant changes are cropping, rotation or flipping, whereas periodicity-variant changes involve increasing or decreasing the speed of a video. &lt;/p>; &lt;p>; To explicitly define how to measure similarity in the context of periodic learning, SimPer proposes periodic feature similarity. This construction allows us to formulate training as a contrastive learning task. A model can be trained with data without any labels and then fine-tuned if necessary to map the learned features to specific frequency values. &lt;/p>; &lt;p>; Given an input sequence &lt;em>;x&lt;/em>;, we know there&#39;s an underlying associated periodic signal. We then transform &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo- speed or frequency labels for the unlabeled input x. &lt;/p>; &lt;p>; Conventional &lt;a href=&quot;https://en.wikipedia.org/wiki/Similarity_measure&quot;>;similarity measures&lt;/a>; such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; emphasize strict proximity between two feature vectors, and are sensitive to index shifted features (which represent different time stamps), reversed features, and features with changed frequencies. In contrast, periodic feature similarity should be high for samples with small temporal shifts and or reversed indexes, while capturing a continuous similarity change when the feature frequency varies. This can be achieved via a similarity metric in the frequency domain, such as the distance between two &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;>;Fourier transforms&lt;/a>;. &lt;/p>; &lt;p>; To harness the intrinsic continuity of augmented samples in the frequency domain, SimPer designs a generalized contrastive loss that extends the classic &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE&lt;/a>; loss to a soft regression variant that enables contrasting over continuous labels (frequency). This makes it suitable for regression tasks, where the goal is to recover a continuous signal, such as a heart beat. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;361&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SimPer constructs negative views of data through transformations in the frequency domain. The input sequence &lt;em>;x&lt;/em>; has an underlying associated periodic signal. SimPer transforms &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo speed or frequency labels for unlabeled input &lt;em>;x&lt;/em>; (periodicity-variant augmentations &lt;em>;τ&lt;/em>;). SimPer takes transformations that do not change the identity of the input and defines these as periodicity-invariant augmentations &lt;em>;σ&lt;/em>;, thus creating different positive views of the sample. Then, it sends these augmented views to the encoder &lt;em>;f&lt;/em>;,&lt;em>; &lt;/em>;which extracts corresponding features. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate SimPer&#39;s performance, we benchmarked it against state-of-the-art SSL schemes (eg, &lt;a href=&quot;https://github.com/google-research/simclr&quot;>;SimCLR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf&quot;>;BYOL&lt;/a>;, &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Spatiotemporal_Contrastive_Video_Representation_Learning_CVPR_2021_paper.html&quot;>;CVRL&lt;/a>;) on a set of six diverse periodic learning datasets for common real-world tasks in human behavior analysis, environmental remote sensing, and healthcare. Specifically, below we present results on heart rate measurement and exercise repetition counting from video. The results show that SimPer outperforms the state-of-the-art SSL schemes across all six datasets, highlighting its superior performance in terms of data efficiency, robustness to spurious correlations, and generalization to unseen targets. &lt;/p>; &lt;p>; Here we show quantitative results on two representative datasets using SimPer pre-trained using various SSL methods and fine-tuned on the labeled data. First, we pre-train SimPer using the &lt;a href=&quot;https://sites.google.com/corp/view/ybenezeth/ubfcrppg&quot;>;Univ. Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy&lt;/a>; (UBFC) dataset, a human &lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram#Remote_photoplethysmography&quot;>;photoplethysmography&lt;/a>; and heart rate prediction dataset, and compare its performance to state-of-the-art SSL methods. We observe that SimPer outperforms SimCLR, MoCo v2, BYOL, and CVRL methods. The results on the human action counting dataset, &lt;a href=&quot;https://sites.google.com/corp/view/repnet&quot;>;Countix&lt;/a>;, further confirm the benefits of SimPer over others methods as it notably outperforms the supervised baseline. For the feature evaluation results and performance on other datasets, please refer to the &lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results of SimCLR, MoCo v2, BYOL, CVRL and SimPer on the Univ. Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy (UBFC) and Countix datasets. Heart rate and repetition count performance is reported as &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;mean absolute error&lt;/a>; (MAE).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and applications&lt;/h2>; &lt;p>; We present SimPer, a self-supervised contrastive framework for learning periodic information in data. We demonstrate that by combining a temporal self-contrastive learning framework, periodicity-invariant and periodicity-variant augmentations, and continuous periodic feature similarity, SimPer provides an intuitive and flexible approach for learning strong feature representations for periodic signals. Moreover, SimPer can be applied to various fields, ranging from environmental remote sensing to healthcare. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Yuzhe Yang, Xin Liu, Ming-Zher Poh, Jiang Wu, Silviu Borac, and Dina Katabi for their contributions to this work. &lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8123471024413959829/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/simper-simple-self-supervised-learning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/simper-simple-self-supervised-learning.html&quot; rel=&quot;alternate&quot; title=&quot;SimPer: Simple self-supervised learning of periodic targets&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s72-c/SimPer%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2013323302512835157&lt;/id>;&lt;published>;2023-07-13T14:01:00.001-07:00&lt;/published>;&lt;updated>;2023-07-13T14:01:18.428-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Symbol tuning improves in-context learning in language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jerry Wei, Student Researcher, and Denny Zhou, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s1200/SymbolTuning.png&quot; style=&quot;display: none;&quot; />; &lt;p>; A key feature of human intelligence is that humans can learn to perform new tasks by reasoning using only a few examples. Scaling up language models has unlocked a range of new applications and paradigms in machine learning, including the ability to perform challenging reasoning tasks via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>;. Language models, however, are still sensitive to the way that prompts are given, indicating that they are not reasoning in a robust manner. For instance, language models often require heavy prompt engineering or phrasing tasks as instructions, and they exhibit unexpected behaviors such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;performance on tasks being unaffected even when shown incorrect labels&lt;/a>;. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.08298&quot;>;Symbol tuning improves in-context learning in language models&lt;/a>;”, we propose a simple fine-tuning procedure that we call &lt;em>;symbol tuning&lt;/em>;, which can improve in-context learning by emphasizing input–label mappings. We experiment with symbol tuning across &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Flan-PaLM&lt;/a>; models and observe benefits across various settings. &lt;/p>; &lt;ul>; &lt;li>;Symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. &lt;/li>;&lt;li>;Symbol-tuned models are much stronger at algorithmic reasoning tasks. &lt;/li>;&lt;li>;Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior knowledge. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s1035/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;663&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of symbol tuning, where models are fine-tuned on tasks where natural language labels are replaced with arbitrary symbols. Symbol tuning relies on the intuition that when instruction and relevant labels are not available, models must use in-context examples to learn the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Motivation&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a common fine-tuning method that has been shown to improve performance and allow models to better follow in-context examples. One shortcoming, however, is that models are not forced to learn to use the examples because the task is redundantly defined in the evaluation example via instructions and natural language labels. For example, on the left in the figure above, although the examples can help the model understand the task (sentiment analysis), they are not strictly necessary since the model could ignore the examples and just read the instruction that indicates what the task is. &lt;/p>; &lt;p>; In symbol tuning, the model is fine-tuned on examples where the instructions are removed and natural language labels are replaced with semantically-unrelated labels (eg, “Foo,” “Bar,” etc.). In this setup, the task is unclear without looking at the in-context examples. For example, on the right in the figure above, multiple in-context examples would be needed to figure out the task. Because symbol tuning teaches the model to reason over the in-context examples, symbol-tuned models should have better performance on tasks that require reasoning between in-context examples and their labels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s1035/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;237&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Datasets and task types used for symbol tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Symbol-tuning procedure&lt;/h2>; &lt;p>; We selected 22 publicly-available &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) datasets that we use for our symbol-tuning procedure. These tasks have been widely used in the past, and we only chose classification-type tasks since our method requires discrete labels. We then remap labels to a random label from a set of ~30K arbitrary labels selected from one of three categories: integers, character combinations, and words. &lt;/p>; &lt;p>; For our experiments, we symbol tune &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, the instruction-tuned variants of &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;. We use three different sizes of Flan-PaLM models: Flan-PaLM-8B, Flan-PaLM-62B, and Flan-PaLM-540B. We also tested &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;Flan-cont-PaLM-62B&lt;/a>; (Flan-PaLM-62B at 1.3T tokens instead of 780B tokens), which we abbreviate as 62B-c. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s861/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;168&quot; data-original-width=&quot;861&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a set of ∼300K arbitrary symbols from three categories (integers, character combinations, and words). ∼30K symbols are used during tuning and the rest are held out for evaluation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experimental setup&lt;/h2>; &lt;p>; We want to evaluate a model&#39;s ability to perform unseen tasks, so we cannot evaluate on tasks used in symbol tuning (22 datasets) or used during instruction tuning (1.8K tasks). Hence, we choose 11 NLP datasets that were not used during fine-tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;In-context learning&lt;/h2>; &lt;p>; In the symbol-tuning procedure, models must learn to reason with in-context examples in order to successfully perform tasks because prompts are modified to ensure that tasks cannot simply be learned from relevant labels or instructions. Symbol-tuned models should perform better in settings where tasks are unclear and require reasoning between in-context examples and their labels. To explore these settings, we define four in-context learning settings that vary the amount of reasoning required between inputs and labels in order to learn the task (based on the availability of instructions/relevant labels) &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s936/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;936&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depending on the availability of instructions and relevant natural language labels, models may need to do varying amounts of reasoning with in-context examples. When these features are not available, models must reason with the given in-context examples to successfully perform the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Symbol tuning improves performance across all settings for models 62B and larger, with small improvements in settings with relevant natural language labels (+0.8% to +4.2%) and substantial improvements in settings without relevant natural language labels (+5.5% to +15.5%). Strikingly, when relevant labels are unavailable, symbol-tuned Flan-PaLM-8B outperforms FlanPaLM-62B, and symbol-tuned Flan-PaLM-62B outperforms Flan-PaLM-540B. This performance difference suggests that symbol tuning can allow much smaller models to perform as well as large models on these tasks (effectively saving ∼10X inference compute). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s900/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;495&quot; data-original-width=&quot;900&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Large-enough symbol-tuned models are better at in-context learning than baselines, especially in settings where relevant labels are not available. Performance is shown as average model accuracy (%) across eleven tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Algorithmic reasoning&lt;/h2>; &lt;p>; We also experiment on algorithmic reasoning tasks from &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;. There are two main groups of tasks: 1) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/list_functions&quot;>;List functions&lt;/a>; — identify a transformation function (eg, remove the last element in a list) between input and output lists containing non-negative integers; and 2) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/simp_turing_concept&quot;>;simple turing concepts&lt;/a>; — reason with binary strings to learn the concept that maps an input to an output (eg, swapping 0s and 1s in a string). &lt;/p>; &lt;p>; On the list function and simple turing concept tasks, symbol tuning results in an average performance improvement of 18.2% and 15.3%, respectively. Additionally, Flan-cont-PaLM-62B with symbol tuning outperforms Flan-PaLM-540B on the list function tasks on average, which is equivalent to a ∼10x reduction in inference compute. These improvements suggest that symbol tuning strengthens the model&#39;s ability to learn in-context for unseen task types, as symbol tuning did not include any algorithmic data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s908/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;496&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models achieve higher performance on list function tasks and simple turing concept tasks. (A–E): categories of list functions tasks. (F): simple turing concepts task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In the flipped-label experiment, labels of in-context and evaluation examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as “negative sentiment”), thereby allowing us to study whether models can override prior knowledge. &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;Previous work&lt;/a>; has shown that while pre-trained models (without instruction tuning) can, to some extent, follow flipped labels presented in-context, instruction tuning degraded this ability. &lt;/p>; &lt;p>; We see that there is a similar trend across all model sizes — symbol-tuned models are much more capable of following flipped labels than instruction-tuned models. We found that after symbol tuning, Flan-PaLM-8B sees an average improvement across all datasets of 26.5%, Flan-PaLM-62B sees an improvement of 33.7%, and Flan-PaLM-540B sees an improvement of 34.0%. Additionally, symbol-tuned models achieve similar or better than average performance as pre-training–only models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s914/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;914&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models are much better at following flipped labels presented in-context than instruction-tuned models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented symbol tuning, a new method of tuning models on tasks where natural language labels are remapped to arbitrary symbols. Symbol tuning is based off of the intuition that when models cannot use instructions or relevant labels to determine a presented task, it must do so by instead learning from in-context examples. We tuned four language models using our symbol-tuning procedure, utilizing a tuning mixture of 22 datasets and approximately 30K arbitrary symbols as labels. &lt;/p>; &lt;p>; We first showed that symbol tuning improves performance on unseen in-context learning tasks, especially when prompts do not contain instructions or relevant labels. We also found that symbol-tuned models were much better at algorithmic reasoning tasks, despite the lack of numerical or algorithmic data in the symbol-tuning procedure. Finally, in an in-context learning setting where inputs have flipped labels, symbol tuning (for some datasets) restores the ability to follow flipped labels that was lost during instruction tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; Through symbol tuning, we aim to increase the degree to which models can examine and learn from input–label mappings during in-context learning. We hope that our results encourage further work towards improving language models&#39; ability to reason over symbols presented in-context. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. This work was conducted by Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. We would like to thank our colleagues at Google Research and Google DeepMind for their advice and helpful discussions.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2013323302512835157/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/symbol-tuning-improves-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/symbol-tuning-improves-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Symbol tuning improves in-context learning in language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s72-c/SymbolTuning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8984134419460793359&lt;/id>;&lt;published>;2023-07-11T10:00:00.010-07:00&lt;/published>;&lt;updated>;2023-07-11T10:44:15.111-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;An open-source gymnasium for machine learning assisted computer architecture design&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Amir Yazdanbakhsh, Research Scientist, and Vijay Janapa Reddi, Visiting Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s1200/ArchGym-animation2.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_architecture&quot;>;Computer Architecture&lt;/a>; research has a long history of developing simulators and tools to evaluate and shape the design of computer systems. For example, the &lt;a href=&quot;https://ieeexplore.ieee.org/iel5/2/21180/00982917.pdf?casa_token=M_ZAQmgCbKkAAAAA:Y9wFTB9OQBwzXXpw7kNbq4asdlCCHYRaq7qsqcRBpYyh6734aHmr57ll4Vb1_zcG5ukNvNONNQ&quot;>;SimpleScalar&lt;/a>; simulator was introduced in the late 1990s and allowed researchers to explore various &lt;a href=&quot;https://en.wikipedia.org/wiki/Microarchitecture&quot;>;microarchitectural&lt;/a>; ideas. Computer architecture simulators and tools, such as &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;gem5&lt;/a>;, &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>;, and many more have played a significant role in advancing computer architecture research. Since then, these shared resources and infrastructure have benefited industry and academia and have enabled researchers to systematically build on each other&#39;s work, leading to significant advances in the field. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Nonetheless, computer architecture research is evolving, with industry and academia turning towards machine learning (ML) optimization to meet stringent domain-specific requirements, such as &lt;a href=&quot;https://ai.googleblog.com/2021/02/machine-learning-for-computer.html&quot;>;ML for computer architecture&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2201.01863.pdf&quot;>;ML for TinyML acceleration&lt;/a>;,&amp;nbsp;&lt;a href=&quot;https://ai.googleblog.com/2022/03/offline-optimization-for-architecting.html&quot;>;DNN&lt;/a>; &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9804604&quot;>;accelerator&lt;/a>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3503222.3507767&quot;>;datapath optimization&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1394608.1382172?casa_token=3g46kAHeN0QAAAAA:5pKdYXamA_vstsO5LqA0_4rRy5o2Z46Ks-OJLDUe7ZSLtDfkaSS5i0zLfMe3Y7gAuY2bFMZ1yGOEMA&quot;>;memory controllers&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40&quot;>;power consumption&lt;/a>;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/7430287&quot;>;security&lt;/a>;, and &lt;a href=&quot;https://www.sigarch.org/tag/privacy-preserving-computing/&quot;>;privacy&lt;/a>;. Although prior work has demonstrated the benefits of ML in design optimization, the lack of strong, reproducible baselines hinders fair and objective comparison across different methods and poses several challenges to their deployment. To ensure steady progress, it is imperative to understand and tackle these challenges collectively. &lt;/p>; &lt;p>; To alleviate these challenges, in “&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3579371.3589049&quot;>;ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design&lt;/a>;”, accepted at &lt;a href=&quot;https://www.iscaconf.org/isca2023/program/&quot;>;ISCA 2023&lt;/a>;, we introduced ArchGym, which includes a variety of computer architecture simulators and ML algorithms. Enabled by ArchGym, our results indicate that with a sufficiently large number of samples, any of a diverse collection of ML algorithms are capable of finding the optimal set of architecture design parameters for each target problem; &lt;i>;no one solution is necessarily better than another&lt;/i>;. These results further indicate that selecting the optimal hyperparameters for a given ML algorithm is essential for finding the optimal architecture design, but choosing them is non-trivial. We &lt;a href=&quot;https://bit.ly/ArchGym&quot;>;release&lt;/a>; the code and dataset across multiple computer architecture simulations and ML algorithms.&lt;/p>; &lt;br />; &lt;h2>;Challenges in ML-assisted architecture research &lt;/h2>; &lt;p>; ML-assisted architecture research poses several challenges, including: &lt;/p>; &lt;ol>; &lt;li>;For a specific ML-assisted computer architecture problem (eg, finding an optimal solution for a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_random-access_memory&quot;>;DRAM&lt;/a>; controller) there is no systematic way to identify optimal ML algorithms or hyperparameters (eg, learning rate, warm-up steps, etc.). There is a wider range of ML and heuristic methods, from &lt;a href=&quot;https://arxiv.org/abs/2008.03639&quot;>;random walk&lt;/a>; to &lt;a href=&quot;http://incompleteideas.net/book/RLbook2020.pdf&quot;>;reinforcement learning&lt;/a>; (RL), that can be employed for &lt;a href=&quot;https://en.wikipedia.org/wiki/Design_space_exploration&quot;>;design space exploration&lt;/a>; (DSE). While these methods have shown noticeable performance improvement over their choice of baselines, it is not evident whether the improvements are because of the choice of optimization algorithms or hyperparameters.&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;Thus, to ensure reproducibility and facilitate widespread adoption of ML-aided architecture DSE, it is necessary to outline a systematic benchmarking methodology.&lt;/em>; &lt;br />;&lt;br />; &lt;/li>; &lt;li>;While computer architecture simulators have been the backbone of architectural innovations, there is an emerging need to address the trade-offs between accuracy, speed, and cost in architecture exploration. The accuracy and speed of performance estimation widely varies from one simulator to another, depending on the underlying modeling details (eg, &lt;a href=&quot;https://biblio.ugent.be/publication/2968322/file/6776727.pdf&quot;>;cycle&lt;/a>;-&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;accurate&lt;/a>; vs. &lt;a href=&quot;https://arxiv.org/abs/2210.03894&quot;>;ML&lt;/a>;-&lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;based&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1808.07412&quot;>;proxy&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1803.02329&quot;>;models&lt;/a>;). While analytical or ML-based proxy models are nimble by virtue of discarding low-level details, they generally suffer from high prediction error. Also, due to commercial licensing, there can be strict &lt;a href=&quot;https://ieeexplore.ieee.org/document/7945172&quot;>;limits on the number of runs collected from a simulator&lt;/a>;. Overall, these constraints exhibit distinct performance vs. sample efficiency trade-offs, affecting the choice of optimization algorithm for architecture exploration. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;It is challenging to delineate how to systematically compare the effectiveness of various ML algorithms under these constraints.&lt;/em>; &lt;br />; &lt;br />; &lt;/li>; &lt;li>;Finally, the landscape of ML algorithms is rapidly evolving and some ML algorithms need data to be useful. Additionally, rendering the outcome of DSE into meaningful artifacts such as datasets is critical for drawing insights about the design space. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;In this rapidly evolving ecosystem, it is consequential to ensure how to amortize the overhead of search algorithms for architecture exploration. It is not apparent, nor systematically studied how to leverage exploration data while being agnostic to the underlying search algorithm.&lt;/em>; &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;ArchGym design&lt;/h2>; &lt;p>; ArchGym addresses these challenges by providing a unified framework for evaluating different ML-based search algorithms fairly. It comprises two main components: 1) the ArchGym environment and 2) the ArchGym agent. The environment is an encapsulation of the architecture cost model — which includes latency, throughput, area, energy, etc., to determine the computational cost of running the workload, given a set of architectural parameters — paired with the target workload(s). The agent is an encapsulation of the ML algorithm used for the search and consists of hyperparameters and a guiding policy. The hyperparameters are intrinsic to the algorithm for which the model is to be optimized and can significantly influence performance. The policy, on the other hand, determines how the agent selects a parameter iteratively to optimize the target objective. &lt;/p>; &lt;p>; Notably, ArchGym also includes a standardized interface that connects these two components, while also saving the exploration data as the ArchGym Dataset. At its core, the interface entails three main signals: &lt;i>;hardware state&lt;/i>;, &lt;i>;hardware parameters&lt;/i>;, and &lt;i>;metrics&lt;/i>;. These signals are the bare minimum to establish a meaningful communication channel between the environment and the agent.&amp;nbsp;Using these signals, the agent observes the state of the hardware and suggests a set of hardware parameters to iteratively optimize a (user-defined) reward. The reward is a function of hardware performance metrics, such as performance, energy consumption, etc.&amp;nbsp;&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s1226/ArchGym-animation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;722&quot; data-original-width=&quot;1226&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s16000/ArchGym-animation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ArchGym comprises two main components: the ArchGym environment and the ArchGym agent. The ArchGym environment encapsulates the cost model and the agent is an abstraction of a policy and hyperparameters. With a standardized interface that connects these two components, ArchGym provides a unified framework for evaluating different ML-based search algorithms fairly while also saving the exploration data as the ArchGym Dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;ML algorithms could be equally favorable to meet user-defined target specifications&lt;/h2>; &lt;p>; Using ArchGym, we empirically demonstrate that across different optimization objectives and DSE problems, &lt;em>;at least one set of hyperparameters exists that results in the same hardware performance as other ML algorithms&lt;/em>;. A poorly selected (random selection) hyperparameter for the ML algorithm or its baseline can lead to a misleading conclusion that a particular family of ML algorithms is better than another. We show that with sufficient hyperparameter tuning, different search algorithms, even &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;random walk&lt;/a>; (RW), are able to identify the best possible reward. However, note that finding the right set of hyperparameters may require exhaustive search or even luck to make it competitive. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1158&quot; data-original-width=&quot;1999&quot; height=&quot;371&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/w640-h371/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With a sufficient number of samples, there exists at least one set of hyperparameters that results in the same performance across a range of search algorithms. Here the dashed line represents the maximum normalized reward. &lt;i>;Cloud-1&lt;/i>;, &lt;i>;cloud-2&lt;/i>;, &lt;i>;stream&lt;/i>;, and &lt;i>;random&lt;/i>; indicate four different memory traces for &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>; (DRAM subsystem design space exploration framework).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset construction and high-fidelity proxy model training&lt;/h2>; &lt;p>; Creating a unified interface using ArchGym also enables the creation of datasets that can be used to design better data-driven ML-based proxy architecture cost models to improve the speed of architecture simulation. To evaluate the benefits of datasets in building an ML model to approximate architecture cost, we leverage ArchGym&#39;s ability to log the data from each run from DRAMSys to create four dataset variants, each with a different number of data points. For each variant, we create two categories: (a) Diverse Dataset, which represents the data collected from different agents (&lt;a href=&quot;https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms&quot;>;ACO&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_algorithm&quot;>;GA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;RW&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions.&quot;>;BO&lt;/a>;), and (b) ACO only, which shows the data collected exclusively from the ACO agent, both of which are released along with ArchGym. We train a proxy model on each dataset using &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;>;random forest regression&lt;/a>; with the objective to predict the latency of designs for a &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAM simulator&lt;/a>;. Our results show that: &lt;/p>; &lt;ol>; &lt;li>;As we increase the dataset size, the average normalized &lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;root mean squared error&lt;/a>; (RMSE) slightly decreases. &lt;/li>;&lt;li>;However, as we introduce diversity in the dataset (eg, collecting data from different agents), we observe 9× to 42× lower RMSE across different dataset sizes. &lt;/li>; &lt;/ol>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/s1826/ArchGym2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1143&quot; data-original-width=&quot;1826&quot; height=&quot;401&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/w640-h401/ArchGym2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diverse dataset collection across different agents using ArchGym interface.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/s1803/ArchGym1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1803&quot; height=&quot;407&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/w640-h407/ArchGym1.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The impact of a diverse dataset and dataset size on the normalized RMSE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;The need for a community-driven ecosystem for ML-assisted architecture research&lt;/h2>; &lt;p>;While, ArchGym is an initial effort towards creating an open-source ecosystem that (1) connects a broad range of search algorithms to computer architecture simulators in an unified and easy-to-extend manner, (2) facilitates research in ML-assisted computer architecture, and (3) forms the scaffold to develop reproducible baselines, there are a lot of open challenges that need community-wide support. Below we outline some of the open challenges in ML-assisted architecture design. Addressing these challenges requires a well coordinated effort and a community driven ecosystem.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;969&quot; data-original-width=&quot;1999&quot; height=&quot;310&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/w640-h310/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Key challenges in ML-assisted architecture design.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We call this ecosystem &lt;a href=&quot;https://www.sigarch.org/architecture-2-0-why-computer-architects-need-a-data-centric-ai-gymnasium/&quot;>;Architecture 2.0&lt;/a>;.&amp;nbsp;We outline the key challenges and a vision for building an inclusive ecosystem of interdisciplinary researchers to tackle the long-standing open problems in applying ML for computer architecture research.&amp;nbsp;If you are interested in helping shape this ecosystem, please fill out the &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSfIYeSBoEi-DIHizPx4-FTEcZUSY_uUcKe0rdHC0tkCWp3Gag/viewform&quot;>;interest survey&lt;/a>;.&lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>;&lt;a href=&quot;https://bit.ly/ArchGym&quot;>;ArchGym&lt;/a>; is an open source gymnasium for ML architecture DSE and enables an standardized interface that can be readily extended to suit different use cases. Additionally, ArchGym enables fair and reproducible comparison between different ML algorithms and helps to establish stronger baselines for computer architecture research problems. &lt;/p>; &lt;p>; We invite the computer architecture community as well as the ML community to actively participate in the development of ArchGym. We believe that the creation of a gymnasium-type environment for computer architecture research would be a significant step forward in the field and provide a platform for researchers to use ML to accelerate research and lead to new and innovative designs. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;This blogpost is based on joint work with several co-authors at Google and Harvard University.&amp;nbsp;&lt;/i>;&lt;i>;We would like to acknowledge and highlight Srivatsan Krishnan (Harvard) who contributed several ideas to this project in collaboration with Shvetank Prakash (Harvard), Jason Jabbour (Harvard), Ikechukwu Uchendu (Harvard), Susobhan Ghosh (Harvard), Behzad Boroujerdian (Harvard), Daniel Richins (Harvard), Devashree Tripathy (Harvard), and Thierry Thambe (Harvard).&amp;nbsp; In addition, we would also like to thank James Laudon, Douglas Eck, Cliff Young, and Aleksandra Faust for their support, feedback, and motivation for this work. We would also like to thank John Guilyard for the animated figure used in this post. Amir Yazdanbakhsh is now a Research Scientist at Google DeepMind and Vijay Janapa Reddi is an Associate Professor at Harvard.&lt;/i>;&lt;/p>;&lt;div>;&lt;br />;&lt;/div>;&lt;br />;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8984134419460793359/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/an-open-source-gymnasium-for-computer.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/an-open-source-gymnasium-for-computer.html&quot; rel=&quot;alternate&quot; title=&quot;An open-source gymnasium for machine learning assisted computer architecture design&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s72-c/ArchGym-animation2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7093565002706757508&lt;/id>;&lt;published>;2023-07-10T06:02:00.005-07:00&lt;/published>;&lt;updated>;2023-07-21T13:24:14.452-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ACL 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s1040/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the &lt;a href=&quot;https://2023.aclweb.org/&quot;>;61st annual meeting&lt;/a>; of the &lt;a href=&quot;https://www.aclweb.org/&quot;>;Association for Computational Linguistics&lt;/a>; (ACL), a premier conference covering a broad spectrum of research areas that are concerned with computational approaches to natural language, is taking place in Vancouver, BC. As a leader in natural language processing and understanding, and a&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/sponsors/&quot;>;Diamond Level sponsor&lt;/a>;&amp;nbsp;of&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, Google will showcase the latest research in the field with over 50 publications, and active involvement in a variety of workshops and tutorials.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;If you&#39;re registered for ACL 2023, we hope that you&#39;ll visit the Google booth to learn more about the projects at Google that go into solving interesting problems for billions of people. You can also learn more about Google&#39;s participation below (Google affiliations in &lt;b>;bold&lt;/b>;).&lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Area chairs include: &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>; &lt;br />; Workshop chairs include: &lt;strong>;&lt;em>;Annie Louis&lt;/em>;&lt;/strong>; &lt;br />; Publication chairs include: &lt;strong>;&lt;em>;Lei Shu&lt;/em>;&lt;/strong>; &lt;br />; Program Committee includes: &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Spotlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09648.pdf&quot;>;NusaCrowd: Open Source Initiative for Indonesian NLP Resources&lt;/a>; &lt;br />; &lt;em>;Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Winata, Bryan Wilie, Fajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Muhammad Satrio Wicaksono, Ivan Parmonangan, Ika Alfina, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh Dhole, Arie Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Adilazuarda, Ryan Hadiwijaya, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Haryo Wibowo, Cuk Tho, Ichwanul Karo Karo, Tirana Fatyanosa, Ziwei Ji, Graham Neubig, Timothy Baldwin, &lt;strong>;Sebastian Ruder&lt;/strong>;, Pascale Fung, Herry Sujaini, Sakriani Sakti, Ayu Purwarianti&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2205.12680.pdf&quot;>;Optimizing Test-Time Query Representations for Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Mujeen Sung, Jungsoo Park, Jaewoo Kang, Danqi Chen, &lt;strong>;Jinhyuk Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10750.pdf&quot;>;PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition&lt;/a>; &lt;br />; &lt;em>;Sihao Chen*, &lt;strong>;Senaka Buthpitiya&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, Dan Roth, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02301.pdf&quot;>;Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes&lt;/a>; &lt;br />; &lt;em>;Cheng-Yu Hsieh*, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chih-Kuan Yeh&lt;/strong>;, &lt;strong>;Hootan Nakhost&lt;/strong>;, &lt;strong>;Yasuhisa Fujii&lt;/strong>;, Alex Ratner, Ranjay Krishna, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05110.pdf&quot;>;Large Language Models with Controllable Working Memory&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Daliang Li&lt;/b>;, &lt;b>;Ankit Singh Rawat&lt;/b>;, &lt;b>;Manzil Zaheer&lt;/b>;, &lt;b>;Xin Wang&lt;/b>;, &lt;b>;Michal Lukasik&lt;/b>;, &lt;b>;Andreas Veit&lt;/b>;, &lt;b>;Felix Yu&lt;/b>;,&lt;b>; Sanjiv Kumar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10791.pdf&quot;>;OpineSum: Entailment-Based Self-Training for Abstractive Opinion Summarization&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Annie Louis&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08775.pdf&quot;>;RISE: Leveraging Retrieval Techniques for Summarization Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Uthus&lt;/b>;, &lt;b>;Jianmo Ni&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b5b9bb4ec1e1d7416f88f8b3a1649d1235d747b8.pdf&quot;>;Follow the Leader(board) with Confidence: Estimating p-Values from a Single Test Set with Item and Response Variance&lt;/a>; &lt;br />; &lt;i>; Shira Wein*, Christopher Homan, &lt;strong>;Lora Aroyo&lt;/strong>;, &lt;strong>;Chris Welty&lt;/strong>;&lt;/i>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.02516.pdf&quot;>;SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives&lt;/a>; &lt;br />; &lt;i>;&lt;strong>;Fedor Moiseev&lt;/strong>;, &lt;strong>;Gustavo Hernandez Abrego&lt;/strong>;, &lt;strong>;Peter Dornbach&lt;/strong>;, &lt;strong>;Imed Zitouni&lt;/strong>;, &lt;strong>;Enrique Alfonseca&lt;/strong>;, &lt;strong>;Zhe Dong&lt;/strong>;&lt;/i>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10266.pdf&quot;>;Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM&#39;s Translation Capability&lt;/a>; &lt;br />; &lt;em>;Eleftheria Briakou, &lt;strong>;Colin Cherry&lt;/strong>;, &lt;strong>;George Foster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09102.pdf&quot;>;Prompting PaLM for Translation: Assessing Strategies and Performance&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Vilar&lt;/b>;, &lt;b>;Markus Freitag&lt;/b>;, &lt;b>;Colin Cherry&lt;/b>;, &lt;b>;Jiaming Luo&lt;/b>;, &lt;b>;Viresh Ratnakar&lt;/b>;, &lt;b>;George Foster&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.17525.pdf&quot;>;Query Refinement Prompts for Closed-Book Long-Form QA&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kellie Webster&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10381.pdf&quot;>;To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering&lt;/a>; &lt;br />; &lt;em>;Dheeru Dua*, &lt;strong>;Emma Strubell&lt;/strong>;, Sameer Singh, &lt;strong>;Pat Verga&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00193.pdf&quot;>;FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/02/frmt-benchmark-for-few-shot-region.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;b>;Parker Riley&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Jan A. Botha&lt;/b>;, &lt;b>;Xavier Garcia&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Jason Riesa&lt;/b>;, &lt;b>;Orhan Firat&lt;/b>;,&lt;b>; Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.00397.pdf&quot;>;Conditional Generation with a Question-Answering Blueprint&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kuzman Ganchev&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;, &lt;b>;Fantine Huot&lt;/b>;, &lt;b>;Anders Sandholm&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Mirella Lapata&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12142.pdf&quot;>;Coreference Resolution Through a Seq2Seq Transition-Based System&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Bernd Bohnet&lt;/b>;, &lt;b>;Chris Alberti&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00106.pdf&quot;>;Cross-Lingual Transfer with Language-Specific Subnetworks for Low-Resource Dependency Parsing&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni, &lt;strong>;Dan Garrette&lt;/strong>;, Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08054.pdf&quot;>;DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue&lt;/a>; &lt;br />; &lt;em>;William Held*, &lt;strong>;Christopher Hidey&lt;/strong>;, &lt;strong>;Fei Liu&lt;/strong>;, &lt;strong>;Eric Zhu&lt;/strong>;, &lt;strong>;Rahul Goel&lt;/strong>;, Diyi Yang, &lt;strong>;Rushin Shah&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.08726.pdf&quot;>;RARR: Researching and Revising What Language Models Say, Using Language Models&lt;/a>; &lt;br />; Luyu Gao*, &lt;strong>;Zhuyun Dai&lt;/strong>;, &lt;strong>;Panupong Pasupat&lt;/strong>;, Anthony Chen*, &lt;strong>;Arun Tejasvi Chaganty&lt;/strong>;, &lt;strong>;Yicheng Fan&lt;/strong>;, &lt;strong>;Vincent Y. Zhao&lt;/strong>;, &lt;strong>;Ni Lao&lt;/strong>;, &lt;strong>;Hongrae Lee&lt;/strong>;, &lt;strong>;Da-Cheng Juan&lt;/strong>;, &lt;strong>;Kelvin Guu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.16793.pdf&quot;>;Benchmarking Large Language Model Capabilities for Conditional Generation&lt;/a>; &lt;br />; &lt;em>;Joshua Maynez, Priyanka Agrawal, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.01786.pdf&quot;>;Crosslingual Generalization Through Multitask Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Niklas Muennighoff, Thomas Wang, Lintang Sutawika, &lt;strong>;Adam Roberts&lt;/strong>;, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05655.pdf&quot;>;DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering&lt;/a>; &lt;br />; &lt;em>;Ella Neeman, &lt;strong>;Roee Aharoni&lt;/strong>;, Or Honovich, Leshem Choshen, &lt;strong>;Idan Szpektor&lt;/strong>;, Omri Abend&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10933.pdf&quot;>;Resolving Indirect Referring Expressions for Entity Selection&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mohammad Javad Hosseini&lt;/b>;, &lt;b>;Filip Radlinski&lt;/b>;, &lt;b>;Silvia Pareti&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11840.pdf&quot;>;SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models&lt;/a>; &lt;br />; &lt;em>;Akshita Jha*, &lt;strong>;Aida Mostafazadeh Davani&lt;/strong>;, Chandan K Reddy, &lt;strong>;Shachi Dave&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;, &lt;strong>;Sunipa Dev&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.10040.pdf&quot;>;The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks&lt;/a>; &lt;br />; &lt;em>;Nikil Selvam, &lt;strong>;Sunipa Dev&lt;/strong>;, Daniel Khashabi, Tushar Khot, Kai-Wei Chang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10562.pdf&quot;>;Character-Aware Models Improve Visual Text Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Rosanne Liu&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Chitwan Saharia&lt;/b>;, &lt;b>;William Chan&lt;/b>;, &lt;b>;Adam Roberts&lt;/b>;, &lt;b>;Sharan Narang&lt;/b>;, &lt;b>;Irina Blok&lt;/b>;, &lt;b>;RJ Mical&lt;/b>;, &lt;b>;Mohammad Norouzi&lt;/b>;, &lt;b>;Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06995.pdf&quot;>;Cold-Start Data Selection for Better Few-Shot Language Model Fine-Tuning: A Prompt-Based Uncertainty Propagation Approach&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment &lt;br />; &lt;em>;&lt;b>;Roni Rabin&lt;/b>;, &lt;b>;Alexandre Djerbetian&lt;/b>;, &lt;b>;Roee Engelberg&lt;/b>;, &lt;b>;Lidan Hackmon&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;, &lt;b>;Amir Globerson&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02549.pdf&quot;>;FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Chen-Yu Lee&lt;/b>;, &lt;b>;Chun-Liang Li&lt;/b>;, &lt;b>;Hao Zhang&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Vincent Perot&lt;/b>;, &lt;b>;Guolong Su&lt;/b>;, &lt;b>;Xiang Zhang&lt;/b>;,&lt;b>; Kihyuk Sohn&lt;/b>;, &lt;b>;Nikolay Glushinev&lt;/b>;, &lt;b>;Renshen Wang&lt;/b>;, &lt;b>;Joshua Ainslie&lt;/b>;, &lt;b>;Shangbang Long&lt;/b>;, &lt;b>;Siyang Qin&lt;/b>;,&lt;b>; Yasuhisa Fujii&lt;/b>;, &lt;b>;Nan Hua&lt;/b>;, &lt;b>;Tomas Pfister&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00922.pdf&quot;>;Dialect-Robust Evaluation of Generated Text&lt;/a>; &lt;br />; &lt;em>;Jiao Sun*, &lt;strong>;Thibault Sellam&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Tu Vu*, &lt;strong>;Timothy Dozat&lt;/strong>;, &lt;strong>;Dan Garrette&lt;/strong>;, &lt;strong>;Aditya Siddhant&lt;/strong>;, &lt;strong>;Jacob Eisenstein&lt;/strong>;, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.03950.pdf&quot;>;MISGENDERED: Limits of Large Language Models in Understanding Pronouns&lt;/a>; &lt;br />; &lt;em>;Tamanna Hossain, &lt;strong>;Sunipa Dev&lt;/strong>;, Sameer Singh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13894.pdf&quot;>;LAMBADA: Backward Chaining for Automated Reasoning in Natural Language&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mehran Kazemi&lt;/b>;, &lt;b>;Najoung Kim&lt;/b>;, &lt;b>;Deepti Bhatia&lt;/b>;, &lt;b>;Xin Xu&lt;/b>;, &lt;b>;Deepak Ramachandran&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.19585.pdf&quot;>;LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction&lt;/a>; &lt;br />; &lt;em>;Jeremiah Milbauer*, &lt;strong>;Annie Louis&lt;/strong>;, &lt;strong>;Mohammad Javad Hosseini&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, &lt;strong>;Donald Metzler&lt;/strong>;, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05392.pdf&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Andy Zeng&lt;/strong>;, Trevor Darrell, Dan Klein&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10001.pdf&quot;>;Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters&lt;/a>; &lt;br />; &lt;em>;Boshi Wang, Sewon Min, Xiang Deng, &lt;strong>;Jiaming Shen&lt;/strong>;, &lt;strong>;You Wu&lt;/strong>;, Luke Zettlemoyer and Huan Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14106.pdf&quot;>;Better Zero-Shot Reasoning with Self-Adaptive Prompting&lt;/a>; &lt;br />; &lt;em>;Xingchen Wan*, &lt;strong>;Ruoxi Sun&lt;/strong>;, Hanjun Dai, &lt;strong>;Sercan Ö. Arik&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.00186.pdf&quot;>;Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Paul Roit&lt;/b>;, &lt;b>;Johan Ferret&lt;/b>;, &lt;b>;Lior Shani&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Geoffrey Cideron&lt;/b>;, &lt;b>;Robert Dadashi&lt;/b>;, &lt;b>;Matthieu Geist&lt;/b>;,&lt;b>; Sertan Girgin&lt;/b>;, &lt;b>;Léonard Hussenot&lt;/b>;, &lt;b>;Orgad Keller&lt;/b>;, &lt;b>;Nikola Momchev&lt;/b>;, &lt;b>;Sabela Ramos&lt;/b>;, &lt;b>;Piotr Stanczyk&lt;/b>;, &lt;b>;Nino Vieillard&lt;/b>;, &lt;b>;Olivier Bachem&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Avinatan Hassidim&lt;/b>;, &lt;b>;Olivier Pietquin&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09248.pdf&quot;>;Natural Language to Code Generation in Interactive Data Science Notebooks&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Pengcheng Yin&lt;/b>;, &lt;b>;Wen-Ding Li&lt;/b>;, &lt;b>;Kefan Xiao&lt;/b>;, &lt;b>;Abhishek Rao&lt;/b>;, &lt;b>;Yeming Wen&lt;/b>;, &lt;b>;Kensen Shi&lt;/b>;, &lt;b>;Joshua Howland&lt;/b>;,&lt;b>; Paige Bailey&lt;/b>;, &lt;b>;Michele Catasta&lt;/b>;, &lt;b>;Henryk Michalewski&lt;/b>;, &lt;b>;Oleksandr Polozov&lt;/b>;, &lt;b>;Charles Sutton&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08410.pdf&quot;>;Teaching Small Language Models to Reason&lt;/a>; &lt;br />; &lt;em>;Lucie Charlotte Magister*, &lt;strong>;Jonathan Mallinson&lt;/strong>;, &lt;strong>;Jakub Adamek&lt;/strong>;, &lt;strong>;Eric Malmi&lt;/strong>;, &lt;strong>;Aliaksei Severyn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/assets/pdf/papers/NeuPSL.pdf&quot;>;Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic&lt;/a>; &lt;br />; &lt;em>;Connor Pryor*, &lt;strong>;Quan Yuan&lt;/strong>;, &lt;strong>;Jeremiah Liu&lt;/strong>;, &lt;strong>;Mehran Kazemi&lt;/strong>;, &lt;strong>;Deepak Ramachandran&lt;/strong>;, &lt;strong>;Tania Bedrax-Weiss&lt;/strong>;, Lise Getoor&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10397.pdf&quot;>;A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization&lt;/a>; &lt;br />; &lt;em>;Lining Zhang, Simon Mille, Yufang Hou, &lt;strong>;Daniel Deutsch&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Yixin Liu, Saad Mahamood, &lt;strong>;Sebastian Gehrmann&lt;/strong>;, Miruna Clinciu, Khyathi Raghavi Chandu and João Sedoc&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Industry Track papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18465.pdf&quot;>;Federated Learning of Gboard Language Models with Differential Privacy&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Zheng Xu&lt;/b>;, &lt;b>;Yanxiang Zhang&lt;/b>;, &lt;b>;Galen Andrew&lt;/b>;, &lt;b>;Christopher Choquette&lt;/b>;, &lt;b>;Peter Kairouz&lt;/b>;, &lt;b>;Brendan McMahan&lt;/b>;, &lt;b>;Jesse Rosenstock&lt;/b>;, &lt;b>;Yuanbo Zhang&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18373.pdf&quot;>;KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models&lt;/a>; &lt;br />; &lt;em>;Zhiwei Jia*, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Arjun Akula&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, Hao Su, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;ACL Findings papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10622.pdf&quot;>;Multilingual Summarization with Factual Consistency Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Jonathan Herzig&lt;/b>;, &lt;b>;Elizabeth Clark&lt;/b>;, &lt;b>;Mirella Lapata &lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06767.pdf&quot;>;Parameter-Efficient Fine-Tuning for Robust Continual Multilingual Learning&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Kartikeya Badola&lt;/b>;, &lt;b>;Shachi Dave&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08153.pdf&quot;>;FiDO: Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference&lt;/a>; &lt;br />; &lt;em>;Michiel de Jong*, &lt;strong>;Yury Zemlyanskiy&lt;/strong>;, &lt;strong>;Joshua Ainslie&lt;/strong>;, &lt;strong>;Nicholas FitzGerald&lt;/strong>;, &lt;strong>;Sumit Sanghai&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;William Cohen&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00609.pdf&quot;>;A Simple, Yet Effective Approach to Finding Biases in Code Generation&lt;/a>; &lt;br />; &lt;em>;Spyridon Mouselinos, Mateusz Malinowski, &lt;strong>;Henryk Michalewski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09261.pdf&quot;>;Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mirac Suzgun&lt;/b>;, &lt;b>;Nathan Scales&lt;/b>;, &lt;b>;Nathanael Scharli&lt;/b>;, &lt;b>;Sebastian Gehrmann&lt;/b>;, &lt;b>;Yi Tay&lt;/b>;, &lt;b>;Hyung Won Chung&lt;/b>;,&lt;b>; Aakanksha Chowdhery&lt;/b>;, &lt;b>;Quoc Le&lt;/b>;, &lt;b>;Ed Chi&lt;/b>;, &lt;b>;Denny Zhou&lt;/b>;, &lt;b>;Jason Wei&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.07730.pdf&quot;>;QueryForm: A Simple Zero-Shot Form Entity Query Framework&lt;/a>; &lt;br />; &lt;em>;Zifeng Wang*, &lt;strong>;Zizhao Zhang&lt;/strong>;, &lt;strong>;Jacob Devlin&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Guolong Su&lt;/strong>;, &lt;strong>;Hao Zhang&lt;/strong>;, Jennifer Dy, &lt;strong>;Vincent Perot&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10703.pdf&quot;>;ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09682.pdf&quot;>;Multilingual Sequence-to-Sequence Models for Hebrew NLP&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Matan Eyal&lt;/b>;, &lt;b>;Hila Noga&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.04009.pdf&quot;>;Triggering Multi-Hop Reasoning for Question Answering in Language Models Using Soft Prompts and Random Walks&lt;/a>; &lt;br />; &lt;em>;Kanishka Misra*, &lt;strong>;Cicero Nogueira dos Santos&lt;/strong>;, Siamak Shakeri&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Complex Reasoning in Natural Language&lt;/a>;&lt;br />; &lt;em>;Wenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, &lt;strong>;Aman Madaan&lt;/strong>;, Tao Yu&lt;/em>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Generating Text from Language Models&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Afra Amini&lt;/b>;, &lt;b>;Ryan Cotterell&lt;/b>;, &lt;b>;John Hewitt&lt;/b>;, &lt;b>;Clara Meister&lt;/b>;, &lt;b>;Tiago Pimentel&lt;/b>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/sustainlp2023&quot;>;Simple and Efficient Natural Language Processing (SustaiNLP)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Tal Schuster&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.workshopononlineabuse.com/&quot;>;Workshop on Online Abuse and Harms (WOAH)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Aida Mostafazadeh Davani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://doc2dial.github.io/workshop2023/&quot;>;Document-Grounded Dialogue and Conversational Question Answering (DialDoc)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/5thnlp4convai/home?authuser=0&quot;>;NLP for Conversational AI&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cawl.wellformedness.com/&quot;>;Computation and Written Language (CAWL)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;b>;Kyle Gorman&lt;/b>;, &lt;b>;Brian Roark&lt;/b>;, &lt;b>;Richard Sproat&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sigmorphon.github.io/workshops/2023/&quot;>;Computational Morphology and Phonology (SIGMORPHON)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Kyle Gorman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/umass.edu/wnu2023&quot;>;Workshop on Narrative Understanding (WNU)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7093565002706757508/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-acl-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-acl-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ACL 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s72-c/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6926843365781180400&lt;/id>;&lt;published>;2023-07-07T11:01:00.001-07:00&lt;/published>;&lt;updated>;2023-07-07T11:09:20.724-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;video&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Modular visual question answering via code generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sanjay Subramanian, PhD student, UC Berkeley, and Arsha Nagrani, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://visualqa.org/&quot;>;Visual question answering&lt;/a>; (VQA) is a machine learning task that requires a model to answer a question about an image or &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;a set of images&lt;/a>;. Conventional VQA approaches need a large amount of labeled training data consisting of thousands of human-annotated question-answer pairs associated with images. In recent years, advances in large-scale pre-training have led to the development of VQA methods that perform well with &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;fewer than fifty training examples&lt;/a>; (few-shot) and &lt;a href=&quot;https://ai.googleblog.com/2022/07/rewriting-image-captions-for-visual.html&quot;>;without any human-annotated VQA training data&lt;/a>; (zero-shot). However, there is still a significant performance gap between these methods and state-of-the-art fully supervised VQA methods, such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html&quot;>;MaMMUT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;. In particular, few-shot methods struggle with spatial reasoning, counting, and multi-hop reasoning. Furthermore, few-shot methods have generally been limited to answering questions about single images. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To improve accuracy on VQA examples that involve complex reasoning, in “&lt;a href=&quot;https://arxiv.org/abs/2306.05392&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>;,” to appear at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, we introduce CodeVQA, a framework that answers visual questions using program synthesis. Specifically, when given a question about an image or set of images, CodeVQA generates a Python program (code) with simple visual functions that allow it to process images, and executes this program to determine the answer. We demonstrate that in the few-shot setting, CodeVQA outperforms prior work by roughly 3% on the &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; dataset and 2% on the &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; dataset. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;CodeVQA&lt;/h2>; &lt;p>; The CodeVQA approach uses a code-writing large language model (LLM), such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PALM&lt;/a>;, to generate &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;Python&lt;/a>; programs (code). We guide the LLM to correctly use visual functions by crafting a prompt consisting of a description of these functions and fewer than fifteen “in-context” examples of visual questions paired with the associated Python code for them. To select these examples, we compute embeddings for the input question and of all of the questions for which we have annotated programs (a randomly chosen set of fifty). Then, we select questions that have the highest similarity to the input and use them as in-context examples. Given the prompt and question that we want to answer, the LLM generates a Python program representing that question. &lt;/p>; &lt;p>; We instantiate the CodeVQA framework using three visual functions: (1) &lt;code>;query&lt;/code>;, (2) &lt;code>;get_pos&lt;/code>;, and (3) &lt;code>;find_matching_image&lt;/code>;. &lt;/p>; &lt;ul>; &lt;li>;&lt;code>;Query&lt;/code>;, which answers a question about a single image, is implemented using the few-shot &lt;a href=&quot;https://arxiv.org/abs/2210.08773&quot;>;Plug-and-Play VQA&lt;/a>; (PnP-VQA) method. PnP-VQA generates captions using &lt;a href=&quot;https://arxiv.org/abs/2201.12086&quot;>;BLIP&lt;/a>; — an image-captioning &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformer&lt;/a>; pre-trained on millions of image-caption pairs — and feeds these into a LLM that outputs the answers to the question. &lt;/li>;&lt;li>;&lt;code>;Get_pos&lt;/code>;, which is an object localizer that takes a description of an object as input and returns its position in the image, is implemented using &lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GradCAM&lt;/a>;. Specifically, the description and the image are passed through the BLIP joint text-image encoder, which predicts an image-text matching score. GradCAM takes the gradient of this score with respect to the image features to find the region most relevant to the text. &lt;/li>;&lt;li>;&lt;code>;Find_matching_image&lt;/code>;, which is used in multi-image questions to find the image that best matches a given input phrase, is implemented by using BLIP text and image encoders to compute a text embedding for the phrase and an image embedding for each image. Then the dot products of the text embedding with each image embedding represent the relevance of each image to the phrase, and we pick the image that maximizes this relevance. &lt;/li>; &lt;/ul>; &lt;p>; The three functions can be implemented using models that require very little annotation (eg, text and image-text pairs collected from the web and a small number of VQA examples). Furthermore, the CodeVQA framework can be easily generalized beyond these functions to others that a user might implement (eg, object detection, image segmentation, or knowledge base retrieval). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s1024/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;501&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the CodeVQA method. First, a large language model generates a Python program (code), which invokes visual functions that represent the question. In this example, a simple VQA method (&lt;code>;query&lt;/code>;) is used to answer one part of the question, and an object localizer (&lt;code>;get_pos&lt;/code>;) is used to find the positions of the objects mentioned. Then the program produces an answer to the original question by combining the outputs of these functions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; The CodeVQA framework correctly generates and executes Python programs not only for single-image questions, but also for multi-image questions. For example, if given two images, each showing two pandas, a question one might ask is, “Is it true that there are four pandas?” In this case, the LLM converts the counting question about the pair of images into a program in which an object count is obtained for each image (using the &lt;em>;query&lt;/em>; function). Then the counts for both images are added to compute a total count, which is then compared to the number in the original question to yield a yes or no answer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s1080/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We evaluate CodeVQA on three visual reasoning datasets: &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; (single-image), &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; (multi-image), and &lt;a href=&quot;https://lil.nlp.cornell.edu/nlvr/&quot;>;NLVR2&lt;/a>; (multi-image). For GQA, we provide 12 in-context examples to each method, and for COVR and NLVR2, we provide six in-context examples to each method. The table below shows that CodeVQA improves consistently over the baseline few-shot VQA method on all three datasets. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;GQA&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;COVR&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;NLVR2&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.56 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.06 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;63.37 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.03 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;54.11 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64.04 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the GQA, COVR, and NLVR2 datasets, showing that CodeVQA consistently improves over few-shot PnP-VQA. The metric is exact-match accuracy, ie, the percentage of examples in which the predicted answer exactly matches the ground-truth answer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that in GQA, CodeVQA&#39;s accuracy is roughly 30% higher than the baseline on spatial reasoning questions, 4% higher on “and” questions, and 3% higher on “or” questions. The third category includes multi-hop questions such as “Are there salt shakers or skateboards in the picture?”, for which the generated program is shown below. &lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;img = open_image(&quot;Image13.jpg&quot;) salt_shakers_exist = query(img, &quot;Are there any salt shakers?&quot;) skateboards_exist = query(img, &quot;Are there any skateboards?&quot;) if salt_shakers_exist == &quot;yes&quot; or skateboards_exist == &quot;yes&quot;: answer = &quot;yes&quot; else: answer = &quot;no&quot; &lt;/font>;&lt;/pre>; &lt;br />; &lt;p>; In COVR, we find that CodeVQA&#39;s gain over the baseline is higher when the number of input images is larger, as shown in the table below. This trend indicates that breaking the problem down into single-image questions is beneficial. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Number of images&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;3&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>;&amp;nbsp; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;91.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;51.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;47.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;75.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.2 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present CodeVQA, a framework for few-shot visual question answering that relies on code generation to perform multi-step visual reasoning. Exciting directions for future work include expanding the set of modules used and creating a similar framework for visual tasks beyond VQA. We note that care should be taken when considering whether to deploy a system such as CodeVQA, since vision-language models like the ones used in our visual functions &lt;a href=&quot;https://arxiv.org/abs/2108.02818&quot;>;have been shown to exhibit social biases&lt;/a>;. At the same time, compared to monolithic models, CodeVQA offers additional interpretability (through the Python program) and controllability (by modifying the prompts or visual functions), which are useful in production systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was a collaboration between &lt;a href=&quot;https://bair.berkeley.edu/baircommons.html&quot;>;UC Berkeley&#39;s Artificial Intelligence Research lab&lt;/a>; (BAIR) and Google Research, and was conducted by Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6926843365781180400/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/modular-visual-question-answering-via.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/modular-visual-question-answering-via.html&quot; rel=&quot;alternate&quot; title=&quot;Modular visual question answering via code generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-561916049750681872&lt;/id>;&lt;published>;2023-07-06T12:50:00.000-07:00&lt;/published>;&lt;updated>;2023-07-06T12:50:04.339-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Pic2Word: Mapping pictures to words for zero-shot composed image retrieval&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kuniaki Saito, Student Researcher, Google Research, Cloud AI Team, and Kihyuk Sohn, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s1100/Pic2Word.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Image retrieval plays a crucial role in search engines. Typically, their users rely on either image or text as a query to retrieve a desired target image. However, text-based retrieval has its limitations, as describing the target image accurately using words can be challenging. For instance, when searching for a fashion item, users may want an item whose specific attribute, eg, the color of a logo or the logo itself, is different from what they find in a website. Yet searching for the item in an existing search engine is not trivial since precisely describing the fashion item by text can be challenging. To address this fact, &lt;a href=&quot;https://arxiv.org/pdf/1812.07119.pdf&quot;>;composed image retrieval&lt;/a>; (CIR) retrieves images based on a query that combines both an image and a text sample that provides instructions on how to modify the image to fit the intended retrieval target. Thus, CIR allows precise retrieval of the target image by combining image and text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, CIR methods require large amounts of labeled data, ie, triplets of a 1) query image, 2) description, and 3) target image. Collecting such labeled data is costly, and models trained on this data are often tailored to a specific use case, limiting their ability to generalize to different datasets. &lt;/p>; &lt;p>; To address these challenges, in “&lt;a href=&quot;https://arxiv.org/abs/2302.03084&quot;>;Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval&lt;/a>;”, we propose a task called zero-shot CIR (ZS-CIR). In ZS-CIR, we aim to build a single CIR model that performs a variety of CIR tasks, such as &lt;a href=&quot;https://www.zheyuanliu.me/CIRR/&quot;>;object composition,&lt;/a>; &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;attribute editing&lt;/a>;, or domain conversion, without requiring labeled triplet data. Instead, we propose to train a retrieval model using large-scale image-caption pairs and unlabeled images, which are considerably easier to collect than supervised CIR datasets at scale. To encourage reproducibility and further advance this space, we also &lt;a href=&quot;https://github.com/google-research/composed_image_retrieval&quot;>;release the code&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s1062/image9.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;342&quot; data-original-width=&quot;1062&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Description of existing composed image retrieval model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCWi9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s949/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;476&quot; data-original-width=&quot;949&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCWi9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We train a composed image retrieval model using image-caption data only. Our model retrieves images aligned with the composition of the query image and text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Method overview&lt;/h2>; &lt;p>; We propose to leverage the language capabilities of the language encoder in the &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;contrastive language-image pre-trained model&lt;/a>; (CLIP), which excels at generating semantically meaningful language embeddings for a wide range of textual concepts and attributes. To that end, we use a lightweight mapping sub-module in CLIP that is designed to map an input picture (eg, a photo of a cat) from the image embedding space to a word token (eg, “cat”) in the textual input space. The whole network is optimized with the vision-language contrastive loss to again ensure the visual and text embedding spaces are as close as possible given a pair of an image and its textual description. Then, the query image can be treated as if it is a word. This enables the flexible and seamless composition of query image features and text descriptions by the language encoder. We call our method Pic2Word and provide an overview of its training process in the figure below. We want the mapped token &lt;em>;s&lt;/em>; to represent the input image in the form of word token. Then, we train the mapping network to reconstruct the image embedding in the language embedding, &lt;em>;p&lt;/em>;. Specifically, we optimize the contrastive loss proposed in CLIP computed between the visual embedding &lt;em>;v&lt;/em>; and the textual embedding &lt;em>;p&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYHXfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s611/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;611&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYHXfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Training of the mapping network (&lt;em>;f&lt;sub>;M&lt;/sub>;&lt;/em>;) using unlabeled images only. We optimize only the mapping network with a frozen visual and text encoder.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Given the trained mapping network, we can regard an image as a word token and pair it with the text description to flexibly compose the joint image-text query as shown in the figure below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s827/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-width=&quot;827&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With the trained mapping network, we regard the image as a word token and pair it with the text description to flexibly compose the joint image-text query.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We conduct a variety of experiments to evaluate Pic2Word&#39;s performance on a variety of CIR tasks. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Domain conversion&lt;/h3>; &lt;p>; We first evaluate the capability of compositionality of the proposed method on domain conversion — given an image and the desired new image domain (eg, sculpture, origami, cartoon, toy), the output of the system should be an image with the same content but in the new desired image domain or style. As illustrated below, we evaluate the ability to compose the category information and domain description given as an image and text, respectively. We evaluate the conversion from real images to four domains using &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; and &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>;. &lt;/p>; &lt;p>; To compare with approaches that do not require supervised training data, we pick three approaches: (i) &lt;em>;image only&lt;/em>; performs retrieval only with visual embedding, (ii) &lt;em>;text only &lt;/em>;employs only text embedding, and (iii) &lt;em>;image + text &lt;/em>;averages the visual and text embedding to compose the query. The comparison with (iii) shows the importance of composing image and text using a language encoder. We also compare with &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;Combiner&lt;/a>;, which trains the CIR model on &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; or &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRR&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s965/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;513&quot; data-original-width=&quot;965&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We aim to convert the domain of the input query image into the one described with text, eg, origami.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As shown in figure below, our proposed approach outperforms baselines by a large margin. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s754/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;754&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results (&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;recall&lt;/a>;@10, ie, the percentage of relevant instances in the first 10 images retrieved.) on composed image retrieval for domain conversion.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Fashion attribute composition&lt;/h3>; &lt;p>; Next, we evaluate the composition of fashion attributes, such as the color of cloth, logo, and length of sleeve, using the &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; dataset. The figure below illustrates the desired output given the query. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s818/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;309&quot; data-original-width=&quot;818&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of CIR for fashion attributes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In the figure below, we present a comparison with baselines, including supervised baselines that utilized triplets for training the CIR model: (i) &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;CB&lt;/a>; uses the same architecture as our approach, (ii) &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRPLANT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2203.08101.pdf&quot;>;ALTEMIS&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2007.00145.pdf&quot;>;MAAF&lt;/a>; use a smaller backbone, such as ResNet50. Comparison to these approaches will give us the understanding on how well our zero-shot approach performs on this task. &lt;/p>; &lt;p>; Although CB outperforms our approach, our method performs better than supervised baselines with smaller backbones. This result suggests that by utilizing a robust CLIP model, we can train a highly effective CIR model without requiring annotated triplets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s807/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;498&quot; data-original-width=&quot;807&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results (&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;recall&lt;/a>;@10, ie, the percentage of relevant instances in the first 10 images retrieved.) on composed image retrieval for Fashion-IQ dataset (higher is better). Light blue bars train the model using triplets. Note that our approach performs on par with these supervised baselines with shallow (smaller) backbones.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Qualitative results&lt;/h3>; &lt;p>; We show several examples in the figure below. Compared to a baseline method that does not require supervised training data (text + image feature averaging), our approach does a better job of correctly retrieving the target image. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s1999/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Qualitative results on diverse query images and text description.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; In this article, we introduce Pic2Word, a method for mapping pictures to words for ZS-CIR. We propose to convert the image into a word token to achieve a CIR model using only an image-caption dataset. Through a variety of experiments, we verify the effectiveness of the trained model on diverse CIR tasks, indicating that training on an image-caption dataset can build a powerful CIR model. One potential future research direction is utilizing caption data to train the mapping network, although we use only image data in the present work. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Also thanks to Zizhao Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/561916049750681872/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/pic2word-mapping-pictures-to-words-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/pic2word-mapping-pictures-to-words-for.html&quot; rel=&quot;alternate&quot; title=&quot;Pic2Word: Mapping pictures to words for zero-shot composed image retrieval&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s72-c/Pic2Word.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-667019077470952746&lt;/id>;&lt;published>;2023-06-29T14:08:00.000-07:00&lt;/published>;&lt;updated>;2023-06-29T14:08:24.386-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Announcing the first Machine Unlearning Challenge&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Fabian Pedregosa and Eleni Triantafillou, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s900/Unlearning.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Deep learning has recently driven tremendous progress in a wide array of applications, ranging from &lt;a href=&quot;https://imagen.research.google/&quot;>;realistic image generation&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;impressive retrieval systems&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/ai/bard-google-ai-search-updates/&quot;>;language models that can hold human-like conversations&lt;/a>;. While this progress is very exciting, the widespread use of deep neural network models requires caution: as guided by Google&#39;s AI &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Principles&lt;/a>;, we seek to develop AI technologies responsibly by understanding and mitigating potential risks, such as the propagation and amplification of unfair biases and protecting user privacy. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Fully erasing the influence of the data requested to be deleted is challenging since, aside from simply deleting it from databases where it&#39;s stored, it also requires erasing the influence of that data on other artifacts such as trained machine learning models. Moreover, recent research [&lt;a href=&quot;https://arxiv.org/abs/1610.05820&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;2&lt;/a>;] has shown that in some cases it may be possible to infer with high accuracy whether an example was used to train a machine learning model using &lt;a href=&quot;https://en.wikipedia.org/wiki/Adversarial_machine_learning#Model_extraction&quot;>;membership inference attacks&lt;/a>; (MIAs). This can raise privacy concerns, as it implies that even if an individual&#39;s data is deleted from a database, it may still be possible to infer whether that individual&#39;s data was used to train a model. &lt;/p>; &lt;p>; Given the above, &lt;em>;machine unlearning&lt;/em>; is an emergent subfield of machine learning that aims to remove the influence of a specific subset of training examples — the &quot;forget set&quot; — from a trained model. Furthermore, an ideal unlearning algorithm would remove the influence of certain examples &lt;em>;while maintaining&lt;/em>; other beneficial properties, such as the accuracy on the rest of the train set and generalization to held-out examples. A straightforward way to produce this unlearned model is to retrain the model on an adjusted training set that excludes the samples from the forget set. However, this is not always a viable option, as retraining deep models can be computationally expensive. An ideal unlearning algorithm would instead use the already-trained model as a starting point and efficiently make adjustments to remove the influence of the requested data. &lt;/p>; &lt;p>; Today we&#39;re thrilled to announce that we&#39;ve teamed up with a broad group of academic and industrial researchers to organize the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>;. The competition considers a realistic scenario in which after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. The competition will be hosted on &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>;, and submissions will be automatically scored in terms of both forgetting quality and model utility. We hope that this competition will help advance the state of the art in machine unlearning and encourage the development of efficient, effective and ethical unlearning algorithms. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Machine unlearning applications&lt;/h2>; &lt;p>; Machine unlearning has applications beyond protecting user privacy. For instance, one can use unlearning to erase inaccurate or outdated information from trained models (eg, due to errors in labeling or changes in the environment) or remove harmful, manipulated, or outlier data. &lt;/p>; &lt;p>; The field of machine unlearning is related to other areas of machine learning such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1802.07569&quot;>;life-long learning&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Fairness_(machine_learning)&quot;>;fairness&lt;/a>;. Differential privacy aims to guarantee that no particular training example has too large an influence on the trained model; a stronger goal compared to that of unlearning, which only requires erasing the influence of the designated forget set. Life-long learning research aims to design models that can learn continuously while maintaining previously-acquired skills. As work on unlearning progresses, it may also open additional ways to boost fairness in models, by correcting unfair biases or disparate treatment of members belonging to different groups (eg, demographics, age groups, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s720/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Anatomy of unlearning.&lt;/b>; An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the &quot;forget set&quot;). From the model, forget set, and retain set, the unlearning algorithm produces an updated model. An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Challenges of machine unlearning&lt;/h2>; &lt;p>; The problem of unlearning is complex and multifaceted as it involves several conflicting objectives: forgetting the requested data, maintaining the model&#39;s utility (eg, accuracy on retained and held-out data), and efficiency. Because of this, existing unlearning algorithms make different trade-offs. For example, full retraining achieves successful forgetting without damaging model utility, but with poor efficiency, while &lt;a href=&quot;https://arxiv.org/abs/2007.02923&quot;>;adding noise&lt;/a>; to the weights achieves forgetting at the expense of utility. &lt;/p>; &lt;p>; Furthermore, the evaluation of forgetting algorithms in the literature has so far been highly inconsistent. While some &lt;a href=&quot;https://arxiv.org/abs/1911.04933&quot;>;works&lt;/a>; report the classification accuracy on the samples to unlearn, &lt;a href=&quot;https://proceedings.mlr.press/v119/wu20b.html&quot;>;others&lt;/a>; report distance to the fully retrained model, and yet others use the error rate of membership inference attacks as a metric for forgetting quality [&lt;a href=&quot;https://arxiv.org/abs/2302.09880&quot;>;4&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2010.10981&quot;>;5&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.02205&quot;>;6&lt;/a>;]. &lt;/p>; &lt;p>; We believe that the inconsistency of evaluation metrics and the lack of a standardized protocol is a serious impediment to progress in the field — we are unable to make direct comparisons between different unlearning methods in the literature. This leaves us with a myopic view of the relative merits and drawbacks of different approaches, as well as open challenges and opportunities for developing improved algorithms. To address the issue of inconsistent evaluation and to advance the state of the art in the field of machine unlearning, we&#39;ve teamed up with a broad group of academic and industrial researchers to organize the first unlearning challenge. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Announcing the first Machine Unlearning Challenge&lt;/h2>; &lt;p>; We are pleased to announce the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>;, which will be held as part of the &lt;a href=&quot;https://neurips.cc/Conferences/2023/CompetitionTrack&quot;>;NeurIPS 2023 Competition Track.&lt;/a>; The goal of the competition is twofold. First, by unifying and standardizing the evaluation metrics for unlearning, we hope to identify the strengths and weaknesses of different algorithms through apples-to-apples comparisons. Second, by opening this competition to everyone, we hope to foster novel solutions and shed light on open challenges and opportunities. &lt;/p>; &lt;p>; The competition will be hosted on &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; and run between mid-July 2023 and mid-September 2023. As part of the competition, today we&#39;re announcing the availability of the &lt;a href=&quot;https://github.com/unlearning-challenge/starting-kit&quot;>;starting kit&lt;/a>;. This starting kit provides a foundation for participants to build and test their unlearning models on a toy dataset. &lt;/p>; &lt;p>; The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. For this, we will make available as part of the starting kit a dataset of synthetic faces (samples shown below) and we&#39;ll also use several real-face datasets for evaluation of submissions. The participants are asked to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We will also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s2000/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;2000&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Excerpt images from the &lt;a href=&quot;https://github.com/microsoft/FaceSynthetics&quot;>;Face Synthetics&lt;/a>; dataset together with age annotations. The competition considers the scenario in which an age predictor has been trained on face images like the above, and, after training, a certain subset of the training images must be forgotten.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For evaluating forgetting, we will use tools inspired by MIAs, such as &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;LiRA&lt;/a>;. MIAs were first developed in the privacy and security literature and their goal is to infer which examples were part of the training set. Intuitively, if unlearning is successful, the unlearned model contains no traces of the forgotten examples, causing MIAs to fail: the attacker would be &lt;em>;unable&lt;/em>; to infer that the forget set was, in fact, part of the original training set. In addition, we will also use statistical tests to quantify how different the distribution of unlearned models (produced by a particular submitted unlearning algorithm) is compared to the distribution of models retrained from scratch. For an ideal unlearning algorithm, these two will be indistinguishable. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Machine unlearning is a powerful tool that has the potential to address several open problems in machine learning. As research in this area continues, we hope to see new methods that are more efficient, effective, and responsible. We are thrilled to have the opportunity via this competition to spark interest in this field, and we are looking forward to sharing our insights and findings with the community. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. We are writing this blog post on behalf of the organization team of the Unlearning Competition: Eleni Triantafillou*, Fabian Pedregosa* (*equal contribution), Meghdad Kurmanji, Kairan Zhao, Gintare Karolina Dziugaite, Peter Triantafillou, Ioannis Mitliagkas, Vincent Dumoulin, Lisheng Sun Hosoya, Peter Kairouz, Julio CS Jacques Junior, Jun Wan, Sergio Escalera and Isabelle Guyon.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/667019077470952746/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/announcing-first-machine-unlearning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/announcing-first-machine-unlearning.html&quot; rel=&quot;alternate&quot; title=&quot;Announcing the first Machine Unlearning Challenge&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s72-c/Unlearning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9161751123508054082&lt;/id>;&lt;published>;2023-06-29T12:10:00.004-07:00&lt;/published>;&lt;updated>;2023-07-18T14:55:41.583-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;On-device diffusion plugins for conditioned text-to-image generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao and Tingbo Hou, Software Engineers, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s800/MediaPipeDiffusion-hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown great success in text-to-image generation, achieving high image quality, improved inference performance, and expanding our creative inspiration. Nevertheless, it is still challenging to efficiently control the generation, especially with conditions that are difficult to describe with text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we announce &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; diffusion plugins, which enable controllable text-to-image generation to be run on-device. Expanding upon our &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;prior work&lt;/a>; on GPU inference for on-device large generative models, we introduce new low-cost solutions for controllable text-to-image generation that can be plugged into existing diffusion models and their Low-Rank Adaptation (&lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;>;LoRA&lt;/a>;) variants. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s1080/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;780&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Text-to-image generation with control plugins running on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Background&lt;/h2>; &lt;p>; With diffusion models, image generation is modeled as an iterative denoising process. Starting from a noise image, at each step, the diffusion model gradually denoises the image to reveal an image of the target concept. Research shows that leveraging language understanding via text prompts can greatly improve image generation. For text-to-image generation, the text embedding is connected to the model via cross-attention layers. Yet, some information is difficult to describe by text prompts, eg, the position and pose of an object. To address this problem, researchers add additional models into the diffusion to inject control information from a condition image. &lt;/p>; &lt;p>; Common approaches for controlled text-to-image generation include &lt;a href=&quot;https://arxiv.org/abs/2211.12572&quot;>;Plug-and-Play&lt;/a>;, &lt;a href=&quot;https://github.com/lllyasviel/ControlNet&quot;>;ControlNet&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2302.08453&quot;>;T2I Adapter&lt;/a>;. Plug-and-Play applies a widely used denoising diffusion implicit model (&lt;a href=&quot;https://arxiv.org/abs/2010.02502&quot;>;DDIM&lt;/a>;) inversion approach that reverses the generation process starting from an input image to derive an initial noise input, and then employs a copy of the diffusion model (860M parameters for Stable Diffusion 1.5) to encode the condition from an input image. Plug-and-Play extracts spatial features with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;self-attention&lt;/a>; from the copied diffusion, and injects them into the text-to-image diffusion. ControlNet creates a trainable copy of the encoder of a diffusion model, which connects via a convolution layer with zero-initialized parameters to encode conditioning information that is conveyed to the decoder layers. However, as a result, the size is large, half that of the diffusion model (430M parameters for Stable Diffusion 1.5). T2I Adapter is a smaller network (77M parameters) and achieves similar effects in controllable generation. T2I Adapter only takes the condition image as input, and its output is shared across all diffusion iterations. Yet, the adapter model is not designed for portable devices. &lt;/p>; &lt;br />; &lt;h2>;The MediaPipe diffusion plugins&lt;/h2>; &lt;p>; To make conditioned generation efficient, customizable, and scalable, we design the MediaPipe diffusion plugin as a separate network that is: &lt;/p>; &lt;ul>; &lt;li>;&lt;i>;Plugable&lt;/i>;: It can be easily connected to a pre-trained base model. &lt;/li>;&lt;li>;&lt;i>;Trained from scratch&lt;/i>;: It does not use pre-trained weights from the base model. &lt;/li>;&lt;li>;&lt;i>;Portable&lt;/i>;: It runs outside the base model on mobile devices, with negligible cost compared to the base model inference. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Parameter Size&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Plugable&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;From Scratch&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Portable&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Plug-and-Play &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;860M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;430M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;T2I Adapter &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;77M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Plug-and-Play, ControlNet, T2I Adapter, and the MediaPipe diffusion plugin.&lt;br />;* The number varies depending on the particulars of the diffusion model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The MediaPipe diffusion plugin is a portable on-device model for text-to-image generation. It extracts multiscale features from a conditioning image, which are added to the encoder of a diffusion model at corresponding levels. When connecting to a text-to-image diffusion model, the plugin model can provide an extra conditioning signal to the image generation. We design the plugin network to be a lightweight model with only 6M parameters. It uses depth-wise convolutions and inverted bottlenecks from &lt;a href=&quot;https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html&quot;>;MobileNetv2&lt;/a>; for fast inference on mobile devices. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s1724/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;628&quot; data-original-width=&quot;1724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the MediaPipe diffusion model plugin. The plugin is a separate network, whose output can be plugged into a pre-trained text-to-image generation model. Features extracted by the plugin are applied to the associated downsampling layer of the diffusion model (blue).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Unlike ControlNet, we inject the same control features in all diffusion iterations. That is, we only run the plugin once for one image generation, which saves computation. We illustrate some intermediate results of a diffusion process below. The control is effective at every diffusion step and enables controlled generation even at early steps. More iterations improve the alignment of the image with the text prompt and generate more detail. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s1280/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the generation process using the MediaPipe diffusion plugin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Examples&lt;/h2>; &lt;p>; In this work, we developed plugins for a diffusion-based text-to-image generation model with MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmark&lt;/a>;, MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmark&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_map&quot;>;depth maps&lt;/a>;, and &lt;a href=&quot;https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html&quot;>;Canny edge&lt;/a>;. For each task, we select about 100K images from a &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;web-scale image-text dataset&lt;/a>;, and compute control signals using corresponding MediaPipe solutions. We use refined captions from &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; for training the plugins. &lt;/p>; &lt;br />; &lt;h3>;Face Landmark&lt;/h3>; &lt;p>; The MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmarker&lt;/a>; task computes 478 landmarks (with attention) of a human face. We use the &lt;a href=&quot;https://github.com/google/mediapipe/blob/26a7ca5c64cd885978677931a7218d33cd7d1dec/mediapipe/python/solutions/drawing_utils.py&quot;>;drawing utils&lt;/a>; in MediaPipe to render a face, including face contour, mouth, eyes, eyebrows, and irises, with different colors. The following table shows randomly generated samples by conditioning on face mesh and prompts. As a comparison, both ControlNet and Plugin can control text-to-image generation with given conditions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s1346/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1018&quot; data-original-width=&quot;1346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Face-landmark plugin for text-to-image generation, compared with ControlNet.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Holistic Landmark&lt;/h3>; &lt;p>; MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmarker&lt;/a>; task includes landmarks of body pose, hands, and face mesh. Below, we generate various stylized images by conditioning on the holistic features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s1351/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;970&quot; data-original-width=&quot;1351&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Holistic-landmark plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Depth&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s1344/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depth-plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Canny Edge&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s1345/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;955&quot; data-original-width=&quot;1345&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Canny-edge plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We conduct a quantitative study of the &lt;em>;face landmark&lt;/em>; plugin to demonstrate the model&#39;s performance. The evaluation dataset contains 5K human images. We compare the generation quality as measured by the widely used metrics, &lt;a href=&quot;https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance&quot;>;Fréchet Inception Distance&lt;/a>; (FID) and &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>; scores. The base model is a pre-trained text-to-image diffusion model. We use &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>; v1.5 here. &lt;/p>; &lt;p>; As shown in the following table, both ControlNet and the MediaPipe diffusion plugin produce much better sample quality than the base model, in terms of FID and CLIP scores. Unlike ControlNet, which needs to run at every diffusion step, the MediaPipe plugin only runs once for each image generated. We measured the performance of the three models on a server machine (with Nvidia V100 GPU) and a mobile phone (Galaxy S23). On the server, we run all three models with 50 diffusion steps, and on mobile, we run 20 diffusion steps using the &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/image_generator&quot;>;MediaPipe image generation app&lt;/a>;. Compared with ControlNet, the MediaPipe plugin shows a clear advantage in inference efficiency while preserving the sample quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Model&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;FID↓&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;CLIP↑&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Inference Time (s)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Nvidia V100&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;10.32 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.26 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.5 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.51 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.31 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.4 (+48%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;18.2 (+58.3%) &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.30 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 (+0.2%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.8 (+2.6%) &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quantitative comparison on FID, CLIP, and inference time.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We test the performance of the plugin on a wide range of mobile devices from mid-tier to high-end. We list the results on some representative devices in the following table, covering both Android and iOS. &lt;/p>; &lt;p>; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Device&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;7&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Android&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;iOS&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 4&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 6&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 7&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 12 Pro&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 13 Pro&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;strong>;Time&lt;/strong>; (ms) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;128 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;68 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;48 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;73 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;63 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Inference time (ms) of the plugin on different mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we present MediaPipe, a portable plugin for conditioned text-to-image generation. It injects features extracted from a condition image to a diffusion model, and consequently controls the image generation. Portable plugins can be connected to pre-trained diffusion models running on servers or devices. By running text-to-image generation and plugins fully on-device, we enable more flexible applications of generative AI. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank all team members who contributed to this work: Raman Sarokin and Juhyun Lee for the GPU inference solution; Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, and Matthias Grundmann for leadership. Special thanks to Jiuqiang Tang&lt;/em>;&lt;i>;, Joe Zou and Lu wang,&lt;/i>;&lt;em>;&amp;nbsp;who made this technology and all the demos running on-device.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/9161751123508054082/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/on-device-diffusion-plugins-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/on-device-diffusion-plugins-for.html&quot; rel=&quot;alternate&quot; title=&quot;On-device diffusion plugins for conditioned text-to-image generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s72-c/MediaPipeDiffusion-hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6036162561497757163&lt;/id>;&lt;published>;2023-06-27T14:19:00.000-07:00&lt;/published>;&lt;updated>;2023-06-27T14:19:04.029-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Unifying image-caption and image-classification datasets with prefix conditioning&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kuniaki Saito, Student Researcher, Cloud AI Team, and Kihyuk Sohn, Research Scientist, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s320/Prefix%20conditioning%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Pre-training visual language (VL) models on web-scale image-caption datasets has recently emerged as a powerful alternative to traditional pre-training on image classification data. Image-caption datasets are considered to be more “open-domain” because they contain broader scene types and vocabulary words, which result in models with &lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;strong performance in few- and zero-shot recognition tasks&lt;/a>;. However, images with fine-grained class descriptions can be rare, and the class distribution can be imbalanced since image-caption datasets do not go through manual curation. By contrast, large-scale classification datasets, such as &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>;, are often curated and can thus provide fine-grained categories with a balanced label distribution. While it may sound promising, directly combining caption and classification datasets for pre-training is often unsuccessful as it can result in biased representations that do not generalize well to various downstream tasks. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2206.01125&quot;>;Prefix Conditioning Unifies Language and Label Supervision&lt;/a>;”, presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we demonstrate a pre-training strategy that uses both classification and caption datasets to provide complementary benefits. First, we show that naïvely unifying the datasets results in sub-optimal performance on downstream zero-shot recognition tasks as the model is affected by dataset bias: the coverage of image domains and vocabulary words is different in each dataset. We address this problem during training through &lt;em>;prefix conditioning&lt;/em>;, a novel simple and effective method that uses prefix tokens to disentangle dataset biases from visual concepts. This approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. Prefix conditioning is a generic method that can be easily integrated into existing VL pre-training objectives, such as &lt;a href=&quot;https://openai.com/research/clip&quot;>;Contrastive Language-Image Pre-training&lt;/a>; (CLIP) or &lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;Unified Contrastive Learning&lt;/a>; (UniCL). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;High-level idea&lt;/h2>; &lt;p>; We note that classification datasets tend to be biased in at least two ways: (1) the images mostly contain single objects from restricted domains, and (2) the vocabulary is limited and lacks the linguistic flexibility required for zero-shot learning. For example, the class embedding of “a photo of a dog” optimized for ImageNet usually results in a photo of one dog in the center of the image pulled from the ImageNet dataset, which does not generalize well to other datasets containing images of multiple dogs in different spatial locations or a dog with other subjects. &lt;/p>; &lt;p>; By contrast, caption datasets contain a wider variety of scene types and vocabularies. As shown below, if a model simply learns from two datasets, the language embedding can entangle the bias from the image classification and caption dataset, which can decrease the generalization in zero-shot classification. If we can disentangle the bias from two datasets, we can use language embeddings that are tailored for the caption dataset to improve generalization. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/s1150/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1150&quot; data-original-width=&quot;856&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/w476-h640/image1.png&quot; width=&quot;476&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Top&lt;/strong>;: Language embedding entangling the bias from image classification and caption dataset. &lt;strong>;Bottom&lt;/strong>;: Language embeddings disentangles the bias from two datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prefix conditioning&lt;/h2>; &lt;p>; Prefix conditioning is partially inspired by &lt;a href=&quot;https://ai.googleblog.com/2022/02/guiding-frozen-language-models-with.html&quot;>;prompt tuning&lt;/a>;, which prepends learnable tokens to the input token sequences to instruct a pre-trained model backbone to learn task-specific knowledge that can be used to solve downstream tasks. The prefix conditioning approach differs from prompt tuning in two ways: (1) it is designed to unify image-caption and classification datasets by disentangling the dataset bias, and (2) it is applied to VL pre-training while the standard prompt tuning is used to fine-tune models. Prefix conditioning is an explicit way to specifically steer the behavior of model backbones based on the type of datasets provided by users. This is especially helpful in production when the number of different types of datasets is known ahead of time. &lt;/p>; &lt;p>; During training, prefix conditioning learns a text token (prefix token) for each dataset type, which absorbs the bias of the dataset and allows the remaining text tokens to focus on learning visual concepts. Specifically, it prepends prefix tokens for each dataset type to the input tokens that inform the language and visual encoder of the input data type (eg, classification vs. caption). Prefix tokens are trained to learn the dataset-type-specific bias, which enables us to disentangle that bias in language representations and utilize the embedding learned on the image-caption dataset during test time, even without an input caption. &lt;/p>; &lt;p>; We utilize prefix conditioning for CLIP using a language and visual encoder. During test time, we employ the prefix used for the image-caption dataset since the dataset is supposed to cover broader scene types and vocabulary words, leading to better performance in zero-shot recognition. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s1038/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;1038&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the Prefix Conditioning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; We apply prefix conditioning to two types of contrastive loss, &lt;a href=&quot;http://proceedings.mlr.press/v139/radford21a&quot;>;CLIP&lt;/a>; and &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf&quot;>;UniCL,&lt;/a>; and evaluate their performance on zero-shot recognition tasks compared to models trained with &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet21K&lt;/a>; (IN21K) and &lt;a href=&quot;https://github.com/google-research-datasets/conceptual-12m&quot;>;Conceptual 12M&lt;/a>; (CC12M). CLIP and UniCL models trained with two datasets using prefix conditioning show large improvements in zero-shot classification accuracy. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s440/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;440&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Zero-shot classification accuracy of models trained with only IN21K or CC12M compared to CLIP and UniCL models trained with both two datasets using prefix conditioning (“Ours”). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on test-time prefix&lt;/h3>; &lt;p>; The table below describes the performance change by the prefix used during test time. We demonstrate that by using the same prefix used for the classification dataset (“Prompt”), the performance on the classification dataset (&lt;a href=&quot;https://www.image-net.org/&quot;>;IN-1K&lt;/a>;) improves. When using the same prefix used for the image-caption dataset (“Caption”), the performance on other datasets (Zero-shot AVG) improves. This analysis illustrates that if the prefix is tailored for the image-caption dataset, it achieves better generalization of scene types and vocabulary words. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/w640-h396/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis of the prefix used for test-time. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on robustness to image distribution shift &lt;/h3>; &lt;p>; We study the shift in image distribution using ImageNet variants. We see that the “Caption” prefix performs better than “Prompt” in &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>; (IN-R) and &lt;a href=&quot;https://github.com/HaohanWang/ImageNet-Sketch&quot;>;ImageNet-Sketch&lt;/a>; (IN-S), but underperforms in &lt;a href=&quot;https://github.com/modestyachts/ImageNetV2&quot;>;ImageNet-V2&lt;/a>; (IN-V2). This indicates that the “Caption” prefix achieves generalization on domains far from the classification dataset. Therefore, the optimal prefix probably differs by how far the test domain is from the classification dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis on the robustness to image-level distribution shift. IN: ImageNet, IN-V2: ImageNet-V2, IN-R: Art, Cartoon style ImageNet, IN-S: ImageNet Sketch. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We introduce prefix conditioning, a technique for unifying image caption and classification datasets for better zero-shot classification. We show that this approach leads to better zero-shot classification accuracy and that the prefix can control the bias in the language embedding. One limitation is that the prefix learned on the caption dataset is not necessarily optimal for the zero-shot classification. Identifying the optimal prefix for each test dataset is an interesting direction for future work. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Thanks to Zizhao Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6036162561497757163/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/unifying-image-caption-and-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/unifying-image-caption-and-image.html&quot; rel=&quot;alternate&quot; title=&quot;Unifying image-caption and image-classification datasets with prefix conditioning&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s72-c/Prefix%20conditioning%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;