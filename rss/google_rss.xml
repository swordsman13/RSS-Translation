<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-12-05T22:55:44.361-08:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Health"></category><category term="Robotics"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="AI for Social Good"></category><category term="Security and Privacy"></category><category term="Computer Science"></category><category term="HCI"></category><category term="MOOC"></category><category term="Quantum AI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="accessibility"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="ACL"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="Android"></category><category term="EMNLP"></category><category term="ICML"></category><category term="Physics"></category><category term="TPU"></category><category term="Awards"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Search"></category><category term="Structured Data"></category><category term="video"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Responsible AI"></category><category term="TTS"></category><category term="User Experience"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="Google Accelerated Science"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="Video Analysis"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Diversity"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Vision Research"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="RAI-HCT Highlights"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="grants"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="Google Cloud Platform"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Climate"></category><category term="Computational Imaging"></category><category term="Differential Privacy"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="Large Language Models"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="Weather"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="Graphs"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1315&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8414954450937764241&lt;/id>;&lt;发布>;2023-12-05T17:32:00.000-08:00&lt;/发布>;&lt;更新>;2023-12- 05T19:31:02.159-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;EMNLP&quot;>;&lt; /category>;&lt;title type=&quot;text&quot;>;Google 在 EMNLP 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 项目经理 Malaya Jules &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm- bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s320/EMNLP%202023.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google 很荣幸成为 &lt;a href=&quot;https://2023.emnlp.org/sponsors/&quot;>;钻石赞助商&lt;/a>;。 org/&quot;>;自然语言处理的经验方法&lt;/a>; (EMNLP 2023)，这是一个重要的年度会议，本周在新加坡圣淘沙举行。 Google 在今年的会议上表现强劲，收到了超过 65 篇论文，并积极参与了 11 个研讨会和教程。 Google 也很高兴成为&lt;a href=&quot;https://www.winlp.org/winlp-2023-workshop/winlp-2023-sponsors/&quot;>;主要赞助商&lt;/a>; ://www.google.com/url?q=https://www.winlp.org/winlp-2023-workshop/&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1701403043253017&amp;amp;usg=AOvVaw2oPPnFe0AEcdP1iSblNV91&quot;>;拓宽 NLP &lt;/a>; 研讨会 (WiNLP)，旨在强调人工智能和机器学习领域的人员、观点和文化的全球代表性。我们期待分享我们一些广泛的 NLP 研究，并扩大我们与更广泛的研究界的合作伙伴关系。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 我们希望您能够参观 Google 展位，与积极追求 NLP 最新创新的研究人员交谈，并查看一些预定的展位活动（例如，下面列出的演示和问答环节）。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) 和 &lt;a href=&quot;https://www.linkedin.com/showcase/googleresearch/?viewAsMember =true&quot;>;LinkedIn&lt;/a>; 帐户，了解有关 EMNLP 2023 上 Google 展位活动的更多信息。&lt;/p>; &lt;p>; 请查看下文，了解有关 EMNLP 2023 上展示的 Google 研究的更多信息（Google 附属机构&lt;strong>;粗体&lt;/strong>;）。 &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;板和线高度组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 赞助主席：&lt;strong>;&lt;em>;Shyam Upadyay&lt;/em>;&lt;/strong>; &lt;br />; 行业分会主席：&lt; strong>;&lt;em>;Imed Zitouni&lt;/em>;&lt;/strong>; &lt;br />; 高级项目委员会：&lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt;em>;&lt;strong>;Annie Louis&lt;/ &lt;strong>;&lt;/em>;、&lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Brian Roark&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展台活动&lt;/h2>; &lt;p>; &lt;i>;此时间表可能会发生变化。请访问 Google 展位了解更多信息。&lt;/i>;&lt;/p>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 开发和利用机器翻译和翻译的评估指标。改进多语言 NLP &lt;br />; 演讲者：&lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dan Deutch&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jan -Thorsten Peter&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;David Vilar Torres&lt;/em>;&lt;/strong>; &lt;br />; 12 月 8 日星期五 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; 可微搜索索引和索引生成检索&lt;br />;演讲者：&lt;strong>;&lt;em>;Sanket Vaibhav Mehta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vinh Tran&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; Kai Hui&lt;/em>;&lt;/strong>;、&lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;br />; 12 月 8 日星期五 | 3:30PM -4:00PM SST &lt;/p>; &lt;p>; 单次检索和生成&lt;br />; 演讲者：&lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;Livio Baldini Soares&lt;/em>;&lt;/strong>; &lt;br />; 12 月 9 日，星期六 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; 放大对抗性攻击&lt;br />; 演讲者：&lt;strong>; &lt;em>;Anu Sinha&lt;/em>;&lt;/strong>; &lt;br />; 12 月 9 日，星期六 | 12:30PM -1:45PM SST &lt;/p>; &lt;p>; 自动提示设计：通用自适应提示（请参阅&lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive -prompting-of-large.html&quot;>;博文&lt;/a>;) &lt;br />;演讲者：&lt;strong>;&lt;em>;钱星辰&lt;sup>;*&lt;/sup>;&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;钱星辰&lt;sup>;*&lt;/sup>;&lt;/em>;&lt;/strong>; >;&lt;em>;孙若曦&lt;/em>;&lt;/strong>; &lt;br />; 12 月 9 日，星期六 | 3:30PM -4:00PM SST &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;论文&lt;/h2>; &lt;div style= &quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03291.pdf&quot;>;SynJax：JAX 的结构化概率分布&lt;/a>; &lt;br />; &lt;strong>; &lt;em>;米洛什·斯塔诺耶维奇&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;洛朗·萨特兰&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/ pdf/2311.11077.pdf&quot;>;适配器：用于参数高效和模块化迁移学习的统一库&lt;/a>; &lt;br />; &lt;em>;Clifton Poth&lt;/em>;、&lt;em>;Hannah Sterz&lt;/em>;、&lt;em>; >;Indraneil Paul&lt;/em>;、&lt;em>;Sukannya Purkayastha&lt;/em>;、&lt;em>;Leon Engländer&lt;/em>;、&lt;em>;Timo Imhof&lt;/em>;、&lt;em>;Ivan Vulić&lt;/em>;、&lt;strong >;&lt;em>;塞巴斯蒂安·鲁德&lt;/em>;&lt;/strong>;、&lt;em>;伊琳娜·古列维奇&lt;/em>;、&lt;strong>;&lt;em>;乔纳斯·菲佛&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.08937.pdf&quot;>;DocumentNet：弥合文档预训练中的数据差距&lt;/a>; &lt;br />; &lt;em>;Lijun Yu&lt;/em>;，&lt;strong>; &lt;em>;苗金&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;孙晓宇&lt;/em>;&lt;/strong>;、&lt;em>;陈嘉仪&lt;/em>;、&lt;em>;Alexander Hauptmann&lt;/em>; , &lt;strong>;&lt;em>;戴汉俊&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;魏巍&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// arxiv.org/pdf/2311.08592.pdf&quot;>;AART：人工智能辅助红队为新的法学硕士支持的应用程序提供多样化的数据生成&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Bhaktipriya Radharapu&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;凯文·罗宾逊&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.15239.pdf&quot;>;CRoW：现实世界任务中常识推理的基准测试&lt;/a>; &lt;br />; &lt; em>;Mete Ismayilzada&lt;/em>;、&lt;em>;Debjit Paul&lt;/em>;、&lt;em>;Syrielle Montariol&lt;/em>;、&lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;、&lt;em>;Antoine Bosselut&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11610.pdf&quot;>;大型语言模型可以自我改进&lt;/a>; &lt;br />; &lt;em>;黄嘉欣&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;顾世祥&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;侯乐&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;吴跃新&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;王学智&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;于鸿坤&lt;/em>;&lt;/strong>; , &lt;em>;Jiawei Han&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14767.pdf&quot;>;剖析自回归语言模型中事实关联的回忆&lt;/ a>; &lt;br />; &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Katja Filippova&lt;/em>; em>;&lt;/strong>;，&lt;strong>;&lt;em>;阿米尔·格罗伯森&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10160.pdf&quot;>;停止以纯文本形式上传测试数据：通过评估基准减轻数据污染的实用策略&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong >;、&lt;em>;Omer Goldman&lt;/em>;、&lt;em>;Yoav Goldberg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.16391.pdf&quot;>;选择性标签: 如何从根本上降低文档提取模型的数据标签成本&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;周一超&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;James Bradley Wendt&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;Navneet Potti&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;谢静&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sandeep Tata&lt;/em>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2112.12870.pdf&quot;>;测量自然语言生成模型中的归因&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;汉娜·拉什金&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;维塔利·尼古拉耶夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;马修·拉姆&lt;/em>;&lt;/strong>;、&lt;强>; &lt;em>;洛拉·阿罗约&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;迈克尔·柯林斯&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Dipanjan&lt;/em>; &lt;em>;Das&lt;/ em>;&lt;/strong>;、&lt;strong>; &lt;em>;斯拉夫·彼得罗夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;高拉夫·辛格·托马尔&lt;/em>;&lt;/strong>;、&lt;em>; &lt;strong>;尤利亚·图尔克&lt;/strong>;&lt;/em>;，&lt;strong>;&lt;em>;大卫·雷特&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.02011.pdf &quot;>;逆缩放可以变成 U 形&lt;/a>; &lt;br />; &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong >;、&lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;Quoc Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //arxiv.org/pdf/2305.14282.pdf&quot;>;INSTRUCTSCORE：通过自动反馈进行可解释文本生成评估&lt;/a>; &lt;br />; &lt;em>;Wenda Xu&lt;/em>;，&lt;em>;Danqing Wang&lt;/em>; , &lt;em>;潘亮明&lt;/em>;, &lt;em>;宋振桥&lt;/em>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;, &lt;em>;王威廉杨&lt;/em>;, &lt; em>;李雷&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.14796.pdf&quot;>;论会话问答中对话历史表征的鲁棒性：一项综合研究以及一种新的基于提示的方法&lt;/a>; &lt;br />; &lt;em>;Zorik Gekhman&lt;/em>;、&lt;em>;Nadav Oved&lt;/em>;、&lt;strong>; &lt;em>;Orgad Keller&lt;/em>;&lt;/strong >;，&lt;strong>; &lt;em>;伊丹·斯佩克托&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Roi Reicart&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2208.04347.pdf&quot;>;研究有效扩展 Transformer 以实现长输入汇总&lt;/a>; &lt;br />; &lt;em>;Jason Phang&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>; &lt;em>;赵耀&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Peter J Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2212.09744.pdf&quot;>;DSI++：使用新文档更新变压器内存&lt;/a>; &lt;br />; &lt;em>;Sanket Vaibhav Mehta&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>;&lt;em>;Jai古普塔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; Vinh Q. Tran&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;饶金峰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Marc Najork&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;艾玛·斯特鲁贝尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;唐纳德·梅茨勒&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/ pdf/2305.12029.pdf&quot;>;MultiTurnCleanup：多轮口语对话记录清理基准&lt;/a>; &lt;br />; &lt;em>;沉华&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>;&lt;em >;维琪·扎亚茨&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;约翰·C·罗霍尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丹尼尔·大卫·沃克&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;德克·帕德菲尔德&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EMNLP 的调查结果&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11689.pdf&quot;>;通过自我评估来改进选择性预测法学硕士&lt;/a>; &lt;br />; &lt;em>;Jiefeng Chen&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;Jinsung Yoon&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; Sayna Ebrahimi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sercan O Arik&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Somesh Jha&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.10062.pdf&quot;>;工具辅助生成策略的综合评估&lt;/ a>; &lt;br />; &lt;em>;阿隆·雅科维&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;阿维·卡丘拉鲁&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔纳森·赫齐格&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mor Geva &lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16568.pdf&quot;>;1-PAGER：一次性答案生成和证据检索&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05401.pdf&quot;>;MaXM：走向多语言视觉问答&lt;/a>; &lt;br />; &lt;strong>; &lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;薛林婷、Michal Yarom&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ashish V. Thapliyal&lt;/em>;&lt;/em>;&lt;/em>;强>;,&lt;strong>;&lt;em>;伊丹·斯佩克托&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;朱利安&lt;/em>; &lt;em>;阿姆洛特&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;陈曦&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.18431 v1.pdf&quot;>;SDOH-NLI：从临床记录推断健康社会决定因素的数据集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;，&lt;em>; Eric Loreaux&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;Tal Schuster&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;陈明俊&lt;/em>;&lt;/strong>; ,&lt;strong>; &lt;em>;Alvin Rajkomar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14815.pdf&quot;>;机器阅读理解使用案例-基于推理&lt;/a>; &lt;br />; &lt;em>;Dung Ngoc Thai&lt;/em>;、&lt;em>;Dhruv Agarwal&lt;/em>;、&lt;em>;Mudit Chaudhary&lt;/em>;、&lt;em>;Wenlong Zhao&lt;/em>; 、&lt;em>;Rajarshi Das&lt;/em>;、&lt;em>;Jay-Yoon Lee&lt;/em>;、&lt;em>;Hannaneh Hajishirzi&lt;/em>;、&lt;strong>;&lt;em>;Manzil Zaheer&lt;/em>;&lt;/strong>;、 &lt;em>;Andrew McCallum&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.06897.pdf&quot;>;非洲语言跨语言开放检索问答&lt;/a >; &lt;br />; &lt;em>;Odunayo Ogundepo&lt;/em>;、&lt;em>;Tajuddeen Gwadabe&lt;/em>;、&lt;strong>;&lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; Jonathan H. Clark&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;塞巴斯蒂安·鲁德&lt;/em>;&lt;/strong>;、&lt;em>;David Ifeoluwa Adelani&lt;/em>;、&lt;em>;Bonaventure FP Dossou&lt;/em>; >;、&lt;em>;Abdou Aziz DIOP&lt;/em>;、&lt;em>;Claytone Sikasote&lt;/em>;、&lt;em>;Gilles HACHEME&lt;/em>;、&lt;em>;快乐 Buzaaba&lt;/em>;、&lt;em>;Ignatius Ezeani&lt;/em>; em>;、&lt;em>;Rooweither Mabuya&lt;/em>;、&lt;em>;Salomey Osei&lt;/em>;、&lt;em>;Chris&lt;/em>;、&lt;em>;Chinenye Emezue&lt;/em>;、&lt;em>;Albert Kahira&lt;/em>; 、&lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;、&lt;em>;Akintunde Oladipo&lt;/em>;、&lt;em>;Abraham Toluwase Owodunni&lt;/em>;、&lt;em>;Atnafu Lambebo Tonja&lt;/em>;、&lt;em>;Iyanuoluwa Shode&lt; /em>;、&lt;em>;Akari Asai&lt;/em>;、&lt;em>;Anuoluwapo Aremu&lt;/em>;、&lt;em>;Ayodele Awokoya&lt;/em>;、&lt;em>;Bernard Opoku&lt;/em>;、&lt;em>;Chiamaka Ijeoma Chukwuneke &lt;/em>;、&lt;em>;Christine Mwase&lt;/em>;、&lt;em>;Clemencia Siro&lt;/em>;、&lt;em>;Stephen Arthur&lt;/em>;、&lt;em>;Tunde Oluwaseyi Ajayi&lt;/em>;、&lt;em>;Verrah Akinyi Otiende&lt;/em>;、&lt;em>;Andre Niyongabo Rubungo&lt;/em>;、&lt;em>;Boyd Sinkala&lt;/em>;、&lt;em>;Daniel Ajisafe&lt;/em>;、&lt;em>;Emeka Felix Onwuegbuzia&lt;/em>;、&lt; em>;Falalu Ibrahim Lawan&lt;/em>;、&lt;em>;Ibrahim Said Ahmad&lt;/em>;、&lt;em>;Jesujoba Oluwadara Alabi&lt;/em>;、&lt;em>;CHINEDU EMMANUEL&lt;/em>; &lt;em>;MBONU&lt;/em>;、 &lt;em>;Mofetoluwa Adeyemi&lt;/em>;、&lt;em>;Mofya Phiri&lt;/em>;、&lt;em>;Orevaoghene Ahia&lt;/em>;、&lt;em>;Ruqayya Nasir Iro&lt;/em>;、&lt;em>;Sonia Adhiambo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.08653.pdf&quot;>;概率神经总结中的不确定性校准和选择性生成：基准研究&lt;/a>; &lt;br />; &lt;强>;&lt;em>;Polina Zablotskaia&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杜潘&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;约书亚·梅内斯&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;沙什·纳拉扬&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;任杰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘喆&lt;/em>;&lt;/strong>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.09860.pdf&quot;>;Epsilon 采样摇滚：研究机器翻译最小贝叶斯风险解码的采样策略&lt;/a>; &lt;br / >; &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;、&lt;em>;Behrooz Ghorbani&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>;Patrick Fernandes&lt;sup>;*&lt;/sup>;&lt; /em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14552.pdf&quot;>;推理任务中大型语言模型的幻觉来源&lt;/a>; &lt;br />; &lt;em >;Nick McKenna&lt;/em>;、&lt;em>;李天一&lt;/em>;、&lt;em>;梁程&lt;/em>;、&lt;strong>;&lt;em>;Mohammad Javad Hosseini&lt;/em>;&lt;/strong>;、&lt;em>;Mark约翰逊，&lt;em>;马克·斯蒂德曼&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.09017v2.pdf&quot;>;不要添加，不要&#39; t Miss：有效保留预选文本跨度的内容生成&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、&lt;em>; >;Eran Hirsch&lt;/em>;、&lt;em>;Ido Dagan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07686.pdf&quot;>;是什么造就了 Chain-of-思维提示有效吗？反事实研究&lt;/a>; &lt;br />; &lt;em>;Aman Madaan&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>; &lt;strong>;凯瑟琳·赫尔曼&lt;/strong>;&lt;/em>;、&lt;strong>;&lt; em>;Amir Yazdanbakhsh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03945.pdf&quot;>;使用大型语言模型理解 HTML&lt;/a>; &lt; br />; &lt;strong>;&lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Ofir Nachum&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;苗英杰&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;穆斯塔法·萨夫达里&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;奥斯汀·黄&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha&lt;/em>; &lt; em>;Chowdery&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;诺亚·菲德尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Aleksandra Faust&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09928.pdf&quot;>;通过检测和删除输入来提高摘要模型的鲁棒性噪音&lt;/a>; &lt;br />; &lt;em>;Kundan Krishna&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;姚照&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;任杰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;罗家明&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; >;穆罕默德&lt;/em>; &lt;em>;萨利赫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Peter J. Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://arxiv.org/pdf/2310.15916.pdf&quot;>;情境学习创建任务向量&lt;/a>; &lt;br />; &lt;em>;Roee Hendel&lt;/em>;，&lt;strong>;&lt;em>;Mor Geva&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;阿米尔·格罗伯森&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10544.pdf&quot;>;预-无注意力训练&lt;/a>; &lt;br />; &lt;em>;王俊雄&lt;/em>;、&lt;em>;Jing Nathan Yan&lt;/em>;、&lt;strong>;&lt;em>;Albert Gu&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;Alexander M Rush&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12441.pdf&quot;>;MUX-PLM：数据复用用于高吞吐量语言模型&lt;/a>; &lt;br />; &lt;em>;Vishvak Murahari&lt;/em>;、&lt;em>;Ameet Deshpande&lt;/em>;、&lt;em>;Carlos E Jimenez&lt;/em>;、&lt;em>; &lt;strong >;Izhak Shafran&lt;/strong>;&lt;/em>;、&lt;strong>;&lt;em>;王明秋&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;袁&lt;/em>; &lt;em>;曹&lt;/em>;&lt;/em>; strong>;, &lt;em>;Karthik R Narasimhan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14408.pdf&quot;>;PaRaDe：使用法学硕士演示进行段落排名&lt;/ a>; &lt;br />; &lt;em>;安德鲁·德罗兹多夫&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;庄红雷&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;戴朱云&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;秦臻&lt;/em>;&lt;/strong>;、&lt;em>;Razieh Rahimi&lt;/em>;、&lt;strong>;&lt;em>;王宣辉&lt;/em>;&lt;/strong>; >;、&lt;strong>;&lt;em>;达纳·阿隆&lt;/em>;&lt;/strong>;、&lt;em>;莫希特·艾耶&lt;/em>;、&lt;em>;安德鲁·麦卡勒姆&lt;/em>;、&lt;em>;唐纳德·梅茨勒&lt;sup>;*&lt;/ support>;&lt;/em>;,&lt;strong>; &lt;em>;凯辉&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13678.pdf&quot;>;通过大型语言模型上有限状态解码约束的分段进行长格式语音翻译&lt;/a>; &lt;br />; &lt;em>;Arya D. McCarthy&lt;/em>;，&lt;strong>;&lt;em>;张浩&lt;/em>;&lt; /strong>;,&lt;strong>;&lt;em>;尚卡尔·库马尔&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;菲利克斯·斯塔尔伯格&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;吴克&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07496.pdf&quot;>;使用近似测地线进行无监督意见总结&lt;/a>; &lt;br />; &lt;em>;Somnath Basu罗伊·乔杜里&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;尼古拉斯·莫纳特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;库马尔·阿维纳瓦·杜贝&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;Amr Ahmed&lt;/em>;&lt;/strong>;，&lt;em>;Snigdha Chaturvedi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02883。 pdf&quot;>;SQLPrompt：使用最少标记数据的上下文文本到 SQL&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Sercan O . Arik&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Rajarishi Sinha&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Hootan Nakhost&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; >;戴汉军&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;殷鹏程&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://zi-lin.com/pdf/EMNLP_2023_retrieval.pdf&quot;>;利用结构和不确定性进行复杂图的检索增强解析&lt;/a>; &lt;br />; &lt;em>;子林&lt; /em>;、&lt;strong>;&lt;em>;泉源&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Panupong Pasupat&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jeremiah Zhe Liu&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;尚静波&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.08740.pdf&quot;>;A结构化反射计算机控制的零样本语言智能体&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;李涛&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;李刚&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;邓志伟&lt;/em>;&lt;/strong>;、&lt;em>;王布莱恩&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;李杨&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08371.pdf&quot;>;语言基础中的语用学：现象、任务和建模方法&lt;/a>; &lt;br / >; &lt;em>;丹尼尔·弗里德&lt;/em>;、&lt;em>;尼古拉斯·汤姆林&lt;/em>;、&lt;em>;詹妮弗·胡&lt;/em>;、&lt;strong>; &lt;em>;罗马·帕特尔&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13535.pdf&quot;>;通过主动生成成对反事实来提高分类器的鲁棒性&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ananth Balashankar&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;王学智&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;秦瑶&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Nithum Thain&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;吉林陈&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Ed H.&lt;/em>; &lt;em>;Chi&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Alex Beutel&lt;/em>;&lt;/ strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14224.pdf&quot;>;mmT5：模块化多语言预训练解决源语言幻觉&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;乔纳斯·菲佛&lt;/em>;&lt;/strong>;、&lt;em>; &lt;strong>;弗朗西斯科·皮奇诺&lt;/strong>;&lt;/em>;、&lt;strong>;&lt;em>;马西莫·尼科西亚&lt;/em>;&lt;/strong>;、&lt; strong>;&lt;em>;王欣怡&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Machel Reid&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安&lt;/em>; &lt;em>;Ruder&lt;/ em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.10551.pdf&quot;>;缩放法则与模型架构：归纳偏差如何影响缩放？&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Samira Abnar&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;郑亨元&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;William Fedus&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;饶金峰&lt;/strong>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dani瑜伽塔玛&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;唐纳德·梅茨勒&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00142。 pdf&quot;>;TaTA：非洲语言的多语言表到文本数据集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sebastian Ruder&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;维塔利·尼古拉耶夫&lt;/em>; &lt;em>;Jan A. Botha&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;迈克尔·查文达&lt;/em>;&lt; /strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2305.11938.pdf&quot;>;XTREME-UP：针对代表性不足的语言的以用户为中心的稀缺数据基准&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Ruder ，&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;乔纳森·克拉克&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;亚历山大·古特金&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em >;Mihir Kale&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马敏&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马西莫·尼科西亚&lt;/em>;&lt;/strong>;、&lt;strong>;&lt; em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;帕克·莱利&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jean Michel Amath Sarr&lt;/em>;&lt;/strong>;、&lt;强>; &lt;em>;王欣怡&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;约翰·弗雷德里克&lt;/em>; &lt;em>;Wieting&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Nitish Gupta&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;安娜·卡塔诺娃&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;克里斯托·基洛夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dana L迪金森&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;布莱恩·罗克&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;比迪莎·萨曼塔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;陶康妮&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;David Ifeoluwa Adelani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vera Axelrod&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;艾萨克·雷伯恩&lt;/em>; &lt;em>;卡斯韦尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;科林·切里&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丹·加勒特&lt;/em>; &lt;/strong>;、&lt;strong>; &lt;em>;里夫·英格尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;梅尔文·约翰逊&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;德米特里·潘捷列夫&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14318.pdf&quot;>;q2d ：将问题转化为对话来教模型如何搜索&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shlomi Cohen-Ganor&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;Ido Hakimi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Yoad Lewenberg&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Roee Aharoni&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;Enav Weinreb&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02171.pdf&quot;>;出现具身序列建模中抽象状态表示的研究&lt;/a>; &lt;br />; &lt;em>;Tian Yun&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;em>;Zilai Zeng&lt;/​​em>;，&lt;em>;Kunal Handa&lt; /em>;, &lt;strong>;&lt;em>;Ashish V Thapliyal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bo Pang&lt;/em>;&lt;/strong>;, &lt;em>;Ellie Pavlick&lt;/em>;, &lt; em>;陈孙&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14332.pdf&quot;>;跨语言问答的归因评估和建模&lt;/a>; &lt; br />; &lt;em>;本杰明·穆勒&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;约翰·维廷&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·克拉克&lt;/em>; em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安·鲁德&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;利维奥·巴尔迪尼·苏亚雷斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Xinyi王&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2305.14281.pdf&amp;amp;sa =D&amp;amp;source=docs&amp;amp;ust=1701762357021319&amp;amp;usg=AOvVaw2Q_4he8TIMmr8URRV1w4uX&quot;>;多模态预训练中视觉关系的弱监督学习&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Emanuele Bugliarello&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丽莎·安妮·亨德里克斯&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://arxiv.org/pdf/2305.13286.pdf&quot;>;语言如何相互影响？研究 LM 微调过程中的跨语言数据共享&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni&lt;/em>;、&lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>;、&lt;em>;Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14214.pdf&quot;>;CompoundPiece：评估和提高语言模型的分解性能&lt;/a>; &lt;br>; &lt;本杰明·米尼克斯霍夫 (Benjamin Minixhofer)、&lt;strong>;&lt;em>;乔纳斯·菲佛 (Jonas Pfeiffer)&lt;/em>;&lt;/strong>;、&lt;em>;伊万·武利奇 (Ivan Vulić)&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2302.01328.pdf&quot;>;IC3：委员会共识的图像说明&lt;/a>; &lt;br />; &lt;em>;David Chan&lt;/em>;，&lt;strong>;&lt;em>;Austin Myers&lt;/em>;&lt; /strong>;,&lt;strong>;&lt;em>;Sudheendra Vijayanarasimhan&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;David A Ross&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;John Canny&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11877.pdf&quot;>;幻觉（无法）回答的奇怪案例：在隐藏的状态中寻找真相- 自信的大型语言模型&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;、&lt;em>;Omer Goldman&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、 &lt;em>;Ido Dagan&lt;/em>;、&lt;em>;Shauli Ravfogel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14542.pdf&quot;>;评估大型语言模型受控生成任务研究&lt;/a>; &lt;br />; &lt;em>;孙焦&lt;/em>;、&lt;em>;田宇飞&lt;/em>;、&lt;em>;周望春树&lt;/em>;、&lt;em>;徐楠&lt;/em>; >;, &lt;em>;胡钱&lt;/em>;, &lt;em>;拉胡尔·古普塔&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;em>;彭南云&lt;/em>;, &lt; em>;马学哲&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14324.pdf&quot;>;关系很重要：通过成对精度和关系校准对现代指标进行元评估&lt; /a>; &lt;br />; &lt;strong>;&lt;em>;丹尼尔·多伊奇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔治·福斯特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;马库斯·弗雷塔格&lt; /em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11399.pdf&quot;>;以 0.1% 的额外计算超越缩放定律&lt;/a>; &lt;br />; &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>;郑亨元&lt;sup>;*&lt;/sup>; &lt;/em>;、&lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;、&lt;em>;David R. So&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>; Siamak Shakeri&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;郑怀秀&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;饶金峰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;唐纳德&lt;/em>; &lt;em>;梅茨勒&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;斯拉夫·彼得罗夫&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;尼尔·霍尔斯比&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;Quoc V. Le&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2311.09006.pdf&quot;>;数据相似性不足以解释语言模型的性能&lt;/a>; &lt;br />; &lt;em>;Gregory Yauney&lt;sup>;*&lt;/sup>;&lt;/ em>;、&lt;strong>;&lt;em>;艾米丽·赖夫&lt;/em>;&lt;/strong>;、&lt;em>;大卫·米姆诺&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf /2311.00913.pdf&quot;>;语言模型预训练的自我影响引导数据重新加权&lt;/a>; &lt;br />; &lt;em>;Megh Thakkar&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>;&lt;em>; Tolga Bolukbasi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sriram Ganapathy&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shikhar Vashishth&lt;/em>;&lt;/strong>;、&lt;em>; Sarath Chandar &lt;/em>;，&lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11826.pdf&quot;>;重新标记：推理感知表到分析文本生成&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Deepanway Ghosal&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Preksha Nema&lt;/em>;&lt;/strong>;，&lt; strong>; &lt;em>;Aravindan Raghuveer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15265v1.pdf&quot;>;GATITOS：使用新的多语言词典适用于低资源机器翻译&lt;/a>; &lt;br />; &lt;em>;Alex Jones&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;、&lt; strong>; &lt;em>;Ishank Saxena&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.20201v1.pdf&quot;>;视频有用的多模式机器翻译&lt; /a>; &lt;br />; &lt;em>;李一航、清水秀一郎&lt;/em>;、&lt;em>;褚晨辉&lt;/em>;、&lt;em>;黑桥贞雄&lt;/em>;、&lt;strong>;&lt;em>;李伟&lt;/em>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.08298.pdf&quot;>;符号调优改善语言模型中的情境学习&lt;/a>; &lt;br / >; &lt;em>;Jerry Wei&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;乐厚&lt;/em>;&lt;/strong>;、&lt;em>; &lt;strong>;Andrew Kyle Lampinen&lt;/strong>;&lt; /em>;、&lt;em>;陈向宁&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;大黄&lt;/em>;&lt;/strong>;、&lt;em>;Yi Tay&lt;sup>;*&lt;/ super>;&lt;/em>;、&lt;strong>;&lt;em>;新云&lt;/em>; &lt;em>;陈&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;卢一峰&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;、&lt;em>;马腾宇&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Quoc V Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14755.pdf&quot;>;“不要断章取义！”文体重写对语境模型和评估的需求&lt;/a>; &lt;br />; &lt;em>;Akhila Yerukola&lt;/em>;、&lt;em>;周旭辉&lt;/em>;、&lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>; >;&lt;/strong>;、&lt;em>;Maarten Sap&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08264.pdf&quot;>;QAmeleon：只有 5 个示例的多语言 QA &lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Priyanka Agrawal&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;克里斯·阿尔贝蒂&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Fantine Huot &lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;约书亚·梅内斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;吉马&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安Ruder&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;库兹曼·甘切夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; Mirella Lapata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03540.pdf&quot;>;说、读和提示：高保真文本到-在最低限度监督下的演讲&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;尤金·哈里托诺夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;达米安·文森特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Zalán Borsos&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Raphaël Marinier&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sertan Girgin&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马特·沙里菲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Marco Tagliasacchi&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;Neil Zeghidour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09939.pdf&quot;>;AnyTOD：面向任务的可编程对话系统&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;赵杰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;曹源&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;拉加夫·古普塔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Harrison Lee&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;明秋王&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;哈根索尔陶&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;伊扎克·沙夫兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;吴永辉&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14613.pdf&quot;>;有选择地回答模棱两可的问题&lt;/a>; &lt;br />; &lt;强>;&lt;em>;杰里米·R.科尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;张俊泉&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;丹尼尔·吉利克&lt;/em>;&lt;/ &lt;strong>;、&lt;strong>;&lt;em>;朱利安·马丁·艾森施洛斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Bhuwan Dhingra&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;雅各布·爱森斯坦&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08954.pdf&quot;>;PRESTO：用于解析现实的面向任务的对话框的多语言数据集&lt;/a>;（参见&lt; a href=&quot;https://blog.research.google/2023/03/presto-multilingual-dataset-for-parsing.html&quot;>;博客文章&lt;/a>;）&lt;br />; &lt;strong>;&lt;em>;Rahul Goel &lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;瓦利德·阿马尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Aditya Gupta&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Siddharth Vashishtha&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;佐野元树&lt;/em>;&lt;/strong>;、&lt;em>; Faiz Surani&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em >;Max Chang&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;HyunJeong Choe&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;大卫·格林&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Chuan He&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rattima Nitisaroj&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Anna&lt;/em>; &lt;em>;Trukhina&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shachi Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pararth Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rushin Shah&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;周瑜&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13281.pdf&quot;>;LM vs LM：通过交叉检查检测事实错误&lt;/a>; &lt;br />; &lt;em>;Roi Cohen&lt;/em>;、&lt;em>;May Hamri&lt;/em>;、&lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.03668.pdf&quot;>;生成套件多级多模式网页理解任务&lt;/a>; &lt;br />; &lt;em>;Andrea Burns&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Krishna Srinivasan&lt;/em>;&lt;/strong>; ,&lt;strong>;&lt;em>;约书亚·安斯利&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杰夫·布朗&lt;/em>;&lt;/strong>;、&lt;em>;布莱恩 A.普卢默&lt;/em>;、&lt;em>; Kate&lt;/em>; &lt;em>;Saenko&lt;/em>;、&lt;strong>;&lt;em>;倪建模&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;郭曼迪&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2302.08956.pdf&amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701763216883702&amp;amp;usg =AOvVaw1QisabqC-Gy-U4ou1jbHAY&quot;>;AfriSenti：非洲语言的 Twitter 情绪分析基准&lt;/a>; &lt;br />; &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;、&lt;em>;Idris Abdulmumin&lt;/em>;、&lt;em>; Abinew Ali Ayele&lt;/em>;、&lt;em>;Nedjma Ousidhoum&lt;/em>;、&lt;em>;David Ifeoluwa Adelani&lt;/em>;、&lt;em>;Seid Muhie Yimam&lt;/em>;、&lt;em>;Ibrahim Said Ahmad&lt;/em>; , &lt;em>;Meriem Beloucif&lt;/em>;, &lt;em>;Saif M.&lt;/em>; &lt;em>;Mohammad&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>; Oumaima Hourrane&lt;/em>;、&lt;em>;Alipio Jorge&lt;/em>;、&lt;em>;Pavel Brazdil&lt;/em>;、&lt;em>;Felermino D. M&lt;/em>;。 &lt;em>;A. Ali&lt;/em>;、&lt;em>;Davis David&lt;/em>;、&lt;em>;Salomey Osei&lt;/em>;、&lt;em>;Bello Shehu-Bello&lt;/em>;、&lt;em>;Falalu Ibrahim Lawan&lt;/em>;、&lt; em>;Tajuddeen&lt;/em>; &lt;em>;Gwadabe&lt;/em>;、&lt;em>;Samuel Rutunda&lt;/em>;、&lt;em>;Tadesse Destaw Belay&lt;/em>;、&lt;em>;Wendimu Baye Messelle&lt;/em>;、&lt;em>; >; Hailu Beshada&lt;/em>; &lt;em>;Balcha&lt;/em>;、&lt;em>;Sisay Adugna Chala&lt;/em>;、&lt;em>;Hagos Tesfahun Gebrmichael&lt;/em>;、&lt;em>; Bernard Opoku&lt;/em>;、&lt;em>; >;Stephen Arthur&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13682.pdf&quot;>;通过令牌消除优化检索增强阅读器模型&lt;/a>; &lt;br / >; &lt;em>;Moshe Berchansky&lt;/em>;、&lt;em>;Peter Izsak&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、&lt;em>;Ido Dagan&lt;/em>;、&lt;em>; >;Moshe Wasserblat&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13194.pdf&quot;>;SEAHORSE：用于摘要评估的多语言、多方面数据集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;伊丽莎白·克拉克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安·格尔曼&lt;/em>;&lt;/ &lt;strong>;、&lt;strong>; &lt;em>;约书亚·梅内斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伊·阿哈罗尼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;维塔利·尼古拉耶夫&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Thibault Sellam&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Aditya Siddhan&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Dipanjan Das&lt;/em>; &lt;/strong>;，&lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13245.pdf&quot;>;GQA ：从多头检查点训练广义多查询变压器模型&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;James Lee-Thorp&lt;/a>; &lt;em>; em>;&lt;/strong>;、&lt;em>;米契尔·德容&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;尤里·泽姆良斯基&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;费德里科勒布朗&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;苏米特桑海&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09752。 pdf&quot;>;CoLT5：具有条件计算的更快的远程变压器&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;陶雷&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;米契尔·德容&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;圣地亚哥·翁塔农&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;悉达多·梵天&lt;/em>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;尤里·泽姆良斯基&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;David Uthus&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;郭曼迪&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;詹姆斯·李-索普&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;宋云轩&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf /2310.16523.pdf&quot;>;通过集体批评和自我投票提高大型语言模型中人口表征的多样性&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>;,&lt;strong >; &lt;em>;尼古拉斯·布鲁姆&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;小马&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Raghavendra Kotikalapudi&lt;/em>;&lt;/strong>;、&lt; strong>;&lt;em>;Sahitya Potluri&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Qijun Tan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Hansa Srinivasan&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;本·帕克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;艾哈迈德·贝拉米&lt;/em>;&lt;/strong>;、&lt;em>;亚历克斯·博伊特尔&lt;/em>;、&lt;strong>;&lt;em>; Jilin Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14926.pdf&quot;>;通用自适应提示&lt;/a>;（参见&lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;博客文章&lt;/a>;) &lt;br />; &lt;em>;万星辰&lt;sup >;*&lt;/sup>;&lt;/em>;、&lt;em>; &lt;strong>;孙若曦、Hootan Nakhost&lt;/strong>;&lt;/em>;、&lt;strong>;&lt;em>;戴汉俊&lt;/em>;&lt;/strong>;、&lt;strong >;&lt;em>;朱利安·马丁·艾森施洛斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sercan O. Arik&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;托马斯·普菲斯特&lt;/em>;&lt;/strong>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11171.pdf&quot;>;TrueTeacher：使用大型语言模型学习事实一致性评估&lt;/a>; &lt;br />; &lt;strong>;&lt; em>;佐里克·格赫曼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔纳森·赫齐格&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伊·阿哈罗尼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Chen Elkind&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/ pdf/2310.07871.pdf&quot;>;多模态电子健康记录分层预训练&lt;/a>; &lt;br />; &lt;em>;Xiaochen Wang&lt;/em>;，&lt;em>;Junyu Luo&lt;/em>;，&lt;em>;Jiaqi Wang&lt; /em>;、&lt;em>;尹子仪&lt;/em>;、&lt;em>;崔素涵&lt;/em>;、&lt;em>;袁忠&lt;/em>;、&lt;strong>;&lt;em>;王亚庆&lt;/em>;&lt;/strong>; , &lt;em>;马凤龙&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14499.pdf&quot;>;NAIL：具有高效非自回归解码器的词汇检索索引&lt;/ a>; &lt;br />; &lt;strong>;&lt;em>;利维奥·巴尔迪尼·苏亚雷斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;丹尼尔·吉利克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杰里米·R.科尔&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;汤姆·科维亚特科斯基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11841。 pdf&quot;>;生成检索如何扩展到数百万个段落？&lt;/a>; &lt;br />; &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Kai Hui&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;庄红雷&lt;/em>;&lt;/strong>;、&lt;em>;林志颖&lt;/em>;、&lt;strong>;&lt;em>;唐纳德·梅茨勒&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.13959.pdf&quot;>;让每个例子都有意义：论自我影响力从嘈杂的 NLP 中学习的稳定性和实用性数据集&lt;/a>; &lt;br />; &lt;em>;Irina Bejan&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Artem Sokolov&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; Katja Filippova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/&amp;amp;sa=D&amp;amp;source =docs&amp;ust=1701752203060961&amp;amp;usg=AOvVaw3sRozMNVYuwvyXeLluOgKI&quot;>;第七届拓宽 NLP 研讨会&lt;/a>;（WiNLP）&lt;br />; 主要赞助商&lt;br />; 组织者：&lt;strong>;&lt;em>;Sunipa Dev&lt;/em>;&lt;/ strong>; &lt;br />; 小组成员：&lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ crac2023/?pli=1&quot;>;第六届参考、照应和共指计算模型研讨会&lt;/a>;（CRAC）&lt;br />;特邀演讲嘉宾：&lt;strong>;&lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nlposs.github.io/2023/index.html&quot;>;第三届自然语言处理开源软件研讨会&lt;/a>;（NLP-OSS）&lt;br />; 主办方：&lt;strong>;&lt;em>;Geeticka Chauhan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://splu-robonlp-2023.github.io/&quot;>;联合研讨会机器人空间语言理解与基础沟通&lt;/a>; (SpLU-RoboNLP) &lt;br />; 特邀演讲嘉宾：&lt;strong>;&lt;em>;Andy Zeng&lt;/​​em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gem-benchmark.com/workshop&quot;>;自然语言生成、评估和度量&lt;/a>;（GEM）&lt;br />;组织者：&lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arabicnlp2023.sigarab.org/&quot;>;首届阿拉伯语自然语言处理会议&lt;/a>;（ArabicNLP）&lt;br />;主办方：&lt;strong>; &lt;em>;Imed Zitouni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.bigpictureworkshop.com/&quot;>;大局：制定研究叙述&lt;/a>; ( BigPicture）&lt;br />;组织者：&lt;strong>;&lt;em>;Nora Kassner&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://blackboxnlp.github.io/&quot;>;BlackboxNLP 2023：第六届 NLP 分析和解释神经网络研讨会&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Najoung Kim&lt;/em >;&lt;/strong>; &lt;br />; 小组成员：&lt;strong>;&lt;em>;Neel Nanda&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.conll.org/2023 &quot;>;SIGNLL 计算自然语言学习会议&lt;/a>; (CoNLL) &lt;br />; 联合主席：&lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt;/strong>; &lt;br />; 领域和 AC：&lt; strong>;&lt;em>;Kyle Gorman&lt;/em>;&lt;/strong>;（语音与音韵学）、&lt;strong>;&lt;em>;刘飞&lt;/em>;&lt;/strong>;（自然语言生成）&lt;/p>; &lt;p>; &lt; a href=&quot;https://sigtyp.github.io/ws2023-mrl.html&quot;>;第三届多语言表征学习研讨会&lt;/a>;（MRL）&lt;br />;主办方：&lt;strong>;&lt;em>;Omer Goldman&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;br />; 特邀演讲嘉宾：&lt;strong>;&lt;em>;Orhan Firat&lt;/em>;&lt;/strong>; &lt; /p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://emnlp2023-creative-nlg.github.io/&quot;>;创意自然语言生成&lt;/a>; &lt;br />;组织者：&lt;em>;Tuhin Chakrabarty&lt;sup>;*&lt;/sup >;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font- size:small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/ 8414954450937764241/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at -emnlp-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/8414954450937764241&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot; self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp-2023.html&quot; rel=&quot;alternate&quot; title=&quot; Google 在 EMNLP 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply @blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img /b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl /AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQF VpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s72-c/EMNLP%202023.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3665865722098768988&lt;/id>;&lt;已发布>;2023-12-04T14:41 :00.000-08:00&lt;/已发布>;&lt;更新>;2023-12-04T14:46:06.688-08:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;物理&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;量子计算&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;一种具有指数加速的经典力学新量子算法&lt;/stitle>;&lt;content type=&quot; html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院量子 AI 团队研究科学家 Robin Kothari 和 Rolando Somma&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWs xVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s1100/glued-trees.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 量子计算机承诺解决一些问题的速度比传统计算机快得多，但只有少数例子具有如此显着的加速，例如 &lt;a href=&quot;https://en.wikipedia.org/wiki /Shor%27s_algorithm&quot;>;Shor 因式分解算法&lt;/a>;和&lt;a href=&quot;https://blog.research.google/2023/10/developing-industrial-use-cases-for.html&quot;>;量子模拟&lt;/一个>;。在这几个例子中，大多数都涉及模拟本质上是量子力学的物理系统——这是量子计算机的自然应用。但是模拟本质上不是量子的系统又如何呢？量子计算机能否为此提供指数级优势？ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://link.aps.org/doi/10.1103/PhysRevX.13.041041&quot;>;模拟中的指数量子加速耦合经典振荡器&lt;/a>;”，发表于 &lt;a href=&quot;https://journals.aps.org/prx/&quot;>;Physical Review X&lt;/a>; (PRX) 并在&lt;a href=&quot;https ://focs.computer.org/2023/&quot;>;计算机科学基础研讨会&lt;/a>; (FOCS 2023)，我们报告了一种新量子算法的发现，该算法为模拟耦合提供了指数级优势&lt;a href= “https://en.wikipedia.org/wiki/Harmonic_oscillator&quot;>;经典谐振子&lt;/a>;。这些是自然界中一些最基本、普遍存在的系统，可以描述无数自然系统的物理学，从电路到分子振动再到桥梁力学。我们与麦考瑞大学的 Dominic Berry 和多伦多大学的 Nathan Wiebe 合作，发现了一种映射，可以将任何涉及耦合振荡器的系统转化为描述量子系统时间演化的问题。在某些限制下，使用量子计算机解决这个问题的速度比使用经典计算机快得多。此外，我们使用这种映射来证明，任何可以通过量子算法有效解决的问题都可以重新转换为涉及耦合振荡器网络的问题，尽管耦合振荡器的数量呈指数级增长。除了解锁以前未知的量子计算机应用之外，这一结果还提供了一种通过纯粹推理经典系统来设计新量子算法的新方法。 &lt;/p>; &lt;br />; &lt;h2>;模拟耦合振荡器&lt;/h2>; &lt;p>;我们考虑的系统由经典谐振子组成。单谐振子的一个例子是连接到弹簧上的质量（例如球）。如果将质量块从其静止位置移开，则弹簧将产生恢复力，沿相反方向推动或拉动质量块。该恢复力导致质量块来回振荡。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE 19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/s359/oscillator.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;116&quot; data-original-width=&quot;359&quot; height=&quot;129&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXr O1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/w400-h129/oscillator.gif&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;谐振子的一个简单示例是通过弹簧连接到墙壁的质量。 [图片来源：&lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Simple_harmonic_oscillator.gif&quot;>;维基媒体&lt;/a>;]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 现在考虑&lt;em>;耦合&lt;/em>;谐波振荡器，其中&lt;em>;多个&lt;/em>;质量通过弹簧相互连接。移动一个质量，就会引起一波振荡穿过系统。正如人们所预料的那样，在经典计算机上模拟大量质量的振荡变得越来越困难。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNV VBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkuKE-mFxJ3e9sdM-ct3lU81GS1 /s1129/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;832&quot; data-original-width=&quot;1129&quot; 高度=“295”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8s VxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkuKE-mFxJ3e9sdM-ct3lU81Gs1/w400-h295/image4.png&quot;宽度=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过弹簧连接的质量示例系统，可以使用以下命令进行模拟量子算法。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;为了能够模拟大量耦合谐振子，我们提出了一个映射，对所有谐振子的位置和速度进行编码质量和弹簧进入量子位系统的量子波函数。由于描述量子位系统波函数的参数数量随着量子位数量呈指数增长，因此我们可以将 &lt;em>;N&lt;/em>; 个球的信息编码到仅约 log(&lt;em>;N &lt;/em>;）量子位。只要对系统有一个紧凑的描述（即质量和弹簧的属性），我们就可以演化波函数来稍后学习球和弹簧的坐标，并且所用的资源比我们使用时要少得多模拟球和弹簧的朴素经典方法。 &lt;/p>; &lt;p>; 我们证明了一类耦合经典振荡器系统可以在量子计算机上有效地模拟。但仅此一点并不能排除存在某种尚不为人知的巧妙经典算法的可能性，该算法在资源使用方面同样高效。为了证明我们的量子算法比任何可能的经典算法实现指数加速，我们提供了两个额外的证据。 &lt;/p>; &lt;br />; &lt;h2>;粘合树问题和量子预言&lt;/h2>; &lt;p>; 对于第一个证据，我们使用我们的映射来表明量子算法可以有效地解决一个著名的问题关于已知难以经典解决的图，称为&lt;a href=&quot;https://arxiv.org/abs/quant-ph/0209131&quot;>;粘合树问题&lt;/a>;。该问题需要两个分支树（一个图，其节点每个分支到另外两个节点，类似于树的分支路径）并通过一组随机边将它们的分支粘合在一起，如下图所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6 mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s930/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;930&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0- 69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;粘合树问题的直观表示。在这里，我们从标记为 ENTRANCE 的节点开始，并允许本地探索该图，该图是通过将两个二叉树随机粘合在一起而获得的。目标是找到标记为 EXIT 的节点。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 粘合树问题的目标是找到退出节点 - 的“根”第二棵树——尽可能高效。但粘合树的节点和边缘的确切配置最初对我们来说是隐藏的。要了解系统，我们必须查询&lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_machine&quot;>;oracle&lt;/a>;，它可以回答有关设置的具体问题。这个神谕允许我们探索树木，但仅限于本地。几十年前，&lt;a href=&quot;https://doi.org/10.1145/780542.780552&quot;>;研究表明&lt;/a>;在经典计算机上找到退出节点所需的查询数量与多项式因子成正比&lt;em>;N&lt;/em>;，节点总数。 &lt;/p>; &lt;p>; 但是将其重新转换为球和弹簧的问题，我们可以将每个节点想象为一个球，将两个节点之间的每个连接想象为一个弹簧。拨动入口节点（第一棵树的根），振荡将穿过树木。只需花费与树的&lt;em>;深度&lt;/em>;成比例的时间（比&lt;em>;N&lt;/em>;指数小）即可到达出口节点。因此，通过将胶合树球弹簧系统映射到量子系统并对其进行演化，我们可以检测出口节点的振动，并比使用经典计算机更快地确定它。 &lt;/p>; &lt;br />; &lt;h2>;BQP 完整性&lt;/h2>; &lt;p>; 通过检查一组问题，我们揭示了我们的算法比任何可能的经典算法效率指数级更高的第二个也是最有力的证据：量子计算机可以高效求解（即可以在&lt;a href=&quot;https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time&quot;>;多项式时间&lt;/a>;内求解），简称为&lt;a href=&quot;https: //en.wikipedia.org/wiki/BQP&quot;>;有界误差量子多项式时间&lt;/a>;或BQP。 BQP 中最难的问题称为“BQP-complete”。 &lt;/p>; &lt;p>; 虽然人们普遍认为存在一些量子算法可以有效解决而经典算法无法解决的问题，但这一点尚未得到证实。所以，我们能提供的最好证据是我们的问题是 BQP 完备的，也就是说，它是 BQP 中最难的问题之一。如果有人找到一种有效的经典算法来解决我们的问题，那么量子计算机有效解决的每个问题都将是经典可解决的！甚至连构成&lt;a href= “https://en.wikipedia.org/wiki/RSA_(cryptosystem)&quot;>;现代加密&lt;/a>;，并由 Shor 算法著名地解决，预计将是 BQP 完整的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0 ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBET4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s574/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;563&quot; data-original-width=&quot;574&quot; height=&quot;314&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXi WgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s320/image1.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;显示 BPP 和 BQP 类的可信关系的图表，它们是可以有效解决的问题集分别在经典计算机和量子计算机上解决。 BQP 完备问题是 BQP 中最难的问题。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 为了证明我们模拟球和弹簧的问题确实是 BQP 完备的，我们从模拟通用量子电路的标准 BQP 完全问题，并表明每个量子电路都可以表示为许多与弹簧耦合的球的系统。因此，我们的问题也是 BQP 完备的。 &lt;/p>; &lt;br />; &lt;h2>;影响和未来的工作&lt;/h2>; &lt;p>; 这项工作还揭示了 2002 年的工作，当时理论计算机科学家 Lov K. Grover 和他的同事 Anirvan M. Sengupta 使用&lt;a href=&quot;https://journals.aps.org/pra/abstract/10.1103/PhysRevA.65.032319&quot;>;耦合摆的类比&lt;/a>;来说明 Grover 著名的量子&lt;a href=&quot;https:// en.wikipedia.org/wiki/Grover%27s_algorithm&quot;>;搜索算法&lt;/a>;可以比传统方法更快地在未排序的数据库中找到正确的元素。通过正确的设置和初始条件，在系统进化一段时间后，就有可能判断一个&lt;em>;N&lt;/em>;摆是否与其他摆不同——类似于在数据库中找到正确的元素那只是~√(&lt;em>;N)&lt;/em>;。虽然这暗示了某些经典振荡系统和量子算法之间的联系，但它无法解释为什么格罗弗的量子算法实现了量子优势。 &lt;/p>; &lt;p>; 我们的结果使这种联系更加精确。我们证明，任何经典谐振子系统的动力学确实可以等价地理解为相应的指数较小的量子系统的动力学。这样我们就可以在 log(&lt;em>;N&lt;/em>;) 个量子位的量子计算机上模拟 Grover 和 Sengupta 的钟摆系统，并找到一种不同的量子算法，可以及时找到正确的元素 ~√(&lt;em>; N&lt;/em>;)。我们在经典系统和量子系统之间发现的类比可用于构建其他提供指数加速的量子算法，其中加速的原因现在从经典波的传播方式中更加明显。 &lt;/p>; &lt;p>; 我们的工作还表明，每种量子算法都可以等效地理解为经典波在耦合振荡器系统中的传播。这意味着，例如，我们原则上可以构建一个经典系统，在它的演化时间比任何已知的解决因式分解的经典算法的运行时间小得多的时间之后，它可以解决因式分解问题。这可能看起来像是一种高效的经典因式分解算法，但问题是振荡器的数量呈指数级增长，这使得它成为解决因式分解的不切实际的方法。耦合谐振子在自然界中无处不在，描述了从电路到分子链再到桥梁等结构的广泛系统。虽然我们在这里的工作重点是这类广泛问题的基本复杂性，但我们希望它将指导我们寻找谐振子问题的现实示例，在这些示例中量子计算机可以提供指数优势。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢我们的量子计算科学传播者 Katie McCormick 帮助撰写这篇博文。&lt;/em>; &lt;/em>; &lt;/p>; p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3665865722098768988/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; 类型=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;链接 href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog .research.google/2023/12/a-new-quantum-algorithm-for-classical.html&quot; rel=&quot;alternate&quot; title=&quot;一种具有指数加速的经典力学新量子算法&quot; type=&quot;text/html&quot; />;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image高度=“16” rel=“http://schemas.google.com/g/2005#thumbnail” src=“https://img1.blogblog.com/img/b16-rounded.gif” 宽度=“16” >;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPke O9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s72 -c/glued-trees.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total >;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-68038970217457115&lt;/id>;&lt;已发布>;2023-12-04T10:00:00.000-08:00&lt;/已发布>; &lt;更新>;2023-12-04T11:29:40.224-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ads&quot;>;&lt;/category>; &lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“优化”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;安全和隐私&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Privacy Sandbox 归因报告 API 中的摘要报告优化&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author &quot;>;发布者：Google 软件工程师 Hidayet Aksu 和研究科学家 Adam Sealfon&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpW Qo5mT0Vdplv5uBFQmdSePT1LPlwN-hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s320 /hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 近年来，推出了&lt;a href=&quot;https://privacysandbox.com/&quot;>;隐私沙盒&lt;/a>;计划，旨在探索广告商如何负责任地衡量其广告活动的有效性，其目标是&lt;a href=&quot;https://blog.chromium.org/2020/01/building-more-private-web-path-towards.html&quot;>;弃用第三方 Cookie&lt;/a>;（受&lt;a href =&quot;https://www.gov.uk/cma-cases/investigation-into-googles-privacy-sandbox-browser-changes&quot;>;与英国竞争和市场管理局解决任何竞争问题&lt;/a>;）。 &lt;a href=&quot;https://en.wikipedia.org/wiki/HTTP_cookie#Third-party_cookie&quot;>;Cookie&lt;/a>; 是网站存储在用户设备上的包含用户偏好设置的小数据；它们可用于提供更好的浏览体验（例如，允许用户自动登录）并提供相关内容或广告。 Privacy Sandbox 试图通过提供隐私保护替代方案来解决有关使用 cookie 跟踪整个网络浏览数据的问题。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 许多浏览器使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;差分隐私&lt;/a>;（DP ）提供隐私保护 API，例如&lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/attribution-reporting&quot;>;归因报告 API&lt;/a>; (ARA)，不依赖 cookie 来衡量广告转化。 ARA 会对个人用户操作进行加密，并将其收集到聚合的&lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/&quot;>;摘要报告&lt;/a>;中，该报告会估计测量目标，例如归因于广告活动的转化（网站上的有用操作，例如购买或注册邮件列表）的数量和价值。 &lt;/p>; &lt;p>; 配置 API 参数的任务（例如，在不同的转化之间分配贡献预算）对于最大化摘要报告的效用非​​常重要。在“&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;Privacy Sandbox 归因报告 API 中的摘要报告优化&lt;/a>;”中，我们介绍了用于对摘要报告进行建模的正式数学框架。然后，我们将最大化摘要报告效用的问题表述为优化问题，以获得最佳 ARA 参数。最后，我们使用真实和合成数据集评估该方法，并证明与基线非优化摘要报告相比，实用性显着提高。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ARA 摘要报告&lt;/h2>; &lt;p>; 我们使用以下示例来说明我们的符号。想象一家名为 &lt;em>;Du &amp;amp; 的虚构礼品店。 Penc&lt;/em>; 使用数字广告来吸引客户。下表记录了他们的假日销售情况，其中每条记录都包含展示次数特征，其中包括 (i) 展示次数 ID、(ii) 广告活动和 (iii) 展示广告的城市，以及转化特征，包括 (i)购买的物品数量以及 (ii) 这些物品的总美元价值。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqrhtaPxDlj8o5sbubBQLdCq_RRnT2ZPmhxuvEIC_9SvLfhKkXh1t_elS4eRCQyHv2pYX4YkAX1UkEX3c3BdbB 5PejqLF0uq_MAXuXKVA1YPKVnZ5tqertr02eNDFjJ0nnoeD_rGdDFWxLrTsCNGXOE9LrGSsuPhrtZ6SgqRomWRjpun1JdrwWDfnfJF0n/s1035/image4.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;397&quot; data-original-width=&quot;1035&quot; height=&quot;245&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqrhtaPxDlj8o5sbubBQLdCq_RRnT2ZPmhxuvEIC_9SvLfhKkXh1t_elS4eRCQyHv2pYX4YkAX1UkEX3c3BdbB5PejqLF0uq_MAXuXKVA1YPKVnZ5 tqertr02eNDFjJ0nnoeD_rGdDFWxLrTsCNGXOE9LrGSsuPhrtZ6SgqRomWRjpun1JdrwWDfnfJF0n/w640-h245/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;Du &amp;amp; 的展示和转化功能日志Penc.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;数学模型&lt; /h3>; &lt;p>; ARA 摘要报告可以通过四种算法建模：(1) 贡献向量，(2) &lt;a href=&quot;https://github.com/WICG/attribution-reporting-api/blob/main/ AGGREGATE.md#contribution-bounding-and-budgeting&quot;>;贡献边界&lt;/a>;，(3) &lt;a href=&quot;https://github.com/WICG/attribution-reporting-api/blob/main/AGGREGATION_SERVICE_TEE。 md?&quot;>;摘要报告&lt;/a>;，以及 (4) 重构值。贡献边界和摘要报告由 ARA 执行，而贡献向量和重构值由 AdTech 提供商（使企业能够购买和销售数字广告的工具和系统）执行。这项工作的目的是协助广告技术公司优化摘要报告算法。 &lt;/p>; &lt;p>; 贡献向量算法将测量结果转换为离散化和缩放的 ARA 格式。缩放需要考虑每次展示的总体贡献限制。在这里，我们提出了一种剪辑并执行随机舍入的方法。该算法的结果是可聚合键和值的直方图。 &lt;/p>; &lt;p>; 接下来，贡献边界算法在客户端设备上运行，并对归因报告强制执行贡献边界，其中任何超出限制的进一步贡献都会被丢弃。输出是归因转化的直方图。 &lt;/p>; &lt;p>; 摘要报告算法在&lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot;>;可信执行环境&lt;/a>;内的服务器端运行，并返回嘈杂的聚合结果满足DP。噪声是从离散的&lt;a href=&quot;https://en.wikipedia.org/wiki/Laplace_distribution&quot;>;拉普拉斯分布&lt;/a>;中采样的，并且为了执行隐私预算，可以仅查询报告一次。 &lt;/p>; &lt;p>; 最后，重建值算法将测量值转换回原始比例。重建值和贡献向量算法由 AdTech 设计，两者都会影响从摘要报告中收到的效用。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKys IXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s1999/image1.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7 spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;ARA 摘要报告的说明性用法，其中包括贡献向量（算法 A）、贡献边界（算法 C）、摘要报告（算法 S）和重构值（算法 R）。算法C和S在API中是固定的。 AdTech 设计了 ​​A 和 R。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt; h2>;误差度量&lt;/h2>; &lt;p>;在选择误差度量来评估近似质量时需要考虑几个因素。为了选择特定的度量，我们考虑了误差度量的理想属性，该属性可以进一步用作目标函数。考虑到所需的属性，我们选择了&lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/design-decisions/#rmsre&quot;>;𝜏-截断均方根相对误差&lt;/ a>; (RMSRE&lt;sub>;𝜏&lt;/sub>;) 作为其属性的误差指标。有关详细讨论以及与其他可能指标的比较，请参阅&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;优化&lt;/h2>; &lt;p>; 根据 RMSRE 测量优化效用&lt;sub>;𝜏&lt;/ sub>;，我们为每个切片选择一个上限参数 C 和隐私预算 𝛼。两者的组合决定了如何在 AdTech 端对实际测量（例如总价值为 3 美元的两次转化）进行编码，然后传递给 ARA 进行贡献边界算法处理。 RMSRE&lt;sub>;𝜏&lt;/sub>; 可以精确计算，因为它可以用限幅偏差和噪声分布方差来表示。按照这些步骤，我们发现固定隐私预算 𝛼&lt;sub>; &lt;/sub>; 或上限参数 C 的 RMSRE&lt;sub>;𝜏&lt;/sub>; 是 &lt;a href=&quot;https://en. wikipedia.org/wiki/Convex_optimization&quot;>;凸&lt;/a>;（因此可以有效地获得其他参数的误差最小化值），而对于联合变量（C，𝛼），它变成非凸的（因此我们可能不会始终能够选择最佳的参数）。在任何情况下，任何现成的优化器都可以用于选择隐私预算和上限参数。在我们的实验中，我们使用&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html&quot;>;SLSQP&lt;/a>;最小化器https://docs.scipy.org/doc/scipy/reference/ generated/scipy.optimize.minimize.html&quot;>;scipy.optimize&lt;/a>; 库。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;合成数据&lt;/h2>; &lt;p>; 不同的 ARA 配置可以通过在转换数据集。然而，由于隐私问题，对此类数据的访问可能会受到限制或缓慢，或者根本不可用。解决这些限制的一种方法是使用复制真实数据特征的合成数据。 &lt;/p>; &lt;p>; 我们提出了一种通过现实世界转换数据集的统计建模来负责任地生成合成数据的方法。我们首先对真实转换数据集进行实证分析，以揭示 ARA 的相关特征。然后，我们设计一个管道，使用这种分布知识来创建可以通过输入参数进行自定义的真实合成数据集。 &lt;/p>; &lt;p>; 管道首先根据&lt;a href=&quot;https://en.wikipedia.org/wiki/Power_law&quot;>;幂律分布&lt;/a>;生成展示次数（第 1 步），然后每次展示都会根据&lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_distribution&quot;>;泊松分布&lt;/a>;生成转化（第 2 步），最后，对于每次转化，它会生成提取的转化值来自&lt;a href=&quot;https://en.wikipedia.org/wiki/Log-normal_distribution&quot;>;对数正态分布&lt;/a>;（步骤 3）。通过与数据集相关的参数，我们发现这些分布与广告数据集特征密切匹配。因此，人们可以从历史或公共数据集中学习参数并生成用于实验的合成数据集。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_ acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s841 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;841&quot; data-original-width=&quot;840&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_ aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;总体数据集生成步骤以及用于说明的功能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>; 实验评估 &lt;/h2>; &lt;p>; 我们在三个真实数据集上评估我们的算法（&lt;a href=&quot; https://ailab.criteo.com/criteo-spoke-search-conversion-log-dataset/&quot;>;Criteo&lt;/a>;、AdTech Real Estate 和 AdTech Travel）和三个综合数据集。 Criteo 包含 1500 万次点击，房地产包含 10 万次转化，旅游包含 3 万次转化。每个数据集被划分为训练集和测试集。训练集用于选择贡献预算、裁剪阈值参数和转化计数限制（现实数据集每次点击只有一次转化），并在测试集上评估误差。每个数据集都使用印象特征划分为切片。对于现实世界的数据集，我们考虑每个切片的三个查询；对于合成数据集，我们考虑每个切片的两个查询。 &lt;/p>; &lt;p>; 对于每个查询，我们选择 RMSRE&lt;sub>;𝝉&lt;/sub>; 𝜏 值作为训练数据集查询中值的五倍。这确保了误差度量对数据重新缩放的不变性，并允许我们通过对每个特征使用 𝝉 来组合来自不同尺度特征的误差。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm 5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s1971/image5.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1028&quot; data-original-width=&quot;1971&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUBNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6t KKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;现实世界数据集的散点图，说明观察转换值的概率。拟合曲线代表了最佳对数正态分布模型，可以有效捕获数据中的基本模式。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;结果&lt;/h3>; &lt;p>; 我们将基于优化的算法与简单的基线方法进行比较。对于每个查询，基线使用相等的贡献预算和训练数据的固定分位数来选择裁剪阈值。我们的算法产生的误差远低于现实世界和合成数据集的基线。我们基于优化的方法适应隐私预算和数据。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMi hk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s1999/image3 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1730&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x 0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RMSRE&lt;sub>;τ&lt;/sub>; 用于我们的算法和基线的隐私预算 {1, 2, 4, 8, 16, 32, 64}三个真实世界数据集和三个合成数据集。我们基于优化的方法始终比使用固定分位数作为裁剪阈值并在查询之间平均分配贡献预算的基线实现更低的错误。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们研究了 ARA 中摘要报告的优化，目前已部署在数百个设备上数以百万计的 Chrome 浏览器。我们提出了 ARA 缴款预算优化问题的严格表述，目的是为研究人员提供稳健的抽象，以促进实际改进。 &lt;/p>; &lt;p>; 我们的方案利用历史数据来限制和扩展差异隐私下未来数据的贡献，非常通用，适用于广告以外的设置。基于这项工作的一种方法是使用过去的数据来学习数据分布的参数，然后应用从该分布导出的合成数据来为未来数据的查询进行隐私预算。请参阅&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;论文&lt;/a>;和&lt;a href=&quot;https://github.com/google-research/google-research/tree/ master/ara_optimization&quot;>;附带代码&lt;/a>;，了解详细的算法和证明。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是与 Badih Ghazi 合作完成的，普里蒂什·卡马斯、拉维·库马尔、帕辛·马努朗西和阿维纳什·瓦拉达拉詹。我们感谢 Akash Nadan 的帮助。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/68038970217457115/comments/default&quot; rel=&quot;replies&quot; title= “发表评论” type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/summary-report-optimization-in-privacy.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/68038970217457115&quot; rel=&quot;edit&quot;类型=“application/atom+xml”/>;&lt;link href=“http://www.blogger.com/feeds/8474926331452026626/posts/default/68038970217457115”rel=“self”类型=“application/atom+xml” />;&lt;link href=&quot;http://blog.research.google/2023/12/summary-report-optimization-in-privacy.html&quot; rel=&quot;alternate&quot; title=&quot;Privacy Sandbox 归因报告中的摘要报告优化API&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger。 com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16- rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5 mT0Vdplv5uBFQmdSePT1LPlwN- hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:总计>;0&lt;/ thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8288223977991952319&lt;/id>;&lt;发布>;2023-12-01T09:19:00.000-08:00&lt; /published>;&lt;更新>;2023-12-05T09:38:21.187-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器翻译&quot;>; &lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“语音”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom” /ns#&quot; term=&quot;Translate&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;单语言数据的无监督语音到语音翻译&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;发布者：Google 研究部研究科学家 Eliya Nachmani 和软件工程师 Michelle Tadmor Ramanovich&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2 zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1 -3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s1600/T3.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 语音转语音翻译 (S2ST) 是一种&lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;>;机器翻译&lt;/a>;，它将口语从一种语言转换为一种语言。语言到另一种语言。这项技术有潜力打破语言障碍，促进不同文化和背景的人们之间的沟通。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 之前，我们介绍了&lt;a href=&quot;https://ai.googleblog.com/2019/05/introducing-translatotron-end-to- end.html&quot;>;翻译机 1&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html&quot;>;翻译机 2&lt;/a>; ，第一个能够直接在两种语言之间翻译语音的模型。然而，他们是在监督环境中使用并行语音数据进行训练的。并行语音数据的稀缺是该领域的一个主要挑战，以至于大多数公共数据集都是从文本半或完全合成的。这给学习翻译和重建语音属性增加了额外的障碍，这些属性没有在文本中表示，因此没有反映在合成的训练数据中。 &lt;/p>; &lt;p>; 在这里，我们展示了 &lt;a href=&quot;https://arxiv.org/abs/2305.17547&quot;>;Translatotron 3&lt;/a>;，一种新颖的无监督语音到语音翻译架构。在 Translatotron 3 中，我们证明可以仅从单语言数据学习语音到语音翻译任务。这种方法不仅为更多语言对之间的翻译打开了大门，而且还为非文本语音属性（例如停顿、语速和说话者身份）的翻译打开了大门。我们的方法不包括对目标语言的任何直接监督，因此我们认为这是在翻译过程中保留源语音的副语言特征（例如语气、情感）的正确方向。为了实现语音到语音的翻译，我们使用&lt;a href=&quot;https://arxiv.org/abs/1511.06709&quot;>;反向翻译&lt;/a>;，这是一种来自无监督机器翻译 (UMT) 的技术，其中源语言的合成翻译用于&lt;a href=&quot;https://arxiv.org/abs/1710.11041&quot;>;翻译没有双语文本数据集的文本&lt;/a>;。西班牙语和英语之间的语音到语音翻译任务的实验结果表明，Translatotron 3 的性能优于基线级联系统。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Translatotron 3 &lt;/h2>; &lt;p>; Translatotron 3 解决了无监督 S2ST 的问题，可以消除双语语音数据集的要求。为此，Translatotron 3 的设计包含三个关键方面：&lt;/p>; &lt;ol>; &lt;li>;将整个模型预训练为 &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/html /He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html&quot;>;带有 &lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;>;SpecAugment&lt;/a>; 的掩码自动编码器&lt;/a>;，这是一种用于语音识别的简单数据增强方法，在对数上运行输入音频（而不是原始音频本身）的&lt;a href=&quot;https://en.wikipedia.org/wiki/Mel-Frequency_cepstrum&quot;>;梅尔频谱图&lt;/a>;，并被证明可以有效提高编码器。 &lt;/li>;&lt;li>;基于&lt;a href=&quot;https://arxiv.org/abs/1710.04087&quot;>;多语言无监督嵌入&lt;/a>;（MUSE）的无监督嵌入映射，它在不配对的语言上进行训练，但允许模型来学习源语言和目标语言之间共享的嵌入空间。 &lt;/li>;&lt;li>;基于反向翻译的重建损失，以完全无监督的方式训练编码器-解码器直接 S2ST 模型。 &lt;/li>; &lt;/ol>; &lt;p>; 该模型使用无监督 MUSE 嵌入损失、重建损失和 S2S 反向翻译损失的组合进行训练。在推理过程中，共享编码器用于将输入编码到多语言嵌入空间中，随后由目标语言解码器进行解码。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;架构&lt;/h3>; &lt;p>; Translatotron 3 采用共享编码器对源和目标进行编码语言。解码器由语言解码器、声学合成器（负责翻译语音的声学生成）和单一注意模块组成，如 Translatotron 2。但是，对于 Translatotron 3 有两个解码器，一个用于源语言，另一个用于源语言为目标语言。在训练过程中，我们使用单语语音文本数据集（即，这些数据由语音文本对组成；它们不是翻译）。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;编码器&lt;/h3>; &lt;p>; 编码器与 Translatotron 中的语音编码器具有相同的架构2. 编码器的输出分为两部分：第一部分包含语义信息，第二部分包含声学信息。通过使用 MUSE 损失，输出的前半部分被训练为输入语音频谱图文本的 MUSE 嵌入。后半部分更新后没有MUSE损失。值得注意的是，源语言和目标语言之间共享相同的编码器。此外，MUSE 嵌入本质上是多语言的。因此，编码器能够学习跨源语言和目标语言的多语言嵌入空间。这允许对输入进行更高效和有效的编码，因为编码器能够将两种语言的语音编码到公共嵌入空间中，而不是为每种语言维护单独的嵌入空间。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;解码器&lt;/h3>; &lt;p>; 与 Translatotron 2 一样，解码器由三个不同的组件组成：即语言解码器、声学合成器和注意力模块。然而，为了有效处理源语言和目标语言的不同属性，Translatotron 3 针对源语言和目标语言配备了两个独立的解码器。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;两部分训练&lt;/h3>; &lt;p>; 训练方法由两部分组成：(1)带重建的自动编码和（2）反向翻译项。在第一部分中，网络被训练为使用 MUSE 损失和重建损失将输入自动编码到多语言嵌入空间。此阶段的目的是确保网络生成有意义的多语言表示。在第二部分中，网络被进一步训练以利用反向翻译损失来翻译输入频谱图。缓解 &lt;a href=&quot;https://en.wikipedia.org/wiki/Catastropic_interference#:~:text=Catastropic%20interference%2C%20also%20known%20as,information%20upon%20learning%20new%20information 的问题.&quot;>;灾难性遗忘&lt;/a>;和强制潜在空间为多语言、MUSE 损失和重建损失也应用于训练的第二部分。为了确保编码器学习输入的有意义的属性，而不是简单地重建输入，我们在两个阶段将 SpecAugment 应用于编码器输入。事实证明，它可以通过增加输入数据来有效提高编码器的泛化能力。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;训练目标&lt;/h3>; &lt;p>; 在反向翻译训练阶段（如章节所示）如下），网络经过训练将输入频谱图翻译为目标语言，然后再翻译回源语言。反向翻译的目标是强制潜在空间是多语言的。为了实现这一目标，应用了以下损失： &lt;/p>; &lt;ul>; &lt;li>;MUSE 损失：MUSE 损失衡量输入频谱图的多语言嵌入与反向翻译频谱图的多语言嵌入之间的相似性。 &lt;/li>;&lt;li>;重建损失：重建损失衡量输入频谱图和反向翻译频谱图之间的相似性。 &lt;/li>; &lt;/ul>; &lt;p>; 除了这些损失之外，SpecAugment 还应用于两个阶段的编码器输入。在反翻译训练阶段之前，网络被训练为使用 MUSE 损失和重建损失将输入自动编码到多语言嵌入空间。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MUSE 损失&lt;/h3>; &lt;p>; 确保编码器生成有意义的多语言表示对于这两个解码器，我们在训练期间都采用了 MUSE 损失。 MUSE 损失迫使编码器通过使用预先训练的 MUSE 嵌入来生成这样的表示。在训练过程中，给定输入文本转录本，我们从输入语言的嵌入中提取相应的 MUSE 嵌入。然后，MUSE 嵌入和编码器输出向量之间的误差被最小化。请注意，由于嵌入的多语言性质，编码器在推理过程中对输入的语言无关。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9 sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s1999 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;999&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd 6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Translatotron 3 中的训练和推理。训练包括通过自动编码路径的重建损失，并通过反向翻译使用重建损失。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;音频样本&lt;/h2>; &lt;p>; 以下是 Translatotron 3 直接语音到语音翻译的示例：&lt;/p>; &lt;h3>;西班牙语到英语（在会话数据集上）&lt;/h3>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left ： 汽车; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;输入（西班牙语）&lt;/td>; &lt;td>;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://google-research.github.io/ lingvo-lab/translatotron3/examples/conv_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS 合成参考（英文）&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/ td>; &lt;td>;&lt;音频控制=“控制”src =“https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/ref.wav”>;&lt;/音频>;&lt;/ td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3（英语）&lt;/td>; &lt;td>;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/ translatotron3/examples/conv_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;西班牙语到英语（在 CommonVoice11 合成数据集上） )&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;输入（西班牙语）&lt;/td>; &lt;td>;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://google-research.github.io/ lingvo-lab/translatotron3/examples/cm-s_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS 合成参考（英文） &lt;/td>; &lt;td>;&lt;音频控制=“控制”src =“https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/ref.wav”>;&lt;/音频>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3（英语）&lt;/td>; &lt;td>;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://google-research.github.io/ lingvo-lab/translatotron3/examples/cm-s_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;西班牙语-英语（在 CommonVoice11 数据集上）&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;输入（西班牙语）&lt;/td>; &lt;td>;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://google-research.github.io/ lingvo-lab/translatotron3/examples/cm-e/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS 参考（英文）&lt;/td>; &lt;td>;&lt;音频控件=“控制”src =“https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/ref.wav”>;&lt;/audio>;&lt;/td>; &lt;/ tr>; &lt;tr>; &lt;td>;Translatotron 3（英语）&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo- lab/translatotron3/examples/cm-e/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height: 40%;&quot; >; &lt;br />; &lt;/div>; &lt;h2>;性能&lt;/h2>; &lt;p>;为了凭经验评估所提出方法的性能，我们使用各种数据集（包括&lt;a href=&quot;https: //aclanthology.org/2020.lrec-1.520/&quot;>;Common Voice 11&lt;/a>; 数据集，以及源自 &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>; 的两个合成数据集会话&lt;/a>;和 Common Voice 11 数据集。 &lt;/p>; &lt;p>; 翻译质量通过 ASR（自动语音识别）上的 &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; 来衡量（越高越好）翻译语音的转录，与相应的参考翻译文本进行比较。而语音质量是通过 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>; 分数来衡量的（越高越好）。此外，说话者相似度是通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;平均余弦相似度&lt;/a>;来衡量的（越高越好）。 &lt;/p>; &lt;p>; 由于 Translatotron 3 是一种&lt;em>;无监督&lt;/em>;方法，因此我们使用级联 S2ST 系统作为基线，该系统结合了 ASR、无监督机器翻译 (UMT) 和 TTS（文本到文本） -演讲）。具体来说，我们采用 UMT，它使用嵌入空间中的最近邻居来创建翻译。 &lt;/p>; &lt;p>; Translatotron 3 在我们测量的各个方面都大幅优于基准：翻译质量、说话者相似度和语音质量。它在&lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;会话语料库&lt;/a>;上表现尤其出色。此外，Translatotron 3 实现了与地面真实音频样本相似的语音自然度（通过 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>; 测量，越高越好）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9 vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s1200/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2H tiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;在三个西班牙语-英语语料库上评估翻译质量（通过 BLEU 衡量，越高越好）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;表对齐=“中心”cellpadding=“0”cellspacing=“0”类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZju v6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0- dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s1200/image3.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD 8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;在三个西班牙语-英语语料库上评估语音相似度（通过输入说话者和输出说话者之间的平均余弦相似度来衡量，越高越好）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;中心&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE- WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFh HE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s1200/image1.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3 DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;在三个西班牙语-英语语料库上评估的平均意见得分（通过平均 MOS 指标衡量，越高越好）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line -height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;未来的工作&lt;/h2>; &lt;p>; 作为未来的工作，我们希望将工作扩展到更多语言，并研究零样本 S2ST 是否可以与反向翻译技术一起应用。我们还想研究反向翻译在不同类型的语音数据中的使用，例如噪声语音和低资源语言。&lt;/p>; &lt;div style=&quot;line-height : 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作的直接贡献者包括 Eliya Nachmani、Alon Levkovitch、Yifan Ding、Chulayutsh Asawaroengchai、Heigazhen、和米歇尔·塔德莫·拉马诺维奇。我们还要感谢张宇、Yuma Koizumi、Soroosh Mariooryad、RJ Skerry-Ryan、Neil Zeghidour、Christian Frank、Marco Tagliasacchi、Nadav Bar、Benny Schlesinger 和 Yonghui Wu。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href =&quot;http://blog.research.google/feeds/8288223977991952319/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/12/unsupervised-speech-to-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com /feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech- to-speech.html&quot; rel=&quot;alternate&quot; title=&quot;单语言数据的无监督语音到语音翻译&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http ://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/ g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot; 72“网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1-3de kdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s72-c/T3.png&quot; width=&quot;72&quot; xmlns:media=&quot;http ://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：博客-8474926331452026626.post-1055595481112584523&lt;/id>;&lt;发布>;2023-11-22T08:03:00.000-08:00&lt;/发布>;&lt;更新>;2023-11-30T13:46:35.804-08:00&lt;/更新>; &lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;高性能计算&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ ns#&quot; term=&quot;物理&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Weather&quot;>;&lt;/category>;&lt;title type=&quot;text&quot; >;改进云及其对气候影响的模拟&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 客座研究员 Tapio Schneider 和工程负责人 Yi-fan Chen研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53m iz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s320/hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 当今的气候模型成功地捕捉了广泛的全球变暖趋势。然而，由于&lt;a href=&quot;https://physicalstoday.sitation.org/doi/abs/10.1063/PT.3.4772&quot;>;规模小但具有全球重要性&lt;/a>;的过程存在不确定性，例如&lt;a href=&quot;http://rdcu.be/ohot&quot;>;云&lt;/a>;和&lt;a href=&quot;https://doi.org/10.3389/fmars.2019.00065&quot;>;海洋湍流&lt;/a>;，这些模型对即将到来的气候变化的预测在细节上不是很准确。例如，预测地球全球平均表面温度相对于工业化前时期将变暖2℃的时间，&lt;a href=&quot;https://www.ipcc.ch/report/ar6/wg1/downloads/ report/IPCC_AR6_WGI_TS.pdf&quot;>;当今的模型相差 40-50 年&lt;/a>;（整整一代人）。因此，我们没有规划弹性基础设施所需的&lt;a href=&quot;https://www.nature.com/articles/s41558-020-00984-6&quot;>;准确且精细的预测&lt;/a>; ，调整供应链以适应气候破坏，并评估脆弱社区面临的气候相关危害的风险。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 这在很大程度上是因为云在未来几十年的气候预测中主导着错误和不确定性 [&lt;a href=&quot;https://doi.org /10.1029/2005GL023851&quot;>;1&lt;/a>;，&lt;a href=&quot;https://link.springer.com/article/10.1007/s00382-013-1725-9&quot;>;2&lt;/a>;，&lt;a href= “https://doi.org/10.1038/nclimate3402&quot;>;3&lt;/a>;]。云反射阳光并产生&lt;a href=&quot;https://en.wikipedia.org/wiki/Greenhouse_effect&quot;>;温室效应&lt;/a>;，这使得它们对于调节地球的能量平衡和调节气候系统对气候变化的响应至关重要。温室气体浓度的变化。然而，它们的规模太小，无法在当今的气候模型中直接解析。目前的气候模型可以解决数十到一百公里尺度的运动，&lt;a href=&quot;https://link.springer.com/article/10.1186/s40645-019-0304-z&quot;>;很少有推动&lt;/a >; &lt;a href=&quot;https://journals.ametsoc.org/view/journals/bams/101/5/bams-d-18-0167.1.xml&quot;>;千米规模&lt;/a>;。然而，维持覆盖大片热带海洋的低云等的湍流空气运动的尺度为数米至数十米。由于规模差异巨大，气候模型使用云的经验参数化，而不是直接模拟它们，这会导致较大的误差和不确定性。 &lt;/p>; &lt;p>; 虽然云无法在全球气候模型中直接解析，但可以通过使用高分辨率&lt;a href=&quot;https://en.wikipedia.org/wiki/Large_eddy_simulation 在有限区域内模拟云的湍流动力学&quot;>;大涡模拟&lt;/a>; (LES)。然而，使用 LES 模拟云的高计算成本阻碍了广泛和系统的数值实验，并且阻碍了用于训练参数化方案以在较粗分辨率的全球气候模型中表示云的大型数据集的生成。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;使用张量处理单元加速云的大涡模拟&lt;/a>; ”，发表于&lt;em>;&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/journal/19422466&quot;>;地球系统建模进展杂志&lt;/a>;&lt;/em>; (JAMES)，以及我们与 Google 客座研究员&lt;a href=&quot;https://clima.caltech.edu/&quot;>;气候模拟联盟&lt;/a>; (CliMA) 负责人合作，证明&lt;a href=&quot;https:/ /cloud.google.com/tpu&quot;>;张量处理单元&lt;/a>; (TPU) — 最初为机器学习 (ML) 应用开发的专用集成电路 — 可有效用于执行云的 LES。我们证明，TPU 与定制的软件实现相结合，可用于在特定条件下模拟计算上特别具有挑战性的&lt;a href=&quot;https://en.wikipedia.org/wiki/Marine_stratocumulus&quot;>;海洋层积云&lt;/a>;在&lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;海洋层积云动力学和化学&lt;/a>; (DYCOMS) 实地研究期间观察到的结果。这个基于 TPU 的成功 LES 代码揭示了 TPU 的实用性，即其庞大的计算资源和紧密的互连，用于云模拟。 &lt;/p>; &lt;p>; 在过去 20 年中，气候模型对降水量或大气层顶部能量平衡等关键指标的准确性每十年提高了约 10%。我们这项研究的目标是通过改善云的表现，将气候模型的误差减少 50%。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TPU 上的大涡模拟&lt;/h2>; &lt;p>; 在这项工作中，我们重点关注层积云云，覆盖约 20% 的热带海洋，是地球上最普遍的云类型。当前的气候模型尚无法正确再现层积云的行为，这一直是这些模型中最大的错误来源之一。我们的工作将为大规模气候模型提供更准确的地面事实。 &lt;/p>; &lt;p>; 我们在 TPU 上对云的模拟表现出前所未有的计算吞吐量和扩展能力，例如，可以在最大约 35 × 54 km 的区域内模拟层积云，其实时演化速度提高 10 倍&lt; sup>;2&lt;/sup>;。这样的域大小接近典型全球气候模型网格盒的横截面积。我们的结果为计算实验开辟了新途径，并大幅扩大了可用于训练全球气候模型云参数化的 LES 样本。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;8&quot; cellspacing=&quot;4&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2 KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz -dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s540/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width= “540”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq _bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt; a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNx t_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s540/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79ULI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7Kq BOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot; 0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;模拟 285 x 285 x 2 公里的云演变渲染&lt;sup>;3&lt;/sup>; 层积云片。这是有史以来模拟的最大的同类云片。&lt;strong>;左&lt;/strong>;：相机巡航时云场的斜视图。&lt;strong>;右&lt; /strong>;：云场俯视图，摄像机逐渐拉开。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; LES 代码是用 TensorFlow 编写的，TensorFlow 是一个开放的-Google 为 ML 应用程序开发的源软件平台。该代码利用了 TensorFlow 的图形计算和&lt;a href=&quot;https://www.tensorflow.org/xla&quot;>;加速线性代数&lt;/a>; (XLA) 优化，可以充分利用 TPU 硬件，包括高速、低延迟的&lt;a href=&quot;https://patents.google.com/patent/US9372800&quot;>;芯片间互连&lt;/a>; (ICI)帮助我们实现了前所未有的性能。同时，TensorFlow 代码可以轻松地将 ML 组件直接合并到基于物理的流体求解器中。 &lt;/p>; &lt;p>; 我们通过模拟大气流动求解器的典型测试用例来验证代码，例如在中性分层中上升的浮力气泡，以及下沉并撞击表面的负浮力气泡。这些测试用例表明，基于 TPU 的代码忠实地模拟了流动，随着分辨率的提高，湍流细节也越来越精细。验证测试的最终结果是模拟 DYCOMS 现场活动期间的条件。基于 TPU 的代码可靠地再现了飞机在野外活动中观察到的云场和湍流特征，这一壮举&lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;对于LES&lt;/a>;是因为层积云顶部的&lt;a href=&quot;https://doi.org/10.1029/2018MS001312&quot;>;温度和其他热力学特性的快速变化&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs 7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/s1023/image2.png “ style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;815&quot; data-original-width=&quot;1023&quot; height=&quot;510&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZ TDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/w640-h510/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用于验证我们的 TPU 云模拟器的测试用例之一。与低分辨率网格（200 m，顶行）相比，高分辨率网格（10 m，底行）可以更好地解析负浮力气泡撞击表面所产生的密度电流的精细结构。&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;外观&lt;/h2>; &lt;p>; 与建立了这个基础后，我们的下一个目标是大幅扩大现有的高分辨率云模拟&lt;a href=&quot;https://doi.org/10.1029/2021MS002631&quot;>;数据库&lt;/a>;，构建气候模型的研究人员可以使用这些数据库来开发更好的云参数化——无论是基于物理的模型、机器学习模型还是两者的混合。这需要超出&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;论文&lt;/a>;中描述的额外物理过程；例如，需要将辐射传输过程集成到代码中。我们的目标是生成各种云类型的数据，例如雷暴云。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6 c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s540/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;303&quot; data-original-width=&quot;540&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4y Bye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用与层积云模拟工作相同的模拟器渲染雷暴模拟。在地面附近也可以观察到降雨。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;这项工作说明了机器学习硬件的进步在重新用于其他研究时如何能够取得惊人的效果领域——在本例中是气候建模。这些模拟为云内湍流等过程提供了详细的训练数据，这些过程无法直接观察到，但对于气候建模和预测至关重要。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢以下内容的合著者：论文作者：Sheide Chammas、Qing Wang、Matthias Ihme 和 John Anderson。我们还要感谢 Carla Bromberg、Rob Carver、Fei Sha 和 Tyler Russell 对这项工作的见解和贡献。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog .research.google/feeds/1055595481112584523/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2023/11/improving-simulations-of-clouds-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/1055595481112584523&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and .html&quot; rel=&quot;alternate&quot; title=&quot;改进云的模拟及其对气候的影响&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www .blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#缩略图&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTkuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQW kePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search .yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post -3970135078050750650&lt;/id>;&lt;发布>;2023-11-21T10:09:00.000-08:00&lt;/发布>;&lt;更新>;2023-11-21T10:09:33.541-08:00&lt;/更新>;&lt;类别方案= “http://www.blogger.com/atom/ns#” term=&quot;accessibility&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI社会公益&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www .blogger.com/atom/ns#&quot; term=&quot;机器感知&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;开源&quot;>;&lt;/ category>;&lt;title type=&quot;text&quot;>;开源项目指南：计算机视觉辅助技术平台&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Dave Hawkey，软件Google 研究工程师&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGf IXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s320/hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 两年前，我们&lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;发布了项目指南&lt;/a>;， Google Research 与&lt;a href=&quot;https://www.guidingeyes.org/&quot;>;Guiding Eyes for the Blind&lt;/a>; 合作，帮助视力障碍（例如失明和弱视）人士步行、慢跑，并独立运行。 Project Guideline 仅使用 Google Pixel 手机和耳机，利用设备上的机器学习 (ML) 引导用户沿着标有画线的户外路径行驶。该技术已经&lt;a href=&quot;https://projectguidelinejp.withgoogle.com/intl/en/&quot;>;在世界各地进行了测试&lt;/a>;，甚至在&lt;a href=&quot;https://www. youtube.com/live/2cW1-plwqeQ?si=MTIX2uJkyWuLluht&amp;amp;t=7334&quot;>;2020 年东京残奥会开幕式&lt;/a>;。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 自最初发布以来，我们开始通过嵌入障碍物检测和高级路径规划等新功能来改进项目指南，以安全可靠地导航用户通过更复杂的场景（例如急转弯和附近的行人）。早期版本具有简单的逐帧图像分割功能，可检测路径线相对于图像帧的位置。这足以将用户定向到线路，但提供了有关周围环境的有限信息。改善导航信号，例如障碍物和即将转弯的警报，需要更好地理解和绘制用户环境。为了解决这些挑战，我们构建了一个平台，可用于无障碍空间及其他领域的各种空间感知应用程序。 &lt;/p>; &lt;p>; 今天，我们宣布&lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;开源版本的项目指南&lt;/a>;，供任何人使用用于改进和构建新的无障碍体验。该版本包括核心平台的&lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;源代码&lt;/a>;，以及&lt;a href=&quot;https://github.com/ google-research/project-guideline/tree/main/project_guideline/android&quot;>;Android 应用&lt;/a>;，经过预训练&lt;a href=&quot;https://github.com/google-research/project-guideline/tree/ main/project_guideline/vision/models&quot;>;ML 模型&lt;/a>;，以及 &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/unreal&quot;>;3D 模拟框架&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;系统设计&lt;/h2>; &lt;p>; 主要用例是 Android 应用程序，但是我们希望能够以可重现的方式在各种环境中运行、测试和调试核心逻辑。这促使我们使用 C++ 设计和构建系统，以便与 &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; 和其他核心库紧密集成，同时仍然能够与Android 使用 &lt;a href=&quot;https://developer.android.com/ndk&quot;>;Android NDK&lt;/a>;。 &lt;/p>; &lt;p>; 在底层，Project Guideline 使用 &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; 来估计用户在导航时的位置和方向。课程。基于 &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLabV3+&lt;/a>; 框架构建的分割模型，处理每个相机帧以生成指南的二进制掩码（请参阅&lt;a href =&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;上一篇博文&lt;/a>;了解更多详情）。然后，使用 ARCore 提供的相机位姿和镜头参数（内在参数）将分段引导线上的点从图像空间坐标投影到世界空间地平面上。由于每个帧都提供不同的线条视图，因此世界空间点会聚合在多个帧上以构建现实世界指南的虚拟映射。该系统对指导世界空间坐标进行分段曲线逼近，以构建时空一致的轨迹。当用户沿着路径前进时，这允许细化估计的线路。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2 rMnoI5wKosp3L8hliQJivnl4LdWDqZ_ck3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s800/image1.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;355&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDq Z_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Project Guideline 构建指南的 2D 地图，聚合每帧中检测到的点（&lt;strong>;红色&lt;/strong>;）以构建有状态的表示（&lt;strong>;蓝色&lt;/strong>;）随着跑步者沿着路径前进。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;控制系统根据用户当前的位置、速度、和方向。然后向用户提供音频反馈信号以调整其航向以与即将到来的线段一致。通过使用跑步者的速度矢量而不是相机方向来计算导航信号，我们消除了跑步过程中常见的不规则相机运动引起的噪声。我们甚至可以在用户不在摄像机视野范围内时将用户导航回线路，例如，如果用户转弯过头。这是可能的，因为 ARCore 继续跟踪相机的姿势，可以将其与从先前相机图像推断出的状态线图进行比较。 &lt;/p>; &lt;p>; 项目指南还包括障碍物检测和回避功能。 ML 模型用于估计单个图像的深度。为了训练这个单目深度模型，我们使用了 &lt;a href=&quot;https://blog.research.google/2023/10/sanpo-scene-understanding-accessibility.html&quot;>;SANPO&lt;/a>;，这是一个大型户外数据集来自内部策划的城市、公园和郊区环境的图像。该模型能够检测各种障碍物的深度，包括人、车辆、柱子等。深度图被转换为 3D 点云，类似于线分割过程，用于检测用户路径上是否存在障碍物，然后通过音频信号提醒用户。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2Y H9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/s800/image2.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; height=&quot;480&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrm n5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/w640-h480/image2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Project Guideline 使用单目深度 ML 模型构建环境的 3D 点云，以检测并提醒用户路径沿线的潜在障碍物.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;基于&lt;a href=&quot;https://developer.android.com/ndk/guides/audio/ 的低延迟音频系统aaudio/aaudio&quot;>;AAudio API&lt;/a>; 的实现是为了向用户提供导航声音和提示。项目指南中提供了多个&lt;em>;声音包&lt;/em>;，包括使用&lt;a href=&quot;https://resonance-audio.github.io/resonance-audio/&quot;>;Resonance Audio API&lt;的空间声音实现/a>;.这些声音包是由 Google 的声音研究人员和工程师团队开发的，他们设计并测试了许多不同的声音模型。这些声音结合了平移、音高和空间化来引导用户沿着线走。例如，向右转向的用户可能会在左耳中听到嘟嘟声，以指示线路向左，随着频率的增加，以获得更大的路线修正。如果用户进一步转向，可能会听到高音调的警告声，表明正在接近路径边缘。此外，如果用户偏离线路太远、检测到异常情况或系统无法提供导航信号，则始终会发出清晰的“停止”音频提示。 &lt;/p>; &lt;p>; 项目指南专为具有 &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/ 的 Google Pixel 手机而构建&quot;>;Google Tensor&lt;/a>; 芯片。 Google Tensor 芯片使优化的机器学习模型能够在设备上运行，并具有更高的性能和更低的功耗。这对于以最小的延迟向用户提供实时导航指令至关重要。在 Pixel 8 上，在 &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>; 上运行深度模型时，延迟提高了 28 倍张量处理单元（TPU）代替CPU，与GPU相比提升9倍。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMy quc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s1124/image3 .jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;748&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wD wDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s16000/image3.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;测试和模拟&lt;/h2>; &lt;p>; 项目指南包括模拟器，可以在虚拟环境中快速测试系统并进行原型设计。从机器学习模型到音频反馈系统的所有内容都在模拟器中本地运行，无需设置所有硬件和物理环境即可提供完整的项目指南体验。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh -9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/s1200/image4.jpg&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1200&quot; height=&quot;480&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT7 0F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/w640-h480/image4.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Project Guideline 模拟器的屏幕截图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line- height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;未来方向&lt;/h2>; &lt;p>;为了推动技术向前发展，&lt;a href=&quot;https://www.wear.works&quot;>;WearWorks &lt;/a>; 已成为早期采用者，并与 Project Guideline 合作，整合其获得专利的触觉导航体验，除了声音之外还利用触觉反馈来引导跑步者。 WearWorks 开发触觉技术已超过 8 年，此前曾帮助第一位盲人马拉松运动员在没有视力帮助的情况下完成纽约马拉松比赛。我们希望这样的整合能够带来新的创新，让世界变得更加无障碍。&lt;br />; &lt;/p>; &lt;p>; 项目指南团队也在努力利用最新的技术进步，彻底消除画线。移动机器学习技术，例如 &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;ARCore Scene Semantics API&lt;/a>;，可以识别人行道、建筑物和其他物体在室外场景中。我们邀请无障碍社区在这项技术的基础上进行开发和改进，同时探索其他领域的新用例。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;许多人参与了项目指南的制定及其背后的技术。我们要感谢项目指南团队成员：Dror Avalon、Phil Bayer、Ryan Burke、Lori Dooley、Song Chun Fan、Matt Hall、Amélie Jean-aimée、Dave Hawkey、Amit Pitaru、Alvin Shi、Mikhail Sirotenko、Sagar Waghmare、约翰·沃特金森、金佰利·威尔伯、马修·威尔森、杨轩、马克·扎里奇、史蒂文·克拉克、吉姆·库西、乔什·埃利斯、汤姆·霍兹、迪克·里昂、克里斯·米切尔、荒尾悟、郑有珍、乔·弗莱、古市和人、小林郁美、丸山凯西、Minh Nguyen、Alto Okamura、Yosuke Suzuki 和 Bryan Tanaka。感谢 ARCore 贡献者：Ryan DuToit、Abhishek Kar 和 Eric Turner。感谢 Alec Go、Jing Li、Liviu Panait、Stefano Pellegrini、Abdullah Rashwan、Lu Wang、Qifei Wang 和 Fan Yang 提供 ML 平台支持。我们还要感谢 Hartwig Adam、Tomas Izo、Rahul Sukthankar、Blaise Aguera y Arcas 和 Huisheng Wang 的领导支持。特别感谢我们的合作伙伴 Guiding Eyes for the Blind 和 Achilles International。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3970135078050750650/comments/default&quot; rel =&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-commerce-project-guideline.html# comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel =&quot;编辑&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;self&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-commerce-project-guideline.html&quot; rel=&quot;alternate&quot; title=&quot;开源项目指南：一个平台计算机视觉辅助技术” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIetF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30G hT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0 &lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-6563276834599281497&lt;/id>;&lt;发布>;2023-11-17T11：36：00.000-08： 00&lt;/published>;&lt;updated>;2023-11-17T11:36:32.036-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for社会公益&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;研究奖&quot;>;&lt;/category>;&lt;category schema=&quot;http://www. blogger.com/atom/ns#&quot; term=&quot;大学关系&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;以社会为中心的人工智能的新兴实践&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class =&quot;byline-author&quot;>;发布者：Anoop Sinha，技术与研究总监协会和 Google 研究部副总裁 Yossi Matias&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn 0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj- bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google 人工智能原则&lt;/a>;的第一条是“对社会有益”。作为人工智能从业者，我们受到人工智能技术的变革潜力的启发，人工智能技术能够以前所未有的规模和速度造福社会和我们的共享环境。从&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;帮助应对气候危机&lt;/a>;到&lt;a href=&quot;https:// /blog.google/technology/health/how-were-using-ai-to-help-transform-healthcare/&quot;>;帮助医疗保健转型&lt;/a>;，&lt;a href=&quot;https://blog.google/outreach -initiatives/accessibility/global-accessibility-awareness-day-google-product-update/&quot;>;让数字世界变得更加无障碍&lt;/a>;，我们的目标是负责任地应用人工智能，为全球更多的人提供帮助。实现全球规模需要研究人员和社区在整个人工智能生态系统中共同思考并采取行动。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 我们将这种方法称为“以社会为中心的人工智能”。它既是&lt;a href=&quot;https://hcil.umd.edu/ human-centered-ai/&quot;>;以人为中心的人工智能的延伸和扩展，&lt;/a>;关注社会的总体需求仍然根据个人用户的需求提供信息，特别是在更大的、共享的人类经验的背景下。最近的人工智能进步提供了前所未有的社会层面的能力，如果我们将集体的、多学科的人工智能研究应用于社会层面的共同挑战，从预测饥饿到预测疾病，再到提高生产力，我们现在可以有条不紊地满足这些需求。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/AI_Opportunity_Agenda.pdf&quot;>;人工智能造福社会的机会&lt;/a>;每增加一次天。我们审视了我们在这些领域的工作以及我们支持的研究项目。最近，Google &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;宣布选出 70 名教授&lt;/a>;参加&lt;a href=&quot;https://research.google /outreach/air-program/&quot;>;2023 年包容性研究计划奖&lt;/a>;，该计划支持满足全球历史上边缘化群体需求的学术研究。通过对这项工作的评估，我们确定了一些以社会为中心的人工智能的新兴实践：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;了解社会的需求&lt;/strong>; &lt;br />;倾听社区和合作伙伴的意见对于深入理解重大问题并确定需要解决的优先挑战。作为一种新兴的通用技术，人工智能有潜力解决对人们生活产生重大影响的全球重大社会问题（例如，教育工人、改善医疗保健和提高生产力）。我们发现影响力的关键是以社会需求为中心。为此，我们将重点放在社会一致同意应优先考虑的目标上，例如联合国的&lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/globalgoals&quot;>;17 项可持续发展目标&lt;/a>;是由 190 多个国家共同制定的一系列相互关联的目标，旨在应对全球挑战。 &lt;/li>;&lt;li>;&lt;strong>;集体努力满足这些需求&lt;br />;&lt;/strong>;集体努力将利益相关者（例如，当地和学术界、非政府组织、公私合作）纳入设计的联合过程，人工智能技术的开发、实施和评估，因为它们正在开发和部署以满足社会需求。 &lt;/li>;&lt;li>;&lt;strong>;通过努力满足社会需求的程度来衡量成功&lt;br />;&lt;/strong>;衡量人工智能解决方案满足社会需求的程度非常重要且具有挑战性。在我们的每个案例中，我们都确定了通过与利益相关者合作优化的主要和次要影响指标。 &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;为什么以社会为中心的人工智能很重要？&lt;/h2>; &lt;p>; 案例下面描述的示例展示了以社会为中心的人工智能方法如何对无障碍、健康和气候等主题产生影响。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;了解言语不标准的个人的需求&lt;/h3>; &lt;p>;有&lt;a href=&quot;https://www.nidcd.nih.gov/health/statistics/quick-statistics-voice-speech-language&quot;>;数以百万计的人&lt;/a>;有不标准的言语（例如，发音障碍，&lt;a href=&quot;https://en.wikipedia.org/wiki/Dysarthria&quot;>;构音障碍&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Spasmodic_dysphonia&quot;>;发声困难&lt;/a>;）仅在美国。 &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/impaired-speech-recognition/&quot;>;2019 年&lt;/a>;，Google Research 推出了&lt;a href=&quot;https://blog.research. google/2019/08/project-euphonias-personalized-speech.html&quot;>;Project Euphonia&lt;/a>;，一种允许使用非标准语音的个人用户训练个性化语音识别模型的方法。我们的成功始于我们对现在能够在移动设备上使用语音听写的每个人的影响。 &lt;/p>; &lt;p>; Euphonia 从以社会为中心的人工智能方法开始，包括与非营利组织 &lt;a href=&quot;http://als.net/&quot;>;ALS 治疗发展研究所&lt;/a>; 和&lt;a href=&quot;http://www.alsri.org/&quot;>;ALS 居住计划&lt;/a>;，了解&lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot; 患者的需求>;肌萎缩侧索硬化症&lt;/a>;（ALS）及其使用自动语音识别系统的能力。后来，我们开发了&lt;a href=&quot;https://blog.research.google/2021/09/personalized-asr-models-from-large-and.html&quot;>;世界上最大的非标准语料库&lt;/a>;语音录音，这使我们能够训练&lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;通用语音模型&lt;/ a>;&lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;真实识别混乱语音的能力提高 37%&lt;/a>;对话&lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;单词错误率&lt;/a>; (WER) 测量。这也促成了伊利诺伊大学香槟分校、Alphabet 和 Apple 之间的 &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/speech-accessibility-project/&quot;>;2022&lt;/a>; 合作、Meta、微软和亚马逊启动&lt;a href=&quot;https://speechaccessibilityproject.beckman.illinois.edu/&quot;>;语音无障碍项目&lt;/a>;，这是一项持续的计划，旨在创建一个公开的无序语音样本数据集改进产品并使语音识别更加包容不同的语音模式。其他使用人工智能帮助消除模式和语言障碍的技术包括&lt;a href=&quot;https://about.google/stories/making-conversation-more-accessible-with-live-transcribe/&quot;>;实时转录&lt;/ a>;、&lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/first-time-i-was-able-call-my-23-year-old-son/&quot;>;实时字幕&lt;/ a>; 和&lt;a href=&quot;https://blog.google/intl/en-in/products/explore-communicate/easier-access-to-web-pages-let/&quot;>;大声朗读&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;关注社会健康需求&lt;/h3>; &lt;p>;及时获取孕产妇健康信息可挽救生命全球：&lt;a href=&quot;https://www.who.int/news/item/23-02-2023-a-woman-dies-every-two-mins- due-to-pregnancy-or-childbirth--联合国机构&quot;>;每两分钟就有一名妇女在怀孕或分娩期间死亡&lt;/a>;和&lt;a href=&quot;https://data.unicef.org/topic/child-survival/under- Five-mortality/&quot;>;1 26 名儿童在五岁之前死亡&lt;/a>;。在印度农村地区，针对与怀孕和婴儿相关的关键健康问题对孕妇和新妈妈进行教育需要可扩展、低成本的技术解决方案。 Google Research 与 &lt;a href=&quot;https://armman.org/&quot;>;ARMMAN&lt;/a>; 一起支持&lt;a href=&quot;https://blog.research.google/2022/08/using-ml-to -boost-engagement-with.html&quot;>;该计划&lt;/a>;使用移动消息传递和机器学习 (ML) 算法来预测女性何时可以从接受干预措施（即有针对性的预防性护理信息）中受益，并鼓励她们参与&lt;a href=&quot;https://armman.org/mmitra/&quot;>;mMitra&lt;/a>; 免费语音通话程序。一年之内，mMitra 计划显示出生体重增加三倍的婴儿数量增加了 17%，了解怀孕期间服用铁片重要性的女性数量增加了 36%。这一自动化解决方案已惠及超过 175,000 名母亲和成长中的母亲，公共卫生工作者利用该解决方案来提高信息传递的质量。 &lt;/p>; &lt;p>; 由于社区和人工智能技术开发人员之间密切的集体合作伙伴关系，这些努力在改善健康方面取得了成功。我们通过与护理人员合作，采用了同样的方法来满足各种医疗需求。一些示例包括：使用&lt;a href=&quot;https://health.google/caregivers/arda/&quot;>;自动视网膜疾病评估&lt;/a>; (ARDA) 来&lt;a href=&quot;https://blog. google/technology/health/5-myths-about-medical-ai-debunked/&quot;>;帮助全世界诊所的 250,000 名患者筛查糖尿病视网膜病变&lt;/a>;；我们与 &lt;a href=&quot;https://www.icadmed.com/&quot;>;iCAD&lt;/a>; 合作，将我们的&lt;a href=&quot;https://blog.google/technology/ai/icad-partnership-breast -癌症筛查/&quot;>;乳房X线照相术&lt;/a>;人工智能模型应用于临床环境，以帮助乳腺癌检测；以及 &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>; 的开发，这是一种医学大语言模型，目前正在&lt;a href=&quot;https: //cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model&quot;>;经过云合作伙伴测试&lt;/a>;，帮助医生提供更好的服务病人护理。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;持续努力应对危机所产生的复合影响&lt;/h3>; &lt;p>; Google 研究开始开展洪水预测工作2018 年印度的&lt;a href=&quot;https://www.blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;洪水预报&lt;/a>;和&lt; a href=&quot;https://www.undp.org/bangladesh/blog/climate-change-google-and-bangladesh-floods&quot;>;扩展到孟加拉国&lt;/a>;，以帮助应对每年洪水造成的灾难性破坏。最初的努力始于与&lt;a href=&quot;https://cwc.gov.in/&quot;>;印度中央水务委员会&lt;/a>;、地方政府和社区的合作。这些工作的实施使用了搜索和地图上的 &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS 警报&lt;/a>;，并且最近更广泛地使用了通过&lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>;扩大访问范围。持续&lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;合作&lt;/a>;并推进基于人工智能的全球洪水预报模型使我们能够&lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;将此功能扩展&lt;/a>;至&lt;a href=&quot;https://blog .google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;非洲、亚太地区、欧洲以及南部、中部和北部的 80 多个国家&lt;/a>;美国。我们还与社区志愿者网络合作，进一步加强洪水警报。通过与政府和社区合作衡量这些努力对社会的影响，我们每年都会改进我们的方法和算法。 &lt;/p>; &lt;p>; 我们能够利用这些方法和一些底层技术，例如&lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>; SOS 警报&lt;/a>;，从&lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;洪水预报&lt;/a>;到类似的社会需求，例如&lt;a href=&quot;https://blog .google/products/search/mapping-wildfires-with-satellite-data/&quot;>;野火预报&lt;/a>;和&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support /&quot;>;高温警报&lt;/a>;。我们与各组织的持续合作促成了&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/early-warning-system-wmo-google/&quot;>;对其他工作的支持&lt;/a>;，例如世界气象组织 (WMO) 的&lt;a href=&quot;https://public.wmo.int/en/earlywarningsforall&quot;>;全民预警倡议&lt;/a>;。与社区的持续接触使我们能够随着时间的推移了解用户在社会层面的需求，扩大我们的努力，并扩大我们努力的社会影响力。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;进一步支持以社会为中心的人工智能研究&lt;/h2>; &lt;p>; &lt;a href=&quot;https: //research.google/outreach/air-program/recipients/&quot;>;我们最近资助了&lt;/a>; 18 项大学研究提案，这些提案体现了以社会为中心的人工智能方法，这是&lt;a href=&quot;https://research 中的一个新方向.google/outreach/air-program/&quot;>;Google 包容性研究计划奖&lt;/a>;。这些研究人员正在采用以社会为中心的人工智能方法，并帮助在世界各地创建有益的应用程序。一些资助项目的例子包括：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;人工智能驱动的受冲突影响国家态度极化监测，以促进包容性和平进程和妇女赋权：&lt;/strong>;该项目的目标是创建由法学硕士支持的工具，可用于监控发展中国家在线对话的和平。最初的目标社区是和平不断变化的社区，这项工作将特别强调减轻影响妇女的两极分化和促进和谐。&lt;/li>; &lt;li>;&lt;strong>;人工智能辅助分布式协作室内污染计：案例研究印度社区的需求分析和低成本健康家居解决方案：&lt;/strong>;该项目正在研究如何使用低成本污染监测仪与人工智能辅助方法相结合，为社区和家庭提供改善空气质量的建议健康。最初的目标社区受到污染的严重影响，与他们的联合工作包括制定如何衡量当地社区成果改善的目标。 &lt;/li>; &lt;li>;&lt;strong>;协作开发人工智能解决方案，以扩大乌干达青少年获得性健康和生殖健康教育和服务的机会：&lt;/strong>;该项目的目标是创建法学硕士支持的工具，以提供个性化的指导和服务。满足用户对撒哈拉以南非洲低收入地区性健康和生殖健康教育主题的需求。当地社会需求巨大，估计&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9652643/&quot;>;少女怀孕率达到 25%&lt;/a>;，并且项目旨在通过人工智能解决方案的集体开发流程来满足需求。&lt;strong>; &lt;/strong>; &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;未来方向&lt;/h2>; &lt;p>; 关注社会需求，通过多学科集体研究开展工作，并衡量对社会的影响，有助于产生相关、持久、赋权和有益的人工智能解决方案。请参阅&lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/&quot;>;人工智能实现全球目标&lt;/a>;，详细了解潜在的以社会为中心的人工智能研究问题。 &lt;a href=&quot;https://blog.google/outreach-initiatives/google-org/httpsbloggoogleoutreach-initiativesgoogle-orgunited-nations-global-goals-google-ai-/&quot;>;我们与非营利组织的合作&lt;/a>;这些领域与我们正在开展和鼓励的研究相辅相成。我们相信，使用以社会为中心的人工智能的进一步举措将帮助集体研究社区解决问题并对整个社会产生积极影响。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;非常感谢许多参与此项目的人员这些 Google 项目包括 Shruti Sheth、Reena Jana、Amy Chung-Yu Chou、Elizabeth Adkison、Sophie Allweis、Dan Altman、Eve Andersson、Ayelet Benjamini&lt;/em>;、&lt;em>;Julie Cattiau、Yuval Carny、Richard Cave、Katherine Chou , Greg Corrado, Carlos De Segovia, Remi Denton, Dotan Emanuel, Ashley Gardner, Oren Gilon, Taylor Goddu, Brigitte Hoyer Gosselink, Jordan Green, Alon Harris&lt;/em>;, &lt;em>;Avinatan Hassidim, Rus Heywood, Sunny Jansen, Pan -潘江、安东·卡斯特、玛丽莲·拉德维格、罗尼特·莱维·莫拉德、鲍勃·麦克唐纳、艾丽西亚·马丁、沙基尔·穆罕默德、菲利普·尼尔森、莫里亚·罗伊兹、凯蒂·西弗、乔尔·肖尔、米林德·坦贝、阿帕娜·塔内贾、迪维·塔卡、吉米·托宾、卡特琳·托马内克、 Blake Walsh、Gal Weiss、Kasumi Widner、Lihong Xi 和团队。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6563276834599281497/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered .html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 6563276834599281497&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;self&quot; type= &quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot; rel=&quot;alternate&quot; title=&quot;新兴实践以社会为中心的人工智能” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG3 9hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>; &lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6928081948246813203&lt;/id>;&lt;已发布>; 2023-11-16T13:11:00.000-08:00&lt;/published>;&lt;updated>;2023-11-17T07:42:58.683-08:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Google 研究院的负责任的人工智能：生成式人工智能安全的对抗性测试&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Kathy Meier-Hellstern，构建负责任的人工智能和人工智能数据系统，Google 研究总监&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_O uPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/ lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>;&lt;em>; &lt;/em>;&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;负责任的人工智能和以人为本的技术&lt;/a>; (RAI-HCT) Google 研究团队致力于通过文化意识研究的视角推进负责任的、以人为本的人工智能的理论和实践，以满足当今数十亿用户的需求，并为更美好的人工智能未来开辟道路。 RAI-HCT 内的 BRAIDS（构建负责任的 AI 数据和解决方案）团队旨在通过利用可扩展的工具、高质量数据、简化的流程和新颖的研究来简化 RAI 实践的采用，目前的重点是解决独特的挑战由&lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;生成人工智能&lt;/a>; (GenAI) 提出。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; GenAI 模型实现了前所未有的功能，导致创新应用程序迅速激增。 Google 积极利用 &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;GenAI&lt;/a>; 来&lt;a href=&quot;https://cloud.google.com/vertex-ai/docs /generative-ai/code/code-models-overview&quot;>;增强&lt;/a>;其&lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai-in-workspace-now -available&quot;>;产品的实用性&lt;/a>;并改善生活。 GenAI 带来巨大好处的同时，也带来了虚假信息、偏见和安全方面的风险。 2018 年，Google 率先提出了&lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;人工智能原则&lt;/a>;，强调有益利用和预防伤害。从那时起，Google 就致力于通过 1) 全面的风险评估框架有效落实我们在&lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;负责任的 AI 实践&lt;/a>;中的原则，2）内部治理结构，3）教育，使谷歌员工能够将人工智能原则融入到他们的工作中，以及4）开发用于识别、衡量和分析人工智能产品整个生命周期中道德风险的流程和工具。 BRAIDS 团队专注于最后一个领域，创建用于识别 GenAI 产品中道德和安全风险的工具和技术，使 Google 内部的团队能够应用适当的缓解措施。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;是什么让 GenAI 难以负责任地构建？&lt;/h2>; &lt;p>; GenAI 模型前所未有的功能伴随着一系列新的潜在故障，凸显了在模型广泛使用之前采取全面、系统的 RAI 方法来理解和减轻潜在安全问题的紧迫性。用于了解潜在风险的一项关键技术是对抗性测试，该测试是为了系统地评估模型而进行的测试，以了解它们在一系列场景中被提供恶意或无意中有害的输入时的行为方式。为此，我们的研究集中在三个方向：&lt;/p>; &lt;ol>; &lt;li>;&lt;em>;规模化对抗性数据生成&lt;/em>;&lt;br />; 考虑到不同的用户社区、用例和行为，它在推出产品或服务之前很难全面识别关键安全问题。人机交互的大规模对抗性数据生成通过创建包含各种不同且可能不安全的模型输入的测试集来满足这一需求，这些输入会在不利情况下强调模型的能力。我们在 BRAIDS 中的独特重点在于确定受我们的模型影响的不同用户社区的社会危害。 &lt;/li>; &lt;li>;&lt;em>;自动化测试集评估和社区参与&lt;/em>;&lt;br />;扩展测试流程，以便快速评估数千个模型响应，以了解模型如何在广泛的范围内响应自动化测试集评估有助于发现潜在的有害场景。除了使用对抗性测试集进行测试之外，社区参与是我们识别“未知的未知数”并为数据生成过程提供种子的方法的关键组成部分。&lt;/li>; &lt;li>;&lt;em>;评估者多样性&lt;/em>;&lt;br />;安全评估依赖于人的判断，而人的判断是由社区和文化塑造的，并且不容易自动化。为了解决这个问题，我们优先研究评估者多样性。&lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;规模化对抗数据生成&lt;/ h2>; &lt;p>; 高质量、全面的数据支撑着 Google 的许多关键项目。最初依赖手动数据生成，我们在自动化对抗性数据生成过程方面取得了重大进展。具有用例和政策一致提示的集中式数据存储库可用于快速启动新的对抗性测试的生成。我们还开发了多种基于大型语言模型 (LLM) 的合成数据生成工具，这些工具优先生成反映不同社会背景的数据集，并集成数据质量指标以提高数据集质量和多样性。 &lt;/p>; &lt;p>; 我们的数据质量指标包括：&lt;/p>; &lt;ul>; &lt;li>;语言风格分析，包括查询长度、查询相似性和语言风格的多样性。&lt;/li>; &lt;li>;跨维度的衡量广泛的社会和多元文化维度，利用 &lt;a href=&quot;https://github.com/google-research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>;、&lt;a href=&quot; https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;，&lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness -with-societal-context-be73d4ad38e2&quot;>;社会情境存储库&lt;/a>;。&lt;/li>; &lt;li>;衡量与 Google 的一致性&lt;a href=&quot;https://policies.google.com/terms/generative-ai /use-policy&quot;>;生成式人工智能策略&lt;/a>;和预期用例。&lt;/li>; &lt;li>;分析对抗性，以确保我们检查显式（输入明显设计为产生不安全的输出）和隐式（输入无害但输出有害）查询。 &lt;/li>; &lt;/ul>; &lt;p>; 我们的大规模数据生成方法之一在我们关于&lt;a href=&quot;https://arxiv.org/abs/2311.08592&quot;>;人工智能辅助红队&lt;/ a>;（AART）。 AART 生成具有高度多样性的评估数据集（例如，特定于广泛文化和地理区域的敏感和有害概念），并由人工智能辅助方法引导，以在应用程序环境中定义、范围和优先考虑多样性。与一些最先进的工具相比，AART 在概念覆盖率和数据质量方面显示出了有希望的结果。另外，我们还与 MLCommons 合作，为&lt;a href=&quot;https://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot;>;人工智能安全公共基准做出贡献&lt;/一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;对抗性测试和社区见解&lt;/h2>; &lt;p>; 使用对抗性测试集评估模型输出使我们能够在部署之前识别关键安全问题。我们的初步评估完全依赖于人工评级，由于缺乏标准化的安全定义和政策，导致周转时间缓慢且不一致。我们通过引入与政策一致的评估者指南来提高人类评估者的准确性，从而提高了评估的质量，并且正在研究其他改进措施，以更好地反映不同社区的观点。此外，使用基于 LLM 的自动评分器进行自动化测试集评估可提高效率和规模，同时允许我们将复杂或模糊的案例交给人类进行专家评分。 &lt;/p>; &lt;p>; 除了使用对抗性测试集进行测试之外，收集社区见解对于不断发现“未知的未知数”至关重要。为了提供规模化流程所需的高质量人力投入，我们与 &lt;a href=&quot;https://sites.google.com/corp/google.com/earr-external-research-group 等团体合作/home&quot;>;公平人工智能研究圆桌会议&lt;/a>; (EARR)，并与我们的内部道德和分析团队合作，以确保我们代表使用我们模型的多元化社区。 &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;对抗性 Nibbler 挑战&lt;/a>;吸引外部用户了解&lt;a href=&quot;https://arxiv.org/abs /2305.14384&quot;>;向最终用户大规模输出不安全、有偏见或暴力的内容&lt;/a>;。我们对社区参与的持续承诺包括收集不同社区的反馈并与研究社区合作，例如在&lt;a href=&quot;https://sites.google.com/view/art-of-safety&quot;>;安全的艺术期间&lt;a href=&quot;http://www.ijcnlp-aacl2023.org/&quot;>;计算语言学协会会议亚太分会&lt;/a>; (IJCNLP-AACL 2023) 举办的研讨会&lt;/a>;，以解决对抗性问题GenAI 的测试挑战。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;安全评估中的评估者多样性&lt;/h2>; &lt;p>;了解和减轻 GenAI 安全风险既是技术和社会挑战。安全认知本质上是主观的，并受到多种交叉因素的影响。我们对人口统计对安全认知的影响进行了深入研究，探讨了&lt;a href=&quot;https://arxiv.org/abs/2306.11530&quot;>;评估者人口统计数据的交叉影响&lt;/a>;（例如种族/民族、性别、年龄） ）和 GenAI 输出安全评估的内容特征（例如危害程度）。传统方法在很大程度上忽视了固有的主观性和评估者之间的系统性分歧，这可能掩盖重要的文化差异。我们的&lt;a href=&quot;https://arxiv.org/abs/2311.05074&quot;>;分歧分析框架&lt;/a>;揭示了来自不同背景的评估者之间的各种分歧模式，包括“基本事实”专家评级。这为评估人类注释质量和模型评估的新方法铺平了道路，而不仅仅是简单地使用金标签。我们的 &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;NeurIPS 2023 出版物&lt;/a>;介绍了&lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset &quot;>;DICES&lt;/a>;（对话式人工智能安全评估的多样性）数据集，有助于对法学硕士进行细致入微的安全评估，并考虑不同文化背景下的差异、模糊性和多样性。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;总结&lt;/h2>; &lt;p>; GenAI带来了技术变革，为快速发展提供了可能性甚至无需编码即可进行定制。然而，它也存在产生有害输出的风险。我们的主动对抗性测试计划可识别并减轻 GenAI 风险，以确保包容性的模型行为。对抗性测试和红队是安全策略的重要组成部分，以全面的方式进行它们至关重要。创新的快速步伐要求我们不断挑战自我，与内部合作伙伴、多元化的用户社区和其他行业专家合作，寻找“未知的未知”。 &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6928081948246813203/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; 类型=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;链接 href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog .research.google/2023/11/responsible-ai-at-google-research_16.html&quot; rel=&quot;alternate&quot; title=&quot;Google Research 的负责任 AI：生成式 AI 安全的对抗性测试&quot; type=&quot;text/html&quot;/ >;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>; &lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0 -5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y- 6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>; 0&lt;/ thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-1338549955376716163&lt;/id>;&lt;发布>;2023-11-14T12:28:00.000-08:00&lt; /已发布>;&lt;更新>;2023-11-14T14:09:51.250-08:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“AI”>;&lt; /category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;视频分析&quot;>;&lt;/category>;&lt;title type= &quot;text&quot;>;将多模态理解扩展到长视频&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院软件工程师 Isaac Noble 和研究科学家 Anelia Angelova，谷歌 DeepMind &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5Y wIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s1600/mirasol.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 在为现实生活应用构建机器学习模型时，我们需要考虑多种模式的输入，以捕获我们周围世界的各个方面。例如，音频、视频和文本都提供有关视觉输入的各种补充信息。然而，由于模态的异质性，构建多模态模型具有挑战性。一些模式可能在时间上很好地同步（例如，音频、视频），但与文本不对齐。此外，视频和音频信号中的数据量比文本中的数据量大得多，因此当将它们组合成多模态模型时，视频和音频通常不能被充分消耗，并且需要不成比例地压缩。对于较长的视频输入，这个问题会更加严重。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2311.05698&quot;>;Mirasol3B：一种多模态自回归模型，用于时间对齐和上下文模态&lt;/a>;”，我们引入了一种多模态&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;自回归&lt;/a>;模型（Mirasol3B），用于跨音频、视频和文本学习方式。主要思想是将多模态建模解耦为单独的聚焦自回归模型，根据模态的特征处理输入。我们的模型由一个用于时间同步模态（音频和视频）的自回归组件和一个单独的自回归组件组成，该自回归组件用于不一定是时间对齐但仍然是顺序的模态，例如文本输入，如标题或描述。此外，时间对齐的模态在时间上进行划分，可以共同学习局部特征。通过这种方式，音视频输入被及时建模，并且比以前的工作分配了相对更多的参数。与其他多模式模型相比，通过这种方法，我们可以轻松处理更长的视频（例如，128-512 帧）。在 3B 参数下，Mirasol3B 比之前的 &lt;a href=&quot;https://arxiv.org/abs/2204.14198&quot;>;Flamingo&lt;/a>; (80B) 和 &lt;a href=&quot;https://arxiv.org/ abs/2305.18565&quot;>;PaLI-X&lt;/a>; (55B) 型号。最后，Mirasol3B 在&lt;a href=&quot;https://blog.research.google/2022/08/efficient-video-text-learning-with.html&quot;>;视频问答&lt;方面优于最先进的方法&lt; /a>;（视频 QA）、长视频 QA 和音频-视频-文本基准。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZH HQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s1240/image5.gif “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1240&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG 7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s16000/image5.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Mirasol3B 架构由一个用于时间对齐模态（音频和视频）的自回归模型（按块划分）和一个单独的自回归模型组成未对齐的上下文模式（例如文本）。联合特征学习由组合器进行，它学习紧凑但信息丰富的特征，允许处理长视频/音频输入。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line- height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;协调时间对齐和上下文模式&lt;/h2>; &lt;p>; 视频、音频和文本是具有不同特征的多种模式。例如，视频是每秒 30-100 帧的时空视觉信号，但由于数据量较大，当前模型通常每个视频仅消耗 32-64 帧。音频是一种以比视频高得多的频率获得的一维时间信号（例如，16 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hertz&quot;>;Hz&lt;/a>;），而文本输入适用于整个视频的单词序列通常为 200-300 个单词序列，并用作音频-视频输入的上下文。为此，我们提出了一种模型，该模型由一个自回归组件和另一个自回归组件组成，该自回归组件融合并共同学习出现在高频且大致同步的时间对齐信号，以及另一个用于处理非对齐信号的自回归组件。时间对齐和上下文模式的组件之间的学习通过交叉注意机制进行协调，该机制允许两者在按顺序学习的同时交换信息，而无需及时同步它们。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;视频和音频的时间对齐自回归建模&lt;/h2>; &lt;p>;长视频可以传达丰富的信息以及按顺序发生的活动。然而，当前的模型通过一次提取所有信息来进行视频建模，而没有足够的时间信息。为了解决这个问题，我们应用了一种自回归建模策略，在该策略中，我们根据先前时间间隔的特征表示来调节一个时间间隔内联合学习的视频和音频表示。这保留了时间信息。 &lt;/p>; &lt;p>; 视频首先被分割成更小的视频块。每个块本身可以是 4-64 帧。然后，与每个块相对应的特征由称为组合器（如下所述）的学习模块进行处理，该模块在当前步骤中生成联合音频和视频特征表示——此步骤提取并压缩每个块最重要的信息。接下来，我们使用自回归 Transformer 处理这个联合特征表示，它将注意力集中到之前的特征表示上，并生成下一步的联合特征表示。因此，模型不仅学习如何表示每个单独的块，还学习如何表示块在时间上的关联。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5 q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s1259/image1.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;744&quot; data-original-width=&quot;1259&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSq Ve-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s16000/image1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们对音频和视频输入使用自回归建模，按时间对它们进行分区并学习联合特征表示，然后按顺序自回归学习。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;使用模态组合器对长视频进行建模&lt;/h2>; &lt;为了组合每个视频块中视频和音频信息的信号，我们提出了一个称为组合器的学习模块。通过获取与特定视频时间帧相对应的音频输入来对齐视频和音频信号。然后，我们在时空上处理视频和音频输入，提取与输入变化特别相关的信息（对于我们使用的视频&lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;稀疏视频管&lt;/a>;，对于音频，我们应用&lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;>;频谱图&lt;/a>;表示，两者均由&lt;a处理href=&quot;https://arxiv.org/abs/2010.11929&quot;>;视觉转换器）&lt;/a>;。我们将这些特征连接并输入到组合器中，组合器旨在学习捕获这两个输入的新特征表示。为了解决视频和音频信号中大量数据的挑战，组合器的另一个目标是降低联合视频/音频输入的维度，这是通过选择要生成的较少数量的输出特征来完成的。组合器可以简单地实现为因果变压器，它在时间方向上处理输入，即仅使用先前步骤或当前步骤的输入。或者，组合器可以具有可学习存储器，如下所述。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Combiner 样式&lt;/h2>; &lt;p>; Combiner 的简单版本采用了 Transformer 架构。更具体地说，来自当前块（以及可选的先前块）的所有音频和视频特征被输入到变换器并投影到较低维度，即，选择较少数量的特征作为输出“组合”特征。虽然 Transformer 通常不会在这种情况下使用，但我们发现通过选择 Transformer 的最后 &lt;em>;m&lt;/em>; 个输出（如果 &lt;em>;m&lt;/em>; 是），可以有效地降低输入特征的维度。所需的输出尺寸（如下所示）。或者，组合器可以具有存储器组件。例如，我们使用&lt;a href=&quot;https://arxiv.org/abs/2211.09119&quot;>;令牌图灵机&lt;/a>;（TTM），它支持可微分的内存单元，累积和压缩所有先前时间步长的特征。使用固定内存允许模型在每一步处理更紧凑的特征集，而不是处理先前步骤的所有特征，从而减少计算量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbk XZ-HzNh-DCh6UNSSIwxUlWSOTihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s1798/image8.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;619&quot; data-original-width=&quot;1798&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlW SOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s16000/image8.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们使用一个简单的基于 Transformer 的组合器（&lt;b>;左&lt;/b>;）和一个内存组合器（&lt;b>;右&lt;/b>;） ，基于令牌图灵机 (TTM)，它使用内存来压缩以前的特征历史记录。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们根据多个基准评估我们的方法，&lt;a href=&quot;https://www.microsoft.com/en-us/research/wp- content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1906.02467&quot;>;ActivityNet-QA &lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>;，用于视频 QA 任务，其中发出有关视频的基于文本的问题，并且模型需要回答。这评估了模型理解基于文本的问题和视频内容并形成答案（仅关注相关信息）的能力。在这些基准测试中，后两个针对长视频输入并具有更复杂的问题。 &lt;/p>; &lt;p>; 我们还在更具挑战性的开放式文本生成设置中评估了我们的方法，其中模型以不受约束的方式生成自由格式文本的答案，需要与真实答案完全匹配。虽然这种更严格的评估将同义词视为不正确，但它可能更好地反映模型的泛化能力。 &lt;/p>; &lt;p>; 我们的结果表明，对于大多数基准测试（包括所有开放式生成评估），其性能均优于最先进的方法 - 值得注意的是，考虑到我们的模型只有 3B 参数，比以前的方法小得多，例如，火烈鸟80B。我们仅使用视频和文本输入来与其他工作进行比较。重要的是，我们的模型可以处理 512 帧，而无需增加模型参数，这对于处理更长的视频至关重要。最后，通过 TTM 组合器，我们看到了更好或相当的性能，同时减少了 18% 的计算量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenF RP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s1200/image3.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZGdSo EblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/ 上的结果06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>;（视频 QA）数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center “ cellpadding =“0”cellspacing =“ 0”类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IK iqsySkDDyvlBFiy3VkmcDRWJ- WRoZ-c7Tl-fozWXSEkznQSmJland23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s1200/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;860&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJland23S faA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;&lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>; 基准测试的结果，其中包含用于视频 QA 任务的长视频。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;音频视频基准测试结果&lt;/h2>; &lt;p>; 流行音频结果-视频数据集&lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG-Sound&lt;/a>;和&lt;a href=&quot;https://epic- kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; 如下所示。由于这些基准仅用于分类，因此我们将它们视为开放式文本生成设置，我们的模型在其中生成想要的班级；例如，对于与“打鼓”活动对应的类ID，我们期望模型生成文本“打鼓”。在某些情况下，即使我们的模型在生成开放式设置中输出结果，我们的方法仍大幅优于现有技术。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdn YOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s1200/image6.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDuQz 1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s16000/image6.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG 上的结果-声音&lt;/a>;（音频-视频QA）数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9- 4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4v2 _Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s1200/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtE Wi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>;（音频-视频 QA）数据集的结果。&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;自回归建模的好处&lt;/h2>; &lt;p>; 我们进行了消融研究将我们的方法与使用相同输入信息但使用标准方法（即没有自回归和组合器）的一组基线进行比较。我们还比较了预训练的效果。因为标准方法不适合处理更长的时间视频中，该实验仅针对 32 帧和 4 个块进行，在所有设置中进行公平比较。我们看到 Mirasol3B 的改进对于相对较短的视频仍然有效。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHu SeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s1200/image4.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEvPd wUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;比较我们模型的主要组成部分的消融实验。使用组合器、自回归建模和预训练都可以提高性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt; /div>; &lt;h2>;结论 &lt;/h2>; &lt;p>; 我们提出了一种多模态自回归模型，该模型通过协调时间对齐模态和时间未对齐模态之间的学习来解决与多模态数据异质性相关的挑战。使用组合器对时间对齐模态进行进一步的时间自回归处理，控制序列长度并产生强大的表示。我们证明了一个相对较小的模型可以成功地表示长视频并有效地与其他模式结合。我们在视频和音频视频问答方面优于最先进的方法（包括一些更大的模型）。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 AJ Piergiovanni、Isaac 共同撰写诺布尔、金大勋、迈克尔·柳、维克多·戈麦斯和阿内莉亚·安吉洛娃。我们感谢 Claire Cui、Tania Bedrax-Weiss、Abhijit Ogale、Yunhsuan Sung、Ching-Chung Chang、Marvin Ritter、Kristina Toutanova、Ming-Wei Chang、Ashish Thapliyal、Xiyang Luo、Wei Cheng Kuo、Aren Jansen、Bryan Seybold、Ibrahim Alabdulmohsin、 Jialin Wu、Luke Friedman、Trevor Walker、Keerthana Gopalakrishnan、Jason Baldridge、Radu Soricut、Mojtaba Seyedhosseini、Alexander D&#39;Amour、Oliver Wang、Paul Natsev、Tom Duerig、Younghui Wu、Slav Petrov、Zoubin Ghahramani 的帮助和支持。我们还感谢汤姆·斯莫尔准备动画。 &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1338549955376716163/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text/html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/11/scaling-multimodal-understanding-to.html&quot; rel=&quot;alternate&quot; title=&quot;将多模态理解扩展到长视频&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name >;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel= “http://schemas.google.com/g/2005#thumbnail” src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>; &lt;/作者>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPC ev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s72-c/mirasol.png&quot; 宽度 = &quot;72 &quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签： blogger.com，1999：blog-8474926331452026626.post-5546775652504697591&lt;/id>;&lt;已发布>;2023-11-10T10:05:00.000-08:00&lt;/已发布>;&lt;更新>;2023-11-10T10:05:13.301- 08:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“可访问性”>;&lt;/类别>;&lt;类别方案=“http://www.blogger. com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category schema =&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;为研究界实现大规模健康研究&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究中心软件工程师 Chintan Ghate 和研究工程师 Diana Mincu&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/r29vz2xl/avvxsehdecrv8m_gtaixn2bwuwshshbj6g4jpo6 jb6uyqso-llcbn6wewcesr01ng1ng1mfcchoifnzgbbbbb_mn4ip_hhup_hhup_hhhup_hhrc_mn4ptor_hhrc_mhhymquhnm1ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssS hfkzbxyylb0c1izfqvcqun-laebhjd96wtylibs03ejrteovnayrwpbhb-a7rytalfcfj9skx/s1100/s1100/msSignals-hero.jpg hero.jpg&#39; />; &lt;p>;作为消费技术，例如&lt;a href =“ https://cloud.google.com/blog/topics/healthcare-care-life-sciences/google-cloud-cloud-fitbit-fitbit-fitbit-haga-collaborate-collaborate-in-pilot-on-pilot-on-pilot-on-pilot-on-pilot-hent-pileot-hend-hernt-hearp-hernt--研究“>;健身追踪器&lt;/a>;和&lt;a href=&quot;https://blog.google/products/pixel/pixel/health-ealth-ai-better-sleep/&quot;>;手机&lt;/a>;相关数据收集，利用这些数据途径来研究和提高我们对医疗状况的理解的机会也是如此。我们有&lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research.html&quot;>;以前在慢性疾病的背景下，特别是多发性硬化症（MS）。这项努力利用&lt;a href=&quot;https://github.com/googleclecloudplatform/fda-mystudies&quot;>; fda mystudies平台&lt;/a>;，一个开放式平台，用于创建临床研究应用程序，使任何人更容易，这使得任何人都更容易以可信赖和安全的方式进行自己的学习并收集高质量的医疗保健数据。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>;今天，我们描述了我们通过扩展FDA Mydudies平台开发的设置，并演示如何使用它来建立数字健康研究。我们还介绍了通过该平台创建的名为MS Signals创建的探索性研究，该研究由MS患者的症状跟踪应用程序组成。该应用程序的目标是双重的：1）确保FDA神秘主义平台的增强功能使更简化的研究创作体验； 2）了解如何使用新的数据收集机制来彻底改变患者的慢性疾病管理和跟踪。我们有&lt;a href=&quot;https://github.com/googleclecloudplatform/fda-mystudies-flutter&quot;>;开放源&lt;/a>;我们在&lt;a href =“ https：// wwwww a href =”下的FDA神秘平台下的扩展。 apache.org/licenses/license-2.0&quot;>; apache 2.0许可证&lt;/a>;为社区提供自己的学习资源。 &lt;/p>; &lt;br />; &lt;h2>;扩展FDA Mydudies Platform &lt;/h2>; &lt;p>;原始FDA Mystudies平台允许人们配置自己的研究应用程序，管理参与者并创建单独的iOS和Android应用程序。为了简化研究创建过程并确保研究的参与度增加，我们进行了许多可访问性更改。一些主要改进包括：通过使用&lt;a href=&quot;https://flutter.dev/&quot;>; Flutter &lt;/a>;，Google的开源框架用于构建，跨平台（iOS和Android）应用程序生成来自单个代码库的多平台应用程序；简化的设置，以便用户可以快速原型（在大多数情况下）迅速进行研究；而且，最重要的是，强调可访问性，以便听到各种患者的声音。可访问性增强功能包括对平台的基本功能以及MS信号研究应用程序的特定研究设计的更改。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>;因为这将是一个点，它将同时生成iOS和Android应用程序，从而减少支持多个平台所需的工作。 Flutter还提供&lt;a href=&quot;https://docs.flutter.dev/tools/hot-reload&quot;>; hot-reloading &lt;/a>;，它允许开发人员构建＆amp;快速预览功能。该应用程序中的设计系统利用此功能提供了一个中心点，品牌＆amp; amp; amp; amp;该应用程序的主题可以更改以符合新研究的基调并立即预览。应用程序中的演示环境还利用此功能来允许开发人员在其机器上本地嘲笑和预览问卷。根据我们的经验，这在A/B测试UX以及问题的格式和措辞中与临床医生相处的格式和措辞一直很节省。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>;我们进行了几种可用性增强：&lt;/p>; &lt;ol>; &lt;li>; Light＆amp;黑暗主题支持&lt;/li>; &lt;li>;粗体文本＆amp;可变字体大小&lt;/li>; &lt;li>;高对比度模式&lt;/li>; &lt;li>;提高用户对可访问性设置的认识因此，必须支持深色主题功能，以使经常使用研究应用程序更容易。某些小或轻的文本元素对于有视力障碍的用户来说是难以辨认的，因此我们添加了1）大胆文本和对较大字体尺寸的支持以及2）高对比度的颜色示意图。为了确保易于查找的可访问性设置，我们放置了一个在应用程序第一次启动期间介绍的介绍性的一次性屏幕，该屏幕将直接将用户带入其系统可访问性设置。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;研究可访问性增强&lt;/h3>; &lt;p>;使研究本身更容易与认知超负荷，我们进行了以下更改：&lt;/p>; &lt;ol>; &lt;li>;澄清了入职过程&lt;/li>; &lt;li>;改进的问卷设计&lt;/li>; &lt;/ol>; &lt;/ol>; &lt;p>;首先，我们澄清了在板载过程中，通过向用户介绍首次打开应用程序时的所需步骤列表，以减少混乱和参与者的下降。 &lt;/p>; &lt;p>;应用程序中的原始问卷设计以卡片格式提出了每个问题，该格式利用屏幕的一部分来获得阴影和卡的深度效果。在许多情况下，这是一个令人愉快的美学，但是在优先可访问性的应用中，这些视觉元素限制了屏幕上可用的空间。因此，当使用更较大的字体大小时，会有更频繁的单词断裂，从而降低了可读性。我们仅通过删除卡设计元素并使用整个屏幕来解决此问题，从而可以更好地使用更大的字体大小视觉效果。 &lt;/p>; &lt;br />; &lt;h2>; MS信号原型研究&lt;/h2>; &lt;p>;测试这些变化的可用性，我们使用了重新设计的平台创建一个名为MS Signals的原型研究应用程序，该应用程序使用调查，该应用程序使用调查收集有关参与者与MS相关症状的信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/s1760/MSSignals.png&quot; style =“边距 - 左：自动;边缘权利：自动;”>; &lt;img border =“ 0” data-Original-height =“ 1728” data-Original-width =“ 1760” 1760“高度=” 628“ src =” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/w640-h628/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left;”>; ms信号应用程序屏幕截图。&lt;/em>; &lt;/em>; &lt;/td>; &lt;/tr >; &lt;/tbody>; &lt;/table>; &lt;！ -  &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-Left：auto; auto; Margin-Right：auto;“>; &lt;tbody>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https：//blogger.googleuserercontent.com/img/img/r29vz2xl/r29vz2xl/avvz2xl/avvxsejecbggkdha7hdha7hdha7hdha7hdha7hdha7hdha7hdha7hdha7hdhdha7hdhdhdqunbbg-- dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/s1999/image2.png&quot; style=&quot;margin-left: auto;边缘权利：自动;”>; &lt;img border =“ 0” data-Original-height =“ 1971” data-eriginal-width =“ 1999” height =“ 632” src =“ https://blogger.googleusercercontent.com /img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/w640-h632/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot; tr-caption“ style =” text-align：center;“>; &lt;em style =” text-align：left;“>; ms信号应用程序屏幕截图。&lt;/em>; &lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;&lt; /table>;  - >; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>; MS研究应用程序设计&lt;/h3>; &lt;p>;作为第一步研究信息，要求参与者完成资格和研究理解问卷，以确保他们已经阅读了潜在的漫长研究参与条款。这可能包括例如“在哪个国家可用？”或“”或“”等问题。你能退出研究吗？”这样的部分在大多数健康研究中都是常见的，它往往是参与者的第一个下降点。 &lt;/p>; &lt;p>;为了最大程度地减少在此早期阶段的研究下降，我们将资格测试简要介绍并反映了正确的答案，以回馈参与者的理解测试。这有助于最大程度地减少用户可能需要浏览初始资格问卷的次数，并确保对研究协议的重要方面进行清楚的次数。 &lt;/p>; &lt;p>;成功入学后，参与者被带到主要的应用程序视图，其中包含三页：&lt;/p>; &lt;ul>; &lt;li>; &lt;li>; &lt;strong>;活动：&lt;/strong>; &lt;br />;页面列出了参与者可用的问卷，是他们大部分时间所花费的地方。问卷的频率各不相同 - 有些是为了收集病史而创建的一次性调查，而另一些则是每天，每周或每月重复的，具体取决于他们正在探索的症状或区域。在一次性调查中，我们在每个问题上都提供一个计数器，向用户发出信号，并剩下多少个问题，类似于资格和理解步骤中的问卷。 &lt;/li>; &lt;li>; &lt;strong>;仪表板：&lt;/strong>; &lt;br />;确保参与者恢复了一些东西，以换取他们在研究中输入的信息，仪表板区域介绍了他们在图中的响应摘要或饼图形式。参与者可以将这些数据显示给他们的护理提供者，以摘要在过去6个月中的状况，这是对当今许多人采用的传统笔和纸张方法的改进。 &lt;/li>; &lt;li>; &lt;strong>;资源：&lt;/strong>; &lt;br />;一组有用的链接，帮助文章和与MS相关的常见问题。 &lt;/li>; &lt;/ul>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;问卷设计&lt;/h3>; &lt;p>;，因为需要经常输入数据可以领导为了认知超负荷，参与者下降和数据质量不佳，我们通过两种方式减轻了负担：&lt;/p>; &lt;ol>; &lt;li>;我们将大型问卷分解为较小的问卷，导致6次每日调查，其中包含3-5每个问题是多项选择的问题，并且与单个症状有关。这样，我们总共涵盖了20种主要症状，并以类似的方式介绍了临床医生在临床环境中如何提出这些问题的方式。&lt;/li>; &lt;li>;我们确保在此处可以轻松地在&lt;/li>; &lt;/ol>; &lt;p>;在设计调查内容时，我们与经验丰富的临床医生和研究人员密切合作，以最终确定措辞和布局。虽然该领域的研究通常使用&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/likert_scale&quot;>; Likert Scale &lt;/a>;收集症状信息，但我们定义了更直观的杂语量表来提供更好的体验对于跟踪疾病的参与者以及临床医生或研究人员，观察疾病史。例如，在视觉问题的情况下，我们没有要求参与者以1到10的比例评估他们的症状，而是提出了一个多项选择问题，我们详细介绍了他们可能遇到的常见视力问题。 &lt;/p>; &lt;p>;这种详细的量表可以通过包括有助于更清楚地定义症状的上下文来更准确地跟踪患者的症状。这种方法还允许研究人员回答超出症状相关性的问题。例如，对于视力问题，使用详细量表收集的数据会向研究人员揭示&lt;a href=&quot;https://www.webmd.com/eye-health/nystagmus&quot;>; nystagmus &lt;/a>;与双视力相比，MS。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/s1780/MSSignals-2.png&quot; style =“边距 - 左：自动;边缘权利：自动;”>; &lt;img border =“ 0” data-Foriginal-height =“ 1754” data-Original-width =“ 1780” height =“ 630” src =“ src =” https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/w640-h630/MSSignals-2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left;”>;并排比较与左侧的李克特秤，然后右侧的详细规模。&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;！ -  &lt;table align =“ center” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container“ style =”边缘左左右：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/ S1999/image1.png“ style =”保证金左：自动;边缘权利：auto;“>; &lt;img border =“ 0” data-original-height =“ 1901” data-eriginal-width =“ 1999” height =“ 608” src =“ https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/w640-h608/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “ text-align：center;”>; &lt;em style =“ text-align：左;”>;并排比较左侧的李克特刻度，右侧是冗长的比例。&lt;/em>; &lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>;  - >; &lt;h2>;专注于可访问性&lt;/h2>; &lt;p>;基于移动的研究通常会给患有慢性条件的参与者带来其他挑战：文本很难阅读，颜色的对比可能使得很难看到某些信息，或者在滚动页面上滚动可能具有挑战性。这可能导致参与者下车，如果经历更多的人，这可能会产生偏见的数据集疾病的高级形式无法提供数据。 &lt;/p>; &lt;p>;为了防止此类问题，我们包括以下可访问性功能：&lt;/p>; &lt;ul>; &lt;li>;在整个过程中，我们都采用了盲目访问的配色方案。这包括改善关键文本和重要的其他信息之间的对比度，否则这些信息可能会以较小的字体和褪色的文本颜色表示。&lt;/li>; &lt;li>;我们通过放置所有按钮减少了访问关键控制所需的移动量靠近页面底部，并确保弹出窗口可以从屏幕的底部控制。&lt;/li>; &lt;/ul>; &lt;p>;测试MS信号的可访问性，我们与&lt;a href =合作https://www.nationalmssociety.org/&quot;>; national MS Society &lt;/a>;招募参与者进行用户体验研究。为此，协会向其成员发出了呼吁，要求9名受访者测试各种应用程序流。大多数表明，他们希望一种比目前的方法更好的方法来跟踪症状数据，他们认为MS信号是一种独特而有价值的工具，可以提高其症状跟踪的准确性，并且他们希望共享仪表板与他们的医疗保健提供者的视图。 &lt;/p>; &lt;br />; &lt;h2>;下一步&lt;/h2>; &lt;p>;我们想鼓励所有人使用&lt;a href =“ https://github.com/googleclecleclodplatform/fda-mystudies-flutter “>;开源&lt;/a>;平台开始建立并运行自己的研究。我们正在努力创建一组标准的研究模板，这些模板将结合我们从上面学到的知识，我们希望尽快释放这些模板。有关任何问题，评论或问题，请查看我们的&lt;a href=&quot;https://goo.gle/ms-signals&quot;>;资源页面&lt;/a>;。 &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/555467756775652504697591/comments/comments/default/default/default” rel =“ reque =” reque =“ reque” “/>; &lt;link href =” http://blog.research.google/2023/11/enabling-large-scale-scale-health-scale-health-studies-for.html#comment-form-comment-form“ rel =” rel =“ requies =” title =“ 0注释” “ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/posts/default/555467756775652504697597597597591 >; &lt;link href =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/554677565652504697591” /blog.research.google/2023/11/enabling-large-scale-scale-health-nealth-studies-for.html“ rel =“替代” title =“启用研究社区的大规模健康研究” “/>; &lt;ause>; &lt;name>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/12098626265147775266161 &lt;/uri>; &lt;emage>; &lt;emage>; noreply@blogger@blogger.com &lt;/email>;图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s72- c/mssignals-hero.jpg“ width =” 72“ xmlns：媒体=” http：//search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr>; &lt;thr>; &lt;thr：thr>; 0 &lt;/thr：thr：thr：thr>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999：Blog-8474926331452026626.POST-7987071997777787277 &lt;/id>; &lt;/id>; &lt;/id>; &lt;出版>; 2023-11-09T14：23：00.000-0008：00.000-08：008：008：008：008：008：008：00 &lt;/出版更新>; 2023-11-09T14：23：38.431-08：00 &lt;/updated>; &lt;类别方案=“ http://www.blogger.com/atom/atom/ns#”类别方案=“ http://www.blogger.com/atom/ns#” term =“ health”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“ rai-hct亮点”>; &lt;/actory>; &lt;title type =“ text”>;在Google研究中负责的AI：AI Research（CAIR）中的上下文&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author&quot;>;Posted by Katherine Heller, Research Scientist, Google Research, on behalf of the CAIR Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s1100 /cair-hero.jpg“ style =” display：none;” />; &lt;p>;人工智能（AI）和相关的机器学习（ML）技术在我们周围的世界中越来越有影响力，因此我们必须考虑我们创造技术各个方面对社会和个人的潜在影响。对于这些目的，AI研究（CAIR）团队的上下文在整个AI管道的背景下开发了新颖的AI方法：&lt;strong>; &lt;/strong>;从数据到最终用户反馈。用于构建AI系统的管道通常以&lt;em>; data &lt;/em>;收集开始，然后设计A &lt;em>; model &lt;/em>;以在该数据上运行，&lt;em>; &lt;em>; &lt;em>; &lt;em>;现实世界，最后是人类反馈&lt;/em>;的编译和结合。 CAIR团队的工作起源于健康领域，现在扩展到其他领域，影响了该管道的各个方面。在专门研究模型构建的同时，我们特别关注具有责任的构建系统，包括公平，鲁棒性，透明度和包容性。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ Margin-Left：auto ; margin-right：auto;“>; &lt;tbody>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https：//blogger.googleusercontent.com/img/r29vz2xl/r29vz2xl/avvxseidxseidxsseidxbmc -Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/s1050/CAIR.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;383&quot; data-original-width=&quot;1050 &quot; height=&quot;234&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC-Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/w640-h234/CAIR.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ TD>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h2>;数据&lt;/h2>; &lt;p>; CAIR团队专注于了解构建ML系统的数据。提高ML数据集透明度的标准对我们的工作有用。首先，我们采用文档框架来阐明数据集和模型特征作为数据和模型文档技术的开发指导 -  &lt;a href=&quot;https://arxiv.org/arxiv.org/abs/1803.09010&quot;>;数据集的数据集&lt;/a>;和aft &lt;a href=&quot;https://arxiv.org/abs/1810.03993&quot;>;用于型号报告的型号卡&lt;/a>;。 &lt;/p>; &lt;p>;例如，健康数据集高度敏感，但影响很大。因此，我们开发了&lt;a href=&quot;https://dl.acm.org/doi/fullhtml/10.1145/3531146.353239&quot;>; healthsheets &lt;/a>;，一种医疗服务的dataSheet的健康定义适应。我们开发特定健康表的动机在于现有的AI和健康监管框架的局限性。 &lt;a href=&quot;https://www.nejm.org/doi/10.1056/nejmp1816373&quot;>;最近的研究&lt;/a>;建议数据隐私调节和标准（例如。 。 Health Sheets旨在在道德数据集分析中填补这一空白。健康网的开发是与许多利益相关者合作完成的，包括临床，法律和监管，生物伦理学，隐私和产品。 &lt;/p>; &lt;p>;此外，我们研究了数据表和健康表如何用作诊断工具，从而表现出数据集的局限性和优势。我们的目的是在社区中开始对话，并随着时间的流逝，为动态的医疗保健方案量身定制健康表。 &lt;/p>; &lt;p>;为了促进这项努力，我们加入了&lt;a href=&quot;http://www.datadadativersity.org/&quot;>;在一起&lt;/a>; &lt;/a>;倡议，旨在开发&lt;a href =的财团” https://www.nature.com/articles/s41591-022-01987-w&quot;>; international，基于共识的多样性和代表性标准&lt;/a>;在健康数据集中，并提供有关如何减轻风险的指导偏见，转化为伤害和健康不平等。成为这种国际，跨学科的合作伙伴关系的一部分，该伙伴关系涵盖了全球学术，临床，监管，政策，行业，病人和慈善组织，使我们能够在国际上进行有关AI责任的对话。来自32个国家/地区的250多个利益相关者已促进了完善标准&lt;strong>;。&lt;/strong>; &lt;/p>; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-aptapion-coption-container “样式=” Margin-Left：Auto; Margin-Right：auto;“>; &lt;tbody>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s1050/Healthsheets.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original- height=&quot;731&quot; data-original-width=&quot;1050&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s16000/Healthsheets.png&quot; />;&lt; /a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left;”>; healthsheets and Stand一起：朝着健康数据文档和标准。&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h2>;模型&lt;/h2>; &lt;p>; &lt;/h2>; &lt;p>;现实世界，他们可能无法以预期的方式行事，从而在新的情况下做出不良的预测。这种失败可能是出于多种原因而发生的，并且可能带来负面后果，尤其是在医疗保健的背景下。我们的工作旨在确定可能发现意外模型行为的情况，然后才成为一个实质性的问题，并减轻意外和不希望的后果。 &lt;/p>; &lt;p>; CAIR团队的大部分建模工作都致力于识别和缓解何时&lt;a href=&quot;https://blog.research.google/2021/2021/10/how-underspecification-precification-presents.html&quot;>;指定&lt;/a>;。我们表明，从训练域中绘制的持有数据上表现良好的模型在分配变化下并不相同或公平，因为模型在依赖虚假相关性的程度上有所不同。这对用户和从业人员构成了风险，因为使用标准模型评估实践可能很难预测模型不稳定性。 &lt;a href=&quot;https://www.jmlr.org/papers/v23/20-1335.html&quot;>;我们已经证明了这种担忧&lt;/a>;成像和电子健康记录的预测。 &lt;/p>; &lt;p>;我们还展示了如何利用因果机制知识来诊断和减轻新环境中的公平和鲁棒性问题。 Knowledge of causal structure allows practitioners to anticipate &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html&quot;>;the generalizability of fairness properties under distribution shift in real - 世界医学设置&lt;/a>;。此外，研究特定因果途径或“快捷方式”引入ML系统中的偏见的能力，我们演示了如何识别&lt;a a href =“ https://www.nature.com/articles/s41467-023-- 39902-7“>;快捷方式学习&lt;/a>;导致ML系统的预测无意识地取决于敏感属性（例如，年龄，性别，种族）。我们已经展示了如何使用因果&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/directed_acyclic_graph&quot;>;定向acycclic graphs &lt;/a>; V206/alabdulmohsin23a“>;在复杂的分配变化形式下，将ML系统适应不断变化的环境。我们的团队目前正在研究如何对不同形式的偏见的因果解释，包括&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/selection_bias&quot;>;选择偏见&lt;/a>;，标签偏见和测量误差，和测量错误，和测量误差，和测量误差， &lt;a href=&quot;https://nips.cc/virtual/2022/58452&quot;>;激励设计在模型开发和评估期间减轻偏见的技术设计&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s727/image6.png&quot; style=&quot;边距左：1EM;边缘权利：1EM;“>; &lt;img border =“ 0” data-Original-height =“ 442” data-eriginal-width =“ 727” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxsei_hhxvww2hiuenxnikoratvfffsgqfsgqfsqfabjbjbjbjbjbjbjbjbjbbbsqfbbsqfqud qulwcytdfor9rwtv​​ffor5forqulfbbbbbbbw quldnhthpnfrhthp.2 w yhthw yhtw yhtwym KJ1OHZJPZRZRZRZEBGB6WAP-WON7NIME6DRZJMISCHFM5JDYJULLZULSHUVEJX10-WNHOANX9BSREFZIL0V0VH4E2LU1EW2N/s1078/s1078/image1.png“ style =” margin-left：auto; auto; auto;边缘权利：自动;“>; &lt;img border =“ 0” data-original-height =“ 382” data-eriginal-width =“ 1078” src =“ https://blogger.googleusercontent.com/img/img/b/b/ R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;“>; &lt;em style =” text-align：left;“>;快捷方式学习：对于某些型号，使用医学图像时，年龄可以作为分类的快捷方式。&lt;/em>; &lt;/em>; &lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/ tbody>; &lt;/table>; &lt;br />; &lt;p>; CAIR团队致力于开发方法来开发更广泛的包容性模型。例如，我们还拥有&lt;a href =“ https://arxiv.org/abs/abs/2302.03874” >;在参与系统的设计上进行&lt;/a>;的工作，该&lt;/a>;允许个人选择是否披露敏感属性，例如种族，当ML系统做出预测时。我们希望我们的方法论研究对AI中包容性的社会理解产生积极影响方法开发。 &lt;/p>; &lt;br />; &lt;h2>;部署&lt;/h2>; &lt;p>; CAIR团队旨在建立通过使用移动设备技术来改善所有人生活的技术。我们旨在减少患有健康状况的苦难，解决系统性不平等，并实现基于设备的透明数据收集。随着消费者技术（例如健身追踪器和手机）成为健康数据收集的核心，我们探索了这些技术在慢性病的背景下的使用，特别是&lt;a href =“ https：//en.wikipedia 。我们开发了新的数据收集机制和预测，我们希望最终将彻底改变患者的慢性病管理，临床试验，医疗逆转和药物开发。 &lt;/p>; &lt;p>;首先，我们扩展了开放式&lt;a href=&quot;https://github.com/googlecleclecloudplatform/fda-mystudies&quot;>; fda mystudies平台&lt;/a>;，用于创建临床研究应用程序，以使任何人更容易以可信赖和安全的方式进行自己的学习并收集高质量的数据。我们的改进包括零核心设置，以便研究人员可以通过使用&lt;a href=&quot;https://flutter.dev/&quot;>; flutter &lt;/a>;和，，，以及，通过使用&lt;a href=&quot;https://flutter.dev/ &lt;/a>;和，，最重要的是，强调可访问性，以便听到所有患者的声音。我们很高兴地宣布，这项工作现在已经&lt;a href=&quot;https://github.com/googleclecloudplatform/fda-mystudies-flutter&quot;>;开放源&lt;/a>;是原始FDA-Mystudies平台的扩展。您可以立即开始建立自己的学业！ &lt;/p>; &lt;p>;为了测试此平台，我们构建了一个原型应用程序，该应用程序称为MS信号，该应用程序使用调查在新颖的消费者环境中与患者接触。我们与&lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;国家MS Society &lt;/a>; &lt;/a>;招募参与者进行该应用程序的用户体验研究，目的是降低辍学率并提高该应用程序的用户体验。平台进一步。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/s915/MSSignals.png&quot; style=&quot;左键左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Original-height =“ 904” data-Original-width =“ 915”高度=“ 632” src =“ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/w640-h632/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left;”>; ms信号应用程序屏幕截图。＆nbsp; &lt;strong>; left：&lt;/strong>;＆nbsp;屏幕。可能会使用它来推动MS中ML研究的前沿。在另一项研究中，我们与&lt;a href=&quot;https://neurolology.duke.edu/&quot;>;杜克大学神经病学系建立了研究合作，并证明ML模型可以&lt;a href =“ https：https：https：https：https：https：https：https：https： //www.researchsquare.com/article/rs-2547289/v1&quot;>; accurccurtinal使用从移动应用程序中连续收集的数据，可以在三个月内预测高度症状的发生率&lt;/a>;。结果表明，训练有素的模型可以由临床医生使用来评估MS参与者的症状轨迹，这可能为管理干预措施做出决定。 &lt;/p>; &lt;p>; CAIR团队参与了许多其他系统的部署，包括内部和外部使用。例如，我们还与&lt;a href=&quot;https://learninglyally.org/&quot;>;学习ally &lt;/a>; to &lt;a href =“ https://ai.googleblog.com/2023/08/study - 社会意识到的临时causal.html“>;为学习障碍儿童（例如阅读障碍）建立一本书推荐系统&lt;/a>;。我们希望我们的工作对未来的产品开发产生积极影响。 &lt;/p>; &lt;br />; &lt;h2>;人类反馈&lt;/h2>; &lt;p>;随着ML模型在整个发达国家中变得无处不在，在较不发达国家的声音落后可能太容易了。 CAIR团队的优先事项是弥合这一差距，与社区建立深厚的关系，并通过社区驱动的方法共同努力解决与ML有关的问题。 &lt;/p>; &lt;p>;我们这样做的一种方法之一是与ML的基层组织合作，例如&lt;a href=&quot;https://www.sisonkebiotik.africa/home&quot;>; sisonkebiotik &lt;/a>;，在ML和医疗保健的交汇处，一个开放且包容性的研究人员，从业者和爱好者共同努力，共同建立能力并推动非洲前进的研究计划。我们与SisonKebiotik社区合作，详细介绍了历史自上而下的全球健康方法的局限性，并建议采用基于互补的健康方法，特别是基层参与式社区（GPC）的方法。我们共同创建了一个&lt;a href=&quot;https://openreview.net/forum?id=jhy_g91r880&quot;>;用于ML和全球健康的框架关于&lt;a href=&quot;https://www.masakhane.io/&quot;>; masakhane &lt;/a>;，Sisonkebiotik和&lt;a href =“ https://ro-ya-cv4africa.github。 io/homepage/“>; ro&#39;ya &lt;/a>;。 &lt;/p>; &lt;p>;我们正在采取公开倡议，以更好地了解非西方国家/地区的人工智能对健康的作用，看法和用例，并在非洲最初重点关注。与&lt;a href=&quot;https://ghananlp.org/&quot;>;加纳NLP &lt;/a>;一起，我们努力详细介绍&lt;a href=&quot;https://arxiv.org/abs/2304.02190&quot;>;在非西方环境中，更好地了解算法公平和健康的偏见&lt;/a>;。我们最近启动了一项研究，以使用人类反馈来扩展这项工作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy /s927/mlpipelinebiases.png“ style =” Margin-Left：auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 386” data-Original-witth =“ 927” SRC =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy/s16000/MLPipelineBiases.png&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left;”>;沿ML管道的偏见及其与非洲文化的关联差异轴。&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;p>; CAIR团队致力于创造机会，以聆听AI开发中更多观点。我们与SisonKebiotik合作，共同组织&lt;a href=&quot;https://www.sisonkebiotik.africa/events/workshops/workshops/dl-indaba-2023&quot;>;健康工作室数据科学&lt;/a>; https://deeplearningindaba.com/2023/&quot;>; deep Learning Indaba 2023 &lt;/a>;在加纳。每个人的声音对于使用AI技术开发更美好的未来至关重要。 &lt;/p>; &lt;br />; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;我们要感谢Negar Rostamzadeh，Stephen Pfohl，Subhrajit Roy，Diana Mincu，Chintan Ghate，Chintan Ghate，Mercy Asiedu，Emily Salkey，Alexander D，Alexander D &#39;Amour, Jessica Schrouff, Chirag Nagpal, Eltayeb Ahmed, Lev Proleev, Natalie Harris, Mohammad Havaei, Ben Hutchinson, Andrew Smart, Awa Dieng, Mahima Pushkarna, Sanmi Koyejo, Kerrie Kauer, Do Hee Park, Lee Hartsell, Jennifer Graves, Berk Ustun，Hailey Joren，Timnit Gebru和Margaret Mitchell的贡献和影响力，以及我们在学习Ally，National MS Society，Duke Universal Soctorm，Duke University Hospital，Colidand Stere，Sisonkebiotik和Masakhane的许多朋友和合作者。&lt;/em>; &lt;/em>; &lt;/em>; &lt;/>; &lt;/em>; &lt;/>; &lt;/em>; &lt;/ p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/79870719977777778787277/comments/comments/default/default” rel =“ requies” rel =“ requeies” title =“ title =“ post注释” >; &lt;&lt;link href =“ http://blog.research.google/2023/11/Responsible-ai-AT-Google-google-research.html#comment-form” rel =“ rel =” re =“回复” title =“ 0注释” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/798707199997777777777777777277 =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/798707199777787787277” 。 &lt;auter>; &lt;name>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777775266161 &lt;/uri>; &lt;Email>; noreply@blogger.com &lt;/email>; “ 16” rel =” http://schemas.google.com/g/g/2005#thumbnail“ src =” https://img1.blog1.blogblog.com/img/img/b16-rounded.gif.gif“ width” width width =“ 16”>; &lt;&lt; /gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s72-c/CAIR-hero.jpg&quot; width =“ 72” xmlns：媒体=“ http://search.yahoo.com/mrss/”>; &lt;/媒体：thumbnail>; &lt;thr：thr>; &lt;thr：thr>; 0 &lt;/thr：thr：thr>; &lt;/entry>; &lt;/entry>; &lt;entry>; &lt;entry>; &lt;entry>; &lt;in >; tag：blogger.com，1999年：Blog-8474926331452026626.POST-1303378857635363504 &lt;/id>; &lt;/id>; &lt;出版>; 2023-11-09T11：20：00.002-08：00.002-08：00 &lt;/00 &lt;/00 &lt;/00 ：02.393-08：00 &lt;/updated>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ Quantum ai”>; &lt;/category>; &lt;类别>; www.blogger.com/atom/ns#“ term =“量子计算”>; &lt;/category>; &lt;title type =“ text”>;克服错误校正的量子处理器上的泄漏&lt;span class =“ byline-author”>;由Kevin Miao和Matt McEwen发表，研究科学家，量子AI团队&lt;/span>; &lt;img src =” jyiial3awbfaxnu9l3g47helknkfnf6xfpu7a_9exgllkfglkfgowvuoat0plheocfofjrazqt2il1fbmtzmtzmwfgjsmwfgjsnrpbrqzwqkyjkyjovyjov6imomhwbwbwihbwihhwih5ocpcccepccepccepckepkgggw -luipdbolytoggin3hjfpmwr/s320/Quantum％20leakage.jpg“ style =“ display：none;” />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/qubit&quot;>; qubits &lt;/a>; &lt;/a>; &lt;a>; Google Quantum设备&lt;/a>;既精致又嘈杂，因此有必要合并错误校正程序，以识别并说明构建有用的量子计算机的量子错误。最普遍的两个错误机制是&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/quantum_error_eror_correction#bit_flip_code&quot;>; bit-flip_code&quot;>; bit-flip errors &lt;/a>; a href =“ https://en.wikipedia.org/wiki/quantum_error_correction#sign_flip_code”>; phode-flip errors &lt;/a>;（其中编码量子信息的相位更改的相位）。 &lt;a href=&quot;https://blog.research.google/2021/08/demstrating-fundamentals-of-quantum.html&quot;>;量子错误校正&lt;/a>;（qec）有望解决并减轻这两个突出的错误。但是，还有各种各样的错误机制来挑战QEC的有效性。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;p>;当我们希望量子量作为理想&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/two-state_quantum_system&quot;>;两个 - 级别的系统&lt;/a>;没有损失机制，实际上并非如此。我们使用量子的最低能量水平（形成&lt;em>;计算基础&lt;/em>;）来执行计算。这两个级别对应于量子中激发的缺失（计算基态）或存在（计算激发态），并标记为|0⟩（“ &lt;a href =” https://en.wikipedia.org/wiki.org/wiki /bra％e2％80％93Ket_notation“>; ket &lt;/a>;零”）和|1⟩（“ ket One”）。但是，我们的Qubits还拥有许多称为&lt;em>; &lt;a href=&quot;https://arxiv.org/abs/1509.05470&quot;>;泄漏状态&lt;/a>; &lt;/em>;的更高级别。按照标记级别标记的惯例，指示量子量中有多少激励，我们将其指定为|2⟩，|3⟩，|4⟩等等。 &lt;/p>; &lt;p>;在“ &lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;克服量子错误校正中的泄漏&lt;/a>;” em>; &lt;a href=&quot;https://www.nature.com/nphys/&quot;>;自然物理学&lt;/a>; &lt;/a>; &lt;/em>;，我们确定何时以及如何将量子泄漏到更高的状态各州可以通过我们的两个Qubit大门腐蚀附近的Qubits。然后，我们识别并实施一种策略，该策略可以消除泄漏并将其转换为QEC可以有效修复的错误。最后，我们表明这些操作显着改善了QEC过程的性能和稳定性。最后的结果特别关键，因为其他操作需要时间，通常会导致更多错误。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;与不完美的Qubits一起工作&lt;/h2>; &lt;p>;我们的量子处理器是由称为&lt;的超导量子构建的em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/transmon&quot;>; transmons &lt;/a>; &lt;/em>;。与理想的量子量量只有两个计算级别（计算基础状态和计算激发态）不同，Transmon Qubt具有比计算激发态更高能量的额外状态。这些较高的泄漏状态对于产生&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/quantum_entanglement&quot;>;纠缠&lt;/a>;的特定操作很有用&lt;/a>;太非线性和难以操作。但是，通过各种过程，也可以无意间将Transmon击中这些泄漏状态，包括我们应用用于操作的对照脉冲中的缺陷，或者是从低温冰箱中剩下的少量流量剩余的。这些过程集体称为&lt;em>;泄漏&lt;/em>;，它描述了量子量从计算状态到泄漏状态的过渡。 &lt;/p>; &lt;p>;考虑在QEC实验中广泛使用的特定两倍操作：&lt;a href =“ https://en.wikipedia.org/wiki/wiki/wiki/list_of_quantum_quantum_quantum_logic_gates_gates 5B1％5D-，受控％2DZ，-％2C％0ACONTROLLOLLED％20SIGN“>; CZ GATE &lt;/a>;。该门在两个量子位上运行，当两个量子位在其|1⟩级别中时，相互作用会导致两个单独的激发在其中一个量子位中简短地“束”形成|2⟩，而另一个量子则变为| 0 ⟩，在返回到每个量子位在|1⟩中的原始配置之前。这束束缚了CZ门的纠缠力量。但是，只要概率很小，门就会遇到错误，激发没有返回其原始配置，从而导致操作在|2⟩（泄漏状态）中留下量子。当我们执行数百个或更多这些CZ门时，这种小的泄漏误差概率会累积。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwtMGSiPIWruSj-S6Nleklj7YF9Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/s559/image1.png&quot; style =“边距 - 左：自动;边缘右：自动;”>; &lt;img border =“ 0” data-Original-height =“ 559” data-Original-width =“ 559” 559“ height =” 400“ src =” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwtMGSiPIWruSj-S6Nleklj7YF9Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/w400-h400/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>; transmon Qubits支持许多泄漏状态（|2⟩，|3⟩，|4⟩，…）超出计算基础（|0⟩和|1⟩）。虽然我们通常仅使用计算基础来表示量子信息，但有时量子位进入这些泄漏状态，并破坏我们的Qubits的正常操作。&lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;p>;单个泄漏事件特别损坏了正常的量子操作，因为它会引起许多单独的错误。当一个量子位以泄漏的状态启动时，CZ门不再正确纠缠量子，从而阻止算法正确执行。不仅如此，在泄漏状态下应用于一个Qubit的CZ门也会导致另一个量子泄漏，从而通过设备扩散泄漏。我们的工作包括造成泄漏的方式以及与我们在量子处理器中使用的各种操作相互作用的广泛表征。 &lt;/p>; &lt;p>;一旦Qubit进入泄漏状态，它可以保留在该状态下进行许多操作，然后再放松回到计算状态。这意味着一个单个泄漏事件会干扰该量子的许多操作，从而创建了随时间堆在一起的操作错误（&lt;em>;时间相关&lt;/em>;错误）。通过CZ门在设备中不同量子位之间泄漏的能力，这意味着我们同时看到了相邻量子位上的一堆错误（&lt;em>; space与空间相关的错误&lt;/em>;错误）。泄漏引起空间和时间相关的错误模式的事实使得从QEC算法的角度诊断和纠正尤其难以诊断。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>; &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-erors-erors-erors-scaling.html&quot;>; Surface Code Qec Qec Qec &lt;/a>;，一套应用于一套不完美的操作物理Qubits形成A &lt;em>;逻辑量子&lt;/em>;，其属性更接近理想的量子。简而言之，我们使用一组称为&lt;em>; data Qubits &lt;/em>;的Qubits保存量子信息，而另一组&lt;em>; MEAKE QUBITS &lt;/em>;检查了数据量量表，报告它们是否报告了遭受了任何错误，而不会破坏数据量量的微妙量子状态。 QEC的主要基础假设之一是，每个操作都独立发生错误，但是泄漏可以持续在许多操作上，并导致多个错误的相关模式。当泄漏导致这种假设违反时，我们的QEC策略的性能受到了限制。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2-15jJ6uX2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g -wzbds3JM-TUGSNXOHFB8KM4SHNQKGPQGTKZS8FFRCPS-HHHH8MJSVKKLRNZA/S1588/image3.png“样式” width=&quot;1588&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2-15jJ6uX2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g-WzBDS3jM-tuGSnxOHFb8kM4shNqKGPqGtKzs8FFRCPs-hH8mjsvkKLrNza/s16000/image3.png&quot; />; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;一旦泄漏在我们的表面代码transmon网格中表现出来，它仍然存在于相对于单个表面代码QEC循环的长时间。更糟糕的是，一个Qubit上的泄漏也可能导致其邻居泄漏。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;p>;我们的&lt;a a href =“ https：/ /BLOG.RESEARCH.GOOGLE/2021/08/dextrating-fundamentals-of-quantum.html#:~: text = the%20Sycamore%20Device.--Sleaky%20Qubits，the%20QUBITS，the%20Goal%20Goal%20Of&quot;>; A>;表明，我们可以使用称为&lt;em>;多级重置&lt;/em>;（MLR）的操作从测量Qubits中删除泄漏。这是可能的，因为一旦我们对量子台进行测量，它们就不再拥有任何重要的量子信息。在这一点上，我们可以与非常有损的频带相互作用，从而导致Qubit所处的任何状态（包括泄漏状态）腐烂到计算基态|0⟩。如果我们想象A &lt;em>; jenga &lt;/em>;塔代表贵族中的激发，我们将整个堆栈翻倒。但是，仅拆卸一块砖块是更具挑战性的。同样，MLR不适用于数据量量位，因为它们始终保留重要的量子信息，因此我们需要一种新的泄漏方法，以最小化计算基础状态。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>; Qubit泄漏去除&lt;/em>;（DQLR），该（DQLR）靶向数据量置位的泄漏状态，并将其转换为数据量量和相邻度量Qubit中的计算状态。 dqlr由一个两倍的门组成（被称为&lt;em>; elecage iswap &lt;/em>;  -  an &lt;a a href =“ https://en.wikipedia.org/wiki/wiki/wiki/list_of_quantum_quantum_logic_gates_gates_gates #clifford_qubit_qubit_gates_text：text：text：text：text = 5B6％5D-，假想％20swap，-2“>; iSWap &lt;/a>;具有泄漏状态的操作）受启发性和类似于我们的CZ Gate的启发，然后快速重置度量符号，以进一步消除错误。 The Leakage iSWAP gate is very efficient and greatly benefits from our extensive characterization and calibration of CZ gates within the surface code experiment. &lt;/p>; &lt;p>; Recall that a CZ gate takes two single excitations on two different qubits and briefly brings them to one qubit, before returning them to their respective qubits. A Leakage iSWAP gate operates similarly, but almost in reverse, so that it takes a single qubit with two excitations (otherwise known as |2⟩) and splits them into |1⟩ on two qubits. The Leakage iSWAP gate (and for that matter, the CZ gate) is particularly effective because it does not operate on the qubits if there are fewer than two excitations present. We are precisely removing the |2⟩ &lt;em>;Jenga&lt;/em>; brick without toppling the entire tower. &lt;/p>; &lt;p>; By carefully measuring the population of leakage states on our transmon grid, we find that DQLR can reduce average leakage state populations over all qubits to about 0.1%, compared to nearly 1% without it. Importantly, we no longer observe a gradual rise in the amount of leakage on the data qubits, which was always present to some extent prior to using DQLR. &lt;/p>; &lt;p>; This outcome, however, is only half of the puzzle. As mentioned earlier, an operation such as MLR could be used to effectively remove leakage on the data qubits, but it would also completely erase the stored quantum state. We also need to demonstrate that DQLR is compatible with the preservation of a logical quantum state. &lt;/p>; &lt;p>; The second half of the puzzle comes from executing the QEC experiment with this operation interleaved at the end of each QEC cycle, and observing the logical performance. Here, we use a metric called &lt;em>;detection probability&lt;/em>; to gauge how well we are executing QEC. In the presence of leakage, time- and space-correlated errors will cause a gradual rise in detection probabilities as more and more qubits enter and stay in leakage states. This is most evident when we perform no reset at all, which rapidly leads to a transmon grid plagued by leakage, and it becomes inoperable for the purposes of QEC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s1532/image2.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1532&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;The prior state-of-the-art in our QEC experiments was to use MLR on the measure qubits to remove leakage. While this kept leakage population on the measure qubits (green circles) sufficiently low, data qubit leakage population (green squares) would grow and saturate to a few percent. With DQLR, leakage population on both the measure (blue circles) and data qubits (blue squares) remain acceptably low and stable.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; With MLR, the large reduction in leakage population on the measure qubits drastically decreases detection probabilities and mitigates a considerable degree of the gradual rise. This reduction in detection probability happens even though we spend more time dedicated to the MLR gate, when other errors can potentially occur. Put another way, the correlated errors that leakage causes on the grid can be much more damaging than the uncorrelated errors from the qubits waiting idle, and it is well worth it for us to trade the former for the latter. &lt;/p>; &lt;p>; When only using MLR, we observed a small but persistent residual rise in detection probabilities. We ascribed this residual increase in detection probability to leakage accumulating on the data qubits, and found that it disappeared when we implemented DQLR. And again, the observation that the detection probabilities end up lower compared to only using MLR indicates that our added operation has removed a damaging error mechanism while minimally introducing uncorrelated errors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s1205/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;1205&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Leakage manifests during surface code operation as increased errors (shown as error detection probabilities) over the number of cycles. With DQLR, we no longer see a notable rise in detection probability over more surface code cycles.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prospects for QEC scale-up&lt;/h2>; &lt;p>; Given these promising results, we are eager to implement DQLR in future QEC experiments, where we expect error mechanisms outside of leakage to be greatly improved, and sensitivity to leakage to be enhanced as we work with larger and larger transmon grids. In particular, our simulations indicate that scale-up of our surface code will almost certainly require a large reduction in leakage generation rates, or an active leakage removal technique over all qubits, such as DQLR. &lt;/p>; &lt;p>; Having laid the groundwork by understanding where leakage is generated, capturing the dynamics of leakage after it presents itself in a transmon grid, and showing that we have an effective mitigation strategy in DQLR, we believe that leakage and its associated errors no longer pose an existential threat to the prospects of executing a surface code QEC protocol on a large grid of transmon qubits. With one fewer challenge standing in the way of demonstrating working QEC, the pathway to a useful quantum computer has never been more promising. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work would not have been possible without the contributions of the entire Google Quantum AI Team.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1303378857635363504/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/overcoming-leakage-on-error-corrected.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1303378857635363504&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1303378857635363504&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/overcoming-leakage-on-error-corrected.html&quot; rel=&quot;alternate&quot; title=&quot;Overcoming leakage on error-corrected quantum processors&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidYny_9v0IyOBy7QbYVgwS5FAHzgHhVUt5FQvjXhi6WWJHyIIal3awBfaXNu9l3g47HElKNkFNf6XfpU7a_9ExGLlKFgOWvUOaT0PLhEoCfofJrazqT2IL1fBMtZMwfGJSnrpBrQZwQkYJOV6iMOmhWBWIH5OCPcsEPkgw-LuiPDBoLYToGGIN3Hjfpmwr/s72-c/Quantum%20leakage.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4661241136828203614&lt;/id>;&lt;published>;2023-11-07T12:34:00.003-08:00&lt;/published>;&lt;updated>;2023-11-07T12:34:21.947-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Alternating updates for efficient transformers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Xin Wang, Software Engineer, and Nishanth Dikkala, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk/s1600/altup.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Contemporary deep learning models have been remarkably successful in many domains, ranging from natural language to computer vision. &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer neural networks&lt;/a>; (transformers) are a popular deep learning architecture that today comprise the foundation for most tasks in natural language processing and also are starting to extend to applications in other domains, such as &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;computer vision&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.html&quot;>;robotics&lt;/a>;, and &lt;a href=&quot;https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/&quot;>;autonomous driving&lt;/a>;. Moreover, they form the backbone of all the current state-of-the-art &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;>;language models&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Increasing scale in Transformer networks has led to improved performance and the &lt;a href=&quot;https://arxiv.org/abs/2206.07682&quot;>;emergence of behavior&lt;/a>; not present in smaller networks. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. A natural question is whether we can reap the benefits of larger models without incurring the computational burden. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2301.13310&quot;>;Alternating Updates for Efficient Transformers&lt;/a>;”, accepted as a Spotlight at &lt;a href=&quot;https://neurips.cc/virtual/2023/poster/72994&quot;>;NeurIPS 2023&lt;/a>;, we introduce AltUp, a method to take advantage of increased token representation without increasing the computation cost. AltUp is easy to implement, widely applicable to any transformer architecture, and requires minimal &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter tuning&lt;/a>;. For instance, using a variant of AltUp on a 770M parameter T5-Large model, the addition of ~100 parameters yields a model with a significantly better quality. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; To understand how we can achieve this, we dig into how transformers工作。 First, they partition the input into a sequence of tokens. Each token is then mapped to an embedding vector (via the means of an embedding table) called the token embedding. We call the dimension of this vector the token representation dimension. The Transformer then operates on this sequence of token embeddings by applying a series of computation modules (called layers&lt;em>;)&lt;/em>; using its network parameters. The number of parameters in each transformer layer is a function of the layer&#39;s &lt;em>;width&lt;/em>;, which is determined by the token representation dimension. &lt;/p>; &lt;p>; To achieve benefits of scale without incurring the compute burden, prior works such as sparse mixture-of-experts (Sparse MoE) models (eg, &lt;a href=&quot;https://arxiv.org/abs/2101.03961&quot;>;Switch Transformer&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2022/11/mixture-of-experts-with-expert-choice.html?m=1&quot;>;Expert Choice&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2022/01/scaling-vision-with-sparse-mixture-of.html?m=1&quot;>;V-MoE&lt;/a>;) have predominantly focused on efficiently scaling up the network parameters (in the self-attention and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;>;feedforward layers&lt;/a>;) by conditionally activating a subset based on the input. This allows us to scale up network size without significantly increasing compute per input. However, there is a research gap on scaling up the token representation dimension itself by conditionally activating parts of the token representation vector. &lt;/p>; &lt;p>; Recent works (for example, &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;>;scaling laws&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2011.14522&quot;>;infinite-width networks)&lt;/a>; have empirically and theoretically established that a wider token representation helps in learning more complicated functions. This phenomenon is also evident in modern architectures of increasing capability. For instance, the representation dimension grows from 512 (small) to 768 (base) and 1024 (corresponding to models with 770M, 3B, and 11B parameters respectively) in &lt;a href=&quot;https://arxiv.org/abs/1910.10683 &quot;>;T5 models&lt;/a>;, and from 4096 (8B) to 8192 (64B) and 18432 (540B) in &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM models&lt;/a>; 。 A widened representation dimension also significantly improves performance for dual encoder retrieval models. However, naïvely widening the representation vector requires one to increase the model dimension accordingly, which quadratically&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;1&lt;/a>;&lt;/sup>; increases the amount of computation in the feedforward computation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Method&lt;/h2>; &lt;p>; AltUp works by partitioning a widened representation vector into equal sized blocks, processing only a single block at each layer, and using an efficient prediction-correction mechanism to infer the outputs of the other blocks (shown below on the right). This allows AltUp to simultaneously keep the model dimension, hence the computation cost, roughly constant and take advantage of using an increased token dimension. The increased token dimension allows the model to pack more information into each token&#39;s embedding. By keeping the width of each transformer layer constant, AltUp avoids incurring the quadratic increase in computation cost that would otherwise be present with a naïve expansion of the representation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s1367/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;823&quot; data-original-width=&quot;1367&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;An illustration of widening the token representation without (&lt;b>;left&lt;/b>;) and with AltUp (&lt;b>;right&lt;/b>;). This widening causes a near-quadratic increase in computation in a vanilla transformer due to the increased layer width. In contrast, Alternating Updates keeps the layer width constant and efficiently computes the output by operating on a sub-block of the representation at each layer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More specifically, the input to each layer is two or more blocks, one of which is passed into the 1x width transformer layer (see figure below). We refer to this block as the “activated” block. This computation results in the exact output for the activated block. In parallel, we invoke a lightweight predictor that computes a weighted combination of all the input blocks. The predicted values, along with the computed value of the activated block, are passed on to a lightweight corrector that updates the predictions based on the observed values. This correction mechanism enables the inactivated blocks to be updated as a function of the activated one. Both the prediction and correction steps only involve a limited number of vector additions and multiplications and hence are much faster than a regular transformer layer. We note that this procedure can be generalized to an arbitrary number of blocks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s957/image3.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;504&quot; data-original-width=&quot;957&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;The predictor and corrector computations: The predictor mixes sub-blocks with trainable scalar coefficients; the corrector returns a weighted average of the predictor output and the transformer output. The predictor and corrector perform scalar-vector multiplications and incur negligible computation cost compared to the transformer. The predictor outputs a linear mixing of blocks with scalar mixing coefficients p&lt;sub>;i, j&lt;/sub>; , and the corrector combines predictor output and transformer output with weights g&lt;sub>;i&lt;/sub>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; At a higher level, AltUp is similar to sparse MoE in that it is a method to add capacity to a model in the form of conditionally accessed (external) parameters. In sparse MoE, the additional parameters take the form of feed forward network (FFN) experts and the conditionality is with respect to the input. In AltUp, the external parameters come from the widened embedding table and the conditionality takes the form of alternating block-wise activation of the representation vector, as in the figure above. Hence, AltUp has the same underpinning as sparse MoE models. &lt;/p>; &lt;p>; An advantage of AltUp over sparse MoE is that it does not necessitate sharding since the number of additional parameters introduced is a factor&lt;sup id=&quot;fnref2&quot;>;&lt;a href=&quot;#fn2&quot; rel=&quot;footnote&quot;>;2&lt;/a>;&lt;/sup>; of the embedding table size, which typically makes up a small fraction of the overall model size. Moreover, since AltUp focuses on conditionally activating parts of a wider token representation, it can be applied synergistically with orthogonal techniques like MoE to obtain complementary performance gains. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; AltUp was evaluated on T5 models on various benchmark language tasks. Models augmented with AltUp are uniformly faster than the extrapolated dense models at the same accuracy. For example, we observe that a T5 Large model augmented with AltUp leads to a 27%, 39%, 87%, and 29% speedup on &lt;a href=&quot;https://gluebenchmark.com/&quot;>;GLUE&lt;/a>;, &lt;a href=&quot;https://super.gluebenchmark.com/&quot;>;SuperGLUE&lt;/a>;, &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;>;SQuAD&lt;/a>;, and &lt;a href=&quot;https://nlp.cs.washington.edu/triviaqa/&quot;>;Trivia-QA&lt;/a>; benchmarks, respectively. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s1386 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;982&quot; data-original-width=&quot;1386&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Evaluations of AltUp on T5 models of various sizes and popular benchmarks. AltUp consistently leads to sizable speedups relative to baselines at the same accuracy. Latency is measured on &lt;a href=&quot;https://cloud.google.com/tpu/docs/system-architecture-tpu-vm&quot;>;TPUv3&lt;/a>; with 8 cores. Speedup is defined as the change in latency divided by the AltUp latency (B = T5 Base, L = T5 Large, XL = T5 XL models).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; AltUp&#39;s relative performance improves as we apply it to larger models — compare the relative speedup of T5 Base + AltUp to that of T5 Large + AltUp. This demonstrates the scalability of AltUp and its improved performance on even larger models. Overall, AltUp consistently leads to models with better predictive performance than the corresponding baseline models with the same speed on all evaluated model sizes and benchmarks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Extensions: Recycled AltUp&lt;/h2>; &lt;p>; The AltUp formulation adds an insignificant amount of per-layer computation, however, it does require using a wider embedding table. In certain scenarios where the vocabulary size (ie, the number of distinct tokens the tokenizer can produce) is very large, this may lead to a non-trivial amount of added computation for the initial embedding lookup and the final &lt;a href=&quot;https://www.google.com/url?q=https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1699300629405641&amp;amp;usg=AOvVaw3tnZ18-LvY0tPP_NClQ_P-&quot;>;linear + softmax operation&lt;/a>;. A very large vocabulary may also lead to an undesirable amount of added embedding parameters. To address this, Recycled-AltUp is an extension of AltUp that avoids these computational and parameter costs by keeping the embedding table&#39;s width the same. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s989/image4.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;515&quot; data-original-width=&quot;989&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Illustration of the Architecture for Recycled-AltUp with &lt;em>;K&lt;/em>; = 2.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In Recycled-AltUp, instead of widening the initial token embeddings, we replicate the embeddings &lt;em>;K&lt;/em>; times to form a wider token representation. Hence, Recycled-AltUp adds virtually no additional parameters relative to the baseline transformer, while benefiting from a wider token representation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRoANGe48-HncQDuNpxAVSUGf2GPp1qp6KpnvqMbQ5Mejlq4gWpq96vu9FxkDub8aO3RqYyDmTki2Xqkj9drOprwqrHDyoQvHI4oziYFguhDnkkYZO4GXdhAKgIlfffHJ5Po7mEVGCaAkOH7oobnt-8qKjWlerZp4EeoAjZg6IG_CEVO3hhsQvlPaV2GHU/s1999/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;643&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRoANGe48-HncQDuNpxAVSUGf2GPp1qp6KpnvqMbQ5Mejlq4gWpq96vu9FxkDub8aO3RqYyDmTki2Xqkj9drOprwqrHDyoQvHI4oziYFguhDnkkYZO4GXdhAKgIlfffHJ5Po7mEVGCaAkOH7oobnt-8qKjWlerZp4EeoAjZg6IG_CEVO3hhsQvlPaV2GHU/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Recycled-AltUp on T5-B/L/XL compared to baselines. Recycled-AltUp leads to strict improvements in pre-training performance without incurring any perceptible slowdown.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also evaluate the lightweight extension of AltUp, Recycled-AltUp, with &lt;em>;K&lt;/em>; = 2 on T5 base, large, and XL models and compare its pre-trained accuracy and speed to those of baselines. Since Recycled-AltUp does not require an expansion in the embedding table dimension, the models augmented with it have virtually the same number of trainable parameters as the baseline models. We again observe consistent improvements compared to the dense baselines. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Why does AltUp work?&lt;/h2>; &lt;p>; AltUp increases a model&#39;s capacity by adding and &lt;em>;efficiently&lt;/em>; leveraging auxiliary parameters to the embedding table, and maintaining the higher dimensional representation across the layers. We believe that a key ingredient in this computation lies in AltUp&#39;s prediction mechanism that performs an ensemble of the different blocks. This weighted combination enables continuous message passing to the entire vector despite activating only sub-blocks of it in each layer. Recycled-AltUp, on the other hand, does not add any additional parameters to the token embeddings. However, it still confers the benefit of simulating computation in a higher dimensional representation space since a higher dimensional representation vector is maintained when moving from one transformer layer to another. We conjecture that this aids the training by augmenting the flow of information through the network. An interesting research direction is to explore whether the benefits of Recycled-AltUp can be explained entirely by more favorable training dynamics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;We thank our collaborators Cenk Baykal, Dylan Cutler, and Rina Panigrahy at Google Research, and Nikhil Ghosh at University of California, Berkeley (work done during research internship at Google).&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;This is because the feedforward layers of a Transformer are typically scaled quadratically with the model dimension.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;br />; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn2&quot;>;&lt;b>;2&lt;/b>;&lt;/a>;&lt;/sup>;This factor depends on the user-specified expansion factor, but is typically 1, ie, we double the embedding table dimension.&amp;nbsp;&lt;a href=&quot;#fnref2&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4661241136828203614/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/alternating-updates-for-efficient.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4661241136828203614&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4661241136828203614&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/alternating-updates-for-efficient.html&quot; rel=&quot;alternate&quot; title=&quot;Alternating updates for efficient transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk/s72-c/altup.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8959024707515398632&lt;/id>;&lt;published>;2023-11-03T11:23:00.001-07:00&lt;/published>;&lt;updated>;2023-11-03T11:27:07.156-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Best of both worlds: Achieving scalability and quality in text clustering&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sara Ahmadian and Mehran Kazemi, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR2mD5afdT_Qg9Ex4Lj94FaxB9KHiq20iLoDlOY8S8Rk5XsV6n_4dU9CFbCvSeBSONGQYqy-yMGY6-KU_y_RGICOpz76GNdzv7ecxor_rVnF31lZOd3STQF4MIE4F_EadrSI1DXY67MXnsCswY3w3X8vX8KgU_rRPs6eTZndYAbVcEezgwaUsdZEAHD59Z/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;Clustering&lt;/a>; is a fundamental, ubiquitous problem in data mining and &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;>;unsupervised&lt;/a>; machine learning, where the goal is to group together similar items. The standard forms of clustering are metric clustering and graph clustering. In metric clustering, a given metric space defines distances between data points, which are grouped together based on their &lt;em>;separation&lt;/em>;. In graph clustering, a given graph connects similar data points through edges, and the clustering process groups data points together based on the &lt;em>;connections&lt;/em>; between them. Both clustering forms are particularly useful for large corpora where class labels can&#39;t be defined. Examples of such corpora are the ever-growing digital text collections of various internet platforms, with applications including organizing and searching documents, identifying patterns in text, and recommending relevant documents to users (see more examples in the following posts: &lt;a href=&quot;https://blog.research.google/2010/10/clustering-related-queries-based-on.html?m=1&quot;>;clustering related queries based on user intent&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2021/10/practical-differentially-private.html&quot;>;practical differentially private clustering&lt;/a>;). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The choice of text clustering method often presents a dilemma. One approach is to use embedding models, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/BERT_(language_model)&quot;>;BERT&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>;, to define a metric clustering problem. Another is to utilize &lt;a href=&quot;https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-cross-attention-3b396266d82e&quot;>;cross-attention&lt;/a>; (CA) models, such as &lt;a href=&quot;https://blog.research.google/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_pre-trained_transformer&quot;>;GPT&lt;/a>;, to define a graph clustering problem. CA models can provide highly accurate similarity scores, but constructing the input graph may require a prohibitive quadratic number of inference calls to the model. On the other hand, a metric space can efficiently be defined by distances of embeddings produced by embedding models. However, these similarity distances are typically of substantial lower-quality compared to the similarity signals of CA models, and hence the produced clustering can be of much lower-quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRHbZ9egdnFlc6ZCmLeODTdPZoOXcjjEGCLHP8Ve3aihMExIjyLiX4n0Zy9l4nmkx-3k6rxYN5696irWyubaYAqecpxFTmP1wzjBo0MKnZMnnTjQrdSxKIWGzsySs8XB-OVHyyxKJBWI0dlrYvb_S204S6aSbkPTx8_FQTJXk8f_WDIoGgqNJTACuXNewu/s835/figure%20top.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;641&quot; data-original-width=&quot;835&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRHbZ9egdnFlc6ZCmLeODTdPZoOXcjjEGCLHP8Ve3aihMExIjyLiX4n0Zy9l4nmkx-3k6rxYN5696irWyubaYAqecpxFTmP1wzjBo0MKnZMnnTjQrdSxKIWGzsySs8XB-OVHyyxKJBWI0dlrYvb_S204S6aSbkPTx8_FQTJXk8f_WDIoGgqNJTACuXNewu/s16000/figure%20top.png&quot; />;&lt;/a>;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt; a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoY4juklU42gca4fdtnUN4t5j8Z-4rEEhyphenhyphenDi4kRfkV1wfqBs_wyldX9sU2pwtMHdVZPZf2vWnDweEPzwiLseYZenC3afIuaDRf_7arpoF-xnMAwgGXs48a_3wNG2trhN0CTkHgMVa6ZQ7ToP7IE6fpIFmbTF7YYGPFgYU3cMtyyBYIfSWVLVvRXY6Dueiy/s835/figure%20bottom.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;641&quot; data-original-width=&quot;835&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoY4juklU42gca4fdtnUN4t5j8Z-4rEEhyphenhyphenDi4kRfkV1wfqBs_wyldX9sU2pwtMHdVZPZf2vWnDweEPzwiLseYZenC3afIuaDRf_7arpoF-xnMAwgGXs48a_3wNG2trhN0CTkHgMVa6ZQ7ToP7IE6fpIFmbTF7YYGPFgYU3cMtyyBYIfSWVLVvRXY6Dueiy/s16000/figure%20bottom.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the embedding-based and cross-attention–based similarity scoring functions and their scalability vs. quality dilemma.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>; Motivated by this, in “&lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals”, presented at ICLR 2023&lt;/a>;, we describe a novel clustering algorithm that effectively combines the scalability benefits from embedding models and the quality from CA models. This graph clustering algorithm has query access to both the CA model and the embedding model, however, we apply a budget on the number of queries made to the CA model. This algorithm uses the CA model to answer edge queries, and benefits from unlimited access to similarity scores from the embedding model. We describe how this proposed setting bridges algorithm design and practical considerations, and can be applied to other clustering problems with similar available scoring functions, such as clustering problems on images and media. We demonstrate how this algorithm yields high-quality clusters with almost a linear number of query calls to the CA model. We have also &lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks.zip&quot;>;open-sourced&lt;/a>; the data used in our experiments. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The clustering algorithm&lt;/h2>; &lt;p>; The KwikBucks algorithm is an extension of the well-known &lt;a href=&quot;https://cesa-bianchi.di.unimi.it/Algo2/Note/kwik.pdf&quot;>;KwikCluster algorithm (Pivot algorithm)&lt;/a>;. The high-level idea is to first select a set of documents (ie, centers) with no similarity edge between them, and then form clusters around these centers. To obtain the quality from CA models and the runtime efficiency from embedding models, we introduce the novel &lt;em>;combo similarity oracle&lt;/em>; mechanism. In this approach, we utilize the embedding model to guide the selection of queries to be sent to the CA model. When given a set of center documents and a target document, the combo similarity oracle mechanism outputs a center from the set that is similar to the target document, if present. The combo similarity oracle enables us to save on budget by limiting the number of query calls to the CA model when selecting centers and forming clusters. It does this by first ranking centers based on their embedding similarity to the target document, and then querying the CA model for the pair (ie, target document and ranked center), as shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s1601/process.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s16000/process.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;A combo similarity oracle that for a set of documents and a target document, returns a similar document from the set, if present.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We then perform a post processing step to merge clusters if there is a strong connection between two of them, ie, when the number of connecting edges is higher than the number of missing edges between two clusters. Additionally, we apply the following steps for further computational savings on queries made to the CA model, and to improve performance at runtime: &lt;/p>; &lt;ol>; &lt;li>;We leverage &lt;a href=&quot;https://arxiv.org /pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; to form a set of centers from a set of randomly selected documents instead of selecting these centers from all the documents (in the illustration below, the center nodes are red ）。 &lt;/li>;&lt;li>;We apply the combo similarity oracle mechanism to perform the cluster assignment step in parallel for all non-center documents and leave documents with no similar center as singletons. In the illustration below, the assignments are depicted by blue arrows and initially two (non-center) nodes are left as singletons due to no assignment. &lt;/li>;&lt;li>;In the post-processing step, to ensure scalability, we use the embedding similarity scores to filter down the potential mergers (in the illustration below, the green dashed boundaries show these merged clusters). &lt;/li>; &lt;/ol>;&lt;div>;&lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1bJ65zsG-PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s1322/Kwikbux%20gif.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;704&quot; data-original-width=&quot;1322&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1bJ65zsG-PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s16000/Kwikbux%20gif.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of progress of the clustering algorithm on a given graph instance. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate the novel clustering algorithm on various datasets with different properties using different embedding-based and cross-attention–based models. We compare the clustering algorithm&#39;s performance with the two best performing baselines (see the &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;paper&lt;/a>; for more details): &lt;/p>; &lt;ul>; &lt;li>;The &lt;a href=&quot;https://arxiv.org/pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; algorithm for budgeted clustering with access to CA only. &lt;/li>;&lt;li>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_clustering&quot;>;Spectral clustering&lt;/a>; on the &lt;em>;&lt;a href=&quot;https://en.wikipedia .org/wiki/Nearest_neighbor_graph&quot;>;k-nearest neighbor graph&lt;/a>;&lt;/em>; (kNN) formed by querying the CA model for the &lt;em>;k&lt;/em>;-nearest neighbors of each vertex from embedding-based相似。 &lt;/li>; &lt;/ul>; &lt;p>; To evaluate the quality of clustering, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision and recall&lt;/a>;. Precision is used to calculate the percentage of similar pairs out of all co-clustered pairs and recall is the percentage of co-clustered similar pairs out of all similar pairs. To measure the quality of the obtained solutions from our experiments, we use the &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1-score&lt;/a>;, which is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Harmonic_mean&quot;>;harmonic mean&lt;/a>; of the precision and recall, where 1.0 is the highest possible value that indicates perfect precision and recall, and 0 is the lowest possible value that indicates if either precision or recall are zero. The table below reports the F1-score for Kwikbucks and various baselines in the case that we allow only a linear number of queries to the CA model. We show that Kwikbucks offers a substantial boost in performance with a 45% relative improvement compared to the best baseline when averaging across all datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s1574/results.jpg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;582&quot; data-original-width=&quot;1574&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s16000/results.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Comparing the clustering algorithm to two baseline algorithms using various public datasets: (1) The &lt;a href=&quot;https://arxiv.org/pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; algorithm for budgeted clustering with access to CA only, and (2) &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_clustering&quot;>;spectral clustering&lt;/a>; on the &lt;em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Nearest_neighbor_graph&quot;>;k-nearest neighbor (kNN) graph&lt;/a>;&lt;/em>; formed by querying the CA model for the &lt;em>;k&lt;/ em>;-nearest neighbors of each vertex from embedding-based similarity. Pre-processed datasets can be downloaded &lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks.zip&quot;>;here&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The figure below compares the clustering algorithm&#39;s performance with baselines using different query budgets. We observe that KwikBucks consistently outperforms other baselines at various budgets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3CboOkZ4lbp9dmH-5EkPGe1zo6JPlOQbXT1n67ke4qN0kXeZXMQKb4SUM-E6TXuyWF9UF7vvKTtl2lecg-Z5tV0mA6Hobh9NKbmy__dqGRIYcTEIzdfGjCCyih2eZW4xF33n7Y1pJGpwyDvQ75rwQsp0kZICGSPoyR8ahiBQwcZvT7ZLLeAA1BQkJtLwU/s1236/stackoverflow.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1236&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3CboOkZ4lbp9dmH-5EkPGe1zo6JPlOQbXT1n67ke4qN0kXeZXMQKb4SUM-E6TXuyWF9UF7vvKTtl2lecg-Z5tV0mA6Hobh9NKbmy__dqGRIYcTEIzdfGjCCyih2eZW4xF33n7Y1pJGpwyDvQ75rwQsp0kZICGSPoyR8ahiBQwcZvT7ZLLeAA1BQkJtLwU/s16000/stackoverflow.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A comparison of KwikBucks with top-2 baselines when allowed different budgets for querying the cross-attention model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Text clustering often presents a dilemma in the choice of similarity function: embedding models are scalable but lack quality, while cross-attention models offer quality but substantially hurt scalability. We present a clustering algorithm that offers the best of both worlds: the scalability of embedding models and the quality of cross-attention models. KwikBucks can also be applied to other clustering problems with multiple similarity oracles of varying accuracy levels. This is validated with an exhaustive set of experiments on various datasets with diverse properties. See the &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This project was initiated during Sandeep Silwal&#39;s summer internship at Google in 2022. We would like to express our gratitude to our co-authors, Andrew McCallum, Andrew Nystrom, Deepak Ramachandran, and Sandeep Silwal, for their valuable contributions to this work. We also thank Ravi Kumar and John Guilyard for assistance with this blog post.&lt;/em>; &lt;/p>;&lt;/div>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8959024707515398632/ comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/best-of-both -worlds-achieving.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/8959024707515398632&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8959024707515398632&quot; rel=&quot; self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/best-of-both-worlds-achieving.html&quot; rel=&quot;alternate&quot; title =&quot;Best of both worlds: Achieving scalability and quality in text clustering&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile /12098626514775266161&lt;/uri>;&lt; ：//img1.blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail height =“ 72” url =“ https：//blogdger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR2mD5afdT_Qg9Ex4Lj94FaxB9KHiq20iLoDlOY8S8Rk5XsV6n_4dU9CFbCvSeBSONGQYqy-yMGY6-KU_y_RGICOpz76GNdzv7ecxor_rVnF31lZOd3STQF4MIE4F_EadrSI1DXY67MXnsCswY3w3X8vX8KgU_rRPs6eTZndYAbVcEezgwaUsdZEAHD59Z/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt; /media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4882962745371313901&lt;/id>;&lt;published>;2023 -11-02T15:01:00.001-07:00&lt;/published>;&lt;updated>;2023-11-02T15:06:20.846-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com /atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt; category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Zero-shot adaptive prompting of large language models&lt;/ stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Xingchen Wan, Student Researcher, and Ruoxi Sun, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqTosaeIs4AMYukkmgUBEii6iQcrr9_dNKM0cHnW6m9Wi8yX0V-QCduQfkqLyQPNpVbze3OFO-nPG5Wm9DLMM8pEAfhQGq-TEJroLJGQyqNr-hlJNkToBCmgJbphrCRvlv95gkDQH0ScT7VrXu2VCTKyiWy8yYM4G8voF0kD0K2oxwg2M2xBcz1yQnU0Zb/s600/USP.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Recent advances in large language models (LLMs) are very promising as reflected in their capability for general problem-solving in &lt;em>;few-shot&lt;/em>; and &lt;em>;zero-shot&lt;/em>; setups, even without explicit training on these tasks. This is impressive because in the few-shot setup, LLMs are presented with only a few question-answer demonstrations prior to being given a test question. Even more challenging is the zero-shot setup, where the LLM is directly prompted with the &lt;em>;test question only&lt;/em>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Even though the few-shot setup has dramatically reduced the amount of data required to adapt a model for a specific use-case, there are still cases where generating sample prompts can be challenging. For example, handcrafting even a small number of demos for the broad range of tasks covered by general-purpose models can be difficult or, for unseen tasks, impossible. For example, for tasks like summarization of long articles or those that require domain knowledge (eg, medical question answering), it can be challenging to generate sample answers. In such situations, models with high zero-shot performance are useful since no manual prompt generation is required. However, zero-shot performance is typically weaker as the LLM is not presented with guidance and thus is prone to spurious output. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://aclanthology.org/2023.findings-acl.216/&quot;>;Better Zero-shot Reasoning with Self-Adaptive Prompting&lt;/a>;”, published at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, we propose &lt;em>;Consistency-Based Self-Adaptive Prompting (COSP) &lt;/em>;to address this dilemma. COSP is a zero-shot automatic prompting method for reasoning problems that carefully selects and constructs &lt;em>;pseudo-&lt;/em>;demonstrations for LLMs using only unlabeled samples (that are typically easy to obtain) and the models&#39; own predictions. With COSP, we largely close the performance gap between zero-shot and few-shot while retaining the desirable generality of zero-shot prompting. We follow this with “&lt;a href=&quot;https://arxiv.org/abs/2305.14926&quot;>;Universal Self-Adaptive Prompting&lt;/a>;“ (USP), accepted at &lt;a href=&quot;https://2023. emnlp.org/&quot;>;EMNLP 2023&lt;/a>;, in which we extend the idea to a wide range of &lt;em>;general &lt;/em>;natural language understanding (NLU) and natural language generation (NLG) tasks and demonstrate its effectiveness 。 &lt;/p>; &lt;br />; &lt;h2>;Prompting LLMs with their own outputs&lt;/h2>; &lt;p>; Knowing that LLMs benefit from demonstrations and have at least &lt;em>;some&lt;/em>; zero-shot abilities, we wondered whether the model&#39;s zero-shot outputs could serve as demonstrations for the model to prompt itself. The challenge is that zero-shot solutions are imperfect, and we risk giving LLMs poor quality demonstrations, which could be worse than no demonstrations at all. Indeed, the figure below shows that adding a correct demonstration to a question can lead to a correct solution of the test question (Demo1 with question), whereas adding an incorrect demonstration (Demo 2 + questions, Demo 3 with questions) leads to incorrect answers 。 Therefore, we need to select reliable self-generated demonstrations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5QShw0e9OdtzA9Ob5vAtWloPZGIZgCGFSL9okO-5pxIN3M2QciE1MOT73NsdaHwOz5LiH94IZRkE0SYSJ2fiiBSNR_qHCrO-URHqZPq7Fhhpvm6wtykLsySfF9l5ERsbOqtT_J1-A8pjoOa9htdvXEnL4-WRbP-Kn85aIz7G-BvWNflcaG4lUPomVlLU8 /s1051/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1051&quot; data-original-width=&quot;776&quot; height =&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5QShw0e9OdtzA9Ob5vAtWloPZGIZgCGFSL9okO-5pxIN3M2QciE1MOT73NsdaHwOz5LiH94IZRkE0SYSJ2fiiBSNR_qHCrO-URHqZPq7Fhhpvm6wtykLsySfF9l5ERsbOqtT_J1-A8pjoOa9htdvXEnL4-WRbP-Kn85aIz7G-BvWNflcaG4lUPomVlLU8/w472-h640/image6.png&quot; width=&quot;472&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>; Example inputs &amp;amp; outputs for reasoning tasks, which illustrates the need for carefully designed selection procedure for in-context demonstrations (&lt;a href=&quot;https://arxiv.org/abs/1608.01413&quot;>;MultiArith&lt;/a>;&amp;nbsp;dataset &amp;amp;&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM-62B&lt;/a>;&amp;nbsp;model): (1) zero-shot&amp;nbsp;&lt;/em>;&lt;a href=&quot;https://blog.research.google/2022/05/language-models-perform-reasoning-via.html&quot; style=&quot;text-align: left;&quot;>;chain-of-thought&lt;/a>;&lt;em style=&quot;text-align: left;&quot;>;&amp;nbsp;with no demo: correct logic but wrong answer; (2) correct demo (Demo1) and correct answer; (3) correct but repetitive demo (Demo2) leads to repetitive outputs; (4) erroneous demo (Demo3) leads to a wrong answer; but (5) combining Demo3 and Demo1 again leads to a correct answer.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; COSP leverages a key observation of LLMs: that confident and consistent predictions are more likely correct. This observation, of course, depends on how good the uncertainty estimate of the LLM is. Luckily, in large models, &lt;a href=&quot;https://arxiv.org/abs/2207.05221&quot;>;previous&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2210.11610&quot;>;works&lt;/a>; suggest that the uncertainty estimates are robust. Since measuring confidence requires only model predictions, not labels, we propose to use this as a zero-shot proxy of correctness. The high-confidence outputs and their inputs are then used as &lt;em>;pseudo&lt;/em>;-demonstrations. &lt;/p>; &lt;p>; With this as our starting premise, we estimate the model&#39;s confidence in its output based on its &lt;a href=&quot;https://arxiv.org/abs/2203.11171&quot;>;self-consistency&lt;/a>; and use this measure to select robust self-generated demonstrations. We ask LLMs the same question multiple times with zero-shot &lt;a href=&quot;https://blog.research.google/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; (CoT) prompting. To guide the model to generate a range of possible rationales and final answers, we include randomness controlled by a “temperature” hyperparameter. In an extreme case, if the model is 100% certain, it should output identical final answers each time. We then compute the entropy of the answers to gauge the uncertainty — the answers that have high self-consistency and for which the LLM is more certain, are likely to be correct and will be selected. &lt;/p>; &lt;p>; Assuming that w&lt;em>;e&lt;/em>; are presented with a collection of unlabeled questions, the COSP method is: &lt;/p>; &lt;ol>; &lt;li>;Input each unlabeled question into an LLM, obtaining multiple rationales and answers by sampling the model multiple times. The most frequent answers are highlighted, followed by a score that measures consistency of answers across multiple sampled outputs (higher is better). In addition to favoring more consistent answers, we also penalize repetition within a response (ie, with repeated words or phrases) and encourage diversity of selected demonstrations. We encode the preference towards consistent, un-repetitive and diverse outputs in the form of a scoring function that consists of a weighted sum of the three scores for selection of the self-generated pseudo-demonstrations.&lt;/li>; &lt;li>;We concatenate the pseudo-demonstrations into test questions, feed them to the LLM, and obtain a final predicted answer.&lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidn_WHASrzLG7rsqvKr5XqTi0VE0hEonQJsCRy0XHnM_DsWP4u1izMPUvfakpLd9FJvu47XXspD3vgIXrnrEhbrGR5vxSkcRRtbrU7HwHh6zLpywepMg39GUAW6uSVYxW-JzdOl5IVt9KPqLDVVCvjz86e6vQgZK79FKAF5wGdpztohbWhe7tVtVwcNBgh/s949/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;396&quot; data-original-width=&quot;949&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidn_WHASrzLG7rsqvKr5XqTi0VE0hEonQJsCRy0XHnM_DsWP4u1izMPUvfakpLd9FJvu47XXspD3vgIXrnrEhbrGR5vxSkcRRtbrU7HwHh6zLpywepMg39GUAW6uSVYxW-JzdOl5IVt9KPqLDVVCvjz86e6vQgZK79FKAF5wGdpztohbWhe7tVtVwcNBgh/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Illustration of COSP: In Stage 1 (&lt;strong>;left&lt;/strong>;), we run zero-shot CoT multiple times to generate a pool of demonstrations (each consisting of the question, generated rationale and prediction) and assign a score. In Stage 2 (&lt;strong>;right&lt;/strong>;), we augment the current test question with pseudo-demos (blue boxes) and query the LLM again. A majority vote over outputs from both stages forms the final prediction.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; COSP focuses on question-answering tasks with CoT prompting for which it is easy to measure self-consistency since the questions have unique correct answers. But this can be difficult for other tasks, such as open-ended question-answering or generative tasks that don&#39;t have unique answers (eg, text summarization). To address this limitation, we introduce USP in which we generalize our approach to other general NLP tasks: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Classification&lt;/em>; (CLS): Problems where we can compute the probability of each class using the neural network output logits of each class. In this way, we can measure the uncertainty without multiple sampling by computing the entropy of the logit distribution.&lt;/li>; &lt;li>;&lt;em>;Short-form generation&lt;/em>; (SFG): Problems like question answering where we can use the same procedure mentioned above for COSP, but, if necessary, without the rationale-generating step.&lt;/li>; &lt;li>;&lt;em>;Long-form generation&lt;/em>; (LFG):&lt;em>; &lt;/em>;Problems like summarization and translation, where the questions are often open-ended and the outputs are unlikely to be identical, even if the LLM is certain. In this case, we use an &lt;em>;overlap metric&lt;/em>; in which we compute the average of the &lt;em>;pairwise&lt;/em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE score&lt;/a>; between the different outputs to the same query.&lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjn2mgVNUmsKVPYKo3zrcQnq3nHT0xIzCk2rIOK0fSrFIOEkyCrx7MWNnTrOdwnFRlGbid1cj8OqV2xBCfOtgv5oiuUPoQjRY9CpMnjM79P0mQmoyQqluMPZsqFQUtS7AtPy5Uw-sf5UT_dV_bRbGWSRQiR5U2tDIYd2zxsk_lboJsKG4mcBZKxp5gEeT_T/s1360/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;640&quot; data-original-width=&quot;1360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjn2mgVNUmsKVPYKo3zrcQnq3nHT0xIzCk2rIOK0fSrFIOEkyCrx7MWNnTrOdwnFRlGbid1cj8OqV2xBCfOtgv5oiuUPoQjRY9CpMnjM79P0mQmoyQqluMPZsqFQUtS7AtPy5Uw-sf5UT_dV_bRbGWSRQiR5U2tDIYd2zxsk_lboJsKG4mcBZKxp5gEeT_T/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Illustration of USP in exemplary tasks (classification, QA and text summarization). Similar to COSP, the LLM first generates predictions on an unlabeled dataset whose outputs are scored with logit entropy, consistency or alignment, depending on the task type, and pseudo-demonstrations are selected from these input-output pairs. In Stage 2, the test instances are augmented with pseudo-demos for prediction.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We compute the relevant confidence scores depending on the type of task on the aforementioned set of unlabeled test samples. After scoring, similar to COSP, we pick the confident, diverse and less repetitive answers to form a model-generated pseudo-demonstration set. We finally query the LLM again in a few-shot format with these pseudo-demonstrations to obtain the final predictions on the entire test set. &lt;/p>; &lt;br />; &lt;h2>;Key Results&lt;/h2>; &lt;p>; For COSP, we focus on a set of six arithmetic and commonsense reasoning problems, and we compare against 0-shot-CoT (ie, “&lt;a href=&quot;https://arxiv.org/abs/2205.11916&quot;>;Let&#39;s think step by step&lt;/a>;“ only). We use self-consistency in all baselines so that they use roughly the same amount of computational resources as COSP. Compared across three LLMs, we see that zero-shot COSP significantly outperforms the standard zero-shot baseline. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsRevpmsBSIsIMti071v4FFnK-khtMXTbqvf92wvdvKQhyphenhyphenTCADcsIOQhAtr1kfxkmoaQ5MNKmcJiYol1JmxCchzYl9Kn_9h9eaGZAvJBrRxsvJorbngS4fLaChBk6e9wtHgE7JvPMHN1gajpnhgZaCwuYOjo-CTUR9x8PbXEEbRG3vp8elQzMib5Kv5Qh-/s1883/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1738&quot; data-original-width=&quot;1883&quot; height=&quot;369&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsRevpmsBSIsIMti071v4FFnK-khtMXTbqvf92wvdvKQhyphenhyphenTCADcsIOQhAtr1kfxkmoaQ5MNKmcJiYol1JmxCchzYl9Kn_9h9eaGZAvJBrRxsvJorbngS4fLaChBk6e9wtHgE7JvPMHN1gajpnhgZaCwuYOjo-CTUR9x8PbXEEbRG3vp8elQzMib5Kv5Qh-/w400-h369/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Key results of COSP in six arithmetic (&lt;a href=&quot;https://arxiv.org/abs/1608.01413&quot; style= &quot;text-align: left;&quot;>;MultiArith&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot; style=&quot;text-align: left;&quot;>;GSM-8K&lt;/a>; , &lt;a href=&quot;https://aclanthology.org/D14-1058/&quot; style=&quot;text-align: left;&quot;>;AddSub&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1162 /tacl_a_00160&quot; style=&quot;text-align: left;&quot;>;SingleEq&lt;/a>;) and commonsense (&lt;a href=&quot;https://doi.org/10.18653/v1/N19-1421&quot; style=&quot;text-align : left;&quot;>;CommonsenseQA&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1162/tacl_a_00370&quot; style=&quot;text-align: left;&quot;>;StrategyQA&lt;/a>;) reasoning tasks using &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM-62B, PaLM-540B&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;GPT-3 ( code-davinci-001)&lt;/a>; models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class =&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcbJaMBl3_Pc5S_mp9lDWz4v_SS6Mzd3QBxyn-tWdtnDNgJLX1YPRHkdPtJBanhADoEDTzJqOsk2x4uOFoV8e0h8Tml_Nt1kOd5eT7TyC-MIqWEVEiY_kZQZ0wkPU9T5HwIHXv2IHeLXdGz1MvWiQFxNNe7CaKDfUs0j9R6F8cgN4T0dgtgp86xsF9cK6l/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1999&quot; height=&quot;325&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcbJaMBl3_Pc5S_mp9lDWz4v_SS6Mzd3QBxyn-tWdtnDNgJLX1YPRHkdPtJBanhADoEDTzJqOsk2x4uOFoV8e0h8Tml_Nt1kOd5eT7TyC-MIqWEVEiY_kZQZ0wkPU9T5HwIHXv2IHeLXdGz1MvWiQFxNNe7CaKDfUs0j9R6F8cgN4T0dgtgp86xsF9cK6l/w640-h325/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;USP improves significantly on 0-shot performance. “CLS” is an average of 15 classification tasks; “SFG” is the average of five short-form generation tasks; “LFG” is the average of two summarization tasks. “SFG (BBH)” is an average of all BIG-Bench Hard tasks, where each question is in SFG format.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For USP, we expand our analysis to a much wider range of tasks, including more than 25 classifications, short-form generation, and long-form generation tasks. Using the state-of-the-art PaLM 2 models, we also test against the &lt;a href=&quot;https://arxiv.org/abs/2210.09261&quot;>;BIG-Bench Hard&lt;/a>; suite of tasks where LLMs have previously underperformed compared to people. We show that in all cases, USP again outperforms the baselines and is competitive to prompting with golden examples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsJh9EIxiF8m6jqOM7vYniG6LX8gFVa0W3V7_DEOwMCEZjbk_M4104Tg-3qMUAbYXaR3RqMTQsrZ8TN3Np5I9SXHBVJGjB6E6_1khoubPbbR-AjUnV37TYGBHu2DJpR50Ek34dwcFkjJB-OTqW4T4E-zjQhJqPX-f_PxSj7322nU2qH2VK0TBMO1QKUkfT/s901 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;833&quot; height=&quot; 640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsJh9EIxiF8m6jqOM7vYniG6LX8gFVa0W3V7_DEOwMCEZjbk_M4104Tg-3qMUAbYXaR3RqMTQsrZ8TN3Np5I9SXHBVJGjB6E6_1khoubPbbR-AjUnV37TYGBHu2DJpR50Ek34dwcFkjJB-OTqW4T4E-zjQhJqPX-f_PxSj7322nU2qH2VK0TBMO1QKUkfT/w592-h640/image5.png&quot; width=&quot;592&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Accuracy on BIG- Bench Hard tasks with PaLM 2-M (each line represents a task of the suite). The gain/loss of USP (green stars) over standard 0-shot (green triangles) is shown in percentages. “Human” refers to average human performance; “AutoCoT” and “Random demo” are baselines we compared against in the&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2305.14926&quot;>;paper&lt;/a>;; and “3-shot” is the few-shot performance for three handcrafted demos in CoT format.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also analyze the working mechanism of USP by validating the key observation above on the relation between confidence and correctness, and we found that in an overwhelming majority of the cases, USP picks confident predictions that are more likely better in all task types considered, as shown in the如下图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjahSci_hJqgqxS-my79ZlINe9Dpi0yaLrMTXeTu-HXjmmAmhVXtylXK7BUdJGIPMFhFvqv31Wd6ux2ti1hJLIi9Rr8j_PIjD9ElUTWx0ViaP1fLg63Q9lqvgd7U74Uqi45IOU3CsqHaqjO94iSLyPD2dZCJf5x5hsHfU_6yukbJHpG6wDzdSVp7nwBHNFx/s1474/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;507&quot; data-original-width=&quot;1474&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjahSci_hJqgqxS-my79ZlINe9Dpi0yaLrMTXeTu-HXjmmAmhVXtylXK7BUdJGIPMFhFvqv31Wd6ux2ti1hJLIi9Rr8j_PIjD9ElUTWx0ViaP1fLg63Q9lqvgd7U74Uqi45IOU3CsqHaqjO94iSLyPD2dZCJf5x5hsHfU_6yukbJHpG6wDzdSVp7nwBHNFx/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;USP picks confident predictions that are more likely better. Ground-truth performance metrics against USP confidence scores in selected tasks in various task types (blue: CLS, orange: SFG, green: LFG) with PaLM-540B.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Zero-shot inference is a highly sought-after capability of modern LLMs, yet the success in which poses unique challenges. We propose COSP and USP, a family of versatile, zero-shot automatic prompting techniques applicable to a wide range of tasks. We show large improvement over the state-of-the-art baselines over numerous task and model combinations. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was conducted by Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Martin Eisenschlos, Sercan Ö. Arık, and Tomas Pfister. We would like to thank Jinsung Yoon Xuezhi Wang for providing helpful reviews, and other colleagues at Google Cloud AI Research for their discussion and feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog .research.google/feeds/4882962745371313901/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2023/11/zero-shot-adaptive-prompting-of-large.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http:/ /www.blogger.com/feeds/8474926331452026626/posts/default/4882962745371313901&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/ 8474926331452026626/posts/default/4882962745371313901&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/zero-shot-adaptive-prompting -of-large.html&quot; rel=&quot;alternate&quot; title=&quot;Zero-shot adaptive prompting of large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http ://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/ g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot; 72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqTosaeIs4AMYukkmgUBEii6iQcrr9_dNKM0cHnW6m9Wi8yX0V-QCduQfkqLyQPNpVbze3OFO-nPG5Wm9DLMM8pEAfhQGq-TEJroLJGQyqNr-hlJNkToBCmgJbphrCRvlv95gkDQH0ScT7VrXu2VCTKyiWy8yYM4G8voF0kD0K2oxwg2M2xBcz1yQnU0Zb/s72-c/USP.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http ://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：博客-8474926331452026626.post-4489259534129284932&lt;/id>;&lt;published>;2023-11-01T10:30:00.002-07:00&lt;/published>;&lt;updated>;2023-11-06T09:07:40.958-08:00&lt;/updated>; &lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MetNet-3: A state-of-the- art neural weather model available in Google products&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Samier Merchant, Google Research, and Nal Kalchbrenner, Google DeepMind&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdgnhML03N9vxEdGH1TkBATtxGpjyO5XYgZwJY5dY0-sPIAvrmCll4J8I9owyJTNOHZdq6MMZskWsYJDZivZA_zvj2atWhUsPoxWnNyifiFAm83GC2EsZ4xgre8bCk32Yzv3vlR4pGn12H7T5Vkbz5BaErZ22JRB-OqveQ7EDHsrCYjKN65Soc1FrZNwvu/s1600/metnethero1.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Forecasting weather variables such as precipitation, temperature, and wind is key to numerous aspects of society, from daily planning and transportation to energy production.随着我们不断看到更多的洪水、干旱和热浪等极端天气事件，准确的预报对于准备和减轻其影响至关重要。未来的前 24 小时尤其重要，因为它们具有高度可预测性和可操作性，可以帮助人们及时做出明智的决策并保证安全。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today we present a new weather model called &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;MetNet-3&lt;/a>;, developed by Google Research and Google DeepMind. Building on the earlier &lt;a href=&quot;https://blog.research.google/2020/03/a-neural-weather-model-for-eight-hour.html?m=1&quot;>;MetNet&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;MetNet-2&lt;/a>; models, MetNet-3 provides high resolution predictions up to 24 hours ahead for a larger set of core variables, including precipitation, surface temperature, wind speed and direction, and dew point. MetNet-3 创建时间上平滑且高度精细的预测，提前时间间隔为 2 分钟，空间分辨率为 1 至 4 公里。 MetNet-3 achieves strong performance compared to traditional methods, outperforming the best single- and multi-member physics-based &lt;a href=&quot;https://en.wikipedia.org/wiki/Numerical_weather_prediction&quot;>;numerical weather prediction&lt;/a>; (NWP) models — such as &lt;a href=&quot;https://rapidrefresh.noaa.gov/hrrr/&quot;>;High-Resolution Rapid Refresh&lt;/a>; (HRRR) and &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts#:~:text=ENS%20is%20a%20probabilistic%20forecast,high%20winds%20or%20heavy%20rain).&quot;>;ensemble forecast suite&lt;/a>; (ENS) — for multiple regions up to 24 hours ahead. &lt;/p>; &lt;p>; Finally, we&#39;ve integrated MetNet-3&#39;s capabilities across various Google &lt;a href=&quot;https://support.google.com/websearch/answer/13692898&quot;>;products and technologies&lt;/a>; where weather is relevant. MetNet-3 目前在美国本土和欧洲部分地区推出，重点是 12 小时降水预报，正在帮助为多个国家和语言的人们提供准确可靠的天气信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQcwPsDQPUe4Uon7vWWSewbqcWsAdfUIJ4yLLFiCvdQKu4ffT6E5qIMeiabtxK5wudSL-jjxa_fW5aOaBvDILq_dQzeT4RMSULORJZrjwkDscDxLnLflUybqHlPf1J8O7KB171g5I9kLVgRbGP0mr0HxbG0pY7J9ojoEZLl4JZHaMQH490XmUR_IUj_YMO/s904/image55.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQcwPsDQPUe4Uon7vWWSewbqcWsAdfUIJ4yLLFiCvdQKu4ffT6E5qIMeiabtxK5wudSL-jjxa_fW5aOaBvDILq_dQzeT4RMSULORJZrjwkDscDxLnLflUybqHlPf1J8O7KB171g5I9kLVgRbGP0mr0HxbG0pY7J9ojoEZLl4JZHaMQH490XmUR_IUj_YMO/s16000/image55.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixRY-kzsepNdP_arXnJbHPJFViN_N4CzjOYH_1YxfjIDI5Nben4u8BoJ-tcYrrw4a3Jp7HFBGmakeBMqKAINeVFssClJHNUjvBhYHY6vpy6nOdpEoFDhCulwIE8OM9e7fRRwXqW01AeWUJjqmnNDn32ScCeQ2S64aNvDgigDes5vWA1_RrT7oMxK8sttG7/s904/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEixRY-kzsepNdP_arXnJbHPJFViN_N4CzjOYH_1YxfjIDI5Nben4u8BoJ-tcYrrw4a3Jp7HFBGmakeBMqKAINeVFssClJHNUjvBhYHY6vpy6nOdpEoFDhCulwIE8OM9e7fRRwXqW01AeWUJjqmnNDn32ScCeQ2S64aNvDgigDes5vWA1_RrT7oMxK8sttG7/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing= “0”类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MetNet-3 precipitation output summarized into actionable forecasts in Google Search on mobile.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Densification of sparse observations&lt;/h2>; &lt;p>; Many recent machine learning weather models use the atmospheric state generated by traditional methods (eg, data assimilation from NWPs) as the primary starting point to build forecasts. In contrast, a defining feature of the MetNet models has been to use direct observations of the atmosphere for training and evaluation. The advantage of direct observations is that they often have higher fidelity and resolution. However, direct observations come from a large variety of sensors at different altitudes, including weather stations at the surface level and satellites in orbit, and can be of varying degrees of sparsity. For example, precipitation estimates derived from radar such as &lt;a href=&quot;https://mrms.nssl.noaa.gov/&quot;>;NOAA&#39;s Multi-Radar/Multi-Sensor System&lt;/a>; (MRMS) are relatively dense images, whereas weather stations located on the ground that provide measurements for variables such as temperature and wind are mere points spread over a region. &lt;/p>; &lt;p>; In addition to the data sources used in previous MetNet models, MetNet-3 includes point measurements from weather stations as both inputs and targets with the goal of making a forecast at all locations.为此，MetNet-3 的关键创新是一种称为致密化的技术，它将基于物理的模型中传统的数据同化和模拟两步过程合并到神经网络的单次传递中。致密化的主要组成部分如下所示。尽管致密化技术单独适用于特定的数据流，但由此产生的致密化预测受益于进入 MetNet-3 的所有其他输入流，包括地形、卫星、雷达和 NWP 分析功能。 MetNet-3 的默认输入中不包含 NWP 预报。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie1m1p0i-MWhS7Ih5RGzV-AQuDDPwgao4SpmnSUTdSsy7fcEwk4Soj5IJ8FqtGjhvi4ot2HKZdaQh3Hpu4CviRsx7FujT_4bbvpV8mu15Zt5bO5KbMGaaqIZoAGUp77ltVYH-zt2HTwVxbuGZHJt-0lbXZT-ukJH_KtB3pnHdRrRpZ2r5WgMSNGXnu-H8j /s1929/image22.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1929&quot; data-original-width=&quot;1600&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie1m1p0i-MWhS7Ih5RGzV-AQuDDPwgao4SpmnSUTdSsy7fcEwk4Soj5IJ8FqtGjhvi4ot2HKZdaQh3Hpu4CviRsx7FujT_4bbvpV8mu15Zt5bO5KbMGaaqIZoAGUp77ltVYH-zt2HTwVxbuGZHJt-0lbXZT-ukJH_KtB3pnHdRrRpZ2r5WgMSNGXnu-H8j/s16000/image22.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;A&lt;/b>;) During training, a fraction of the weather stations are masked out from the input while kept in the target. &lt;b>;B&lt;/b>;) To evaluate generalization to untrained locations, a set of weather stations represented by squares is never used for training and is only used for evaluation. &lt;b>;C&lt;/b>;) Data from these held out weather stations with sparse coverage is included during evaluation to determine prediction quality in these areas. &lt;b>;D&lt;/b>;) The final forecasts use the full set of training weather stations as input and produce fully dense forecasts aided by spatial parameter sharing.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;High resolution in space and time&lt;/h2>; &lt;p>; A central advantage of using direct observations is their high spatial and temporal解决。例如，气象站和地面雷达站分别以 1 公里的分辨率每隔几分钟在特定点提供测量结果； this is in stark contrast with the assimilation state from the state-of-the-art model &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts#:~:text=ENS%20is%20a%20probabilistic%20forecast,high%20winds%20or%20heavy%20rain).&quot;>;ENS&lt;/a>;, which is generated every 6 hours at a resolution of 9 km with hour-by-hour forecasts. To handle such a high resolution, MetNet-3 preserves another of the defining features of this series of models, &lt;em>;lead time conditioning&lt;/em>;.以分钟为单位的预测提前时间直接作为神经网络的输入给出。这使得 MetNet-3 能够有效地对短至 2 分钟的时间间隔内的观测的高时间频率进行建模。 Densification combined with lead time conditioning and high resolution direct observations produces a fully dense 24 hour forecast with a temporal resolution of 2 minutes, while learning from just 1,000 points from the &lt;a href=&quot;https://madis.ncep.noaa.gov/madis_OMO.shtml&quot;>;One Minute Observation&lt;/a>; (OMO) network of weather stations spread across the United States. &lt;/p>; &lt;p>; MetNet-3 predicts a marginal multinomial probability distribution for each output variable and each location that provides rich information beyond just the mean. This allows us to compare the probabilistic outputs of MetNet-3 with the outputs of advanced probabilistic ensemble NWP models, including the ensemble forecast ENS from the &lt;a href=&quot;https://www.ecmwf.int/&quot;>;European Centre for Medium-Range Weather Forecasts&lt;/a>; and the &lt;a href=&quot;https://www.spc.noaa.gov/exper/href/&quot;>;High Resolution Ensemble Forecast&lt;/a>; (HREF) from the &lt;a href=&quot;https://www.noaa.gov/&quot;>;National Oceanic and Atmospheric Administration of the US&lt;/a>;. Due to the probabilistic nature of the outputs of both models, we are able to compute scores such as the &lt;a href=&quot;https://confluence.ecmwf.int/display/FUG/Section+12.B+Statistical+Concepts+-+Probabilistic+Data#:~:text=The%20Continuous%20Ranked%20Probability%20Score,the%20forecast%20is%20wholly%20inaccurate.&quot;>;Continuous Ranked Probability Score&lt;/a>; (CRPS).下图突出显示了致密化结果，并说明 MetNet 的预测不仅具有更高的分辨率，而且在重叠交付周期进行评估时也更加准确。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJsf6Y6gV9VjK_rS_Bf_WLWdsJOq3sQbdaW26VSp2vX1Fq5j7VcWl4VDi3BeBFpEcH_YGrkU9ozJyuP5dh8tWWCU4yGzlmGBTfwM-kXGKZvdvI1DF17V4kSJSGGBIacqaCO4N1Oc8P4PymPWdglJbew_cjP9reFSJuHR3_ikZfZFuzN6aC8F17TAtiJPIg/s768/image44.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiJsf6Y6gV9VjK_rS_Bf_WLWdsJOq3sQbdaW26VSp2vX1Fq5j7VcWl4VDi3BeBFpEcH_YGrkU9ozJyuP5dh8tWWCU4yGzlmGBTfwM-kXGKZvdvI1DF17V4kSJSGGBIacqaCO4N1Oc8P4PymPWdglJbew_cjP9reFSJuHR3_ikZfZFuzN6aC8F17TAtiJPIg/s16000/image44.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;b>;Top&lt;/b>;: MetNet-3&#39;s forecast of wind speed for each 2 minutes over the future 24 hours with a spatial resolution of 4km. &lt;b>;Bottom&lt;/b>;: ENS&#39;s hourly forecast with a spatial resolution of 18 km. &lt;br />;The two distinct regimes in spatial structure are primarily driven by the presence of the Colorado mountain ranges.较暗对应于较高的风速。 More samples available here: &lt;a href=&quot;https://youtube.com/watch?v=iB1DzHNqH_o&quot;>;1&lt;/a>;, &lt;a href=&quot;https://youtube.com/watch?v=LlWB558jKJk&quot;>;2&lt;/a>;, &lt;a href=&quot;https://youtube.com/watch?v=74bFo3nkbe4&quot;>;3&lt;/a>;, &lt;a href=&quot;https://youtube.com/watch?v=MKUzYQZn9sQ&quot;>;4&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBhlSum7x274E9KQGzLnjM9iXNEhifOJjKzt1Cwa5YyABCbaB68Mkr3gFvIVUhyphenhyphenaIGOqUE78MqGTK992NK8zrdKrqKxtFlYf1qeWYNkTa4PVzD3u_9lmQAjKnbLILHAkPhIOCvyAI6qBtfyf-z_xgUys3gXRJd_GSs3-qnyq0yFbjvmxdXAbVldV-xrIRJ/s1120/image11.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;694&quot; data-original-width=&quot;1120&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBhlSum7x274E9KQGzLnjM9iXNEhifOJjKzt1Cwa5YyABCbaB68Mkr3gFvIVUhyphenhyphenaIGOqUE78MqGTK992NK8zrdKrqKxtFlYf1qeWYNkTa4PVzD3u_9lmQAjKnbLILHAkPhIOCvyAI6qBtfyf-z_xgUys3gXRJd_GSs3-qnyq0yFbjvmxdXAbVldV-xrIRJ/s16000/image11.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance comparison between MetNet-3 and NWP baseline for wind speed based on CRPS (lower is better).在超本地设置中，测试气象站的值在评估期间作为网络的输入给出； the results improve further especially in the early lead times.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In contrast to weather station variables, precipitation estimates are more dense as they come from ground radar. MetNet-3 的降水建模与 MetNet-1 和 2 类似，但将 1 公里空间粒度的高分辨率降水预报扩展到与其他变量相同的 24 小时提前时间，如下面的动画所示。在整个 24 小时范围内，MetNet-3 在降水方面的性能实现了比 ENS 更好的 CRPS 值。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidyInWvqdPpdIndLrxzykAODCeJ_p69uqvYOpasjFcBQU5o8Mtr-DiLfZXZrkJel9TD9SxZEmyIb58r6TZjRw57D8aSjl9P2jxCOsK7XZeXY0J3B8UMIFnl6aqXqhd0wft_NQGBi9KqpSUHAgw2c4JoYMdt27sKp6xcvOyMfjASpaZZzlI9o8lesj3GsrL/s720/image14.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;683&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEidyInWvqdPpdIndLrxzykAODCeJ_p69uqvYOpasjFcBQU5o8Mtr-DiLfZXZrkJel9TD9SxZEmyIb58r6TZjRw57D8aSjl9P2jxCOsK7XZeXY0J3B8UMIFnl6aqXqhd0wft_NQGBi9KqpSUHAgw2c4JoYMdt27sKp6xcvOyMfjASpaZZzlI9o8lesj3GsrL/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Case study for Thu Jan 17 2019 00:00 UTC showing the probability of instantaneous precipitation rate being above 1 mm/h on CONUS.较暗对应于较高的概率值。 The maps also show the prediction threshold when optimized towards Critical Success Index &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;CSI&lt;/a>; (dark blue contours).这个具体的案例研究显示了美国中部新的大降水模式的形成；这不仅仅是对现有模式的预测。 &lt;br />;&lt;b>;Top:&lt;/b>; ENS&#39;s hourly forecast. &lt;b>;Center:&lt;/b>; Ground truth, source NOAA&#39;s MRMS. &lt;b>;Bottom:&lt;/b>; Probability map as predicted by MetNet-3. &lt;a href=&quot;https://www.youtube.com/watch?v=TXqR9lL4368&quot;>;Native resolution available here.&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_4M2Sz50c_PDZkyHqZGfc5p5aRGpAS04ztN9N3s3VBn4_AD8GN7Vv6Vw-2phokpqtamutHT_6nGSsXb7271cfijLu3vJT1IV8Mmo1wlq1jfYcUPNs7TL6z0Cls3qGD1jA4Z0uRpj_rNXYLpFSbHEIqNOAA_V8VE_ZhsO7o-D64nDdmRei_hPEY7YT8lcg/s1102/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;682&quot; data-original-width=&quot;1102&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_4M2Sz50c_PDZkyHqZGfc5p5aRGpAS04ztN9N3s3VBn4_AD8GN7Vv6Vw-2phokpqtamutHT_6nGSsXb7271cfijLu3vJT1IV8Mmo1wlq1jfYcUPNs7TL6z0Cls3qGD1jA4Z0uRpj_rNXYLpFSbHEIqNOAA_V8VE_ZhsO7o-D64nDdmRei_hPEY7YT8lcg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance comparison between MetNet-3 and NWP baseline for instantaneous precipitation rate on CRPS (lower is better).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Delivering realtime ML forecasts&lt;/h2>; &lt;p>; Training and evaluating a weather forecasting model like MetNet-3 on historical data is only a part of the process of delivering ML-powered forecasts to users.开发用于天气预报的实时机器学习系统时需要考虑很多因素，例如从多个不同来源获取实时输入数据、运行推理、实现输出的实时验证、从模型的丰富输出中构建洞察带来直观的用户体验，并以 Google 规模提供结果——所有这些都是连续循环的，每隔几分钟刷新一次。 &lt;/p>; &lt;p>; We developed such a real-time system that is capable of producing a precipitation forecast every few minutes for the entire contiguous United States and for 27 countries in Europe for a lead time of up to 12 hours. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg88AA6lzoFtJd9ZOXt6AiiT_gTtFcJwsZNzUJ63kuYtq7XYs0LHUSp3q37zOPolA-rR_WQPciuDZsg-4Y3J0qrLUmNxMi1iBqyR4ICy4MKwRFXHtQhfkWdwPREd4qm9FVlN6rpLEebDC7MfBg7hToXhQvdsFoGObtu-Lqty3ZQSALf1yjna37tJY4fAptE/s1600/image6.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;367&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg88AA6lzoFtJd9ZOXt6AiiT_gTtFcJwsZNzUJ63kuYtq7XYs0LHUSp3q37zOPolA-rR_WQPciuDZsg-4Y3J0qrLUmNxMi1iBqyR4ICy4MKwRFXHtQhfkWdwPREd4qm9FVlN6rpLEebDC7MfBg7hToXhQvdsFoGObtu-Lqty3ZQSALf1yjna37tJY4fAptE/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the process of generating precipitation forecasts using MetNet-3.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The system&#39;s uniqueness stems from its use of near-continuous inference, which allows the model to constantly create full forecasts based on incoming data streams.这种推理模式不同于传统的推理系统，由于输入数据的独特特征，这种推理模式是必要的。该模型采用各种数据源作为输入，例如雷达、卫星和数值天气预报同化。这些输入中的每一个都具有不同的刷新频率以及空间和时间分辨率。一些数据源（例如天气观测和雷达）具有类似于连续数据流的特征，而其他数据源（例如数值天气预报同化）则类似于批量数据。该系统能够在空间和时间上对齐所有这些数据源，从而使模型能够以非常高的节奏创建对未来 12 小时降水的最新了解。 &lt;/p>; &lt;p>; With the above process, the model is able to predict arbitrary discrete probability distributions.我们开发了新颖的技术，将这种密集的输出空间转换为用户友好的信息，从而在整个 Google 产品和技术中提供丰富的体验。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Weather features in Google products&lt;/h2>; &lt;p>; People around the world rely on Google every day to provide helpful, timely, and accurate information about the weather.这些信息可用于多种目的，例如规划户外活动、旅行打包以及在恶劣天气事件中保持安全。 &lt;/p>; &lt;p>; The state-of-the-art accuracy, high temporal and spatial resolution, and probabilistic nature of MetNet-3 makes it possible to create unique hyperlocal weather insights. For the contiguous United States and Europe, MetNet-3 is operational and produces real-time 12 hour precipitation forecasts that are now served across Google &lt;a href=&quot;https://support.google.com/websearch/answer/13692898&quot;>;products and technologies&lt;/a>; where weather is relevant, such as Search.模型的丰富输出被合成为可操作的信息，并立即提供给数百万用户。 &lt;/p>; &lt;p>; For example, a user who searches for weather information for a precise location from their mobile device will receive highly localized precipitation forecast data, including timeline graphs with granular minute breakdowns depending on the product. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4R591KKD1ZkmhTrjo28JovCeeo2bGjb0Tn5Ohr8KEooVqZqSNlgsrJrROaPWn5XXBzEohkhZMjaX2AV3M1RikyLgO7LfIgTFt54-uumb7xxPU6blnuFC8dN8W2SjK85tBKfZQ9Kn4oR-988YKXVUTbu-N5LWWX6JurqN6RRad7Bve59oEdZC-eMsn4HH9/s600/metnet3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;600&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4R591KKD1ZkmhTrjo28JovCeeo2bGjb0Tn5Ohr8KEooVqZqSNlgsrJrROaPWn5XXBzEohkhZMjaX2AV3M1RikyLgO7LfIgTFt54-uumb7xxPU6blnuFC8dN8W2SjK85tBKfZQ9Kn4oR-988YKXVUTbu-N5LWWX6JurqN6RRad7Bve59oEdZC-eMsn4HH9/s16000/metnet3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MetNet-3 precipitation output in weather on the Google app on Android (&lt;b>;left&lt;/b>;) and mobile web Search (&lt;b>;right&lt; /b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2 >; &lt;p>; MetNet-3 is a new deep learning model for weather forecasting that outperforms state-of-the-art physics-based models for 24-hour forecasts of a core set of weather variables.它有潜力为天气预报创造新的可能性，并提高许多活动的安全性和效率，例如交通、农业和能源生产。 MetNet-3 已投入运行，其预报可在多个与天气相关的 Google 产品中提供。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of this effort 。我们要特别感谢 Google DeepMind（Di Li、Jeremiah Harmsen、Lasse Espeholt、Marcin Andrychowicz、Zack Ontiveros）、Google Research（Aaron Bell、Akib Uddin、Alex Merose、Carla Bromberg、Fred Zyda、Isalo Montacute、Jared Sisk）的人员、杰森·希基、卢克·巴林顿、马克·杨、玛雅·托希迪、娜塔莉·威廉姆斯、普拉莫德·古普塔、施瑞亚·阿格拉瓦尔、托马斯·特恩布尔、汤姆·斯莫尔、泰勒·拉塞尔）和 Google 搜索（奥古斯丁·佩斯西亚洛、比尔·迈尔斯、丹尼·切雷斯尼克、乔纳森·卡什、利奥尔·科恩） , Maca Piombi, Maia Diamant, Max Kamenetsky, Maya Ekron, Mor Schlesinger, Neta Gefen-Doron, Nofar Peled Levi, Ofer Lehr, Or Hillel, Rotem Wertman, Tamar Shevach,Vinay Ruelius Shah, Yechie Labai).&lt;/em>;&lt; /p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4489259534129284932/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4489259534129284932&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4489259534129284932&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http: //blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html&quot; rel=&quot;alternate&quot; title=&quot;MetNet-3: A state-of-the-art neural weather model available in Google products&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhdgnhML03N9vxEdGH1TkBATtxGpjyO5XYgZwJY5dY0-sPIAvrmCll4J8I9owyJTNOHZdq6MMZskWsYJDZivZA_zvj2atWhUsPoxWnNyifiFAm83GC2EsZ4xgre8bCk32Yzv3vlR4pGn12H7T5Vkbz5BaErZ22JRB-OqveQ7EDHsrCYjKN65Soc1FrZNwvu/s72-c/metnethero1.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4973261706617691472&lt;/id>;&lt;published>;2023-10-27T13:22:00.001- 07:00&lt;/published>;&lt;updated>;2023-10-31T14:14:34.982-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot; Android Wear&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Audioplethysmography for cardiac monitoring with hearable devices&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Xiaoran &quot;Van&quot; Fan, Experimental Scientist, and Trausti Thormundsson, Director, Google &lt;/span>; &lt;img src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnCItsRGs939CGCBBAi5fA-xwk9JQi7b2CQ3voKK583p4uTkQkLIXUd1lU71SLynom0Yt78bRxDflolUzzfol5UbfkPmCaNn8bIUxwAbLDatqZTPxP7SYOT45g9qJODdM1kvT6NQUsixtFTiBYr_h_Hx-pFRsmbcBKdnW3WkbcD4Bcr_cZE590VL-cSu-a/s320/hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; The market for &lt;a href=&quot;https://www.telink-semi.com/introduction-true-wireless-stereo/&quot;>;true wireless stereo&lt;/a>; (TWS) &lt;a href=&quot; https://en.wikipedia.org/wiki/Active_noise_control&quot;>;active noise canceling&lt;/a>; (ANC) hearables (headphones and earbuds) has been soaring in recent years, and the global shipment volume will nearly &lt;a href=&quot; https://www.idc.com/promo/wearablevendor&quot;>;double&lt;/a>; that of smart wristbands and watches in 2023. The on-head time for hearables has extended significantly due to the recent advances in ANC, transparency mode,和人工智能。 Users frequently wear hearables not just for music listening, but also for exercising, focusing, or simply mood adjustment. However, hearable health is still mostly uncharted territory for the consumer market. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;APG: Audioplethysmography for Cardiac Monitoring in Hearables&lt;/a>;,” presented at &lt;a href=&quot;https://sigmobile.org/mobicom/2023/&quot;>;MobiCom 2023&lt;/a>;, we introduce a novel active in-ear health sensing modality. Audioplethysmography (APG) enables ANC hearables to monitor a user&#39;s physiological signals, such as heart rate and heart rate variability, without adding extra sensors or compromising battery life. APG exhibits high resilience to motion artifacts, adheres to &lt;a href=&quot;https://www.health.belgium.be/sites/default/files/uploads/fields/fpshealth_theme_file/19099349/Canadian%20guidelines%20for%20ultrasound.pdf&quot;>;safety regulations&lt;/a>; with an 80 dB margin below the limit, remains unaffected by seal conditions, and is inclusive of all skin tones. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3if_joqkSgp5nI0YvfWgUtgi3FoOtmeNcwcDdNNaMlPizqHpbuaeVFrp1MguLPQpT0hqrQaldH0ymqwjDzsD-u3qDR-r8Z0rnB7lsDuF9iab9CdR6GRPI7OPtqLFR29mRwfHz06kwqhppe7mvI9iVIBi8DtpIYh6E-UXs1RwUffTRxKVViTuXpSLXLAXI/s1600/image4.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3if_joqkSgp5nI0YvfWgUtgi3FoOtmeNcwcDdNNaMlPizqHpbuaeVFrp1MguLPQpT0hqrQaldH0ymqwjDzsD-u3qDR-r8Z0rnB7lsDuF9iab9CdR6GRPI7OPtqLFR29mRwfHz06kwqhppe7mvI9iVIBi8DtpIYh6E-UXs1RwUffTRxKVViTuXpSLXLAXI/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;APG sends a low intensity ultrasound transmitting wave (TX wave) using an ANC headphone&#39;s speakers and collects the receiving wave (RX wave) via the on-board feedback microphones. The APG signal is a pulse-like waveform that synchronizes with heartbeat and reveals rich cardiac information, such as &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/32403992/#:~:text=The%20dicrotic%20notch%20is%20a,of%20diastole%20in%20these%20arteries.&quot;>;dicrotic notches&lt;/a>;. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Health sensing in the ear canal&lt;/h2>; &lt;p>; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Ear_canal&quot;>;auditory canal&lt;/a>; receives its blood supply from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_auricular_artery&quot;>;arteria auricularis profunda&lt;/a>;, also known as the deep ear artery. This artery forms an intricate network of smaller vessels that extensively permeate the auditory canal. Slight variations in blood vessel shape caused by the heartbeat (and &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8834671&quot;>;blood pressure&lt;/a>;) can lead to subtle changes in the volume and pressure of the ear canals, making the ear canal an ideal location for health sensing. &lt;/p>; &lt;p>; Recent research has explored using hearables for health sensing by packaging together a plethora of sensors — eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram&quot;>;photoplethysmograms&lt;/a>; (PPG) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Electrocardiography&quot;>;electrocardiograms&lt;/a>; (ECG) — with a microcontroller to enable health applications, such as sleep monitoring, heart rate and blood pressure tracking. However, this sensor mounting paradigm inevitably adds cost, weight, power consumption, acoustic design complexity, and form factor challenges to hearables, constituting a strong barrier to its wide adoption. &lt;/p>; &lt;p>; Existing ANC hearables deploy feedback and feedforward microphones to navigate the ANC function. These microphones create &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3550314&quot;>;new opportunities&lt;/a>; for various sensing applications as they can detect or record many bio-signals inside and outside耳道。 For example, feedback microphones can be used to listen to heartbeats and feedforward microphones can hear respirations. &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3458864.3467680&quot;>;Academic research&lt;/a>; on this passive sensing paradigm has prompted many mobile applications, including heart rate monitoring, ear disease diagnosis, respiration monitoring, and body activity recognition. However, microphones in consumer-grade ANC headphones come with &lt;a href=&quot;https://research.google/pubs/pub52579/&quot;>;built-in high-pass filters&lt;/a>; to prevent saturation from body motions or strong wind噪音。 The signal quality of passive listening in the ear canal also heavily relies on the earbud seal conditions. As such, it is challenging to embed health features that rely on the passive listening of low frequency signals (≤ 50 Hz) on commercial ANC headphones. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Measuring tiny physiological signals&lt;/h2>; &lt;p>; APG bypasses the aforementioned ANC headphone hardware constraints by sending a low intensity ultrasound probing signal through an ANC headphone&#39;s speakers. This signal triggers echoes, which are received via on-board feedback microphones. We observe that the tiny ear canal skin displacement and heartbeat vibrations modulate these ultrasound echoes. &lt;/p>; &lt;p>; We build a &lt;a href=&quot;https://en.wikipedia.org/wiki/Acoustic_resonance&quot;>;cylindrical resonance&lt;/a>; model to understand APG&#39;s underlying physics. This phenomenon happens at an extremely small scale, which makes the raw pulse signal invisible in the raw received ultrasound. We adopt &lt;a href=&quot;https://www.rp-photonics.com/optical_heterodyne_detection.html&quot;>;coherent detection&lt;/a>; to retrieve this micro physiological modulation under the noise floor (we term this retrieved signal as mixed-down signal, see the &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;paper&lt;/a>; for more details). The final APG waveform looks strikingly similar to a PPG waveform, but provides an improved view of cardiac activities with more pronounced &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/32403992/#:~:text=The%20dicrotic%20notch%20is%20a,of%20diastole%20in%20these%20arteries.&quot;>;dicrotic notches&lt;/a>; (ie, pressure waveforms that provide rich insights about the central artery system, such as blood pressure). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEoN64wQBOUKAhV6SHbEMqIcAuVCk01zBki8pnV1xoEseaJCT1uNRprkjKyFLYRin4a-iSrPjg3Pr-eEgc_YrOYihU61CaKl8CN5K8ET_g8vOFxHhUKQaq7Fb9xMhGVVLMS_AouTZvfDfXYRVz0ExhCIV4je2Hxqd-CVfKmmpfGjLIGeKawPEff6Ei5gfT/s1600/image5.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEoN64wQBOUKAhV6SHbEMqIcAuVCk01zBki8pnV1xoEseaJCT1uNRprkjKyFLYRin4a-iSrPjg3Pr-eEgc_YrOYihU61CaKl8CN5K8ET_g8vOFxHhUKQaq7Fb9xMhGVVLMS_AouTZvfDfXYRVz0ExhCIV4je2Hxqd-CVfKmmpfGjLIGeKawPEff6Ei5gfT/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A cylindrical model with cardiac activities ℎ(𝑡) that modulates both the phase and amplitude of the &lt;a href=&quot;https://www.digikey.com/en/articles /the-basics-of-mixers&quot;>;mixed-down&lt;/a>; signal. Based on the simulation from our analytical model, the amplitude 𝑅(𝑡) and phase Φ(𝑡) of the mixed-down APG signals both reflect the cardiac activities ℎ(𝑡).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;APG sensing in practice&lt;/h2>; &lt;p>; During our initial experiments, we observed that APG works robustly with bad earbuds seals and with music playing. However, we noticed the APG signal can sometimes be very noisy and could be heavily disturbed by body motion. At that point, we determined that in order to make APG useful, we had to make it more robust to compete with &lt;a href=&quot;https://curiouscyborg.com/history-of-photoplethysmography/#:~:text=The%20Photoplethysmogram%20was%20first%20observed,the%20light%20interaction%20with%20tissue.&quot;>;more than 80 years&lt;/a>; of PPG development. &lt;/p>; &lt;p>; While PPGs are widely used and highly advanced, they do have some limitations. For example, PPGs sensors typically use two to four &lt;a href=&quot;https://en.wikipedia.org/wiki/Diode&quot;>;diodes&lt;/a>; to send and receive light frequencies for sensing. However, due to the ultra high-frequency nature (hundreds of &lt;a href=&quot;https://en.wikipedia.org/wiki/Terahertz_radiation&quot;>;Terahertz&lt;/a>;) of the light, it&#39;s difficult for a single diode to send multiple colors with different frequencies. On the other hand, we can easily design a low-cost and low-power system that generates and receives more than ten audio tones (frequencies). We leverage &lt;a href=&quot;https://en.wikipedia.org/wiki/Diversity_scheme&quot;>;channel diversity&lt;/a>;, a physical phenomenon that describes how wireless signals (eg, light and audio) at different frequencies have different characters (eg, different attenuation and reflection coefficients) when the signal propagates in a medium, to enable a higher quality APG signal and motion resilience. &lt;/p>; &lt;p>; Next, we experimentally demonstrate the effectiveness of using multiple frequencies in the APG signaling. We transmit three probing signals concurrently with their frequencies spanning evenly from 30 KHz to 32 KHz. A participant was asked to shake their head four times during the experiment to introduce interference. The figure below shows that different frequencies can be transmitted simultaneously to gather various information with &lt;a href=&quot;https://www.rp-photonics.com/optical_heterodyne_detection.html&quot;>;coherent detection&lt;/a>;, a unique advantage to APG 。 &lt;/p>; &lt;p>; The 30 kHz phase shows the four head movements and the magnitude (amplitude) of 31 kHz shows the pulse wave signal. This observation shows that some ultrasound frequencies might be sensitive to cardiac activities while others might be sensitive to motion. Therefore, we can use the multi-tone APG as a calibration signal to find the best frequency that measures heart rate, and use only the best frequency to get high-quality pulse waveform. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP_DLCbWWNJRvMPnIbdAbij_YKuIuYGbTiN-drwsJnEkrybDQT_yNHIqKYrqhxfq8CrWSEVZ6kvCvZ1tIUoXtH5GBqEfyQjkjChwVqL_Cxc0dO6i3vJi_sPo7hjElUuxH9f8d3E6nkDBNGNNgCRdFhfllW8OnhGXVS120-e4Yh67vECY7_lQpDOH2I42TU/s1761/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1380&quot; data-original-width=&quot;1761&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP_DLCbWWNJRvMPnIbdAbij_YKuIuYGbTiN-drwsJnEkrybDQT_yNHIqKYrqhxfq8CrWSEVZ6kvCvZ1tIUoXtH5GBqEfyQjkjChwVqL_Cxc0dO6i3vJi_sPo7hjElUuxH9f8d3E6nkDBNGNNgCRdFhfllW8OnhGXVS120-e4Yh67vECY7_lQpDOH2I42TU/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;The mixed-down amplitude (upper row) and phase (bottom row) for a customized multi-tone APG signal that spans from 30 kHz to 32 kHz. With channel diversity, the cardiac activities are captured in some frequencies (eg, magnitude of 31 kHz) and head movements are captured in other frequencies (eg, magnitude of 30 kHz, 30 kHz, and phase of 31 kHz). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; After choosing the best frequency to measure heart rate, the APG pulse waveform becomes more visible with pronounced dicrotic notches , and enables accurate &lt;a href=&quot;https://en.wikipedia.org/wiki/Heart_rate_variability&quot;>;heart rate variability&lt;/a>; measurement. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0mjyX5qTrzW9rN1qexcI3Nm0hgn03m-K_kGn_A8yfqlviJ3Rc3tod-srqQ-6vuoPEo6em7z3Qx7NpgkYTMRiNVtzUidLMc1lp48n6O8hsLMqFQc8z5CKI8mqSowmEbg5ojxdG4vaYreLbv0q2YwQAk3hKwdSd1cdwQMGVtFmgwn46nvCYRuPEi6hdpcVh/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0mjyX5qTrzW9rN1qexcI3Nm0hgn03m-K_kGn_A8yfqlviJ3Rc3tod-srqQ-6vuoPEo6em7z3Qx7NpgkYTMRiNVtzUidLMc1lp48n6O8hsLMqFQc8z5CKI8mqSowmEbg5ojxdG4vaYreLbv0q2YwQAk3hKwdSd1cdwQMGVtFmgwn46nvCYRuPEi6hdpcVh/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;The final APG signal used in the measurement phase (&lt;strong>;left&lt;/strong>;) and chest &lt;a href=&quot;https://en.wikipedia.org/wiki /Electrocardiography&quot;>;ECG&lt;/a>; signal (&lt;strong>;right&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Multi-tone translates to multiple simultaneous observations, which enable the development of &lt;a href=&quot;https://en.wikipedia.org/wiki/Array_processing&quot;>;array signal processing&lt;/a>; techniques. We demonstrate the spectrogram of a running session APG experiment before and after applying &lt;a href=&quot;https://en.wikipedia.org/wiki/Signal_separation&quot;>;blind source separation&lt;/a>; (see the &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;paper&lt;/a>; for more details). We also show the ground truth heart rate measurement in the same running experiment using a &lt;a href=&quot;https://www.polar.com/us-en/sensors/h10-heart-rate-sensor&quot;>;Polar ECG chest strap &lt;/a>;。 In the raw APG, we see the running cadence (around 3.3 Hz) as well as two dim lines (around 2 Hz and 4 Hz) that indicate the user&#39;s heart rate frequency and its harmonics. The heart rate frequencies are significantly enhanced in &lt;a href=&quot;https://en.wikipedia.org/wiki/Signal-to-noise_ratio&quot;>;signal to noise ratio&lt;/a>; (SNR) after the blind source separation, which align with the ground truth heart rate frequencies. We also show the calculated heart rate and running cadence from APG and ECG. We can see that APG tracks the growth of heart rate during the running session accurately. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbwRfWHxYHVk1ARp9gzcaRTMIUO-FjAgaTqOUtfYMzJ1lpAsWSxm1MczllG4aq-3Rbi-M6u7KXqnPOdsiLJS4le05Alnf_68701LYb0DQrpbbqAYGP7jzYivDhoeniPeTmsBU7Uhg4HRyOlYdQCYWxL-xabuULYFlGdH_rIA1b1Q8ZXH8MHIYlPQcaUpYW/s1888/image3 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1423&quot; data-original-width=&quot;1888&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbwRfWHxYHVk1ARp9gzcaRTMIUO-FjAgaTqOUtfYMzJ1lpAsWSxm1MczllG4aq-3Rbi-M6u7KXqnPOdsiLJS4le05Alnf_68701LYb0DQrpbbqAYGP7jzYivDhoeniPeTmsBU7Uhg4HRyOlYdQCYWxL-xabuULYFlGdH_rIA1b1Q8ZXH8MHIYlPQcaUpYW/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;APG tracks the heart rate accurately during the running session and also measures the running cadence.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Field study and closing thoughts&lt;/h2>; &lt;p>; We conducted two rounds of user experience (UX ) studies with 153 participants. Our results demonstrate that APG achieves consistently accurate heart rate (3.21% median error across participants in all activity scenarios) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Heart_rate_variability&quot;>;heart rate variability&lt;/a>; (2.70% median error in inter-beat interval) measurements. Unlike PPG, which exhibits variable performance across skin tones, our study shows that APG is resilient to variation in: skin tone, sub-optimal seal conditions, and ear canal size. More detailed evaluations can be found in the &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;paper&lt;/a>;. &lt;/p>; &lt;p>; APG transforms any TWS ANC headphones into smart sensing headphones with a simple software upgrade, and works robustly across various user activities. The sensing carrier signal is completely inaudible and not impacted by music playing. More importantly, APG represents new knowledge in biomedical and mobile research and unlocks new possibilities for low-cost health sensing. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>; APG is the result of collaboration across Google Health, product, UX and legal teams. We would like to thank David Pearl, Jesper Ramsgaard, Cody Wortham, Octavio Ponce, Patrick Amihood, Sam Sheng, Michael Pate, Leonardo Kusumo, Simon Tong, Tim Gladwin, Russ Mirov, Kason Walker, Govind Kannan, Jayvon Timmons, Dennis Rauschmayer, Chiong Lai, Shwetak Patel, Jake Garrison, Anran Wang, Shiva Rajagopal, Shelten Yuen, Seobin Jung, Yun Liu, John Hernandez, Issac Galatzer-Levy, Isaiah Fischer-Brown, Jamie Rogers, Pramod Rudrapatna, Andrew Barakat, Jason Guss, Ethan Grabau, Pol Peiffer, Bill Park, Helen O&#39;Connor, Mia Cheng, Keiichiro Yumiba, Felix Bors, Priyanka Jantre, Luzhou Xu, Jian Wang, Jaime Lien, Gerry Pallipuram, Nicholas Gillian, Michal Matuszak, Jakub Wojciechowski, Bryan Allen, Jane Hilario, and Phil Carmack for their invaluable insights and support. Thanks to external collaborators Longfei Shangguan and Rich Howard, Rutgers University and University of Pittsburgh.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4973261706617691472/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/audioplethysmography-for-cardiac.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4973261706617691472&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4973261706617691472&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/audioplethysmography-for-cardiac.html&quot; rel=&quot;alternate&quot; title=&quot;Audioplethysmography for cardiac monitoring with hearable devices&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnCItsRGs939CGCBBAi5fA-xwk9JQi7b2CQ3voKK583p4uTkQkLIXUd1lU71SLynom0Yt78bRxDflolUzzfol5UbfkPmCaNn8bIUxwAbLDatqZTPxP7SYOT45g9qJODdM1kvT6NQUsixtFTiBYr_h_Hx-pFRsmbcBKdnW3WkbcD4Bcr_cZE590VL-cSu-a/s72-c/hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6605352025607219737&lt;/id>;&lt;published>;2023-10-26T11:01:00.000-07:00&lt;/published>;&lt;updated>;2023-10-26T11:01:04.276-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Collaboration&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Supporting benchmarks for AI safety with MLCommons&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Anoop Sinha, Technology and Society, and Marian Croak, Google Research, Responsible AI and Human Centered Technology team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5URA9ivhxcgmfXwa0668O0HslgYQ_p_x9JJbg6nDgryyNc2QImQernPyzkLPcj1esCMOQTUnEsldIlb21E0DWPDIiE2m76qSruF0jA6jfl1sNl6mBW8JUPSWzOfE7IahHRtviFpxUTPcEZoADXHNMy3gZ2hg369y5QxhY01QRVj5kwJx4uwRKzdcBFp9v/s1600/GoogleResearch.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Standard benchmarks are agreed upon ways of measuring important product qualities, and they exist in many fields. Some standard benchmarks measure safety: for example, when a car manufacturer touts a “five-star overall safety rating,” they&#39;re citing a benchmark. Standard benchmarks already exist in machine learning (ML) and AI technologies: for instance, the &lt;a href=&quot;https://mlcommons.org/en/&quot;>;MLCommons&lt;/a>; Association operates the &lt;a href=&quot;https://mlcommons.org/en/news/mlperf-inference-storage-q323/&quot;>;MLPerf&lt;/a>; benchmarks that measure the speed of cutting edge AI hardware such as Google&#39;s TPUs. However, though there has been significant work done on &lt;a href=&quot;https://blog.google/technology/ai/our-responsible-approach-to-building-guardrails-for-generative-ai/&quot;>;AI safety&lt;/a>;, there are as yet no similar standard benchmarks for AI safety. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to support a new effort by the non-profit MLCommons Association to develop standard AI safety benchmarks. Developing benchmarks that are effective and trusted is going to require advancing AI safety testing technology and incorporating a broad range of perspectives. The MLCommons effort aims to bring together expert researchers across academia and industry to develop standard benchmarks for measuring the safety of AI systems into scores that everyone can understand. We encourage the whole community, from AI researchers to policy experts, to &lt;a href=&quot;https://mlcommons.org/ai-safety&quot;>;join us&lt;/a>; in contributing to the effort. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Why AI safety benchmarks?&lt;/h2>; &lt;p>; Like most advanced technologies, AI has the potential for tremendous benefits but could also lead to negative outcomes without appropriate care. For example, AI technology can boost human productivity in a wide range of activities (eg, &lt;a href=&quot;https://blog.google/technology/health/how-ai-can-improve-health-for-everyone-everywhere/&quot;>;improve health diagnostics&lt;/a>; and research into diseases, analyze &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-transportation-energy-emissions-reduction/&quot;>;energy usage&lt;/a>;, and more). However, without sufficient precautions, AI could also be used to support harmful or malicious activities and respond in biased or offensive ways. &lt;/p>; &lt;p>; By providing standard measures of safety across categories such as harmful use, out-of-scope responses, AI-control risks, etc., standard AI safety benchmarks could help society reap the benefits of AI while ensuring that sufficient precautions are being taken to mitigate these risks. Initially, nascent safety benchmarks could help drive AI safety research and inform responsible AI development. With time and maturity, they could help inform users and purchasers of AI systems. Eventually, they could be a valuable tool for policy makers. &lt;/p>; &lt;p>; In computer hardware, benchmarks (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Standard_Performance_Evaluation_Corporation&quot;>;SPEC&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Transaction_Processing_Performance_Council&quot;>;TPC&lt;/a>;) have shown an amazing ability to align research, engineering, and even marketing across an entire industry in pursuit of progress, and we believe standard AI safety benchmarks could help do the same in this vital area. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;What are standard AI safety benchmarks?&lt;/h2>; &lt;p>; Academic and corporate research efforts have experimented with a range of AI safety tests (eg, &lt;a href=&quot;https://arxiv.org/abs/2009.11462&quot;>;RealToxicityPrompts&lt;/a>;, &lt;a href=&quot;https://crfm.stanford.edu/2022/11/17/helm.html&quot;>;Stanford HELM&lt;/a>; fairness, bias, toxicity measurements, and &lt;a href=&quot;https://blog.google/technology/ai/our-responsible-approach-to-building-guardrails-for-generative-ai/&quot;>;Google&#39;s guardrails for generative AI&lt;/a>;). However, most of these tests focus on providing a prompt to an AI system and algorithmically scoring the output, which is a useful start but limited to the scope of the test prompts. Further, they usually use open datasets for the prompts and responses, which may already have been (often inadvertently) incorporated into training data. &lt;/p>; &lt;p>; MLCommons proposes a multi-stakeholder process for selecting tests and grouping them into subsets to measure safety for particular AI use-cases, and translating the highly technical results of those tests into scores that everyone can understand. MLCommons is proposing to create a platform that brings these existing tests together in one place and encourages the creation of more rigorous tests that move the state of the art forward. Users will be able to access these tests both through online testing where they can generate and review scores and offline testing with an engine for private testing. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;AI safety benchmarks should be a collective effort&lt;/h2>; &lt;p>; Responsible AI developers use a diverse range of safety measures, including automatic testing, manual testing, red teaming (in which human testers attempt to produce adversarial outcomes), software-imposed restrictions, data and model best-practices, and auditing. However, determining that sufficient precautions have been taken can be challenging, especially as the community of companies providing AI systems grows and diversifies. Standard AI benchmarks could provide a powerful tool for helping the community grow responsibly, both by helping vendors and users measure AI safety and by encouraging an ecosystem of resources and specialist providers focused on improving AI safety. &lt;/p>; &lt;p>; At the same time, development of mature AI safety benchmarks that are both effective and trusted is not possible without the involvement of the community. This effort will need researchers and engineers to come together and provide innovative yet practical improvements to safety testing technology that make testing both more rigorous and more efficient. Similarly, companies will need to come together and provide test data, engineering support, and financial support. Some aspects of AI safety can be subjective, and building trusted benchmarks supported by a broad consensus will require incorporating multiple perspectives, including those of public advocates, policy makers, academics, engineers, data workers, business leaders, and entrepreneurs. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Google&#39;s support for MLCommons&lt;/h2>; &lt;p>; Grounded in our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; that were &lt;a href=&quot;https://blog.google/technology/ai/ai-principles/&quot;>;announced&lt;/a>; in 2018, Google is committed to specific practices for the safe, secure, and trustworthy development and use of AI (see our &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2019-progress-update.pdf&quot;>;2019&lt;/a>;, &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2020-progress-update.pdf&quot;>;2020&lt;/a>;, &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2021-progress-update.pdf&quot;>;2021&lt;/a>;, &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2022-progress-update.pdf&quot;>;2022&lt;/a>; updates). We&#39;ve also made significant &lt;a href=&quot;https://static.googleusercontent.com/media/publicpolicy.google/en//resources/whcommitments.pdf&quot;>;progress&lt;/a>; on key commitments, which will help ensure AI is developed boldly and responsibly, for the benefit of everyone. &lt;/p>; &lt;p>; Google is supporting the MLCommons Association&#39;s efforts to develop AI safety benchmarks in a number of ways. &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Testing platform&lt;/em>;: We are joining with other companies in providing funding to support the development of a testing platform. &lt;li>;&lt;em>;Technical expertise and resources&lt;/em>;: We are providing technical expertise and resources, such as the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;Monk Skin Tone Examples Dataset&lt;/a>;, to help ensure that the benchmarks are well-designed and effective. &lt;li>;&lt;em>;Datasets&lt;/em>;: We are contributing an internal dataset for multilingual representational bias, as well as already externalized tests for stereotyping harms, such as &lt;a href=&quot;https://github.com/google-research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>; and &lt;a href=&quot;https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;. Moreover, we are sharing our datasets that focus on collecting human annotations responsibly and inclusively, like &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;DICES&lt;/a>; and &lt;a href=&quot;https://www.kaggle.com/datasets/google/jigsaw-specialized-rater-pools-dataset&quot;>;SRP&lt;/a>;. &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; We believe that these benchmarks will be very useful for advancing research in AI safety and ensuring that AI systems are developed and deployed in a responsible manner. AI safety is &lt;a href=&quot;https://blog.google/technology/ai/a-shared-agenda-for-responsible-ai-progress/&quot;>;a collective-action problem&lt;/a>;. Groups like the &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/&quot;>;Frontier Model Forum&lt;/a>; and &lt;a href=&quot;https://partnershiponai.org/&quot;>;Partnership on AI&lt;/a>; are also leading important standardization initiatives. We&#39;re pleased to have been part of these groups and MLCommons since their beginning. We look forward to additional collective efforts to promote the responsible development of new generative AI tools. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many thanks to the Google team that contributed to this work: Peter Mattson, Lora Aroyo, Chris Welty, Kathy Meier-Hellstern, Parker Barnes, Tulsee Doshi, Manvinder Singh, Brian Goldman, Nitesh Goyal, Alice Friend, Nicole Delange, Kerry Barker, Madeleine Elish, Shruti Sheth, Dawn Bloxwich, William Isaac, Christina Butterfield.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6605352025607219737/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6605352025607219737&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6605352025607219737&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot; rel=&quot;alternate&quot; title=&quot;Supporting benchmarks for AI safety with MLCommons&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5URA9ivhxcgmfXwa0668O0HslgYQ_p_x9JJbg6nDgryyNc2QImQernPyzkLPcj1esCMOQTUnEsldIlb21E0DWPDIiE2m76qSruF0jA6jfl1sNl6mBW8JUPSWzOfE7IahHRtviFpxUTPcEZoADXHNMy3gZ2hg369y5QxhY01QRVj5kwJx4uwRKzdcBFp9v/s72-c/GoogleResearch.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7441213378686175839&lt;/id>;&lt;published>;2023-10-26T08:57:00.003-07:00&lt;/published>;&lt;updated>;2023-11-01T17:00:03.449-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Spoken question answering and speech continuation using a spectrogram-powered LLM&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eliya Nachmani, Research Scientist, and Alon Levkovitch, Student Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQEWecXit-a9UwHz781_B9s9ZqxsJIGt7ZXx2Lk0bcSQxXBkmLfjhNQxSiq3gqVjiUZ81_178hArCQ8nNL0OaVyAi8mKixeWN2PvXTLL4I08ht-eCVqtrTRo36dxGBSDNMdlathtEd4g_qdU3T4ZmPMdGNSXHwlDP689sxzbI4Wwosyu9wp-mjadKqL3MN/s1100/Spectron-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The goal of natural language processing (NLP) is to develop computational models that can understand and generate natural language. By capturing the statistical patterns and structures of text-based natural language, language models can predict and generate coherent and meaningful sequences of words. Enabled by the increasing use of the highly successful &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>; model architecture and with training on large amounts of text (with proportionate compute and model size), large language models (LLMs) have demonstrated remarkable success in NLP tasks. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, modeling spoken human language remains a challenging frontier. Spoken dialog systems have conventionally been built as a cascade of &lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_recognition&quot;>;automatic speech recognition&lt;/a>; (ASR), &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural-language_understanding&quot;>;natural language understanding&lt;/a>; (NLU), response generation, and &lt;a href=&quot;https://simple.wikipedia.org/wiki/Text_to_speech&quot;>;text-to-speech&lt;/a>; (TTS) systems. However, to date there have been few capable end-to-end systems for the modeling of spoken language: ie, single models that can take speech inputs and generate its continuation as speech outputs. &lt;/p>; &lt;p>;Today we present a new approach for spoken language modeling, called Spectron, published in “&lt;a href=&quot;https://arxiv.org/abs/2305.15255&quot;>;Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM&lt;/a>;.” Spectron is the first spoken language model that is trained end-to-end to directly process spectrograms as both input and output, instead of learning discrete speech representations. Using only a pre-trained text language model, it can be fine-tuned to generate high-quality, semantically accurate spoken language. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;question answering&lt;/a>; datasets.&lt;/p>; &lt;p>; We show that a pre-trained speech encoder and a language model decoder enable end-to-end training and state-of-the-art performance without sacrificing representational fidelity. Key to this is a novel end-to-end training objective that implicitly supervises speech recognition, text continuation, and conditional speech synthesis in a joint manner. A new spectrogram regression loss also supervises the model to match the higher-order derivatives of the spectrogram in the time and frequency domain. These derivatives express information aggregated from multiple frames at once. Thus, they express rich, longer-range information about the shape of the signal. Our overall scheme is summarized in the following figure: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4yanW_7FsK-zlJ5XKgusakSaIGAEvDPuRZpAZ5NGJl8mW6-4hd06ZWOsk4IIf0hl5XwWFcjkdu9gH4VwqcWCqLUfZQXDM80MuDml8Tt_P0RT3fDq5cxfoPCPNagOuU0SZbSz8Vn7x_bLkxNFXKpakBPwCkgXh8T6bB4lfHQevGBjs-U4c5WA6RgC8gukW/s1815/Spectron.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1815&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4yanW_7FsK-zlJ5XKgusakSaIGAEvDPuRZpAZ5NGJl8mW6-4hd06ZWOsk4IIf0hl5XwWFcjkdu9gH4VwqcWCqLUfZQXDM80MuDml8Tt_P0RT3fDq5cxfoPCPNagOuU0SZbSz8Vn7x_bLkxNFXKpakBPwCkgXh8T6bB4lfHQevGBjs-U4c5WA6RgC8gukW/s16000/Spectron.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;The Spectron model connects the encoder of a speech recognition model with a pre-trained Transformer-based decoder language model. At training, speech utterances split into a prompt and its continuation. Then the full transcript (prompt and continuation) is reconstructed along with the continuation&#39;s speech features. At inference, only a prompt is provided; the prompt&#39;s transcription, text continuation, and speech continuations are all generated by the model.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Spectron architecture&lt;/h2>; &lt;p>; The architecture is initialized with a pre-trained speech encoder and a pre-trained decoder language model. The encoder is prompted with a speech utterance as input, which it encodes into continuous linguistic features. These features feed into the decoder as a prefix, and the whole encoder-decoder is optimized to jointly minimize a &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy&quot;>;cross-entropy&lt;/a>; loss (for speech recognition and transcript continuation) and a novel reconstruction loss (for speech continuation). During inference, one provides a spoken speech prompt, which is encoded and then decoded to give both text and speech continuations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Speech encoder&lt;/h3>; &lt;p>; The speech encoder is a 600M-parameter &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;conformer&lt;/a>; encoder pre-trained on &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;large-scale data&lt;/a>; (12M hours). It takes the spectrogram of the source speech as input, generating a hidden representation that incorporates both linguistic and acoustic information. The input spectrogram is first subsampled using a convolutional layer and then processed by a series of conformer blocks. Each conformer block consists of a feed-forward layer, a self-attention layer, a convolution layer, and a second feed-forward layer. The outputs are passed through a projection layer to match the hidden representations to the embedding dimension of the language model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Language model&lt;/h3>; &lt;p>; We use a 350M or 1B parameter decoder language model (for the continuation and question-answering tasks, respectively) trained in the manner of &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;PaLM 2&lt;/a>;. The model receives the encoded features of the prompt as a prefix. Note that this is the only connection between the speech encoder and the LM decoder; ie, there is no cross-attention between the encoder and the decoder. Unlike most spoken language models, during training, the decoder is teacher-forced to predict the text transcription, text continuation, and speech embeddings. To convert the speech embeddings to and from spectrograms, we introduce lightweight modules pre- and post-network. &lt;/p>; &lt;p>; By having the same architecture decode the intermediate text and the spectrograms, we gain two benefits. First, the pre-training of the LM in the text domain allows continuation of the prompt in the text domain before synthesizing the speech. Secondly, the predicted text serves as intermediate reasoning, enhancing the quality of the synthesized speech, analogous to improvements in text-based language models when using intermediate &lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpads&lt;/a>; or &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; (CoT) reasoning. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Acoustic projection layers&lt;/h3>; &lt;p>; To enable the language model decoder to model spectrogram frames, we employ a multi-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Perceptron&quot;>;perceptron&lt;/a>; “pre-net” to project the ground truth spectrogram speech continuations to the language model dimension. This pre-net compresses the spectrogram input into a lower dimension, creating a bottleneck that aids the decoding process. This bottleneck mechanism prevents the model from repetitively generating the same prediction in the decoding process. To project the LM output from the language model dimension to the spectrogram dimension, the model employs a “post-net”, which is also a multi-layer perceptron. Both pre- and post-networks are two-layer multi-layer perceptrons. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Training objective&lt;/h3>; &lt;p>; The training methodology of Spectron uses two distinct loss functions: (i) &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy&quot;>;cross-entropy loss&lt;/a>;, employed for both speech recognition and transcript continuation, and (ii) &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;regression loss&lt;/a>;, employed for speech continuation. During training, all parameters are updated (speech encoder, projection layer, LM, pre-net, and post-net). &lt;/p>; &lt;br />; &lt;h2>;Audio samples&lt;/h2>; &lt;p>; Following are examples of speech continuation and question answering from Spectron: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;&lt;h3>;Speech Continuation&lt;/h3>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/1/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/2/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/2/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/3/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/3/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/4/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/4/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td colspan=&quot;2&quot; style=&quot;text-align: left;&quot;>;&lt;h3>;Question Answering&lt;/h3>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Question:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/1/Q.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Answer:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/1/S.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Question:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/2/Q.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Answer:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/2/S.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Performance&lt;/h2>; &lt;p>; To empirically evaluate the performance of the proposed approach, we conducted experiments on the&lt;a href=&quot;https://arxiv.org/abs/1912.07875&quot;>; Libri-Light dataset&lt;/a>;. Libri-Light is a 60k hour English dataset consisting of unlabelled speech readings from LibriVox audiobooks. We utilized a frozen neural vocoder called &lt;a href=&quot;https://research.google/pubs/pub51736/&quot;>;WaveFit&lt;/a>; to convert the predicted spectrograms into raw audio. We experiment with two tasks, speech continuation and spoken question answering (QA). Speech continuation quality is tested on the LibriSpeech test set. Spoken QA is tested on the Spoken WebQuestions datasets and a new test set named LLama questions, which we created. For all experiments, we use a 3 second audio prompt as input. We compare our method against existing spoken language models: &lt;a href=&quot;https://arxiv.org/abs/2209.03143&quot;>;AudioLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2102.01192&quot;>;GSLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2305.13009&quot;>;TWIST&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2305.11000&quot;>;SpeechGPT&lt;/a>;. For the speech continuation task, we use the 350M parameter version of LM and the 1B version for the spoken QA task. &lt;/p>; &lt;p>; For the speech continuation task, we evaluate our method using three metrics. The first is &lt;a href=&quot;https://en.wikipedia.org/wiki/Perplexity&quot;>;log-perplexity&lt;/a>;, which uses an LM to evaluate the cohesion and semantic quality of the generated speech. The second is &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;mean opinion score&lt;/a>; (MOS), which measures how natural the speech sounds to human evaluators. The third, speaker similarity, uses a speaker encoder to measure how similar the speaker in the output is to the speaker in the input. Performance in all 3 metrics can be seen in the following graphs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_o_DUlZ-28VmNk20zvWBWC5FNl3GMVeigztDAtB4TR5voCdX3zzuCF-6W6DDrMVGZ2szVhyphenhyphendPz1DHvjBrALQXCMkVzuM81oBck4Wb3N1VH1ndDReAPsfbQv5x7msOD3wMiHeZM-_AcN2ekgSI_G9sF5_u0lrLKc-xxSWc2d9qi6Ubt5cutgQfpIt8uKGr/s1200/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_o_DUlZ-28VmNk20zvWBWC5FNl3GMVeigztDAtB4TR5voCdX3zzuCF-6W6DDrMVGZ2szVhyphenhyphendPz1DHvjBrALQXCMkVzuM81oBck4Wb3N1VH1ndDReAPsfbQv5x7msOD3wMiHeZM-_AcN2ekgSI_G9sF5_u0lrLKc-xxSWc2d9qi6Ubt5cutgQfpIt8uKGr/w640-h396/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Log-perplexity for completions of LibriSpeech utterances given a 3-second prompt. Lower is better.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjUncRH61ZdMzRhsx9NEXoY0P0jIMkaXhycc9AjB2I0pr8bEWxGNaY0KEF4UG0dm6ZVjRT3_aXlNKny3mmjoVgPveK4rskOgU5sApzqzIXixabv7kMeZNMxUpxs_RDqSsnij-HILgUzVZT39hoBDimw9Ecnd2lMq2ylyRCVtcU_owo8jx9ndVrLUHbGhNT/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjUncRH61ZdMzRhsx9NEXoY0P0jIMkaXhycc9AjB2I0pr8bEWxGNaY0KEF4UG0dm6ZVjRT3_aXlNKny3mmjoVgPveK4rskOgU5sApzqzIXixabv7kMeZNMxUpxs_RDqSsnij-HILgUzVZT39hoBDimw9Ecnd2lMq2ylyRCVtcU_owo8jx9ndVrLUHbGhNT/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Speaker similarity between the prompt speech and the generated speech using the speaker encoder. Higher is better.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBdPk77at7a-tSVl1cgPTlMsj4tnV391evuyYPzMXxeviG48kljCNy828jCQ5eVdOcB7ML4FHduIHCuDweZwJiv2USbNqi1EXinzyiwQYM3Sxio3g-61xTBvtb9rVt_Y4Cvns-vl3V-4Gvn1RTpBquX3a6W_NNu6u-1DrK_GT5uGXOvP0WivyO7FF2-WTf/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBdPk77at7a-tSVl1cgPTlMsj4tnV391evuyYPzMXxeviG48kljCNy828jCQ5eVdOcB7ML4FHduIHCuDweZwJiv2USbNqi1EXinzyiwQYM3Sxio3g-61xTBvtb9rVt_Y4Cvns-vl3V-4Gvn1RTpBquX3a6W_NNu6u-1DrK_GT5uGXOvP0WivyO7FF2-WTf/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MOS given by human users on speech naturalness. Raters rate 5-scale subjective mean opinion score (MOS) ranging between 0 - 5 in naturalness given a speech utterance. Higher is better.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As can be seen in the first graph, our method significantly outperforms GSLM and TWIST on the log-perplexity metric, and does slightly better than state-of-the-art methods AudioLM and SpeechGPT. In terms of MOS, Spectron exceeds the performance of all the other methods except for AudioLM. In terms of speaker similarity, our method outperforms all other methods. &lt;/p>; &lt;p>; To evaluate the ability of the models to perform question answering, we use two spoken question answering datasets. The first is the LLama Questions dataset, which uses general knowledge questions in different domains generated using the LLama2 70B LLM. The second dataset is the &lt;a href=&quot;https://huggingface.co/datasets/web_questions&quot;>;WebQuestions&lt;/a>; dataset which is a general question answering dataset. For evaluation we use only questions that fit into the 3 second prompt length. To compute accuracy, answers are transcribed and compared to the ground truth answers in text form. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi60RnurzgglmP14m9BIScZH4BmupewxydVJ5GZMKclwDgyGc3-zZdj8CK7WXmGR_-pwno1Aql0N_u0ocmftoAlTCJY0H-1cYU8YTpxZu5sdIdmG-wZAc-hIwlAbCEVDPgGz4J4a1VMcXWctbjcfCTxsDSZPD69eGfBeGyAxvyYtUX0fE2dhdT5zb71gsJ0/s1200 /image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot; 396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi60RnurzgglmP14m9BIScZH4BmupewxydVJ5GZMKclwDgyGc3-zZdj8CK7WXmGR_-pwno1Aql0N_u0ocmftoAlTCJY0H-1cYU8YTpxZu5sdIdmG-wZAc-hIwlAbCEVDPgGz4J4a1VMcXWctbjcfCTxsDSZPD69eGfBeGyAxvyYtUX0fE2dhdT5zb71gsJ0/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Accuracy for Question Answering on the LLama Questions and Spoken WebQuestions datasets. Accuracy is computed using the ASR transcripts of spoken answers.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; First, we observe that all methods have more difficulty answering questions from the Spoken WebQuestions dataset than from the LLama questions dataset. Second, we observe that methods centered around spoken language modeling such as GSLM, AudioLM and TWIST have a completion-centric behavior rather than direct question answering which hindered their ability to perform QA. On the LLama questions dataset our method outperforms all other methods, while SpeechGPT is very close in performance. On the Spoken WebQuestions dataset, our method outperforms all other methods except for SpeechGPT, which does marginally better. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;This project was conceived and initiated by Michelle Tadmor Ramanovich with additional significant contributions from&lt;/i>;&lt;em>;&amp;nbsp;Eliya Nachmani, Alon Levkovitch, Roy Hirsch (&lt;a href=&quot;https://verily.com/&quot;>;Verily&lt;/a>;), Julian Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin (&lt;a href=&quot;https://verily.com/&quot;>;Verily&lt;/a>;) and RJ Skerry-Ryan. We also thank Heiga Zhen, Yifan Ding, Yu Zhang, Yuma Koizumi, Neil Zeghidour, Christian Frank, Marco Tagliasacchi, Nadav Bar, Benny Schlesinger and Blaise Aguera-Arcas. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7441213378686175839/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/spoken-question-answering-and-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7441213378686175839&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7441213378686175839&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/10/spoken-question-answering-and-speech.html&quot; rel=&quot;alternate&quot; title=&quot;Spoken question answering and speech continuation using a spectrogram-powered LLM&quot; type=&quot;text /html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQEWecXit-a9UwHz781_B9s9ZqxsJIGt7ZXx2Lk0bcSQxXBkmLfjhNQxSiq3gqVjiUZ81_178hArCQ8nNL0OaVyAi8mKixeWN2PvXTLL4I08ht-eCVqtrTRo36dxGBSDNMdlathtEd4g_qdU3T4ZmPMdGNSXHwlDP689sxzbI4Wwosyu9wp-mjadKqL3MN/ s72-c/Spectron-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr: total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7317378303167958176&lt;/id>;&lt;published>;2023-10-25T15:10:00.002-07:00&lt;/published >;&lt;updated>;2023-10-26T14:03:20.633-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>; &lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Climate&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Looking back at wildfire research in 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author &quot;>;Posted by Yi-Fan Chen, Software Engineer, and Carla Bromberg, Program Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCepSXB1QxIgipeUbH1YGd8N3TkVoT1tAgxG0fC0PPREhbsygTQ4i58OVw6-Dt_lagwgswT0jtxUorsVmtyO9UgPEDezJHz_QiKvbJlgYF8Db8W- 68KFdyZsq_uvM7YuZo9BRo__NC4BNxD6nHZpzsimeNOaV3X8dh9aliqbAbk8ycXb25s5NfLTfNe42_/s600/WildfireModeling.gif&quot; style=&quot;display: none;&quot; />; &lt;p>;Wildfires are becoming larger and affecting more and more communities around the world, often resulting in large-scale devastation. Just this year, communities have experienced catastrophic wildfires in &lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Greece_wildfires&quot;>;Greece&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Hawaii_wildfires#Maui&quot;>;Maui&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Canadian_wildfires&quot;>;Canada&lt;/a>; to name a few. While the underlying causes leading to such an increase are complex — including changing climate patterns, forest management practices, land use development policies and many more — it is clear that the advancement of technologies can help to address the new challenges.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;At Google Research, we&#39;ve been investing in a number of &lt;a href=&quot;https://research.google/teams/climate-and-sustainability/&quot;>;climate adaptation&lt;/a>; efforts, including the application of machine learning (ML) to aid in wildfire prevention and provide information to people during these events. For example, to help map fire boundaries, our wildfire boundary &lt;a href=&quot;https://blog.research.google/2023/02/real-time-tracking-of-wildfire.html&quot;>;tracker&lt;/a>; uses ML models and satellite imagery to map large fires in near real-time with updates every 15 minutes. To advance our various research efforts, we are partnering with wildfire experts and government agencies around the world.&lt;/p>; &lt;p>;Today we are excited to share more about our ongoing collaboration with the &lt;a href=&quot;https://www.fs.usda.gov/&quot;>;US Forest Service&lt;/a>; (USFS) to advance fire modeling tools and fire spread prediction algorithms. Starting from the newly developed USFS wildfire behavior model, we use ML to significantly reduce computation times, thus enabling the model to be employed in near real time. This new model is also capable of incorporating localized fuel characteristics, such as fuel type and distribution, in its predictions. Finally, we describe an early version of our new high-fidelity 3D fire spread model.&lt;/p>; &lt;br />; &lt;h2>;Current state of the art in wildfire modeling&lt;/h2>; &lt;p>;Today&#39;s most widely used state-of-the-art fire behavior models for fire operation and training are based on the &lt;a href=&quot;https://www.fs.usda.gov/research/treesearch/55928&quot;>;Rothermel fire model&lt;/a>; developed at the US Forest Service Fire Lab, by Rothermel et al., in the 1970s. This model considers many key factors that affect fire spread, such as the influence of wind, the slope of the terrain, the moisture level, the fuel load (eg, the density of the combustible materials in the forest), etc., and provided a good balance between computational feasibility and accuracy at the time. The Rothermel model has gained widespread use throughout the fire management community across the world.&lt;/p>; &lt;p>;Various operational tools that employ the Rothermel model, such as &lt;a href=&quot;https://www.frames.gov/behaveplus/home&quot;>;BEHAVE&lt;/a>;, &lt;a href=&quot;https://www.firelab.org/project/farsite&quot;>;FARSITE&lt;/a>;, &lt;a href=&quot;https://wfdss.usgs.gov/wfdss/pdfs/FSPro.pdf&quot;>;FSPro&lt;/a>;, and &lt;a href=&quot;https://firelab.org/project/flammap&quot;>;FlamMap&lt;/a>;, have been developed and improved over the years. These tools and the underlying model are used mainly in three important ways: (1) for training firefighters and fire managers to develop their insights and intuitions on fire behavior, (2) for fire behavior analysts to predict the development of a fire during a fire operation and to generate guidance for situation awareness and resource allocation planning, and (3) for analyzing forest management options intended to mitigate fire hazards across large landscapes.&amp;nbsp; These models are the foundation of fire operation safety and efficiency today.&lt;/p>; &lt;p>; However, there are limitations on these state-of-the art models, mostly associated with the simplification of the underlying physical processes (which was necessary when these models were created). By simplifying the physics to produce steady state predictions, the required inputs for fuel sources and weather became practical but also more abstract compared to measurable quantities.&amp;nbsp; As a result, these models are typically “adjusted” and “tweaked” by experienced fire behavior analysts so they work more accurately in certain situations and to compensate for uncertainties and unknowable environmental characteristics. Yet these expert adjustments mean that many of the calculations are not repeatable. &lt;/p>; &lt;p>;To overcome these limitations, USFS researchers have been working on a new model to drastically improve the physical fidelity of fire behavior prediction. This effort represents the first major shift in fire modeling in the past 50 years. While the new model &lt;a href=&quot;https://ebooks.publish.csiro.au/content/wildland-fire-behaviour&quot;>;continues to improve in capturing fire behavior&lt;/a>;, the computational cost and inference time makes it impractical to be deployed in the field or for applications with near real-time requirements. In a realistic scenario, to make this model useful and practical in training and operations, a speed up of at least 1000x would be needed.&lt;/p>; &lt;br />; &lt;h2>;Machine learning acceleration&lt;/h2>; &lt;p>;In partnership with the USFS, we have undertaken a program to apply ML to decrease computation times for complex fire models. Researchers knew that many complex inputs and features could be characterized using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;>;deep neural network&lt;/a>;, and if successful, the trained model would lower the computational cost and latency of evaluating new scenarios. Deep learning is a branch of machine learning that uses neural networks with multiple hidden layers of nodes that do not directly correspond to actual observations. The model&#39;s hidden layers allow a rich representation of extremely complex systems — an ideal technique for modeling wildfire spread.&lt;/p>; &lt;p>; We used the USFS physics-based, numerical prediction models to generate many simulations of wildfire behavior and then used these simulated examples to train the deep learning model on the inputs and features to best capture the system behavior accurately. We found that the deep learning model can perform at a much lower computational cost compared to the original and is able to address behaviors resulting from fine-scale processes. In some cases, computation time for capturing the fine-scale features described above and providing a fire spread estimate was 100,000 times faster than running the physics-based numerical models. &lt;/p>; &lt;p>;This project has continued to make great progress since the first report at &lt;a href=&quot;https://www.adai.pt/newevent/event/home/index.php?target=home&amp;amp;event =4&amp;amp;defLang=2&quot;>;ICFFR&lt;/a>; in December 2022. The joint Google–USFS &lt;a href=&quot;http://books.uc.pt/chapter?chapter=978989262298921&quot;>;presentation at ICFFR 2022&lt;/ a>; and the &lt;a href=&quot;https://www.firelab.org/project/deep-learning-high-resolution-wildfire-modeling&quot;>;USFS Fire Lab&#39;s project page&lt;/a>; provides a glimpse into the ongoing work朝这个方向。 Our team has expanded the dataset used for training by an order of magnitude, from 40M up to 550M training examples. Additionally, we have delivered a prototype ML model that our USFS Fire Lab partner is integrating into a training app that is currently being developed for release in 2024.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdwdwG4NlOjrJlRs25p5yGRUdNDErx7I9aZYLVgoPhrlyiOOx6x8toan0DKoBm2-SG2JfWkZsOluT7g_BYJpDYCTqMRu3cUHqQtyeE-Hgli5j9J3ap2Sg9SrjEfOMXj_CYhbi66iIFwSJjzkii0kJv3VV5V01jbIYFQP-DWjBc9RJKKEoJZPpIZKKYO2TJ/s1999/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdwdwG4NlOjrJlRs25p5yGRUdNDErx7I9aZYLVgoPhrlyiOOx6x8toan0DKoBm2-SG2JfWkZsOluT7g_BYJpDYCTqMRu3cUHqQtyeE-Hgli5j9J3ap2Sg9SrjEfOMXj_CYhbi66iIFwSJjzkii0kJv3VV5V01jbIYFQP-DWjBc9RJKKEoJZPpIZKKYO2TJ/w640-h360/image2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Google researchers visiting the USFS Fire Lab in Missoula, MT, stopping by&amp;nbsp;&lt;/span>;&lt;a href=&quot;https://inciweb.nwcg.gov/incident-information/mtfha-big-knife&quot; style=&quot;text-align: left;&quot;>;Big Knife Fire&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;&amp;nbsp;Operation Command Center.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Fine-grained fuel representation&lt;/h2>; &lt;p>;Besides training, another key use-case of the new model is for operational fire prediction. To fully leverage the advantages of the new model&#39;s capability to capture the detailed fire behavior changes from small-scale differences in fuel structures, high resolution fuel mapping and representation are needed. To this end, we are currently working on the integration of high resolution satellite imagery and geo information into ML models to allow fuel specific mapping at-scale. Some of the preliminary results will be presented at the upcoming &lt;a href=&quot;https://afefirecongress.org/schedule/&quot;>;10th International Fire Ecology and Management Congress&lt;/a>; in November 2023.&lt;/p>; &lt;br />; &lt;h2>;Future work&lt;/h2>; &lt;p>; Beyond the collaboration on the new fire spread model, there are many important and challenging problems that can help fire management and safety. Many such problems require even more accurate fire models that fully consider 3D flow interactions and fluid dynamics, thermodynamics and combustion physics. Such detailed calculations usually require high-performance computers (HPCs) or supercomputers. &lt;/p>; &lt;p>; These models can be used for research and longer-term planning purposes to develop insights on extreme fire development scenarios, build ML classification models, or establish a meaningful “danger index” using the simulated results. These high-fidelity simulations can also be used to supplement physical experiments that are used in expanding the operational models mentioned above. &lt;/p>; &lt;p>;In this direction, Google research has also &lt;a href=&quot;https://www.publish.csiro.au/WF/WF22225&quot;>;developed a high-fidelity large-scale 3D fire simulator&lt;/a>; that can be run on Google TPUs. In the near future, there is a plan to further leverage this new capability to augment the experiments, and to generate data to build insights on the development of extreme fires and use the data to design a fire-danger classifier and fire-danger index protocol.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4SgYHTgu7Uv6WNEJsEJCuFWKolnL13g9BxK2cDuMq7-SRM4dIUK9LbNxy3v0VyY4dU87T_NnehBz4iXCfHCWwoOU6V8-15P55Oi6FvT5BvcLgR_vzYB1xMPWUtj3AiwSsoPZY-9K5i9IwIADDAIjG7hzZZZZ00n7N2jVnjlp65ePQDsJdNIuX6avw8kD_/s480/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4SgYHTgu7Uv6WNEJsEJCuFWKolnL13g9BxK2cDuMq7-SRM4dIUK9LbNxy3v0VyY4dU87T_NnehBz4iXCfHCWwoOU6V8-15P55Oi6FvT5BvcLgR_vzYB1xMPWUtj3AiwSsoPZY-9K5i9IwIADDAIjG7hzZZZZ00n7N2jVnjlp65ePQDsJdNIuX6avw8kD_/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;An example of 3D high-fidelity simulation. This is a controlled burn field experiment (&lt;/span>;&lt;a href=&quot;https://www.fireweather.org/fireflux2&quot; style=&quot;text-align: left;&quot;>;FireFlux II&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;) simulated using&amp;nbsp;&lt;/span>;&lt;a href=&quot;https://www.publish.csiro.au/WF/WF22225&quot;>;Google&#39;s high fidelity fire simulator&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;We thank Mark Finney, Jason Forthofer, William Chatham and Issac Grenfell from US Forest Service Missoula Fire Science Laboratory and our colleagues John Burge, Lily Hu, Qing Wang, Cenk Gazen, Matthias Ihme,&amp;nbsp;Vivian Yang, Fei Sha and John Anderson for core contributions and useful discussions. We also thank Tyler Russell for his assistance with program management and coordination.&lt;/i>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7317378303167958176/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7317378303167958176&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7317378303167958176&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html&quot; rel=&quot;alternate&quot; title=&quot;Looking back at wildfire research in 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCepSXB1QxIgipeUbH1YGd8N3TkVoT1tAgxG0fC0PPREhbsygTQ4i58OVw6-Dt_lagwgswT0jtxUorsVmtyO9UgPEDezJHz_QiKvbJlgYF8Db8W-68KFdyZsq_uvM7YuZo9BRo__NC4BNxD6nHZpzsimeNOaV3X8dh9aliqbAbk8ycXb25s5NfLTfNe42_/s72-c/WildfireModeling.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-420799196308935505&lt;/id>;&lt;published>;2023-10-25T10:45:00.000-07:00&lt;/published>;&lt;updated>;2023-10-25T10:45:37.205-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;EMNLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Search&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Grammar checking at Google Search scale&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eric Malmi, Senior Research Scientist, and Jakub Adamek, Senior Software Engineer, Google, Bard Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW7pWsW1-E7U1LH1bxmjBL_2W5fS6uN2iRb2cNPBks57ubvFNJj-SjE2Zk1lFqndKWBVAeO3g3MLeJ96QUIUNJDyLTcWjeO835NT6HfJMYQBEdGqL-X_sxQ1yxMy16a0OcOLb6gSz2lqM6VofToVtN_s_F_wGB41AZwlj146y7ZXQ4PFsdywX10QO54MJ4/s320/HeroGC.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Many people with questions about grammar turn to Google Search for guidance. While existing features, such as “Did you mean”, already handle simple typo corrections, more complex grammatical error correction (GEC) is beyond their scope. What makes the development of new Google Search features challenging is that they must have high precision and recall while outputting results &lt;a href=&quot;https://ai.googleblog.com/2009/06/speed-matters.html&quot;>;quickly&lt; /a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The conventional approach to GEC is to treat it as a &lt;a href=&quot;https://workspace.google.com/blog/ai-and-machine-learning/using-neural-machine-translation-to-correct-grammatical-in-google-docs&quot;>;translation problem&lt;/a>; and use autoregressive &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; models to decode the response token-by-token, conditioning on the previously generated tokens. However, although Transformer models have proven to be effective at GEC, they aren&#39;t particularly efficient because the generation cannot be parallelized due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive&lt;/a>; decoding. Often, only a few modifications are needed to make the input text grammatically correct, so another possible solution is to treat GEC as a &lt;a href=&quot;https://arxiv.org/abs/2206.07043&quot;>;text editing&lt;/a>;问题。 If we could run the autoregressive decoder only to generate the modifications, that would substantially decrease the latency of the GEC model. &lt;/p>; &lt;p>; To this end, in “&lt;a href=&quot;https://aclanthology.org/2022.findings-emnlp.156/&quot;>;EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start&lt;/a>;”, published at Findings of &lt;a href=&quot;https://2022.emnlp.org/&quot; target=&quot;_blank&quot;>;EMNLP 2022&lt;/a>;, we describe a novel text-editing model that is based on the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>; Transformer encoder-decoder architecture. EdiT5 powers the new Google Search &lt;a href=&quot;https://support.google.com/websearch/answer/13420782&quot;>;grammar check feature&lt;/a>; that allows you to check if a phrase or sentence is grammatically correct and provides corrections when needed. Grammar check shows up when the phrase &quot;grammar check&quot; is included in a search query, and if the underlying model is confident about the correction. Additionally, it shows up for some queries that don&#39;t contain the “grammar check” phrase when Search understands that is the likely intent. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-BAXZ3wH-FcJmFoKvppChwvfZay7xH5bhiYmKQocV47sCb3IMfwEec1OpBWY3b1fd2rnA4Vhy4EqOCZiAR4Xpv6xPok9yC-ZqLogbER2lcmaje1NfBCoPAtdW7rEPG22PtNn4dVuYtYH61D9fnx7kudLZilxzUbi01ePhirqtTwMu6g9trhFFuLCDUCgP/s1412/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;1412&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-BAXZ3wH-FcJmFoKvppChwvfZay7xH5bhiYmKQocV47sCb3IMfwEec1OpBWY3b1fd2rnA4Vhy4EqOCZiAR4Xpv6xPok9yC-ZqLogbER2lcmaje1NfBCoPAtdW7rEPG22PtNn4dVuYtYH61D9fnx7kudLZilxzUbi01ePhirqtTwMu6g9trhFFuLCDUCgP/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;Model architecture&lt;/h2>; &lt;p>; For low-latency applications at Google, Transformer models are typically run on &lt;a href=&quot;https://cloud.google.com/tpu/docs/ intro-to-tpu&quot;>;TPUs&lt;/a>;. Due to their fast matrix multiplication units (MMUs), these devices are optimized for performing large matrix multiplications quickly, for example running a Transformer encoder on hundreds of tokens in only a few milliseconds. In contrast, Transformer decoding makes poor use of a TPU&#39;s capabilities, because it forces it to process only one token at a time. This makes autoregressive decoding the most time-consuming part of a translation-based GEC model. &lt;/p>; &lt;p>; In the EdiT5 approach, we reduce the number of decoding steps by treating GEC as a text editing problem. The EdiT5 text-editing model is based on the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>; Transformer encoder-decoder architecture with a few crucial modifications. Given an input with grammatical errors, the EdiT5 model uses an encoder to determine which input tokens to keep or delete. The kept input tokens form a draft output, which is optionally reordered using a non-autoregressive &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf&quot;>;pointer network &lt;/a>;。 Finally, a decoder outputs the tokens that are missing from the draft, and uses a pointing mechanism to indicate where each new token should be placed to generate a grammatically correct output. The decoder is only run to produce tokens that were missing in the draft, and as a result, runs for much fewer steps than would be needed in the translation approach to GEC. &lt;/p>; &lt;p>;To further decrease the decoder latency, we reduce the decoder down to a single layer, and we compensate by increasing the size of the encoder. Overall, this decreases latency significantly because the extra work in the encoder is efficiently parallelized.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2_MGEcdadlbXM1zMBQzROuRbYyqmcdhcytlnwdHeIJ_M0wrtioaRixoDOxMlE3acuMZRf4KB_T5TgzUCfBBmz3eeEMnnAKHZkvFmMzFnNu_2UAI1l3jhOp2dyQyOxElXwt7iEjun_gd77uk1TMiaBLlMfxEOxcFo8W48fdD9WqbYNvUZECUEd-LHTk1-R/s1878/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1332&quot; data-original-width=&quot;1878&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2_MGEcdadlbXM1zMBQzROuRbYyqmcdhcytlnwdHeIJ_M0wrtioaRixoDOxMlE3acuMZRf4KB_T5TgzUCfBBmz3eeEMnnAKHZkvFmMzFnNu_2UAI1l3jhOp2dyQyOxElXwt7iEjun_gd77uk1TMiaBLlMfxEOxcFo8W48fdD9WqbYNvUZECUEd-LHTk1-R/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an input with grammatical errors (“Guess when was I borned”), the EdiT5 model uses an encoder to determine which input tokens to keep (K) or delete (D), a pointer network (pointer) to reorder kept tokens, and a decoder to insert any new tokens that are needed to generate a grammatically correct output.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We applied the EdiT5 model to the &lt;a href=&quot;https://aclanthology.org/W19-4406/&quot;>;public BEA grammatical error correction benchmark&lt;/a>;, comparing different model sizes. The experimental results show that an EdiT5 large model with 391M parameters yields a higher &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F0.5 score&lt;/a>;, which measures the accuracy of the corrections, while delivering a 9x speedup compared to a T5 base model with 248M parameters. The mean latency of the EdiT5 model was merely 4.1 milliseconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQys5EavCOAEs0dy3M8r5Ar-9v7n5qtFkek1VSfeGBXH1pQHNv6Ebem3vDedO60BlNcp3HLGqYtsT4v4Evz2u2ksTgFfQcQIqd87NPTxVzYVyzvA85Vg_jO8a18KrMT2hOQQA5hg2frOJx2L7S5SM88VBCJzd13ClrSzHnl5xg29TMMpvqA4Yl-d98vTc1/s1987/image1.jpeg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1311&quot; data-original-width=&quot;1987&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQys5EavCOAEs0dy3M8r5Ar-9v7n5qtFkek1VSfeGBXH1pQHNv6Ebem3vDedO60BlNcp3HLGqYtsT4v4Evz2u2ksTgFfQcQIqd87NPTxVzYVyzvA85Vg_jO8a18KrMT2hOQQA5hg2frOJx2L7S5SM88VBCJzd13ClrSzHnl5xg29TMMpvqA4Yl-d98vTc1/s16000/image1.jpeg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Performance of the T5 and EdiT5 models of various sizes on the public BEA GEC benchmark plotted against mean latency. Compared to T5, EdiT5 offers a better latency-F0.5 trade-off. Note that the x axis is logarithmic.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Improved training data with large language models&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://arxiv.org/abs/2106.03830&quot;>;earlier research&lt;/a>;, as well as the results above, show that model size plays a crucial role in generating accurate grammatical corrections. To combine the advantages of large language models (LLMs) and the low latency of EdiT5, we leverage a technique called &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;hard distillation&lt;/a>;. First, we train a teacher LLM using similar datasets used for the &lt;a href=&quot;https://ai.googleblog.com/2021/10/grammar-correction-as-you-type-on-pixel.html&quot;>;Gboard grammar model&lt;/a>;. The teacher model is then used to generate training data for the student EdiT5 model. &lt;/p>; &lt;p>; Training sets for grammar models consist of &lt;em>;ungrammatical source / grammatical target&lt;/em>; sentence pairs. Some of the training sets have noisy targets that contain grammatical errors, unnecessary paraphrasing, or unwanted artifacts. Therefore, we generate new pseudo-targets with the teacher model to get cleaner and more consistent training data. Then, we re-train the teacher model with the pseudo-targets using a technique called &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/1909.13788&quot;>;self-training&lt;/a>;&lt;/em>; 。 Finally, we found that when the source sentence contains many errors, the teacher sometimes corrects only part of the errors. Thus, we can further improve the quality of the pseudo-targets by feeding them to the teacher LLM for a second time, a technique called &lt;em>;&lt;a href=&quot;https://aclanthology.org/D19-1435/&quot;>;iterative refinement&lt;/a>;&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJzgocbj86SS8moSqWrxIKz8E3CxZn712Di2CY7pwgXZ355Z4qUnDBxuEc91AU9jAdKwq4tvyaCzIqKfcJzThrjkka5q1LwXGWNa1xPi5AU7RpJDq-XmoNHs386B2bJqKDDnN8_l1mvLCSShlGjb66RWy25Z_1QXwMDpHDgsPON7r0pUqYdXZLPsx06hOC/s1089/image3.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1089&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgJzgocbj86SS8moSqWrxIKz8E3CxZn712Di2CY7pwgXZ355Z4qUnDBxuEc91AU9jAdKwq4tvyaCzIqKfcJzThrjkka5q1LwXGWNa1xPi5AU7RpJDq-XmoNHs386B2bJqKDDnN8_l1mvLCSShlGjb66RWy25Z_1QXwMDpHDgsPON7r0pUqYdXZLPsx06hOC/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Steps for training a large teacher model for grammatical error correction (GEC). Self-training and iterative refinement remove unnecessary paraphrasing, artifacts, and grammatical errors appearing in the original targets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Putting it all together&lt;/h2>; &lt;p>; Using the improved GEC data, we train two EdiT5-based models: a grammatical error correction model, and a grammaticality分类器。 When the grammar check feature is used, we run the query first through the correction model, and then we check if the output is indeed correct with the classifier model. Only then do we surface the correction to the user. &lt;/p>; &lt;p>; The reason to have a separate classifier model is to more easily trade off between precision and recall. Additionally, for ambiguous or nonsensical queries to the model where the best correction is unclear, the classifier reduces the risk of serving erroneous or confusing corrections. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have developed an efficient grammar correction model based on the state-of-the-art EdiT5 model architecture. This model allows users to check for the grammaticality of their queries in Google Search by including the “grammar check” phrase in the query. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We gratefully acknowledge the key contributions of the other team members, including Akash R, Aliaksei Severyn, Harsh Shah, Jonathan Mallinson, Mithun Kumar SR, Samer Hassan, Sebastian Krause, and Shikhar Thakur. We&#39;d also like to thank Felix Stahlberg, Shankar Kumar, and Simon Tong for helpful discussions and pointers.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/420799196308935505/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/grammar-checking-at-google-search-scale.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/420799196308935505&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/420799196308935505&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/grammar-checking-at-google-search-scale.html&quot; rel=&quot;alternate&quot; title=&quot;Grammar checking at Google Search scale&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW7pWsW1-E7U1LH1bxmjBL_2W5fS6uN2iRb2cNPBks57ubvFNJj-SjE2Zk1lFqndKWBVAeO3g3MLeJ96QUIUNJDyLTcWjeO835NT6HfJMYQBEdGqL-X_sxQ1yxMy16a0OcOLb6gSz2lqM6VofToVtN_s_F_wGB41AZwlj146y7ZXQ4PFsdywX10QO54MJ4/s72-c/HeroGC.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2376347423475108178&lt;/id>;&lt;published>;2023-10-20T10:07:00.000-07:00&lt;/published>;&lt;updated>;2023-11-27T16:21:27.895-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;distributed systems&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Publications&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Answering billions of reporting queries each day with low latency&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jagan Sankaranarayanan, Senior Staff Software Engineer, and Indrajit Roy, Head of Napa Product, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIZSvPVFwZPeTv8ivB-lHMGfoLkP-fDBuAy9GEklUVNBPPoqOXRfme5Psui7DbKssImVjFHtxoygKhFBvgpAG_C5852ocu9i7AOfWPeC1mSlaim8jfqsV55wZIULDUPk7WhxW1OISfL_CjZswN3CcZN7GJgVLBdepic8lfYAUCT0rAlXGGbnf-WuA6mRc6/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://ads.google.com/home/&quot;>;Google Ads&lt;/a>; infrastructure runs on an internal data warehouse called &lt;a href=&quot;https://research.google/pubs/pub50617/&quot;>;Napa&lt;/a>;. Billions of reporting queries, which power critical dashboards used by advertising clients to measure campaign performance, run on tables stored in Napa. These tables contain records of ads performance that are keyed using particular customers and the campaign identifiers with which they are associated. Keys are tokens that are used both to associate an ads record with a particular client and campaign (eg, &lt;code>;customer_id&lt;/code>;, &lt;code>;campaign_id&lt;/code>;) and for efficient retrieval. A record contains dozens of keys, so clients use reporting queries to specify keys needed to filter the data to understand ads performance (eg, by region, device and metrics such as clicks, etc.). What makes this problem challenging is that the data is &lt;em>;skewed&lt;/em>; since queries require varying levels of effort to be answered and have stringent latency expectations. Specifically, some queries require the use of millions of records while others are answered with just a few. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To this end, in “&lt;a href=&quot;https://research.google/pubs/pub52572/&quot;>;Progressive Partitioning for Parallelized Query Execution in Napa&lt;/a>;”, presented at &lt;a href=&quot;https://vldb.org/2023/&quot;>;VLDB 2023&lt;/a>;, we describe how the Napa data warehouse determines the amount of machine resources needed to answer reporting queries while meeting strict latency targets. We introduce a new progressive query partitioning algorithm that can &lt;a href=&quot;https://en.wikipedia.org/wiki/Parallel_computing&quot;>;parallelize&lt;/a>; query execution in the presence of complex data skews to perform consistently well in a matter of a few milliseconds. Finally, we demonstrate how Napa allows Google Ads infrastructure to serve billions of queries every day. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Query processing challenges&lt;/h2>; &lt;p>; When a client inputs a reporting query, the main challenge is to determine how to parallelize the query effectively. Napa&#39;s parallelization technique breaks up the query into even sections that are equally distributed across available machines, which then process these in parallel to significantly reduce query latency. This is done by estimating the number of records associated with a specified key, and assigning more or less equal amounts of work to machines. However, this estimation is not perfect since reviewing all records would require the same effort as answering the query. A machine that processes significantly more than others would result in run-time skews and poor performance. Each machine also needs to have sufficient work since needless parallelism leads to underutilized infrastructure. Finally, parallelization has to be a per query decision that must be executed near-perfectly billions of times, or the query may miss the stringent latency requirements. &lt;/p>; &lt;p>; The reporting query example below extracts the records denoted by keys (ie, &lt;code>;customer_id&lt;/code>; and &lt;code>;campaign_id&lt;/code>;) and then computes an aggregate (ie, &lt;code>;SUM(cost)&lt;/code>;) from an advertiser table. In this example the number of records is too large to process on a single machine, so Napa needs to use a subsequent key (eg, &lt;code>;adgroup_id&lt;/code>;) to further break up the collection of records so that equal distribution of work is achieved. It is important to note that at petabyte scale, the size of the data statistics needed for parallelization may be several terabytes. This means that the problem is not just about collecting enormous amounts of metadata, but also how it is managed. &lt;/p>; &lt;br />; &lt;span style=&quot;font-size: small;&quot;>; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>; SELECT customer_id, campaign_id, SUM(cost) FROM advertiser_table WHERE customer_id in (1, 7, ..., x ) AND campaign_id in (10, 20, ..., y) GROUP BY customer_id, campaign_id; &lt;/pre>;&lt;/span>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;This reporting query example extracts records denoted by keys (ie, &lt;code>;customer_id&lt;/code>; and &lt;code>;campaign_id&lt;/code>;) and then computes an aggregate (ie, &lt;code>;SUM(cost)&lt;/code>;) from an advertiser table. The query effort is determined by the keys&#39; included in the query. Keys belonging to clients with larger campaigns may touch millions of records since the data volume directly correlates with the size of the ads campaign. This disparity of matching records based on keys reflects the &lt;em>;skewness&lt;/em>; in data, which makes query processing a challenging problem.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; An effective solution minimizes the amount of metadata needed, focuses effort primarily on the skewed part of the key space to partition data efficiently, and works well within the allotted time. For example, if the query latency is a few hundred milliseconds, partitioning should take no longer than tens of milliseconds. Finally, a parallelization process should determine when it&#39;s reached the best possible partitioning that considers query latency expectations. To this end, we have developed a progressive partitioning algorithm that we describe later in this article. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Managing the data deluge&lt;/h2>; &lt;p>; Tables in Napa are constantly updated, so we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;>;log-structured merge forests&lt;/a>; (LSM tree) to organize the deluge of table updates. LSM is a forest of sorted data that is temporally organized with a &lt;a href=&quot;https://en.wikipedia.org/wiki/B-tree&quot;>;B-tree index&lt;/a>; to support efficient key lookup queries. B-trees store summary information of the sub-trees in a hierarchical manner. Each B-tree node records the number of entries present in each subtree, which aids in the parallelization of queries. LSM allows us to &lt;a href=&quot;https://research.google/pubs/pub50617/&quot;>;decouple&lt;/a>; the process of updating the tables from the mechanics of query serving in the sense that live queries go against a different version of the data, which is atomically updated once the next batch of ingest (called delta) has been fully prepared for querying. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrdgCgdHtjKomZM08fuF3qd6UWrgHHYvTgclc2iMV1UffAAMqVgsJVhGDSP4aPzPh4-kNb0hU7ZWkOAaf37k7PaTaH4k3m45fDzBaX9x0wxRSpvPADOXIDrtdTL62lFGJKzqEaDT6oz-fPxbkSxU3omqQAm8sWKmsA8SR4wHHKh6iAXbDGcU4a_WvEHsaL/s1999/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;667&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrdgCgdHtjKomZM08fuF3qd6UWrgHHYvTgclc2iMV1UffAAMqVgsJVhGDSP4aPzPh4-kNb0hU7ZWkOAaf37k7PaTaH4k3m45fDzBaX9x0wxRSpvPADOXIDrtdTL62lFGJKzqEaDT6oz-fPxbkSxU3omqQAm8sWKmsA8SR4wHHKh6iAXbDGcU4a_WvEHsaL/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div >; &lt;h2>;The partitioning problem&lt;/h2>; &lt;p>; The data partitioning problem in our context is that we have a massively large table that is represented as an LSM tree. In the figure below, Delta 1 and 2 each have their own B-tree, and together represent 70 records. Napa breaks the records into two pieces, and assigns each piece to a different machine. The problem becomes a partitioning problem of a forest of trees and requires a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tree_traversal&quot;>;tree-traversal algorithm&lt;/a>; that can quickly split the trees into two等份。 &lt;/p>; &lt;p>; To avoid visiting all the nodes of the tree, we introduce the concept of “good enough” partitioning. As we begin cutting and partitioning the tree into two parts, we maintain an estimate of how bad our current answer would be if we terminated the partitioning process at that instant. This is the yardstick of how close we are to the answer and is represented below by a total error margin of 40 (at this point of execution, the two pieces are expected to be between 15 and 35 records in size, the uncertainty adds up to 40）。 Each subsequent traversal step reduces the error estimate, and if the two pieces are approximately equal, it stops the partitioning process. This process continues until the desired error margin is reached, at which time we are guaranteed that the two pieces are more or less equal. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP1MUE1iWt7K2hKPBmTmr4LfIgARhI3a-nKsMqrcmiOejHLJbxDyKIoV-9ZF8k9_X4w9EMLikeVOufzTTmBGMqBKdH_AyrWz_UihzrZFsaNPWFXB2dD8o2RIBbXCM0shFN8a6fUMW0g830qrJx0Q3mpOLfzyQwwuYB9nilFeKt3VfKQYde9Eq5VmN8D-2i/s473/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;327&quot; data-original-width=&quot;473&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP1MUE1iWt7K2hKPBmTmr4LfIgARhI3a-nKsMqrcmiOejHLJbxDyKIoV-9ZF8k9_X4w9EMLikeVOufzTTmBGMqBKdH_AyrWz_UihzrZFsaNPWFXB2dD8o2RIBbXCM0shFN8a6fUMW0g830qrJx0Q3mpOLfzyQwwuYB9nilFeKt3VfKQYde9Eq5VmN8D-2i/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;Progressive partitioning algorithm&lt;/h2>; &lt;p>; Progressive partitioning encapsulates the notion of “good enough” in that it makes a series of moves to reduce the error estimate. The input is a set of B-trees and the goal is to cut the trees into pieces of more or less equal size. The algorithm traverses one of the trees (“drill down&#39;&#39; in the figure) which results in a reduction of the error estimate. The algorithm is guided by statistics that are stored with each node of the tree so that it makes an informed set of moves at each step. The challenge here is to decide how to direct effort in the best possible way so that the error bound reduces quickly in the fewest possible steps. Progressive partitioning is conducive for our use-case since the longer the algorithm runs, the more equal the pieces become. It also means that if the algorithm is stopped at any point, one still gets good partitioning, where the quality corresponds to the time spent. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsnglKJOlArk8nIcelyOi8Yej5O2OUWCwWzitU9PsT5h-0zWDyvpVhxrdMsbsB0ECvC_q6Y91peJ6Q2r5HjXJWsH47LSEDODJw1lm-aOt8lpMhcTyMU2LOf7m2KcykAnyRLFrpx_I95spiJi5qNecVwlJ7x_aRhSDTCON6G_o09WeN8x4BwQeXbJOBOzFW/s390/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;281&quot; data-original-width=&quot;390&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsnglKJOlArk8nIcelyOi8Yej5O2OUWCwWzitU9PsT5h-0zWDyvpVhxrdMsbsB0ECvC_q6Y91peJ6Q2r5HjXJWsH47LSEDODJw1lm-aOt8lpMhcTyMU2LOf7m2KcykAnyRLFrpx_I95spiJi5qNecVwlJ7x_aRhSDTCON6G_o09WeN8x4BwQeXbJOBOzFW/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Prior work in this space uses a &lt;a href=&quot;https://research.google/pubs/ pub52572/&quot;>;sampled table to drive the partitioning process&lt;/a>;, while the Napa approach uses a B-tree. As mentioned earlier, even just a sample from a petabyte table can be massive. A tree-based partitioning method can achieve partitioning much more efficiently than a sample-based approach, which does not use a tree organization of the sampled records. We compare progressive partitioning with an alternative approach, where sampling of the table at various resolutions (eg, 1 record sample every 250 MB and so on) aids the partitioning of the query. Experimental results show the relative speedup from progressive partitioning for queries requiring varying numbers of machines. These results demonstrate that progressive partitioning is much faster than existing approaches and the speedup increases as the size of the query increases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3V3INsImX_TRgcEa2VFXMK2LtVwNyCjzqYco_QMSTQWMAIfb1D5rpF7iM4xFesoltnJI9RUQNVkuuZv7nMjDIfjzhIklmdcj1xl9B8uBMfDKLIhQ9UXgH1Vrgrf2xoNvHMrv0icZDI_PKQLo9ZA6bCzrlyvX3eayjxCH_IZjFxZ1Hy9atHm6oEUqFhZz7/s1452/image3.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1192&quot; data-original-width=&quot;1452&quot; height=&quot;525&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3V3INsImX_TRgcEa2VFXMK2LtVwNyCjzqYco_QMSTQWMAIfb1D5rpF7iM4xFesoltnJI9RUQNVkuuZv7nMjDIfjzhIklmdcj1xl9B8uBMfDKLIhQ9UXgH1Vrgrf2xoNvHMrv0icZDI_PKQLo9ZA6bCzrlyvX3eayjxCH_IZjFxZ1Hy9atHm6oEUqFhZz7/w640-h525/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Napa&#39;s progressive partitioning algorithm efficiently optimizes database queries, enabling Google Ads to serve client reporting queries billions of times each day. We note that tree traversal is a common technique that students in introductory computer science courses use, yet it also serves a critical use-case at Google. We hope that this article will inspire our readers, as it demonstrates how simple techniques and carefully designed data structures can be remarkably potent if used well. Check out the &lt;a href=&quot;https://research.google/pubs/pub52572/&quot;>;paper&lt;/a>; and a &lt;a href=&quot;https://www.youtube.com/watch?v=dtWwUWB5JyQ&quot;>;recent talk&lt;/a>; describing Napa to learn more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This blog post describes a collaborative effort between Junichi Tatemura , Tao Zou, Jagan Sankaranarayanan, Yanlai Huang, Jim Chen, Yupu Zhang, Kevin Lai, Hao Zhang, Gokul Nath Babu Manoharan, Goetz Graefe, Divyakant Agrawal, Brad Adelberg, Shilpa Kolhar and Indrajit Roy.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2376347423475108178/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt; link href=&quot;http://blog.research.google/2023/10/answering-billions-of-reporting-queries.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2376347423475108178&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/2376347423475108178&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google /2023/10/answering-billions-of-reporting-queries.html&quot; rel=&quot;alternate&quot; title=&quot;Answering billions of reporting queries each day with low latency&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name >; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/1209862651477775266161 &lt;/uri>; &lt;Email>; &lt;Email>; noreply@blogger.com &lt;/email>; “ http://schemas.google.com/g/2005#thumbnail” src =“ https://img1.blogbblog.com/img/img/b16-rounded.gif” width =“ 16” &lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIZSvPVFwZPeTv8ivB-lHMGfoLkP-fDBuAy9GEklUVNBPPoqOXRfme5Psui7DbKssImVjFHtxoygKhFBvgpAG_C5852ocu9i7AOfWPeC1mSlaim8jfqsV55wZIULDUPk7WhxW1OISfL_CjZswN3CcZN7GJgVLBdepic8lfYAUCT0rAlXGGbnf-WuA6mRc6/s72-c/hero.jpg&quot; width= &quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>; tag:blogger.com,1999:blog-8474926331452026626.post-3886518375378801095&lt;/id>;&lt;published>;2023-10-19T09:24:00.002-07:00&lt;/published>;&lt;updated>;2023-10-19T09:43: 21.465-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Education&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www. blogger.com/atom/ns#&quot; term=&quot;Search&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;English learners can now practice speaking on Search&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Christian Plagemann, Director, and Katya Cox, Product Manager , Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfHhrdHOFN2-ZoUw8hxN1Gdmd5WBtL5kVUe27TeajsI3nrES8Ah13W7MIKpZ2avrsxFTdkzk-sD1noswtk20cMyPS8GjDnVMPquyxc6EhR9r53bMzncRYlj5ZBM3jMF15ndusFp2fe9ipy4bksiTLfWJ1umdcUrQUxg78SOtXfjqMbSxW_OQsSTAVZ9HV6/s600/Tivoli.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Learning a language can open up new opportunities in a person&#39;s life. It can help people connect with those from different cultures, travel the world, and advance their career. English alone is estimated to have 1.5 billion learners worldwide. Yet proficiency in a new language is difficult to achieve, and many learners cite a lack of opportunity to practice speaking actively and receiving actionable feedback as a barrier to learning. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to announce a new feature of Google Search that helps people practice speaking and improve their language skills. Within the next few days, Android users in Argentina, Colombia, India (Hindi), Indonesia, Mexico, and Venezuela can get even more language support from Google through interactive speaking practice in English — expanding to more countries and languages in the future. Google Search is already a valuable tool for language learners, providing translations, definitions, and other resources to improve vocabulary. Now, learners translating &lt;em>;to&lt;/em>; or &lt;em>;from&lt;/em>; English on their Android phones will find a new English speaking practice experience with personalized feedback. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRV7Q3FKxiVB0qI6W7mk7mf_kZ_I6H_FKhzhrHcxHh5rYUNFa4E59XqRG7u0KpGX6lgvz6ihkD8fddSlHbWI18YkN9BF-KoHnyXLZdXqMdGApRNffllQR8neHSI8hFFDzeBpSkvyLjNb5_u7MJPIIIEPBpskiR_IfeN7YWXpz46seBFjEpIjghLs4-lBj2/s939/image9.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;939&quot; data-original-width=&quot;428&quot; height=&quot;640&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRV7Q3FKxiVB0qI6W7mk7mf_kZ_I6H_FKhzhrHcxHh5rYUNFa4E59XqRG7u0KpGX6lgvz6ihkD8fddSlHbWI18YkN9BF-KoHnyXLZdXqMdGApRNffllQR8neHSI8hFFDzeBpSkvyLjNb5_u7MJPIIIEPBpskiR_IfeN7YWXpz46seBFjEpIjghLs4-lBj2/w292-h640/image9.gif&quot; width=&quot;292&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A new feature of Google Search allows learners&lt;br />;to practice speaking words in context.&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;p>; Learners are presented with real-life prompts and then form their own spoken answers using a provided vocabulary word. They engage in practice sessions of 3-5 minutes, getting personalized feedback and the option to sign up for daily reminders to keep practicing. With only a smartphone and some quality time, learners can practice at their own pace, anytime, anywhere. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Activities with personalized feedback, to supplement existing learning tools&lt;/h2>; &lt;p>; Designed to be used alongside other learning services and resources, like personal tutoring, mobile apps, and classes, the new speaking practice feature on Google Search is another tool to assist learners on their journey. &lt;/p>; &lt;p>; We have partnered with linguists, teachers, and &lt;a href=&quot;https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language&quot;>;ESL/EFL&lt;/a>; pedagogical experts to create a speaking practice experience that is effective and motivating. Learners practice vocabulary in authentic contexts, and material is repeated over dynamic intervals to increase retention — approaches that are known to be effective in helping learners become confident speakers. As one partner of ours shared: &lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;em>;&quot;Speaking in a given context is a skill that language learners often lack the opportunity to practice. Therefore this tool is very useful to complement classes and other resources.&quot; - Judit Kormos, Professor, Lancaster University&lt;/em>; &lt;/p>; &lt;/div>; &lt;p>; We are also excited to be working with several language learning partners to surface content they are helping create and to connect them with learners around the世界。 We look forward to expanding this program further and working with any interested partner. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Personalized real-time feedback &lt;/h2>; &lt;p>; Every learner is different, so delivering personalized feedback in real time is a key part of effective practice. Responses are analyzed to provide helpful, real-time suggestions and corrections.&lt;br />; &lt;/p>; &lt;p>; The system gives &lt;em>;semantic feedback&lt;/em>;, indicating whether their response was relevant to the question and may be understood by a conversation partner. &lt;em>;Grammar feedback&lt;/em>; provides insights into possible grammatical improvements, and a set of &lt;em>;example answers&lt;/em>; at varying levels of language complexity give concrete suggestions for alternative ways to respond in this context. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjEZsUjl4VVmcpQhX43ZJgAtH2S46YWrmpYQuoNvtTMovHKXVPCGoDqDVlyRv_XLzgDWfEWTlbGhTsQO-EvaSJC2K5rnxi5xEoM2amjSMVUKwP13r8DDurLkYyIFTcS7yzL7n11rZiCsfeRx0lLnIueV66BmAn9GSelvSCg27IOZNfqvc2JanmrGI1hKE5/s929/image8.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;929&quot; data-original-width=&quot;428&quot; height=&quot;640&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjEZsUjl4VVmcpQhX43ZJgAtH2S46YWrmpYQuoNvtTMovHKXVPCGoDqDVlyRv_XLzgDWfEWTlbGhTsQO-EvaSJC2K5rnxi5xEoM2amjSMVUKwP13r8DDurLkYyIFTcS7yzL7n11rZiCsfeRx0lLnIueV66BmAn9GSelvSCg27IOZNfqvc2JanmrGI1hKE5/w294-h640/image8.gif&quot; width=&quot;294&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The feedback is composed of three elements: Semantic analysis, grammar correction, and example answers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Contextual translation&lt;/h3>; &lt;p>; Among the several new technologies we developed, contextual translation provides the ability to translate individual words and phrases &lt;em>;in context&lt;/em>;. During practice sessions, learners can tap on any word they don&#39;t understand to see the translation of that word considering its context. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfGcZuy2godKFrUvo8R6jmayMNo4pT7C3lBLuApdsJxIxqmaLfXuoQjk3E4Hs5UK5xFrb03QeABCSDgphyphenhyphencnCy8-smihT_WUZ5nPLqPjpGIM4yMhL9PnqqDG5ZsyHqLjag_uxBTIJZqw4w0lkfoF8iJAerlP2tRcE10D9f6RhSIuej6cVP9pl8xjmhiqz/s1999/image3.jpg&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;949&quot; data-original-width=&quot;1999&quot; height=&quot;190&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfGcZuy2godKFrUvo8R6jmayMNo4pT7C3lBLuApdsJxIxqmaLfXuoQjk3E4Hs5UK5xFrb03QeABCSDgphyphenhyphencnCy8-smihT_WUZ5nPLqPjpGIM4yMhL9PnqqDG5ZsyHqLjag_uxBTIJZqw4w0lkfoF8iJAerlP2tRcE10D9f6RhSIuej6cVP9pl8xjmhiqz/w400-h190/image3.jpg&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of contextual translation feature.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This is a difficult technical task, since individual words in isolation often have multiple alternative meanings, and multiple words can form clusters of meaning that need to be translated in unison. Our novel approach translates the entire sentence, then estimates how the words in the original and the translated text relate to each other. This is commonly known as the &lt;a href=&quot;https://aclanthology.org/J03-1002/&quot;>;word alignment problem&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipp97I_J7eGvrIEjtu71XmBw2GDbNTOtF4lEBnI0kP1B7fK541JKlVfle0Qv8lh3ME7rOoAwOCH26TlvsogltG_krPLLoEBRsYLKfHSFq9w28cNisWL7VVU126PxSj3r2VdGJs1Tg3SJ8aLxVFTAFzqL_KPhlVZ4baUTeex7-pUzmRyFQIMZrtfR2DJErM/s1059/image1.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1059&quot; data-original-width=&quot;796&quot; height=&quot;400&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipp97I_J7eGvrIEjtu71XmBw2GDbNTOtF4lEBnI0kP1B7fK541JKlVfle0Qv8lh3ME7rOoAwOCH26TlvsogltG_krPLLoEBRsYLKfHSFq9w28cNisWL7VVU126PxSj3r2VdGJs1Tg3SJ8aLxVFTAFzqL_KPhlVZ4baUTeex7-pUzmRyFQIMZrtfR2DJErM/w301-h400/image1.gif&quot; width=&quot;301&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of a translated sentence pair and its word alignment. A deep learning alignment model connects the different words that create the meaning to suggest a translation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The key technology piece that enables this functionality is a novel &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;>;deep learning&lt;/a>; model developed in collaboration with the Google Translate team, called Deep Aligner. The basic idea is to take a multilingual language model trained on hundreds of languages, then fine-tune a novel alignment model on a set of word alignment examples (see the figure above for an example) provided by human experts, for several language pairs. From this, the single model can then accurately align any language pair, reaching state-of-the-art alignment error rate (AER, a metric to measure the quality of word alignments, where lower is better). This single new model has led to dramatic improvements in alignment quality across all tested language pairs, reducing average AER from 25% to 5% compared to alignment approaches based on &lt;a href=&quot;https://aclanthology.org/C96-2141/&quot;>;Hidden Markov models&lt;/a>; (HMMs). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjlqhOWugi22TAIKRsT6IcyjdZaWRP17p1C80QywrJfNSRqcpwiUmOJw_Km3EEu9f-E3o8ZiH6DKlzk8Yv4xrk-CFmlsVBMoQYf3Zp5UZDF6SD_cUsY2nA9uBBFsbPBz6s5j6YOL1Q0Fx4Kt-eKpyqnP2RdGhYfKLIxjDyOPPuISBwmruGdnxUP5NdPYkF/s1084/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;374&quot; data-original-width=&quot;1084&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjlqhOWugi22TAIKRsT6IcyjdZaWRP17p1C80QywrJfNSRqcpwiUmOJw_Km3EEu9f-E3o8ZiH6DKlzk8Yv4xrk-CFmlsVBMoQYf3Zp5UZDF6SD_cUsY2nA9uBBFsbPBz6s5j6YOL1Q0Fx4Kt-eKpyqnP2RdGhYfKLIxjDyOPPuISBwmruGdnxUP5NdPYkF/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Alignment error rates (lower is better) between English (EN) and other languages.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This model is also incorporated into Google&#39;s translation APIs, greatly improving, for example, the formatting of translated PDFs and websites in Chrome, the translation of YouTube captions, and enhancing Google Cloud&#39;s translation API. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Grammar feedback &lt;/h3>; &lt;p>; To enable grammar feedback for accented spoken language, our research teams adapted grammar correction models for written text (see the &lt;a href=&quot;https://workspace.google.com/blog/ai-and-machine-learning/using-neural-machine-translation-to-correct-grammatical-in-google-docs&quot;>;blog&lt;/a>; and &lt;a href=&quot;https://aclanthology.org/N19-1333/&quot;>;paper&lt;/a>;) to work on automatic speech recognition (ASR) transcriptions, specifically for the case of accented speech. The key step was fine-tuning the written text model on a corpus of human and ASR transcripts of accented speech, with expert-provided grammar corrections. Furthermore, inspired by &lt;a href=&quot;https://aclanthology.org/2020.emnlp-main.418/&quot;>;previous work&lt;/a>;, the teams developed a novel edit-based output representation that leverages the high overlap between the inputs and outputs that is particularly well-suited for short input sentences common in language learning settings. &lt;/p>; &lt;p>; The edit representation can be explained using an example: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Input&lt;/em>;: I&lt;sup>;1&lt;/sup>; am&lt;sup>;2&lt;/sup>; so&lt;sup>;3&lt;/sup>; bad&lt;sup>;4&lt;/sup>; cooking&lt;sup>;5&lt;/sup>; &lt;/li>;&lt;li>;&lt;em>;Correction&lt;/em>;: I&lt;sup>;1&lt;/sup>; am&lt;sup>;2&lt;/sup>; so&lt;sup>;3&lt;/sup>; bad&lt;sup>;4&lt;/sup>; at&lt;sup>;5&lt;/sup>; cooking&lt;sup>;6&lt;/sup>; &lt;/li>;&lt;li>;&lt;em>;Edits&lt;/em>;: (&#39;at&#39;, 4, PREPOSITION, 4) &lt;/li>; &lt;/ul>; &lt;p>; In the above, “at” is the word that is inserted at position 4 and “PREPOSITION” denotes this is an error involving prepositions. We used the error tag to select tag-dependent acceptance thresholds that improved the model further. The model increased the recall of grammar problems from 4.6% to 35%. &lt;/p>; &lt;p>; Some example output from our model and a model trained on written corpora: &lt;/p>; &lt;br>; &lt;table style=&quot;text-align: center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;Example 1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;Example 2 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left&quot;>; User input (transcribed speech) &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I live of my profession.&lt;/em>; &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I need a efficient card and reliable.&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left&quot;>; Text-based grammar model &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I live by my profession.&lt;/em>; &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I need an efficient card and a reliable.&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left&quot;>; New speech-optimized model &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I live off my profession.&lt;/em>; &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I need an efficient and reliable card.&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Semantic analysis&lt;/h3>; &lt;p>; A primary goal of conversation is to communicate one&#39;s intent clearly. Thus, we designed a feature that visually communicates to the learner whether their response was relevant to the context and would be understood by a partner. This is a difficult technical problem, since early language learners&#39; spoken responses can be syntactically unconventional. We had to carefully balance this technology to focus on the clarity of intent rather than correctness of syntax. &lt;/p>; &lt;p>; Our system utilizes a combination of two approaches: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Sensibility&lt;/em>; classification: Large language models like &lt;a href=&quot;https://blog.google/technology/ai/lamda/&quot;>;LaMDA&lt;/a>; or &lt;a href=&quot;https://blog.research.google/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; are designed to give natural responses in a conversation, so it&#39;s no surprise that they do well on the reverse: judging whether a given response is contextually sensible. &lt;/li>;&lt;li>;&lt;em>;Similarity&lt;/em>; to good responses: We used an &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf&quot;>;encoder architecture&lt;/a>; to compare the learner&#39;s input to a set of known good responses in a semantic embedding space. This comparison provides another useful signal on semantic relevance, further improving the quality of feedback and suggestions we provide.&lt;br />; &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjnk1fQyMbEd1RoCpXD7st3-gFzcFlqF-02BbDilw5b2vG5IuZrs_IOkV0nbFCQXNVslsjxsgnKKSHLxHiDUN4riv4xuDGRnrxQe2O3CUhgcuWNzTxJhVMykcZOO0UZTCHCc0S5vG-VimzA_jZ8hrPBSN8KtUqfOpTNYyE6AXYPNOLZgq8ytF08y6X2GB7/s533/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;533&quot; data-original-width=&quot;428&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjnk1fQyMbEd1RoCpXD7st3-gFzcFlqF-02BbDilw5b2vG5IuZrs_IOkV0nbFCQXNVslsjxsgnKKSHLxHiDUN4riv4xuDGRnrxQe2O3CUhgcuWNzTxJhVMykcZOO0UZTCHCc0S5vG-VimzA_jZ8hrPBSN8KtUqfOpTNYyE6AXYPNOLZgq8ytF08y6X2GB7/w321-h400/image4.gif&quot; width=&quot;321&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The system provides feedback about whether the response was relevant to the prompt, and would be understood by a communication partner.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ML-assisted content development&lt;/h2>; &lt;p>; Our available practice activities present a mix of human-expert created content, and content that was created with AI assistance and human review. This includes speaking prompts, focus words, as well as sets of example answers that showcase meaningful and contextual responses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacP_kIzsIz7Aqz_3hnuQsYMxeZ95DXI7ezmYnTvYOM3LXBNYR7mUJApgtw7BDLS1sSEAbjoc760T-D7gaZ8SAeo9qK40AUHkdpfiUJIItvdhagT2B-InWhoFomzhddu6ueT7gyDQcIIzrGGcZXmNjxfFo42VxQpmqkkgGS8dOh7NE3fPYZUwnGH4WhdMe/s1284/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1263&quot; data-original-width=&quot;1284&quot; height=&quot;394&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacP_kIzsIz7Aqz_3hnuQsYMxeZ95DXI7ezmYnTvYOM3LXBNYR7mUJApgtw7BDLS1sSEAbjoc760T-D7gaZ8SAeo9qK40AUHkdpfiUJIItvdhagT2B-InWhoFomzhddu6ueT7gyDQcIIzrGGcZXmNjxfFo42VxQpmqkkgGS8dOh7NE3fPYZUwnGH4WhdMe/w400-h394/image2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A list of example answers is provided when the learner receives feedback and when they tap the help button.&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; Since learners have different levels of ability, the language complexity of the content has to be adjusted appropriately. Prior work on language complexity estimation focuses on text of &lt;a href=&quot;https://cris.fbk.eu/handle/11582/329866&quot;>;paragraph length&lt;/a>; or &lt;a href=&quot;https://amontgomerie.github.io/2021/03/14/cefr-level-prediction.html&quot;>;longer&lt;/a>;, which differs significantly from the type of responses that our system processes. Thus, we developed novel models that can estimate the complexity of a single sentence, phrase, or even individual words. This is challenging because even a phrase composed of simple words can be hard for a language learner (eg, &quot;Let&#39;s cut to the chase”). Our best model is based on &lt;a href=&quot;https://blog.research.google /2018/11/open-sourcing-bert-state-of-art-pre.html&quot;>;BERT&lt;/a>; and achieves complexity predictions closest to human expert consensus. The model was pre-trained using a large set of LLM- labeled examples, and then fine-tuned using a human expert–labeled dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin -左：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2ZhobYkx2Zfqj4BtN-5UkvVEeLlXLTkLxDAjF_fbdBNmT48F4O0MGm6u70837x4cqNlO2uNBdd4yLVFKzFgi9ox0_F27J3eHyuZUyeFKUc2mNcuiI51-iHJIKObdFGlDkZ2QnGQV4IwYIDCgvRYpXOT_4bbRaRItkFtypYSTeAv90v2R4fjUl059Bgqf/s1134/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;1134&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2ZhobYkx2Zfqj4BtN-5UkvVEeLlXLTkLxDAjF_fbdBNmT48F4O0MGm6u70837x4cqNlO2uNBdd4yLVFKzFgi9ox0_F27J3eHyuZUyeFKUc2mNcuiI51-iHJIKObdFGlDkZ2QnGQV4IwYIDCgvRYpXOT_4bbRaRItkFtypYSTeAv90v2R4fjUl059Bgqf/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;Mean squared error&lt;/a>; of various approaches&#39; performance estimating content difficulty on a diverse corpus of ~450 conversational passages (text / transcriptions). &lt;b>;Top row:&lt;/b>; Human raters labeled the items on a scale from 0.0 to 5.0, roughly aligned to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Common_European_Framework_of_Reference_for_Languages&quot;>;CEFR scale&lt;/a>; (from A1 to C2). &lt;b>;Bottom four rows&lt;/b>;: Different models performed the same task, and we show the difference to the human expert consensus.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Using this model, we can evaluate the difficulty of text items, offer a diverse range of suggestions, and most importantly challenge learners appropriately for their ability levels. For example, using our model to label examples, we can fine-tune our system to generate speaking prompts at various language complexity levels. &lt;/p>; &lt;br>; &lt;table>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td style=&quot;text-align: center&quot; colspan=&quot;6&quot;>;Vocabulary focus words, to be elicited by the questions &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;guitar &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;apple &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;lion &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Simple &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;What do you like to play?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Do you like fruit?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Do you like big cats?&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left ;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center ;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Intermediate &lt;/td>; &lt;td>;&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Do you play any musical instruments?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;What is your favorite fruit?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;What is your favorite animal?&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left ;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center ;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Complex &lt;/td>; &lt;td>;&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;What stringed instrument do you enjoy playing?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Which type of fruit do you enjoy eating for its crunchy texture and sweet flavor?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Do you enjoy watching large, powerful predators?&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Furthermore, content difficulty estimation is used to gradually increase the task difficulty over time, adapting to the learner&#39;s progress. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; With these latest updates, which will roll out over the next few days, Google Search has become even more helpful. If you are an Android user in India (Hindi), Indonesia, Argentina, Colombia, Mexico, or Venezuela, give it a try by translating &lt;em>;to&lt;/em>; or &lt;em>;from&lt;/em>; English with Google. &lt;/p>; &lt;p>; We look forward to expanding to more countries and languages in the future, and to start offering partner practice content soon. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of this project 。 Among many others, we thank our external advisers in the language learning field: Jeffrey Davitz, Judit Kormos, Deborah Healey, Anita Bowles, Susan Gaer, Andrea Revesz, Bradley Opatz, and Anne Mcquade.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3886518375378801095/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-search-can-now-help-with-english-speaking-practice.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3886518375378801095&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3886518375378801095&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-search-can-now-help-with-english-speaking-practice.html&quot; rel=&quot;alternate&quot; title=&quot;English learners can now practice speaking on Search&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfHhrdHOFN2-ZoUw8hxN1Gdmd5WBtL5kVUe27TeajsI3nrES8Ah13W7MIKpZ2avrsxFTdkzk-sD1noswtk20cMyPS8GjDnVMPquyxc6EhR9r53bMzncRYlj5ZBM3jMF15ndusFp2fe9ipy4bksiTLfWJ1umdcUrQUxg78SOtXfjqMbSxW_OQsSTAVZ9HV6/s72-c/Tivoli.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2371558111773883902&lt;/id>;&lt;published>;2023-10-18T14:05:00.000-07:00&lt;/published>;&lt;updated>;2023-10-18T14:05:40.676-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Measurement-induced entanglement phase transitions in a quantum circuit&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jesse Hoke, Student Researcher, and Pedram Roushan, Senior Research Scientist, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjD0d2Z3JPDRdCIvFvlT3VvnLnv7sx7HndEKEnek1t_g84zs__aAh3c2TAG5hPEPYjtjJM8hikDRiSHREYGrW_TrSbHeWVC6eCq6lN2nv4ZVqmtAEg2lTyK1G26q1T-Vo9TUL9YCxVD1tG_Q8hiWBU_cbvIUo_NWrs12hPFTvbJvML4x-OU_RBzidMcwAc_/s1100/order_param_blog_1200.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Quantum mechanics allows many phenomena that are classically impossible: a quantum particle can exist in a superposition of two states simultaneously or be entangled with another particle, such that anything you do to one seems to instantaneously also affect the other, regardless of the space between them. But perhaps no aspect of quantum theory is as striking as the act of measurement. In classical mechanics, a measurement need not affect the system being studied. But a measurement on a quantum system can profoundly influence its behavior. For example, when a quantum bit of information, called a qubit, that is in a superposition of both “0” and “1” is measured, its state will suddenly collapse to one of the two classically allowed states: it will be either “0” or “1,” but not both. This transition from the quantum to classical worlds seems to be facilitated by the act of measurement. How exactly it occurs is one of the fundamental unanswered questions in physics. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In a large system comprising many qubits, the effect of measurements can cause new phases of quantum information to emerge. Similar to how changing parameters such as temperature and pressure can cause a phase transition in water from liquid to solid, tuning the strength of measurements can induce a phase transition in the entanglement of qubits. &lt;/p>; &lt;p>; Today in “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06505-7&quot;>;Measurement-induced entanglement and teleportation on a noisy quantum processor&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;Nature&lt;/a>;&lt;/em>;, we describe experimental observations of measurement-induced effects in a system of 70 qubits on our &lt;a href=&quot;https://quantumai.google/hardware&quot;>;Sycamore quantum processor&lt;/a>;. This is, by far, the largest system in which such a phase transition has been observed. Additionally, we detected &quot;quantum teleportation&quot; — when a quantum state is transferred from one set of qubits to another, detectable even if the details of that state are unknown — which emerged from measurements of a random circuit. We achieved this breakthrough by implementing a few clever “tricks” to more readily see the signatures of measurement-induced effects in the system. &lt;/p>; &lt;br />; &lt;h2>;Background: Measurement-induced entanglement&lt;/h2>; &lt;p>; Consider a system of qubits that start out independent and unentangled with one another. If they interact with one another , they will become entangled. You can imagine this as a web, where the strands represent the entanglement between qubits. As time progresses, this web grows larger and more intricate, connecting increasingly disparate points together. &lt;/p>; &lt;p>; A full measurement of the system completely destroys this web, since every entangled superposition of qubits collapses when it&#39;s measured. But what happens when we make a measurement on only a few of the qubits? Or if we wait a long time between measurements? During the intervening time, entanglement continues to grow. The web&#39;s strands may not extend as vastly as before, but there are still patterns in the web. &lt;/p>; &lt;p>; There is a balancing point between the strength of interactions and measurements, which compete to affect the intricacy of the web. When interactions are strong and measurements are weak, entanglement remains robust and the web&#39;s strands extend farther, but when measurements begin to dominate, the entanglement web is destroyed. We call the crossover between these two extremes the &lt;em>;measurement-induced phase transition&lt;/em>;. &lt;/p>; &lt;p>; In our quantum processor, we observe this measurement-induced phase transition by varying the relative strengths between interactions and measurement. We induce interactions by performing entangling operations on pairs of qubits. But to actually see this web of entanglement in an experiment is notoriously challenging. First, we can never actually look at the strands connecting the qubits — we can only infer their existence by seeing statistical correlations between the measurement outcomes of the qubits. So, we need to repeat the same experiment many times to infer the pattern of the web. But there&#39;s another complication: the web pattern is different for each possible measurement outcome. Simply averaging all of the experiments together without regard for their measurement outcomes would wash out the webs&#39; patterns. To address this, some previous experiments used “post-selection,” where only data with a particular measurement outcome is used and the rest is thrown away. This, however, causes an exponentially decaying bottleneck in the amount of “usable” data you can acquire. In addition, there are also practical challenges related to the difficulty of mid-circuit measurements with superconducting qubits and the presence of noise in the system. &lt;/p>; &lt;br />; &lt;h2>;How we did it&lt;/h2>; &lt;p>; To address these challenges, we introduced three novel tricks to the experiment that enabled us to observe measurement-induced dynamics in a system of up to 70 qubits. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Trick 1: Space and time are interchangeable&lt;/h3>; &lt;p>; As counterintuitive as it may seem, interchanging the roles of space and time dramatically reduces the technical challenges of the experiment. Before this “space-time duality” transformation, we would have had to interleave measurements with other entangling operations, frequently checking the state of selected qubits. Instead, after the transformation, we can postpone all measurements until after all other operations, which greatly simplifies the experiment. As implemented here, this transformation turns the original 1-spatial-dimensional circuit we were interested in studying into a 2-dimensional one. Additionally, since all measurements are now at the end of the circuit, the relative strength of measurements and entangling interactions is tuned by varying the number of entangling operations performed in the circuit. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjo5-hDtmBK9Odeg2d1KpiDe_fyn3e1yK0hAcYacblUt-B9-bWo9GrGUWTAdeC9jMsURYwDADlYlRm2RB9YpT6pHtaMqahpwZGMqVVuREn_J9Fi88FKFg-oiZzsW8W14HntQjXrSktkRZ3gXDyTcRUFJ_cYsk6VTrwETric5XpDK7QNhl_UGxZzc7KbmO9P/s4346/fig1_blog_1200 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2555&quot; data-original-width=&quot;4346&quot; height=&quot;376&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjo5-hDtmBK9Odeg2d1KpiDe_fyn3e1yK0hAcYacblUt-B9-bWo9GrGUWTAdeC9jMsURYwDADlYlRm2RB9YpT6pHtaMqahpwZGMqVVuREn_J9Fi88FKFg-oiZzsW8W14HntQjXrSktkRZ3gXDyTcRUFJ_cYsk6VTrwETric5XpDK7QNhl_UGxZzc7KbmO9P/w640-h376/fig1_blog_1200.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Exchanging space and time. To avoid the complication of interleaving measurements into our experiment (shown as gauges in the&amp;nbsp;&lt;strong>;left&lt;/strong>;&amp;nbsp;panel), we utilize a space-time duality mapping to exchange the roles of space and time. This mapping transforms the 1D circuit (&lt;strong>;left&lt;/strong>;) into a 2D circuit (&lt;strong>;right&lt;/strong>;), where the circuit depth (T) now tunes the effective measurement rate.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Trick 2: Overcoming the post-selection bottleneck&lt;/h3>; &lt;p>; Since each combination of measurement outcomes on all of the qubits results in a unique web pattern of entanglement, researchers often use post-selection to examine the details of a particular web. However, because this method is very inefficient, we developed a new “decoding” protocol that compares each instance of the real “web” of entanglement to the same instance in a classical simulation. This avoids post-selection and is sensitive to features that are common to all of the webs. This common feature manifests itself into a combined classical–quantum “order parameter”, akin to the &lt;a href=&quot;https://quantumai.google/cirq/noise/qcvv/xeb_theory#:~:text=Cross%20entropy%20benchmarking%20uses%20the,performance%20of%20a%20large%20device.&quot;>;cross-entropy benchmark&lt;/a>; used in the random circuit sampling used in our &lt;a href=&quot;https://blog.research.google/2019/10/quantum-supremacy-using-programmable.html&quot;>;beyond-classical demonstration&lt;/a>;. &lt;/p>; &lt;p>; This order parameter is calculated by selecting one of the qubits in the system as the “probe” qubit, measuring it, and then using the measurement record of the nearby qubits to classically “decode” what the state of the probe qubit should be. By cross-correlating the measured state of the probe with this “decoded” prediction, we can obtain the entanglement between the probe qubit and the rest of the (unmeasured) qubits. This serves as an order parameter, which is a proxy for determining the entanglement characteristics of the entire web. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIbaxw2gTCg4rZzWx3HpDcwwVtlqe6ES7EzW5VKPhyQECZK2aVgUa0TcFI8s7Ovn41ZDobntlrRVJ3J84cAu_fQRUx4vCqZD7VDQegfA_P9LBQR6Yl64g-0Cp8pur0E4858K-Vl2UKwFZ5rar4NVHDBTL7R2Nkxy-xsAOxmI_1gBrE8FQRRvjYlkdLO9U3/s2320/order_param_blog_1200.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1760&quot; data-original-width=&quot;2320&quot; height=&quot;486&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIbaxw2gTCg4rZzWx3HpDcwwVtlqe6ES7EzW5VKPhyQECZK2aVgUa0TcFI8s7Ovn41ZDobntlrRVJ3J84cAu_fQRUx4vCqZD7VDQegfA_P9LBQR6Yl64g-0Cp8pur0E4858K-Vl2UKwFZ5rar4NVHDBTL7R2Nkxy-xsAOxmI_1gBrE8FQRRvjYlkdLO9U3/w640-h486/order_param_blog_1200.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;In the decoding procedure we choose a “probe” qubit ( pink) and classically compute its expected value, conditional on the measurement record of the surrounding qubits (yellow). The order parameter is then calculated by the cross correlation between the measured probe bit and the classically computed value.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Trick 3: Using noise to our advantage&lt;/h3>; &lt;p>; A key feature of the so-called “disentangling phase” — where measurements dominate and entanglement is less widespread — is its insensitivity to noise. We can therefore look at how the probe qubit is affected by noise in the system and use that to differentiate between the two phases. In the disentangling phase, the probe will be sensitive only to local noise that occurs within a particular area near the probe. On the other hand, in the entangling phase, any noise in the system can affect the probe qubit. In this way, we are turning something that is normally seen as a nuisance in experiments into a unique probe of the system. &lt;/p>; &lt;br />; &lt;h2>;What we saw&lt;/h2>; &lt;p>; We first studied how the order parameter was affected by noise in each of the two phases. Since each of the qubits is noisy, adding more qubits to the system adds more noise. Remarkably, we indeed found that in the disentangling phase the order parameter is unaffected by adding more qubits to the system. This is because, in this phase, the strands of the web are very short, so the probe qubit is only sensitive to the noise of its nearest qubits. In contrast, we found that in the entangling phase, where the strands of the entanglement web stretch longer, the order parameter is very sensitive to the size of the system, or equivalently, the amount of noise in the system. The transition between these two sharply contrasting behaviors indicates a transition in the entanglement character of the system as the “strength” of measurement is increased. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7Z96NKSvSsw83CeFO_C3NDha5dk069lVQQIhY0w9KLzNk-uGpuy2x4B3dWTdd5WK_gu31FP4uSOPHvZT0HbKJWC3K_-YfMKUpEzFZPKdEPJLgfAkDkSgB3d5kSn5Xnevus-0lvvH6s19Zt4ix5H-qjdMixurIH7uew-9rmsXV6ke39BQS8BpgHPhp9eP-/ s2934/noise_1200.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1840&quot; data-original-width=&quot;2934&quot; height= &quot;401&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7Z96NKSvSsw83CeFO_C3NDha5dk069lVQQIhY0w9KLzNk-uGpuy2x4B3dWTdd5WK_gu31FP4uSOPHvZT0HbKJWC3K_-YfMKUpEzFZPKdEPJLgfAkDkSgB3d5kSn5Xnevus-0lvvH6s19Zt4ix5H-qjdMixurIH7uew-9rmsXV6ke39BQS8BpgHPhp9eP-/w640-h401/noise_1200.png&quot; width=&quot;640&quot; />; &lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Order parameter vs. gate density (number of entangling operations) for different numbers of qubits. When the number of entangling operations is low, measurements play a larger role in limiting the entanglement across the system. When the number of entangling operations is high, entanglement is widespread, which results in the dependence of the order parameter on system size (inset).&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In our experiment, we also demonstrated a novel form of quantum teleportation that arises in the entangling phase. Typically, a specific set of operations are necessary to implement quantum teleportation, but here, the teleportation emerges from the randomness of the non-unitary dynamics. When all qubits, except the probe and another system of far away qubits, are measured, the remaining two systems are strongly entangled with each other. Without measurement, these two systems of qubits would be too far away from each other to know about the existence of each other. With measurements, however, entanglement can be generated faster than the limits typically imposed by locality and causality. This “measurement-induced entanglement” between the qubits (that must also be aided with a classical communications channel) is what allows for quantum teleportation to occur. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgj7SwfWiDaUv0pwEPs9frmRcXUnOVzLTQTfR4CkxZeBFCJITsGB5Bm66mF978UAa_b73UGBKFara6m_LjhVOCfOtxpG17sPBp94_nVrNT2OuNKLBLU0Ntl11WEatAI2aJ2U34gkAKoMfQwxpxPcDxGIXnF8Sy9DeX0rZ9AsikPyI7KXeBWF9515HajWJnX/s2969/teleport_1200.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1754&quot; data-original-width=&quot;2969&quot; height=&quot;378&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEgj7SwfWiDaUv0pwEPs9frmRcXUnOVzLTQTfR4CkxZeBFCJITsGB5Bm66mF978UAa_b73UGBKFara6m_LjhVOCfOtxpG17sPBp94_nVrNT2OuNKLBLU0Ntl11WEatAI2aJ2U34gkAKoMfQwxpxPcDxGIXnF8Sy9DeX0rZ9AsikPyI7KXeBWF9515HajWJnX/w640-h378/teleport_1200.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Proxy entropy vs. gate density for two far separated subsystems (pink and black qubits) when all other qubits are measured. There is a finite-size crossing at ~0.9. Above this gate density, the probe qubit is entangled with qubits on the opposite side of the system and is a signature of the teleporting phase.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our experiments demonstrate the effect of measurements on a quantum circuit. We show that by tuning the strength of measurements, we can induce transitions to new phases of quantum entanglement within the system and even generate an emergent form of quantum teleportation. This work could potentially have relevance to quantum computing schemes, where entanglement and measurements both play a role. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was done while Jesse Hoke was interning at Google from Stanford University. We would like to thank Katie McCormick, our Quantum Science Communicator, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2371558111773883902/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/measurement-induced-entanglement-phase.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2371558111773883902&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2371558111773883902&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/measurement-induced-entanglement-phase.html&quot; rel=&quot;alternate&quot; title=&quot;Measurement-induced entanglement phase transitions in a quantum circuit&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjD0d2Z3JPDRdCIvFvlT3VvnLnv7sx7HndEKEnek1t_g84zs__aAh3c2TAG5hPEPYjtjJM8hikDRiSHREYGrW_TrSbHeWVC6eCq6lN2nv4ZVqmtAEg2lTyK1G26q1T-Vo9TUL9YCxVD1tG_Q8hiWBU_cbvIUo_NWrs12hPFTvbJvML4x-OU_RBzidMcwAc_/s72-c/order_param_blog_1200.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-524891095627230116&lt;/id>;&lt;published>;2023-10-16T10:12:00.000-07:00&lt;/published>;&lt;updated>;2023-10-16T10:12:58.579-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Collaboration&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Improving traffic evacuations: A case study&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Damien Pierce, Software Engineer, and John Anderson, Senior Research Director, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiq0gQYBrF4sBptEnwyqdXMyAuYpCBTZJf7JBevRgt8ZuEtflAdzaNbenGSfItI98BCpXDMpUkO3nEJyDs4XTVaCmpGrqBsLGL1E11V6-QbB629_OL_IrMhX1AfJRZIbL_O9AufL5o-kbESrDz6y_zJbP7Kj6MTn_RLQ8B7hOSlbKguvdd6jjthWNRTPZYl/s1700/trafficevac.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Some cities or communities develop an evacuation plan to be used in case of an emergency. There are a number of reasons why city officials might enact their plan, a primary one being a natural disaster, such as a tornado, flood, or wildfire. An evacuation plan can help the community more effectively respond to an emergency, and so could help save lives. However, it can be difficult for a city to evaluate such a plan because it is not practical to have an entire town or city rehearse a full blown evacuation. For example, Mill Valley, a city in northern California, created a wildfire evacuation plan but lacked an estimate for how long the evacuation would take. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today we describe a case study in which we teamed up with the city of Mill Valley to test and improve their evacuation plan. We outline our approach in our paper, “&lt;a href=&quot;https://arxiv.org/abs/2307.07108&quot;>;Mill Valley Evacuation Study&lt;/a>;”. We started by using a traffic simulator to model a citywide evacuation. The research goal was to provide the city with detailed estimates for how long it would take to evacuate the city, and, by studying the egress pattern, to find modifications to make the plan more effective. While our &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2590198220301214&quot;>;prior work&lt;/a>; on this subject provided an estimate for the evacuation time and showed how the time could be reduced if certain road changes were implemented, it turns out the recommendations in that paper — such as changing the number of outgoing lanes on an arterial — were not feasible. The current round of research improves upon the initial study by more accurately modeling the number and starting locations of vehicles, by using a more realistic map, and by working closely with city officials to ensure that recommended changes to the plan are deemed viable. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>;&lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Geography and methodology&lt;/h2>; &lt;p>; Mill Valley is in Marin County, California, north of San Francisco. Many of the residences are located on the steep hillsides of several valleys surrounded by dense redwood forests. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-rEeqPv7eEzcaCjLSS_ch9Nzuwe6cVoFjxFVDO2b-SSyRGrBZv7qybrvcSVyIsiDCTsi6ZthJRz2fESC1eHmSmrEo7aGReWESSakOtYqLfiz0usyE40XQA0Jn_RXXKHrIyWDNvIfQoBeWuT-QDacbz0kx0Wr8jKo9TryGf2drFfPdbSWW8VxRuF0eGD9h/s1999/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-rEeqPv7eEzcaCjLSS_ch9Nzuwe6cVoFjxFVDO2b-SSyRGrBZv7qybrvcSVyIsiDCTsi6ZthJRz2fESC1eHmSmrEo7aGReWESSakOtYqLfiz0usyE40XQA0Jn_RXXKHrIyWDNvIfQoBeWuT-QDacbz0kx0Wr8jKo9TryGf2drFfPdbSWW8VxRuF0eGD9h/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style =&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm-LcwnvHIYfP0WbN0YrA_LsgYRbOrXsP6_wkjU_y0b5NQROw2uPHr9VrJw3uCXzdm5DIDRgHIcdi2VjICkX3_mIYHN7WF1eyf3CjWxD1NJHh6DBaafwaYarEE-NlcqkaCemu2yghi7rUHtjWa6jrLCDKO97rWKtjsBvnVWeQENvU0sx7ne0xQI7M_el-H/s1999/image8.png&quot; style=&quot;margin-左：自动； margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm-LcwnvHIYfP0WbN0YrA_LsgYRbOrXsP6_wkjU_y0b5NQROw2uPHr9VrJw3uCXzdm5DIDRgHIcdi2VjICkX3_mIYHN7WF1eyf3CjWxD1NJHh6DBaafwaYarEE-NlcqkaCemu2yghi7rUHtjWa6jrLCDKO97rWKtjsBvnVWeQENvU0sx7ne0xQI7M_el-H/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Aerial views of Mill Valley, courtesy of the City of Mill Valley. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Many of those residences are in areas that have only one exit direction, toward the town center. From there the best evacuation route is toward Highway 101, which is in the flat part of the city and is the most likely area to be far from potential wildfires. Some neighborhoods have other routes that lead away from both the city and Highway 101, but those routes pass through hilly forested areas, which could be dangerous or impassable during a wildfire. So, the evacuation plan directs all vehicles west of Highway 101 to head east, to the highway (see map below). The neighborhoods east of Highway 101 are not included in the simulation because they are away from areas with a high fire hazard rating, and are close to the highway. &lt;/p>; &lt;p>; Mill Valley has about 11,400 households west of Highway 101. Most Mill Valley households have two vehicles. Evacuation times scale with the number of vehicles, so it is in the common interest to minimize the number of vehicles used during an evacuation. To that end, Mill Valley has a public awareness campaign aimed at having each household evacuate in one vehicle. While no one knows how many vehicles would be used during an evacuation, it is safe to assume it is on average between one and two per household. The basic evacuation problem, then, is how to efficiently get between 11 and 23 thousand vehicles from the various residences onto one of the three sets of Highway 101 on-ramps. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwsFEZpO2_VYt-i0FHVg3C4Jn_PMQMKMl37lNWYQTXEskYSyy4KTpVmTne00MASABWnes1eLqE0XN16dG0WN9WrvKSVL_4dbB9e5NZchWh9VOVLhzxUjfCEJ1kuAlCM5FVXLy8ydwI-SqhC40_VkOBj1rqTRBgjPdFEM6JRZqSPcwE5apKPQL-x7qdWf9u/s549/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;549&quot; data-original-width=&quot;536&quot; height=&quot;640&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwsFEZpO2_VYt-i0FHVg3C4Jn_PMQMKMl37lNWYQTXEskYSyy4KTpVmTne00MASABWnes1eLqE0XN16dG0WN9WrvKSVL_4dbB9e5NZchWh9VOVLhzxUjfCEJ1kuAlCM5FVXLy8ydwI-SqhC40_VkOBj1rqTRBgjPdFEM6JRZqSPcwE5apKPQL-x7qdWf9u/w624-h640/image3.png&quot; width=&quot;624&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The simulated part of Mill Valley west of Highway 101 is inside the blue border. Highway 101 is shown in green. The red squares indicate the three sets of Highway 101 on-ramps. The pink area has the highest fire hazard rating.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The current work uses the same general methodology as the previous research, namely, running the &lt;a href=&quot;https://en.wikipedia.org/wiki/Simulation_of_Urban_MObility&quot;>;open source SUMO agent-based traffic simulator&lt;/a>; on a map of Mill Valley. The traffic simulator models traffic by simulating each vehicle individually. The detailed behaviors of vehicles are dictated by a &lt;a href=&quot;https://sumo.dlr.de/docs/Car-Following-Models.html&quot;>;car-following model&lt;/a>;. Each vehicle is given a point and time at which to start and an initial route. The routes of most vehicles are updated throughout the simulation, depending on conditions. To consider potential changes in driver behavior under the high stress conditions of an evacuation, the effects of the “aggressiveness” of each car is also investigated, but in our case the impacts are minimal. Some simplifying assumptions are that vehicles originate at residential addresses and the roads and highways are initially empty. These assumptions correspond approximately to conditions that could be encountered if an evacuation happens in the middle of the night. The main inputs in the simulation are the road network, the household locations, the average number of vehicles per household, and a departure temporal distribution. We have to make assumptions about the departure distribution. After discussing with the city officials, we chose a distribution such that most vehicles depart within an hour. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Four bottlenecks&lt;/h2>; &lt;p>; Mill Valley has three sets of Highway 101 on-ramps: northern, middle, and southern. All the vehicles must use one of these sets of on-ramps to reach their destination (either the northernmost or southernmost segment of Highway 101 included in our map). Given that we are only concerned with the majority of Mill Valley that lies west of the highway, there are two lanes that approach the northern on-ramps, and one lane that approaches each of the middle and southern on-ramps. Since every vehicle has to pass over one of these four lanes to reach the highway, they are the bottlenecks. Given the geography and existing infrastructure, adding more lanes is infeasible. The aim of this research, then, is to try to modify traffic patterns to maximize the rate of traffic on each of the four lanes. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evacuation plan&lt;/h2>; &lt;p>; When we started this research, Mill Valley had a preliminary evacuation计划。 It included modifying traffic patterns — disabling traffic lights and changing traffic rules — on a few road segments, as well as specifying the resources (traffic officers, signage) necessary to implement the changes. As an example, a two-way road may be changed to a one-way road to double the number of outgoing lanes. Temporarily changing the direction of traffic is called &lt;em>;contraflow&lt;/em>;. &lt;/p>; &lt;p>; The plot below shows the simulated fraction of vehicles that have departed or reached their destinations versus time, for 1, 1.5, and 2 vehicles per household (left to right). The dashed line on the far left shows the fraction that have departed. The solid black lines show the preliminary evacuation plan results and the dotted lines indicate the normal road network (baseline) results. The preliminary evacuation plan significantly speeds up the evacuation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGFqAZbQu6Nt75U1UzXpyDmD0Hei9HCOEY-AeGExQQjIp62ubwFI0LqatEBGYtzwECiHgxtFRgZfDBeaNmFhV-WDM4vo1V0W_UJTh59oJdfjC6IusT3QZZJPcr2BAirIFp5MNe_O1_uu0EzNUFIAMJH0u_K4dmesGxEKmjejv4NMAp6DLwNlfS4mUUJ6_6/s812/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;425&quot; data-original-width=&quot;812&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGFqAZbQu6Nt75U1UzXpyDmD0Hei9HCOEY-AeGExQQjIp62ubwFI0LqatEBGYtzwECiHgxtFRgZfDBeaNmFhV-WDM4vo1V0W_UJTh59oJdfjC6IusT3QZZJPcr2BAirIFp5MNe_O1_uu0EzNUFIAMJH0u_K4dmesGxEKmjejv4NMAp6DLwNlfS4mUUJ6_6/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;The cumulative fraction of vehicles vs. time in hours. The demand curve is shown in the dashed line on the far left. The solid lines show the preliminary evacuation plan curves for 1, 1.5 and 2 vehicles per household (left to right). The dotted lines show the same for the baseline case.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We can understand how effective the preliminary evacuation plan is by measuring the rates at the bottlenecks. The below plots show the rate of traffic on each of the four lanes leading to the highway on-ramps for the case of 1.5 vehicles per household for both the baseline case (the normal road rules; shown shaded in gray) and the preliminary evacuation plan (shown outlined in black). The average rate per lane varies greatly in the different cases. It is clear that, while the evacuation plan leads to increased evacuation rates, there is room for improvement. In particular, the middle on-ramps are quite underutilized.&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilGFPn9SRnlDgD_btIeFw0BM2YqNp9kSivDA0OuGqrezvaDrjyrCKalxWyzlK2HGwkzKzDEJNHKBDKp9YEpvi8AyN20raA_H88JQY2YQ96Bzx4oGEk_dXcadtyhyphenhyphenx5SVJqiLpSEwIq9Vgy9aqhKX4YiIRWwCje2LsfaiEjs1jZvahUsv-xuJV3oQyCWmFD/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1500&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilGFPn9SRnlDgD_btIeFw0BM2YqNp9kSivDA0OuGqrezvaDrjyrCKalxWyzlK2HGwkzKzDEJNHKBDKp9YEpvi8AyN20raA_H88JQY2YQ96Bzx4oGEk_dXcadtyhyphenhyphenx5SVJqiLpSEwIq9Vgy9aqhKX4YiIRWwCje2LsfaiEjs1jZvahUsv-xuJV3oQyCWmFD/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The rates of traffic on the four lanes leading to Highway 101 on-ramps for both the baseline case (normal road rules; shown shaded in gray) and the preliminary evacuation plan (shown outlined in black).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Final evacuation plan&lt;/h2>; &lt;p>; After studying the map and investigating different alternatives, we, working together with city officials, found a minimal set of new road changes that substantially lower the evacuation time compared to the preliminary evacuation plan (shown below). We call this the final evacuation plan. It extends the contraflow section of the preliminary plan 1000 feet further west, to a main intersection. Crucially, this allows for one of the (normally) two outgoing lanes to be dedicated to routing traffic to the middle on-ramps. It also creates two outgoing lanes from that main intersection clear through to the northern on-ramps, over ¾ of a mile to the east. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVZzJcpphq9-XvpGma97D-9IPEsZ-7A_k_j9809MEEwtuI5te14hozh7VAe1i9xH8iOv8TMwErCBoK_Rxfux3GURoOqDMhNFW5VoEMl5bAII5gkoWrjFfdWlSvMhxl2hdbYb00JDAZTA-yiFsXo7Sm057QEia81G-otuKy1lZF04u_PFWY_hroXxTmtDka/s1510 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;705&quot; data-original-width=&quot;1510&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVZzJcpphq9-XvpGma97D-9IPEsZ-7A_k_j9809MEEwtuI5te14hozh7VAe1i9xH8iOv8TMwErCBoK_Rxfux3GURoOqDMhNFW5VoEMl5bAII5gkoWrjFfdWlSvMhxl2hdbYb00JDAZTA-yiFsXo7Sm057QEia81G-otuKy1lZF04u_PFWY_hroXxTmtDka/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A map of the main changes in the final evacuation plan. The red line shows that traffic heading north on Camino Alto gets diverted to the middle Highway 101 on-ramps. The blue line shows traffic in the northern lane of E Blithedale Ave gets routed on the new contraflow section.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The rate per lane plots comparing the preliminary and final evacuation plans are shown below for 1.5 vehicles per household. The simulation indicates that the final plan increases the average rate of traffic on the lane leading to the middle on-ramps from about 4 vehicles per minute to about 18. It also increases the through rate of the northern on-ramps by over 60%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEip473wwGuslqnXu9FY1DsHgVfUbRC-9Unt1yONvUPBfQ0ekAdYVOZYFLWPjS4qtilW61kQJNBa_MKWYju4CJKabqrxu21KIQGGT0rxhOFNQrFIi46PY08lBiZNMRNnpwWozNGKbjynNdUDO5YdlxWocijTh4cvBkgh_jmsYE9PgFAWFDTbem_JLuzVrDqY/s1999/image3.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1504&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEip473wwGuslqnXu9FY1DsHgVfUbRC-9Unt1yONvUPBfQ0ekAdYVOZYFLWPjS4qtilW61kQJNBa_MKWYju4CJKabqrxu21KIQGGT0rxhOFNQrFIi46PY08lBiZNMRNnpwWozNGKbjynNdUDO5YdlxWocijTh4cvBkgh_jmsYE9PgFAWFDTbem_JLuzVrDqY/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;The rates of traffic on the four lanes leading to Highway 101 on-ramps for both the preliminary case (shown shaded in gray) and the final evacuation plan (shown outlined in black).&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;p>; The below plot shows the cumulative fraction of vehicles vs. time, comparing the cases of 1, 1.5 and 2 vehicles per household for the preliminary and final evacuation plans. The speedup is quite significant, on the scale of hours. For example, with 1.5 vehicles per household, it took 5.3 hours to evacuate the city using the preliminary evacuation plan, and only 3.5 hours using the final plan. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhchH8NchEEmevzOxMoC4Rne0WOMAcijZqydDY5Qq_-5eehyphenhyphenSBp4A-GaRPVc_pQgl4etQydVCrX6gY6gNFkqt0Zk7QDkrDEMu630BoTxvppbHKvAL52aCEJ4Azzu2UlGSXhKHpzZ0-8tfa3HUpP36WYDLCRdJD4geCypjLDwatL8qB4Y6a_cYrxsJg-CnOz/s812/image6 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;425&quot; data-original-width=&quot;812&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhchH8NchEEmevzOxMoC4Rne0WOMAcijZqydDY5Qq_-5eehyphenhyphenSBp4A-GaRPVc_pQgl4etQydVCrX6gY6gNFkqt0Zk7QDkrDEMu630BoTxvppbHKvAL52aCEJ4Azzu2UlGSXhKHpzZ0-8tfa3HUpP36WYDLCRdJD4geCypjLDwatL8qB4Y6a_cYrxsJg-CnOz/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The cumulative fraction of vehicles vs. time in hours. The demand curve is shown in the dashed line on the far left. The solid lines show the final evacuation plan curves for 1, 1.5 and 2 vehicles per household (left to right). The dotted lines show the same for the preliminary evacuation plan.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Evacuation plans can be crucial in quickly getting many people to safety in emergency situations. While some cities have traffic evacuation plans in place, it can be difficult for officials to learn how well the plan works or whether it can be improved. Google Research helped Mill Valley test and evaluate their evacuation plan by running traffic simulations. We found that, while the preliminary plan did speed up the evacuation time, some minor changes to the plan significantly expedited evacuation. We worked closely with the city during this research, and Mill Valley has adopted the final plan. We were able to provide the city with more simulation details, including results for evacuating the city one area at a time. Full details can be found in &lt;a href=&quot;https://arxiv.org/abs/2307.07108&quot;>;the paper&lt;/a>;. &lt;/p>; &lt;p>; Detailed recommendations for a particular evacuation plan are necessarily specific to the area under study. So, the specific road network changes we found for Mill Valley are not directly applicable for other cities. However, we used only public data (road network from &lt;a href=&quot;http://openstreetmap.org&quot;>;OpenStreetMap&lt;/a>;; household information from census data) and an open source simulator (&lt;a href=&quot;https://sumo.dlr.de/docs/index.html&quot;>;SUMO&lt;/a>;), so any city or agency could use the methodology used in our paper to obtain results for their area. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank former Mayor John McCauley and City of Mill Valley personnel Tom Welch, Lindsay Haynes, Danielle Staude, Rick Navarro and Alan Piombo for numerous discussions and feedback, and Carla Bromberg for program management.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/524891095627230116/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/524891095627230116&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/524891095627230116&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html&quot; rel=&quot;alternate&quot; title=&quot;Improving traffic evacuations: A case study&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiq0gQYBrF4sBptEnwyqdXMyAuYpCBTZJf7JBevRgt8ZuEtflAdzaNbenGSfItI98BCpXDMpUkO3nEJyDs4XTVaCmpGrqBsLGL1E11V6-QbB629_OL_IrMhX1AfJRZIbL_O9AufL5o-kbESrDz6y_zJbP7Kj6MTn_RLQ8B7hOSlbKguvdd6jjthWNRTPZYl/s72-c/trafficevac.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;