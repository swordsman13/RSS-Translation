<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2024-01-13T15:56:43.750-08:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Health"></category><category term="Robotics"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="AI for Social Good"></category><category term="On-device Learning"></category><category term="Security and Privacy"></category><category term="Computer Science"></category><category term="HCI"></category><category term="MOOC"></category><category term="Quantum AI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="accessibility"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Audio"></category><category term="TPU"></category><category term="Android"></category><category term="EMNLP"></category><category term="ICML"></category><category term="ML"></category><category term="Physics"></category><category term="video"></category><category term="Awards"></category><category term="ML Fairness"></category><category term="Responsible AI"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Google Maps"></category><category term="TTS"></category><category term="User Experience"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="Video Analysis"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Diversity"></category><category term="Interspeech"></category><category term="RAI-HCT Highlights"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Cloud Platform"></category><category term="Google Genomics"></category><category term="Large Language Models"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Differential Privacy"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Climate"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Kaggle"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Generative AI"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="Weather"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="Graphs"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1326&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-1800430129205268706&lt;/id>;&lt;发布>;2024-01-12T09:04:00.000-08:00&lt;/发布>;&lt;更新>;2024-01- 12T10:39:38.341-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;大型语言模型&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AMIE：用于诊断医学推理和对话的研究人工智能系统&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者Alan Karthikesalingam 和 Vivek Natarajan，Google 研究主管&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_z vQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/s16000/AMIE.gif&quot; style=&quot;显示： 没有任何;” />; &lt;p>; 医患对话是医学的基石，熟练且有意的沟通可以推动诊断、管理、同理心和信任。能够进行此类诊断对话的人工智能系统可以通过成为临床医生和患者等有用的对话伙伴来提高护理的可用性、可及性、质量和一致性。但接近临床医生丰富的专业知识是一项重大挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 医学领域之外的大型语言模型 (LLM) 的最新进展表明，它们可以计划、推理并使用相关上下文来进行丰富的对话。然而，良好的诊断对话有许多方面是医学领域独有的。一位高效的临床医生会获取完整的“临床病史”，并提出有助于得出鉴别诊断的明智问题。他们运用相当多的技能来建立有效的关系，清楚地提供信息，与患者共同做出明智的决定，对他们的情绪做出同理心的反应，并在下一步的护理中支持他们。虽然法学硕士可以准确地执行医学总结或回答医学问题等任务，但很少有专门针对开发此类对话诊断能力的工作。 &lt;/p>; &lt;p>; 受这一挑战的启发，我们开发了&lt;a href=&quot;https://arxiv.org/abs/2401.05654&quot;>;Articulate Medical Intelligence Explorer (AMIE)&lt;/a>;，这是一个基于法学硕士，并针对诊断推理和对话进行了优化。我们从临床医生和患者的角度从反映现实世界临床咨询质量的多个维度对 AMIE 进行了培训和评估。为了将 AMIE 扩展到多种疾病状况、专业和场景，我们开发了一种新颖的基于自我游戏的模拟诊断对话环境，具有自动反馈机制，以丰富和加速其学习过程。我们还引入了推理时间链推理策略，以提高 AMIE 的诊断准确性和对话质量。最后，我们通过模拟与训练有素的演员的协商，在多轮对话的真实例子中前瞻性地测试了 AMIE。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcY Pb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s1200/AMIE%20GIF%201%20v2.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkd fshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s16000/AMIE%20GIF%201%20v2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;AMIE 针对诊断对话进行了优化，提出有助于减少不确定性并提高诊断准确性的问题，同时还平衡了有效临床沟通的其他要求，例如同理心、培养&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;对话式诊断AI的评估&lt;/h2>; &lt;p>;除了开发和优化AI系统本身对于诊断对话，如何评估此类系统也是一个悬而未决的问题。受到用于衡量现实环境中咨询质量和临床沟通技巧的公认工具的启发，我们构建了一个试点评估标准，以评估与病史采集、诊断准确性、临床管理、临床沟通技巧、关系培养和共情。 &lt;/p>; &lt;p>; 然后，我们设计了一项基于文本的咨询的随机、双盲交叉研究，其中经过验证的患者参与者与经过委员会认证的初级保健医生 (PCP) 或针对诊断对话优化的人工智能系统进行交互。我们以&lt;a href=&quot;https://en.wikipedia.org/wiki/Objective_structed_clinical_examination&quot;>;客观结构化临床检查&lt;/a>; (OSCE) 的方式进行咨询，这是一种实际评估中常用的方法世界以标准化和客观的方式检查临床医生的技能和能力。在典型的 OSCE 中，临床医生可能会在多个工作站之间轮换，每个工作站都模拟现实生活中的临床场景，在这些场景中，他们执行诸如与标准化患者演员（经过仔细培训以模拟患有特定病症的患者）进行咨询等任务。咨询是使用同步文本聊天工具进行的，模仿了当今大多数使用法学硕士的消费者所熟悉的界面。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETCl VRu8Hv3J9gQRd_WGYwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/s1350/image4.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1350&quot; height=&quot;568&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETClVru8Hv3J9gQRd_WGYwORLiyl KUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/w640-h568/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE 是一个基于法学硕士的研究人工智能系统，用于诊断推理和对话。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;br />; &lt;h2>;AMIE：基于法学硕士的会话诊断研究人工智能系统&lt;/h2>; &lt;p>; 我们在真实世界数据集上训练 AMIE，其中包括医学推理、医学总结和真实世界临床对话。 &lt;/p>; &lt;p>; 使用通过被动收集和转录现场临床就诊而开发的真实对话来培训法学硕士是可行的，但是，两个重大挑战限制了他们在培训法学硕士进行医学对话方面的有效性。首先，现有的现实世界数据往往无法捕捉广泛的医疗状况和场景，阻碍了可扩展性和全面性。其次，来自现实世界对话记录的数据往往很嘈杂，包含模棱两可的语言（包括俚语、行话、幽默和讽刺）、中断、不合语法的话语和隐含的引用。 &lt;/p>; &lt;p>; 为了解决这些限制，我们设计了一个基于自我游戏的模拟学习环境，具有自动反馈机制，可在虚拟护理环境中进行诊断医学对话，使我们能够在许多医疗条件和环境中扩展 AMIE 的知识和能力。除了所描述的真实世界数据的静态语料库之外，我们还使用此环境通过一组不断发展的模拟对话来迭代微调 AMIE。 &lt;/p>; &lt;p>; 这个过程由两个自我对弈循环组成：（1）一个“内部”自我对弈循环，其中 AMIE 利用上下文中的评论家反馈来改进其在与人工智能患者模拟器的模拟对话中的行为； （2）“外部”自我播放循环，其中一组经过改进的模拟对话被纳入后续的微调迭代中。由此产生的新版本的 AMIE 可以再次参与内部循环，从而创建一个良性的持续学习循环。 &lt;/p>; &lt;p>; 此外，我们还采用了推理时间链推理策略，使 AMIE 能够根据当前对话逐步完善其响应，以得出知情且有依据的答复。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRG LoGpkMV99TM1Ru0xpNSNWee-5xfkUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s1400/image1.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1400&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRGLoGpkMV99TM1Ru0xpNSNWee-5xf kUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;AMIE 使用一种新颖的基于自我游戏的模拟对话学习环境来提高多种疾病状况、专业和患者背景下的诊断对话质量。&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyL nfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/ s1834/image4.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYN qWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;AMIE 使用一种新颖的基于自我游戏的模拟对话学习环境来提高多种疾病状况、专业和患者背景下的诊断对话的质量。&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>;-->; &lt;br />; &lt;p>; 我们测试了与模拟患者（由训练有素的演员扮演）会诊时的表现，并与使用上述随机方法的 20 名真实 PCP 的表现进行比较。AMIE 和 PCP在一项随机、盲法交叉研究中，从专科主治医生和模拟患者的角度进行了评估，该研究包括来自加拿大、英国和印度的 OSCE 提供者的 149 个案例场景，涉及不同的专业和疾病。值得注意的是，我们的研究并不是为了模仿传统的面对面 OSCE 评估或临床医生通常使用文本、电子邮件、聊天或远程医疗的方式。相反，我们的实验反映了当今消费者与法学硕士互动的最常见方式，这是人工智能系统参与远程诊断对话的一种潜在可扩展且熟悉的机制。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_Jjw CM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s1999 /image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwh TQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s16000/image8.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过在线多轮同步文本聊天与模拟患者进行虚拟远程 OSCE 的随机研究设计概述。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;AMIE 的性能&lt;/h2>; &lt;p>; 在此设置中，我们观察到，在对两者进行评估时，AMIE 执行模拟诊断对话的效果至少与 PCP 一样好沿着多个具有临床意义的咨询质量轴。从专科医生的角度来看，AMIE 在 32 个轴中的 28 个轴上具有更高的诊断准确性和卓越的性能，从患者参与者的角度来看，在 26 个轴中的 24 个轴上，AMIE 具有更高的诊断准确性和卓越的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVR srwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/s1834/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_S vuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在我们的评估中，AMIE 在诊断对话的多个评估轴上均优于 PCP。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w- ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s1999/image9.png&quot;样式=“左边距：自动”； margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;752&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLV HgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;专家评级的 top-k 诊断准确性。AMIE 和 PCP Top-k 鉴别诊断 (DDx) 准确性在 149 个场景中与地面真实诊断 (a) 和可接受的鉴别诊断 (b) 中列出的所有诊断进行比较。 Bootstrapping (n=10,000) 确认 AMIE 和 PCP DDx 准确度之间的所有前 k 个差异均显着，在&lt;a href=&quot;https://en.wikipedia.org/wiki/False_discovery_rate&quot;>;错误发现率之后 p &lt;0.05 &lt;/a>;&amp;nbsp;(FDR) 修正。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption -container&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mLS635ryiJJOF7ZOuuA4t0rkj1OXWXB 57GW- FQcNcYq_TKfTPyCLm-EV3Ivk5yPgYdjYKxT8-yxQnDz4mNJwKop4yS3XvyNpcUzVOrhm0MrJKs5DVfl-u8hgwhwkI6kZAvCto4Z4JGcnTb/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1864&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mlLS635ryiJJOF7ZOuuA4t0rkj1OXWXB57GW-FQcNcYq_TKfTPyCLm-EV3Ivk5yPgYdjYKxT8-yxQnDz4mN JwKop4yS3XvyNpcUzVOrhm0MrJKs5DVfl-u8hgwhwkI6kZAvCto4Z4JGcnTb/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;由专科医生评估的诊断对话和推理质量。在 32 个轴中的 28 个轴上，AMIE 的表现优于 PCP，而在其他轴上则具有可比性。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; !-- &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left:auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetE ptfRWgoWJ4- 4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpVgGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s1892/image6.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1714&quot; data-original-width=&quot;1892&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4-4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpV gGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;诊断由专科医生评估的对话和推理能力。在 32 个轴中的 28 个轴上，AMIE 的表现优于 PCP，而在其他轴上则相当。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;br />; &lt;h2>;局限性&lt;/h2>; &lt;p>;我们的研究有几个局限性，应该谨慎解释。首先，我们的评估技术可能低估了人类对话的现实价值，因为我们研究中的临床医生仅限于不熟悉的文本聊天界面，允许大规模的法学硕士与患者互动，但并不代表通常的临床实践。其次，任何此类研究都必须被视为漫长旅程中探索性的第一步。从我们在本研究中评估的法学硕士研究原型过渡到可供人们和为其提供护理的人使用的安全而强大的工具，将需要大量的额外研究。有许多重要的限制需要解决，包括现实世界约束下的实验性能，以及对健康公平、隐私、鲁棒性等重要主题的专门探索，以确保技术的安全性和可靠性。 &lt;/p>; &lt;br />; &lt;h2>;AMIE 对临床医生的帮助&lt;/h2>; &lt;p>; 在&lt;a href=&quot;https://arxiv.org/abs/2312.00164&quot;>;最近发布的预印本&lt;/a >;，我们评估了 AMIE 系统早期迭代单独生成 DDx 或作为临床医生辅助的能力。二十 (20) 名全科临床医生评估了来自&lt;em>;&lt;a href=&quot;https://www.nejm.org/&quot;>;新英格兰医学杂志&lt;/a>;&lt;/em>;的 303 个具有挑战性的真实医疗案例>; (NEJM) &lt;a href=&quot;https://www.nejm.org/case-challenges&quot;>;临床病理学会议&lt;/a>; (CPC)。每份病例报告均由两名随机接受两种辅助条件之一的临床医生阅读：搜索引擎和标准医疗资源的帮助，或除这些工具外的 AMIE 帮助。所有临床医生在使用相应的辅助工具之前都提供了基线、无协助的 DDx。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfv MTKASXUVZ5n_I4VytKfa0S3EN5vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s1999/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1022&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfvMTKASXUVZ5n_I4VytKfa0S3EN5 vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;协助随机读者研究设置，以调查 AMIE 对临床医生解决新英格兰医学杂志复杂诊断病例挑战的辅助效果。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; AMIE 表现出的独立性能超过了无人协助的临床医生（前 10 名准确率分别为 59.1% 和 33.6%，p= 0.04）。比较两个辅助研究组，与没有 AMIE 协助的临床医生 (24.6%，p&lt;0.01) 和有搜索的临床医生 (5.45%，p=0.02) 相比，受 AMIE 协助的临床医生的前 10 名准确率更高。此外，与没有 AMIE 协助的临床医生相比，得到 AMIE 协助的临床医生得出了更全面的鉴别列表。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89Qzza HXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/s1999/image7.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1424&quot; data-original-width=&quot;1999&quot; height=&quot;456&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6 Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/w640-h456/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;除了强大的独立性能之外，使用AMIE系统还为临床医生解决这些复杂病例挑战带来了显着的辅助效果和诊断准确性的提高。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 值得注意的是，NEJM CPC 并不代表日常临床实践。它们是仅针对数百人的不寻常病例报告，因此为探讨公平或公平等重要问题提供了有限的范围。 &lt;/p>; &lt;br />; &lt;h2>;医疗保健领域大胆而负责任的研究——可能性的艺术&lt;/h2>; &lt;p>; 在世界各地，获得临床专业知识的机会仍然很少。虽然人工智能在特定的临床应用中显示出了巨大的前景，但参与临床实践的动态、对话式诊断过程需要人工智能系统尚未展示的许多功能。医生不仅拥有知识和技能，还致力于遵守无数原则，包括安全和质量、沟通、伙伴关系和团队合作、信任和专业精神。在人工智能系统中实现这些属性是一项鼓舞人心的挑战，应该负责任地、谨慎地对待。 AMIE 是我们对“可能性的艺术”的探索，这是一个仅供研究的系统，用于安全地探索未来的愿景，其中人工智能系统可能会更好地与委托我们护理的熟练临床医生的属性保持一致。它只是早期的实验工作，而不是产品，并且有一些局限性，我们认为值得进行严格和广泛的进一步科学研究，以设想一个对话式、同理心和诊断式人工智能系统可能变得安全、有用和易于使用的未来。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;此处描述的研究是 Google Research 和 Google Deepmind 多个团队的共同工作。我们感谢所有的合著者——Tao Tu、Mike Schaekermann、Anil Palepu、Daniel McDuff、Jake Sunshine、Khaled Saab、Jan Freyberg、Ryutaro Tanno、Amy Wang、Brenna Li、Mohamed Amin、Sara Mahdavi、Karan Sighal、Shekoofeh阿齐兹、内纳德·托马塞夫、刘云、程勇、侯乐、阿尔伯特·韦伯森、杰克·加里森、亚什·夏尔马、阿努潘·帕塔克、苏珊特·普拉卡什、菲利普·曼斯菲尔德、施韦塔克·帕特尔、布拉德利·格林、埃娃·多米诺斯卡、蕾妮·黄、尤拉吉·戈特维斯、戴尔·韦伯斯特、凯瑟琳·周、克里斯托弗·塞姆图斯、乔尔·巴拉尔、格雷格·科拉多和约西·马蒂亚斯。我们还要感谢萨米·拉赫加尔、劳伦·维纳和约翰·吉利亚德对叙事和视觉效果的支持。最后，我们感谢 Michael Howell、James Maynika、Jeff Dean、Karen DeSalvo、Zoubin Gharahmani 和 Demis Hassabis 在本项目过程中的支持&lt;/em>;。 &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1800430129205268706/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html#comment-form&quot; rel=&quot;replies&quot; 标题=&quot;0条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706&quot; rel=&quot;edit&quot; type=&quot;application/atom +xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href= &quot;http://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html&quot; rel=&quot;alternate&quot; title=&quot;AMIE：用于诊断医学推理和对话的研究人工智能系统&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt; /email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded. gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6 W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/ s72-c/AMIE.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-7998118785777164574&lt;/id>;&lt;已发布>;2024-01-11T14:42:00.000-08:00&lt;/已发布>;&lt;更新>;2024-01-11T14:42:51.944-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;大型语言模型&quot;>;&lt;/category >;&lt;类别方案=“http://www.blogger.com/atom/ns#”term=“ML”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#” &quot; term=&quot;自然语言理解&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;大型语言模型能否识别并纠正错误？&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-作者&quot;>;作者：Gladys Tyen，Google 研究实习生&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsksZ_xATb JDnQegxWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U-UqW0hglZ-cXXCkznmufJIfyPAR/s1600/Backtracking.jpg “样式=“显示：无；” />; &lt;p>; LLM 在推理任务中越来越受欢迎，例如&lt;a href=&quot;https://hotpotqa.github.io/&quot;>;多轮 QA&lt;/a>;、&lt;a href=&quot;https:// arxiv.org/abs/2207.01206&quot;>;任务完成&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2304.05128&quot;>;代码生成&lt;/a>;或&lt;a href=&quot;https:// /github.com/openai/grade-school-math&quot;>;数学&lt;/a>;。然而，就像人类一样，他们并不总是在第一次尝试时就正确解决问题，尤其是在他们没有接受过培训的任务上。因此，为了使此类系统最有用，它们应该能够 1) 确定其推理出错的地方，以及 2) 回溯以找到另一个解决方案。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 这导致了与自我纠正相关的方法激增，其中法学硕士用于识别其自身的问题。自己的输出，然后根据反馈产生改进的结果。自我纠正通常被认为是一个单一的过程，但我们决定将其分为两个部分：&lt;em>;错误查找&lt;strong>;&lt;/strong>;&lt;/em>;和&lt;em>;输出纠正&lt;/em>;。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2311.08516#:~:text=While%20self%2D Correction%20has%20shown,et%20al.%2C%202023) .&quot;>;LLM 无法发现推理错误，但可以纠正它们！&lt;/a>;”，我们分别测试最先进的 LLM 的错误发现和输出纠正。我们提出了 &lt;a href=&quot;https://github.com/WHGTyen/BIG-Bench-Mistake&quot;>;BIG-Bench Mistake&lt;/a>;，这是一个用于错误识别的评估基准数据集，我们用它来解决以下问题： &lt;/p>; &lt;ol>; &lt;li>;法学硕士能否发现&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;思想链&lt;/a>; (CoT) 风格推理中的逻辑错误？ &lt;/li>;&lt;li>;发现错误可以用来衡量正确性吗？ &lt;/li>;&lt;li>;知道错误在哪里，是否可以提示法学硕士回溯并得出正确答案？ &lt;/li>;&lt;li>;错误发现作为一种技能可以推广到法学硕士从未见过的任务吗？ &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;关于我们的数据集&lt;/h2>; &lt;p>; 错误查找是自然语言处理中一个尚未充分研究的问题，该领域特别缺乏评估任务。为了最好地评估法学硕士发现错误的能力，评估任务应该显示明确的错误。据我们所知，由于这个原因，当前大多数错误查找数据集并未超出&lt;a href=&quot;https://github.com/openai/grade-school-math&quot;>;数学&lt;/a>;领域。 &lt;/p>; &lt;p>; 为了评估法学硕士推理数学领域之外的错误的能力，我们生成了一个供研究界使用的新数据集，称为&lt;strong>; &lt;/strong>;&lt;a href=&quot;https: //github.com/WHGTyen/BIG-Bench-Mistake&quot;>;大基准错误&lt;/a>;。该数据集由使用 &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; 针对 &lt;a href=&quot;https:// 中的五个任务生成的思想链轨迹组成github.com/suzgunmirac/BIG-Bench-Hard&quot;>;BIG-Bench&lt;/a>;。每个跟踪都标有第一个逻辑错误的位置。 &lt;/p>; &lt;p>; 为了最大限度地增加数据集中的错误数量，我们对答案不正确的 255 个迹线进行了采样（因此我们知道肯定存在错误），对答案正确的迹线进行了 45 个采样（因此可能存在或可能不是一个错误）。然后，我们要求人工贴标员检查每条痕迹并识别第一个错误步骤。每条迹线均由至少三位标注者进行注释，其答案的&lt;a href=&quot;https://en.wikipedia.org/wiki/Inter-rater_reliability&quot;>;评分者间可靠性&lt;/a>;水平>;0.98（使用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Krippendorff%27s_alpha&quot;>;Krippendorff 的 α&lt;/a>;）。除 &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/dyck_languages&quot;>;Dyck Languages 任务&lt;/a>;（涉及预测）之外的所有任务均已完成标记给定输入序列的右括号序列。我们通过算法标记了该任务。 &lt;/p>; &lt;p>; 该数据集中出现的逻辑错误简单且明确，为测试法学硕士在将其用于更困难、更模糊的任务之前发现自己的错误的能力提供了一个良好的基准。&lt;/p>; &lt;div class =&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvk5zKLBvc2Ou6RpJc9l-lLqwHW6nWARuc2IAckSQ2SPYX6-UQj9Z8FyOB5emaBvXPta4MWqR1gis9FMEXeafffprNpy PmF_XaBOQ7tQpRpEylbnSlbwytNv1BFXlz5I-ulNM0ZBC7kBhx2KkdCT5MIejwdsHKpHu6rrJ4LBVd-Na_XUn5DCy0EKtj1Uy6/ s1354/BBMistakes2.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;410&quot; data-original-width=&quot;1354&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvk5zKLBvc2Ou6RpJc9l-lLqwHW6nWARuc2IAckSQ2SPYX6-UQj9Z8FyOB5emaBvXPta4MWqR1gis9FMEXeafffprNpyPmF_XaBOQ7tQpRpEylbnSlbwyt Nv1BFXlz5I-ulNM0ZBC7kBhx2KkdCT5MIejwdsHKpHu6rrJ4LBVd-Na_XUn5DCy0EKtj1Uy6/s16000/BBMistakes2.png&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;错误识别核心问题&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;1.法学硕士能否发现思想链式推理中的逻辑错误？&lt;/h3>; &lt;p>; 首先，我们想知道法学硕士是否能够独立于其纠正错误的能力来识别错误。我们尝试多种提示方式来测试&lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_pre-trained_transformer&quot;>;GPT&lt;/a>;系列模型定位错误的能力（提示&lt;a href=&quot;https ://github.com/WHGTyen/BIG-Bench-Mistake/tree/main/mistake_finding_prompts&quot;>;此处&lt;/a>;）假设它们通常代表现代法学硕士的表现。 &lt;/p>; &lt;p>; 一般来说，我们发现这些最先进的模型表现不佳，最好的模型总体准确率达到 52.9%。因此，有必要提高法学硕士在这方面的推理能力。 &lt;/p>; &lt;p>; 在我们的实验中，我们尝试了三种不同的提示方法：直接（trace）、直接（step）和CoT（step）。在直接（跟踪）中，我们向法学硕士提供跟踪并询问错误的位置步骤或&lt;em>;没有错误&lt;/em>;。在直接（步骤）中，我们提示法学硕士针对其采取的每一步问自己这个问题。在CoT（步骤）中，我们提示LLM给出每个步骤是否错误的推理。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdt imB-IYhhROTmtTKoxmmWF0jzI1IKfU3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s1061/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;1061&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdtimB-IYhhROTmtTKoxmmWF0jzI1IKfU 3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;直接（trace）、直接（step）和CoT（step）三种提示方式的示意图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们的发现是一致的，并且建立在&lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;之前的结果&lt;/a>;的基础上，但进一步表明，法学硕士即使是简单且明确的错误也难以避免（作为比较） ，我们的人类评估者在没有事先专业知识的情况下以高度一致的方式解决了问题）。我们假设这是法学硕士无法自我纠正推理错误的一个重要原因。有关完整结果，请参阅&lt;a href=&quot;https://arxiv.org/abs/2311.08516&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;2.发现错误可以用来衡量答案的正确性吗？&lt;/h3>; &lt;p>;当人们遇到我们不确定答案的问题时，我们可以逐步解决我们的解决方案。如果没有发现错误，我们可以假设我们做了正确的事情。 &lt;/p>; &lt;p>; 虽然我们假设这对于法学硕士也有类似的作用，但我们发现这是一个糟糕的策略。在我们包含 85% 错误轨迹和 15% 正确轨迹的数据集中，使用此方法并不比始终将轨迹标记为不正确的幼稚策略好多少，后者给出加权平均值 &lt;a href=&quot;https://en.wikipedia. org/wiki/F-score&quot;>;F1&lt;/a>; 为 78。&lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzabllySdTx5vpHv_5T0NwKDB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBf qVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s698/Self- Correcting-LLMs-Tasks.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-原始高度=“427”数据原始宽度=“698”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzablySdTx5vpHv_5T0NwK DB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBfqVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s16000/自校正-LLM -Tasks.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;显示错误查找效果的图表法学硕士可以用作每个数据集答案正确性的代理。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h3>;3. LLM 可以回溯知道错误在哪里吗？&lt;/h3>; &lt;p>; 由于我们已经证明 LLM 在查找 CoT 跟踪中的推理错误方面表现不佳，我们想知道 LLM 是否能够纠正错误&lt;em>;&lt; /em>;，即使他们知道错误在哪里。 &lt;/p>; &lt;p>; 请注意，了解&lt;em>;错误位置&lt;/em>;与了解&lt;em>;正确答案&lt;/em>;不同：即使最终答案是正确的，CoT 跟踪也可能包含逻辑错误，或者反之亦然。在大多数现实情况下，我们不知道正确的答案是什么，但我们也许能够识别中间步骤中的逻辑错误。 &lt;/p>; &lt;p>; 我们提出以下回溯方法：&lt;/p>; &lt;ol>; &lt;li>;在温度 = 0 时照常生成 CoT 迹线。（温度是控制生成响应的随机性的参数，温度越高，值产生更多样化和创造性的输出，通常以牺牲质量为代价。）&lt;/li>;&lt;li>;识别第一个逻辑错误的位置（例如使用分类器，或者在这里我们只使用数据集中的标签）。 &lt;/li>;&lt;li>;在温度 = 1 时重新生成错误步骤并产生一组八个输出。由于已知原始输出会导致不正确的结果，因此目标是在此步骤中找到与原始输出显着不同的替代生成。 &lt;/li>;&lt;li>;从这八个输出中，选择一个与原始错误步骤不同的输出。 （我们在这里只是使用精确匹配，但将来这可能会更加复杂。）&lt;/li>;&lt;li>;使用新步骤，在温度 = 0 时正常生成迹线的其余部分。&lt;/li>; &lt; /ol>; &lt;p>; 这是一种非常简单的方法，不需要任何额外的提示制作，并且避免了重新生成整个跟踪。我们使用 BIG-Bench Mistake 的错误位置数据对其进行测试，发现它可以纠正 CoT 错误。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;最近的工作&lt;/a>;表明自我纠正方法，例如&lt;a href=&quot;https://arxiv. org/abs/2303.11366&quot;>;反射&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2303.17491&quot;>;RCI&lt;/a>;，导致准确性分数下降，因为有更多的正确答案变得不正确反之亦然。另一方面，我们的方法产生的收益（通过纠正错误答案）多于损失（通过将正确答案更改为错误答案）。 &lt;/p>; &lt;p>; 我们还将我们的方法与随机基线进行比较，其中我们随机假设一个步骤是错误的。我们的结果表明，这个随机基线确实产生了一些收益，但不如使用正确的错误位置回溯那么多，并且损失更多。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4CQ3amwJgMJ7OZToizp04vYIOj7F4kTdVO5DgthHi_HQQNa2FrkKjBA7LB249yN44kXFs0lZVuX1W4n3GLQI51Fy 95ls-gK_rMLQQETYNmvqI7OS7U6xHgXx2cUnhTcwmZrpFWrS1vd01G14gWOexxZDTcpOIbGTHfXRgLWj22OteqMh_iTK2Rg_fC7xt/s744/image4.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;744&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi4CQ3amwJgMJ7OZToizp04vYIOj7F4kTdVO5DgthHi_HQQNa2FrkKjBA7LB249yN44kXFs0lZVuX1W4n3GLQI51Fy95ls-gK_rMLQQETYNmvqI7OS7U6xHgXx 2cUnhTcwmZrpFWrS1vd01G14gWOexxZDTcpOIbGTHfXRgLWj22OteqMh_iTK2Rg_fC7xt/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;该图表显示了我们的方法的准确性的增益和损失以及每个数据集的随机基线。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-高度：40%；&quot;>; &lt;br />; &lt;/div>; &lt;h3>;4.错误发现可以推广到法学硕士从未见过的任务吗？&lt;/h3>; &lt;p>;为了回答这个问题，我们在四个 BIG-Bench 任务上微调了一个小模型，并在第五个保留任务上对其进行了测试。我们对每项任务都这样做，总共生成了五个经过微调的模型。然后我们将结果与零样本提示进行比较 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-L-Unicorn &lt;/a>;，一个更大的模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQ dQFW7YAXTGH6MaXwZs9SyrkE4Q4h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s759/image5.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;281&quot; data-original-width=&quot;759&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQdQFW7YAXTGH6MaXwZs9SyrkE4Q4 h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;条形图显示了与 PaLM 2-L-Unicorn 的零样本提示相比，经过微调的小模型的准确性改进。&lt; /span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们的结果表明，较小的微调奖励模型通常比零样本提示大模型表现更好，即使奖励模型模型从未见过测试集中任务的数据。唯一的例外是逻辑演绎，它的表现与零样本提示相当。 &lt;/p>; &lt;p>; 这是一个非常有希望的结果，因为我们可以使用一个小的微调奖励模型来执行回溯并提高任何任务的准确性，即使我们没有相关数据。这个较小的奖励模型完全独立于生成器 LLM，并且可以针对个别用例进行更新和进一步微调。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1G kvhjouJbJBE9mVl1yUCJEZAVY8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s867/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;867&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1GkvhjouJbJBE9mVl1yUCJEZAVY 8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;展示我们的回溯方法如何工作的插图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; 在这项工作中，我们创建了一个评估基准数据集，更广泛的学术界可以使用它来评估未来的法学硕士。我们进一步表明，法学硕士目前很难发现逻辑错误。然而，如果可以的话，我们将展示回溯作为一种可以为任务带来收益的策略的有效性。最后，可以在一般错误查找任务上训练较小的奖励模型，并用于改进域外错误查找，这表明错误查找可以泛化。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢 Peter Chen、Tony Mak、Hassan Mansoor 和 Victor Cărbune 提供的想法以及帮助进行实验和数据收集。我们还要感谢 Sian Gooding 和 Vicky Zayats 对本文的评论和建议。&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google /feeds/7998118785777164574/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/ can-large-language-models-identify-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger .com/feeds/8474926331452026626/posts/default/7998118785777164574&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/默认/7998118785777164574&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/can-large-language-models-identify-and .html&quot; rel=&quot;alternate&quot; title=&quot;大型语言模型可以识别并纠正错误吗？&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt; /email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded. gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsks Z_xATbJDnQegxWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U- UqW0hglZ-cXXCkznmufJIfyPAR/s72-c/Backtracking.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/ thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-7197275876457161088&lt;/id>;&lt;发布>;2024-01-08T14:07:00.000-08:00&lt; /published>;&lt;更新>;2024-01-08T14:07:07.614-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>; &lt;/category>;&lt;title type=&quot;text&quot;>;Google Research 的负责任的 AI：用户体验团队&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：用户体验主管 Ayça Çakmakli , Google 研究部，负责任的人工智能和以人为本的技术团队&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-x yVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked-9QDeDkSOCZMBaIYrL/s16000/英雄。 jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google 的负责任的 AI 用户体验（Responsible AI UX）团队是一个嵌入 Google Research 的以产品为导向的团队。这种独特的定位要求我们将负责任的人工智能开发实践应用于以用户为中心的用户体验 (UX) 设计流程。在这篇文章中，我们描述了用户体验设计和负责任的人工智能在产品开发中的重要性，并分享了一些示例，说明我们团队的能力和跨职能协作如何在整个 Google 中实现负责任的开发。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 首先是用户体验部分。我们是一个由产品设计专家组成的多学科团队：设计师、工程师、研究人员和策略师，他们管理以用户为中心的用户体验设计流程，从早期构思和问题框架到后期用户界面 (UI) 设计、原型设计和细化。我们相信，当未满足的重要用户需求与产品的主要价值主张之间存在明确的一致性时，就会出现有效的产品开发，并且这种一致性可以通过彻底的以用户为中心的用户体验设计流程可靠地实现。 &lt;/p>; &lt;p>; 其次，认识到生成式人工智能 (GenAI) 对社会产生重大影响的潜力，我们接受我们作为主要用户倡导者的角色，同时我们继续发展我们的用户体验设计流程，以应对人工智能带来的独特挑战，最大限度地发挥效益并最大限度地降低风险。当我们经历人工智能驱动的产品设计过程的每个阶段时，我们高度重视我们的决策的道德、社会和长期影响。我们致力于持续开发全面的&lt;a href=&quot;https://ai.google/responsibility/ai-governance-operations&quot;>;安全性和包容性协议&lt;/a>;，围绕内容管理等关键问题定义设计和部署护栏、安全性、隐私、模型功能、模型访问、公平性和公平性，有助于减轻 GenAI 风险。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_ GumzOt491oPcnWsbI74tkmBNh5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s1920/image4。 png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_GumzOt491oPcnWsbI74tkmBN h5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;Responsible AI UX 不断发展其以用户为中心的产品设计流程，以满足 GenAI 驱动的产品格局的需求，对用户和社会的需求更加敏感，强调道德、社会和长期影响。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 产品设计中的责任也体现在我们选择解决和解决的用户和社会问题上。我们提供资源的计划。因此，我们鼓励&lt;a href=&quot;https://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot;>;优先考虑规模和严重程度较高的用户问题&lt;/a>;帮助最大限度地发挥 GenAI 技术的积极影响。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK 8hws3E5sgNhj9cR7bY78gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s1920 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK8hws3E5sgNhj9cR7bY7 8gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;p>; 跨团队和学科的沟通对于负责任的产品设计至关重要。从用户研究团队到产品设计和工程团队（反之亦然）的信息和见解的无缝流动对于良好的产品开发至关重要。我们团队的核心目标之一是通过弥合我们工程师的广泛技术专业知识与我们学术、研究的用户/社会专业知识之间的沟通差距，确保将深入的用户洞察力实际应用到 Google 的人工智能产品设计决策中。科学家和以用户为中心的设计研究专家。我们建立了一支在这些领域拥有专业知识的多学科团队，加深了我们对受众沟通需求的同理心，并使我们能够更好地在用户和用户之间建立联系。社会专家和我们的技术专家。我们创建框架、指南、原型、备忘单和多媒体工具，帮助在正确的时间为正确的人带来生活中的见解。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class =&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0wvd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5 nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s1920/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border= “0”数据原始高度=“1080”数据原始宽度=“1920”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0w vd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s16000/ image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;促进负责任的 GenAI 原型设计和开发 &lt;/h2>; &lt;p>; 期间Responsible AI UX、&lt;a href=&quot;https://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot;>;人员 + AI 研究&lt;/a>;之间的合作（ PAIR）倡议和&lt;a href=&quot;https://labs.google/&quot;>;实验室&lt;/a>;，我们发现&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564 &quot;>;原型设计&lt;/a>;可以提供参与大型语言模型 (LLM) 的创造性机会，并且通常是 GenAI 产品开发的第一步。为了满足将法学硕士引入原型制作过程的需求，我们探索了一系列不同的提示设计。然后，我们深入该领域，采用各种外部的第一人称用户体验设计研究方法来获取洞察力并获得对用户观点的同理心。通过用户/设计师共同创建会议、迭代和原型设计，我们能够让内部利益相关者、产品经理、工程师、作家、销售和营销团队一起参与，以确保用户的观点得到充分理解并加强一致性跨团队。 &lt;/p>; &lt;p>; 这项工作的成果是 &lt;a href=&quot;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>; ，一个在 &lt;a href=&quot;https://blog.research.google/2023/05/google-research-at-io-2023.html&quot;>;Google I/O 2023&lt;/a>; 上推出的生成式 AI 平台，可实现人们，即使是那些没有任何机器学习经验的人，也可以使用法学硕士创造性地进行原型设计。该团队对用户的第一手经验以及对他们所面临挑战的了解使我们能够将&lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI 原则&lt;/a>;融入到 MakerSuite 产品设计中。例如，&lt;a href=&quot;https://developers.generativeai.google/guide/safety_setting&quot;>;安全过滤器&lt;/a>;等产品功能使用户能够管理结果，从而利用 MakerSuite 实现更轻松、更负责任的产品开发。 &lt;/p>; &lt;p>; 由于我们与产品团队密切合作，我们能够调整纯文本原型以支持与 &lt;a href=&quot;https://makersuite.google.com/app/prompts/new_freeform 的多模式交互&quot;>;Google AI Studio&lt;/a>;，MakerSuite 的演变。现在，Google AI Studio 使开发者和非开发者能够无缝利用 Google 最新的 &lt;a href=&quot;https://ai.google.dev/&quot;>;Gemini&lt;/a>; 模型来合并多种模态输入，例如文本和图像，在产品探索中。以这种方式促进产品开发为我们提供了更好地利用AI来识别&lt;a href=&quot;https://arxiv.org/pdf/2310.15428.pdf&quot;>; &lt;a href=&quot;https://arxiv.org.pdf&quot;>; &lt;a href=&quot;https://arxiv.org/arxiv.org/pdf&quot;>;非发育者使用AI沙盒。与我们的合作伙伴一起，我们继续积极地在我们支持的产品中努力。&lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式= “边距 - 左：自动;边缘右：自动;”>; &lt;tbody>; &lt;Tr>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https：//blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQg-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4AblFB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data -original-height=&quot;1406&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQg-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4AblFB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/ s16000/image6.png“/>; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;a href =“ https：https：https：https： //makersuite.google.com/app/prompts/new_freeform&quot;>; google ai studio &lt;/a>;使开发人员和非发展者能够利用Google Cloud Infrstructure，并在其产品探索中合并多种模态输入。&lt;/td>; &lt;/td>; &lt;/tr tr>; &lt;/tr >; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;公平语音识别&lt;/h2>; &lt;p>;多重&lt; a href =“ https://www.pnas.org/doi/10.1073/pnas.1915768117”>;外部研究/10.3389/frai.2021.725911/full&quot;>; own Research，&lt;/a>;已经确定了当前语音识别技术平均而言，相对于白人说话者，当前语音识别技术平均理解黑人说话者的能力不幸。随着多模式AI工具开始更加依赖语音提示，此问题将会越来越大，并继续疏远用户。为了解决这个问题，负责任的AI UX团队是&lt;a href=&quot;https://blog..google/technology/research/project-elevate-elevate-elevate-black-voices-voices-google-research/&quot;>;在霍华德大学&lt;/a>;，一个突出的&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/historally_black_colleges_ccolleges_and_universities&quot; target=&quot;_blank&quot;>; hbcu &lt;/a>;数据集改进我们的语音技术产品的设计，以使其更容易访问。这项工作称为“提升黑色声音”，这将使霍华德大学能够与希望改善语音技术的人共享数据集，同时建立负责任的数据收集框架，从而确保数据对黑人社区有益。霍华德大学将保留数据集的所有权和许可，并作为负责使用的管家。在Google，我们正在提供资金支持，并与霍华德大学的合作伙伴紧密合作，以确保该计划的成功。 &lt; /p>; &lt;br />; &lt;div class =“ saparator” style =“ clear：clear; text-align：center;”>; &lt;iframe lassefullscreen =“” class =“ blog_video_video_class” frameborder =“ 0” height =“ 360 “ src =” https://www.youtube.com/embed/t_pdlru8qhs?si = 5xy1aogc_d2htzqf“ width =” 640“ youtube-src-id =” 5xy1aogc_d2htzqf of div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;公平的计算机视觉&lt;/h2>; &lt;p>; &lt;a href=&quot;http://gendershades.org/&quot;>;性别阴影&lt;/a>;项目强调，计算机视觉系统难以检测肤色深色的人，并且对于肤色深色的女性而言，表现较差。这很大程度上是由于用于训练这些模型的数据集并不包含各种肤色。为了解决这一限制，负责的AI UX团队一直与社会学家&lt;a href=&quot;https://www.ellismonk.com/&quot;>; Dr. Ellis Monk &lt;/a>;释放&lt;a href=&quot;https://blog.google/products/search/monk-skin--tone-scale/&quot;>;僧侣肤色秤&lt;/a>;（MST），皮肤音调量表旨在更加包含世界各地的肤色范围。它提供了一种工具来评估包含在包含在内的肤色的数据集和模型性能的包含性，从而产生了对每个人都更有效的功能和产品。 &lt;/p>; &lt;p>;我们已将MST集成到一系列&lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>; Google Products &lt;/a>;作为搜索，Google照片等。我们还开放了MST，&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3632120&quot;>;发布了我们的研究&lt;/a>;，&lt;a href =“ 。共享一个示例数据集&lt;/a>;鼓励其他人轻松地将其集成到他们的产品中。负责的AI UX团队继续与Monk博士合作，利用MST在多个产品应用程序中，并继续进行国际研究，以确保其在全球范围内具有包容性。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;咨询＆amp;指导&lt;/h2>; &lt;p>;随着Google各地的团队继续开发利用Genai模型功能的产品，我们的团队认识到他们面临的挑战是各种各样的，并且市场竞争是巨大的。为了支持团队，我们开发了可行的资产，以促进考虑可用资源的更简化和负责任的产品设计过程。我们充当以产品为重点的设计咨询公司，确定扩展服务，共享专业知识并应用我们的设计原则的方法。我们的目标是通过出色的负责任的产品设计帮助Google的所有产品团队将重要的未满足用户需求与技术收益联系起来。 &lt;/p>; &lt;p>;我们一直这样做的一种方式是创建&lt;a href=&quot;https://pair.withgoogle.com/guidebook/guidebook/&quot;>; People + AI指南&lt;/a>;我们学到的许多负责任的设计课程的总结资源以及我们为内部和外部利益相关者提出的建议。凭借即将出版的滚动&lt;a href=&quot;https://medium.com/people-ai-research/updating-the-people-ai-guidebook-in-the-age-oge-fenerative-generative-generative-ai-ai-cace6c846db4&quot;>;更新&lt;/a>;专门关注如何最好地设计和考虑Genai的用户需求，我们希望我们的内部团队，外部利益相关者和较大的社区将在产品开发旅程中最关键的里程碑中获得有用的可行指导。 &lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;>; &lt;tbody>;>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOMslkRaMAkYwAUCOHS5tWTuCBQJ7sPNjkDbopWKKl21XD_1Q_VrK3tCctspa72hY63uOMZKipV5flHTW69S5bVjCVBvE8oMKmEax3VWNj7Wx20UlYRPZABdJhq0DJlegrtSIbQBxtkf18ygJtSxk2kY5f8L82WKEw9vLmiRDgrZiIXtsJ5RtbgvmISRw4/s1100/image1.png&quot; style=&quot;margin-左：auto; Margin-right：auto;“>; &lt;img border =“ 0” data-eriginal-height =“ 622” data-eriginal-width =“ 1100” src =“ https://blogger.googger.googleusercercontent.com/ IMG/B/R29VZ2XL/AVVXSEJOMSLKRAMAKYWAUCOHS5TWTUCBQJ7SPNJKDBOPWKKKL21XD_1Q_VRK3TCCTSPSPASPASPASPA72 sibqbxtkf18ygjtsxk2ky5f8l82wkew9vlmirdgrziixtsj5rtbgvmisrw4/s16000/s16000/image1.png“/>; &lt;/a>; &lt;/a>; &lt;/>; &lt;/>; &lt;/>; &lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;tr>; &lt;tr>; >; People + AI指南有六章，旨在涵盖产品生命周期的不同方面。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;如果您有兴趣阅读有关负责AI的更多信息UX以及我们如何专门考虑使用生成AI负责任地设计，请查看此&lt;a href =“ https://medium.com/people-ai-research/meet-aie--ay%C3%A7A-%A7A-%C3%A7AKMAKMAKMAKLI- Google-new-heaf-ai-ai-ux-ux-ux-d8f2700df95b“>; q＆amp; a ipea &lt;/a>;。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;向我们负责的AI UX团队成员大喊大叫: Aaron Donsbach, Alejandra Molina, Courtney Heldreth, Diana Akrong, Ellis Monk, Femi Olanubi, Hope Neveux, Kafayat Abdul, Key Lee, Mahima Pushkarna, Sally Limb, Sarah Post, Sures Kumar Thoddu Srinivasan, Tesh Goyal, Ursula Lauriston, and Zion Mengesha。特别感谢米歇尔·科恩（Michelle Cohn）对这项工作的贡献。 &lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/7197275876457161088/comments/comments/default/default” /atom+xml“/>; &lt;link href =” http://blog.research.google/2024/01/responsible-ai-ai-ai-at-google-research-usearch-user.html#comment-form“ =“ 0注释” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/feeds/847492633145202626/posts/posts/posts/default/7197272727272727272727272727645716161088 +xml“/>; &lt;link href =” http://www.blogger.com/feeds/847492631452026626/posts/default/719727587645716161088 “ http://blog.research.google/2024/01/responsible-ai-ai-at-google-research-user.html” rel =“替代” title =“ Google Research in Google Research：用户体验团队” text/html“/>; &lt;aunder>; &lt;名称>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/1209862651477775266161 &lt;/uri>; &lt;Email>; &lt;Email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =” https://img1.blog1.blogblog.com/img/b16-round.gif =&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-xyVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked -9qdedksoczmbaiyrl/s72-c/hero.jpg“ width =” 72“ xmlns：媒体=” http：//search.yahoo.com/mrss/>; &lt;/media：thumbnail>; &lt;/thrumbnail>; &lt;thr>; &lt;thr>; &lt;thr>; &lt;thr：thr>; 0 &lt;/thr>; 0 &lt;/thr ：Total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：Blogger.com，1999：Blog-8474926331452026626.POST-8816183473385638131 &lt;/id>;已发布>; &lt;更新>; 2024-01-11T16：01：33.313-08：00 &lt;/updated>; &lt;title type =“ text”>; 2023：AI和Computing>; compution>; &lt;/stitle>; &lt;content>; &lt;content>; &lt;content>; &lt;content>; &lt;content>; &lt;content Type =“ html“>; &lt;span class =” Byline-author”>;由Google DeepMind＆amp＆amp; Jeff Dean发布Google Research，Demis Hassabis，首席执行官，Google DeepMind和James Monyika，SVP，Google Research，Technology＆amp;社会&lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxseiu12g_p2dzo4gq _p2dzo4gq-p95ap2kp95ap2krhrg5oik9vzj4wtt_4wtgznggzngngnggtzgngtzgnekgngtzgnegtzgnekgttegttzqguttzgtugttzqguttzqguttzqguttzqguttzqgtutzqgtutzqgtutzqqugtusttt Ofyhwnu-1S4SSBR2JQ5T36TQRBZUCTAP7GMXKGW77XN6S39IKXPAXBO5TW_CQ52ZFPWOCBKKWL-2XTUZSIH6VIRIQGGGGGGCGGCGGCGGCGGCGGCGGCGGCGGCGGCGGCDUDNQ-PHJZL/s1100 sight year play play style play plays plays plays plays play = ： 没有任何;” />; &lt;p>;这是人工智能（AI）研究及其实用应用领域中令人难以置信的一年。 &lt;/p>; &lt;p>;随着正在进行的研究将AI推得更远，我们回顾我们的&lt;a href =“ https://ai.google/static/documents/google-why-why-we-we-we-focus-on-ai.pdf “>;透视&lt;/a>;今年1月出版，标题为“为什么要专注于AI（以及到底）”，我们指出：&lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;/a>; &lt;/a>; &lt; Div style =“保证金左：40px;”>; &lt;p>;我们致力于领导和制定标准开发和运输有用和有益的应用程序，应用以人类价值为基础的道德原则，并在我们从研究中学习时发展我们的方法，经验，用户和更广泛的社区。 &lt;/p>; &lt;p>;我们还认为，使AI正确 - 对我们而言，这涉及创新和为人们和社会带来广泛访问的利益，同时减轻风险 - 必须是我们和其他人的集体努力，包括研究人员，开发人员，开发人员，开发人员，开发人员，开发人员，用户（个人，企业和其他组织），政府，监管机构和公民。 &lt;/p>; &lt;p>;我们坚信，我们专注于开发和交付的AI支持创新是有用的，令人信服的，并且有潜力帮助和改善各地的人们的生活 - 这就是迫使我们的原因。 &lt;/p>; &lt;/div>; &lt;p>;在本年度审查帖子中，我们将介绍一些Google Research的研究和Google DeepMind的努力，将这些段落置于2023年。&lt;/p>; &lt;br />; &lt;/p>; &lt;br />; &lt; H2>;产品＆amp; Technologies &lt;/h2>; &lt;p>;这是一年生成的AI引起了世界的关注，创造了图像，音乐，故事和引人入胜的对话，以了解所有可想象的一切，以创造力和速度几乎令人难以置信。 &lt;/p>; &lt;p>;在2月，我们&lt;a href=&quot;https://blog.google/technology/technology/ai/bard-google-google-ai-search-updates/&quot;>;首次启动&lt;/a>; &lt;/a>; &lt;a href = “ https://bard.google.com”>; bard &lt;/a>;，您可以用来探索创意并简单地解释事物的工具。它可以生成文本，翻译语言，编写不同种类的创意内容等等。 &lt;/p>; &lt;p>; 5月，我们观看了在阶段&lt;a href =“ https://blog.research.google/2023/05/google-research-- at-io-2023.html“>;在Google I/O &lt;/a>;上。主要是，这包括&lt;a href=&quot;https://ai.google/discover/palm2/&quot;>; Palm 2 &lt;/a>;，一种大型语言模型（LLM），它将Compute-Optimal缩放缩放在一起，改进的数据集混合物，一种改进的数据集混合物，和模型体系结构可在高级推理任务中表现出色。 &lt;/p>; &lt;div class =“ siperator” style =“ clear：clear; text-align：center;”>; &lt;a href =“ https：//blogger.googger.googleusercontent.com/img/r29vz2xl/r29vz2xl/avvz2xl/avvxssegxsegxsege5r7e2f0bqisilzm25kbvm42 8EQC22BOG6SV_H8NBC-PKD2PPV7FHC -UBR43ZWPBRGVAGJ769J-UUCTPBFBO9BF-U81GKFU1OP4OGZHS6KKKKKKKKBB2ZNNSVCELERREN5KWKH2NPPJXFJXFJXFJDSVJDSVZZUIPZUIPZUIPZUIPZUIPZUIPZUIPZUIPZUIPPHEN3B_JCQ4/s1920/lockupp._clrmlock _clmnp._clmmplmmp g“ style =”边距 - 左：1EM;边缘右：1EM;“>; &lt;img border =“ 0” data-froliginal-height =“ 555” data-original-width=&quot;1920&quot; height=&quot;116&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5R7E2f0BQIsiLZm25kxfR_Ix_Bvm6HlQQCXI12sB42siKUAf8eZvVEDU5bi8EQc22BoG6SV_H8NbC-PKd2pPv7FhC-uBR43ZWpbrgvaGJ7699j-uUctPbFBO9Bf-u81gkfU1OP4oGZhs6KKkub2znNsvcElREn5kwKh2npPJxFJdSVZZUIPZhyphenhyphen3B_jCq4/w400-h116/lockup_ic_PaLM-2_H_4297x745px_clr_ @1x.jpg“ width =” 400“/>; &lt;/a>; &lt;/div>; &lt;p>;通过微调和指令敲打棕榈2用于不同的目的，我们能够将其集成到许多Google产品和功能中，包括：&lt;/p>; &lt;ul>; &lt;li>;对吟游诗人的更新，该更新启用了多语言功能。自首次推出以来，Bard现在可提供超过&lt;a href=&quot;https://support.google.com/bard/answer/13575153?hl = en&quot;>; 40种语言和230多种国家和领土&lt;/a>; ，and &lt;a href=&quot;https://blog.google/products/bard/google-bard-new-features-update-update-sept-2023/&quot;>;带有扩展名&lt;/a>;，Bard可以从每天都使用的Google工具 - 例如Gmail，Google Maps，YouTube等。 &lt;/li>; &lt;li>; &lt;a href=&quot;https://blog.google/products/search/generative-ai-search/&quot;>;搜索生成的体验&lt;/a>;（sge），使用LLMS来重新想象组织信息以及如何帮助人们浏览信息，为我们的核心搜索产品创建更流畅的对话互动模型。这项工作将搜索引擎的体验从主要关注信息检索扩展到了更多的东西（能够检索，合成，创造性生成和以前搜索的延续），同时继续充当用户与所寻求的Web内容之间的连接点。 &lt;/li>; &lt;li>; &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/musiclm/examples/&quot;>; musiclm &lt;/a>;，一种由&lt;a href提供的文本tok-music模型=“ https://ai.googleblog.com/2022/10/audiolm-language-modeling-apphack-to..html”>; audiolm &lt;/a>;和&lt;a href =“ https://arxiv.org/abs/ 2208.12415“>; Mulan &lt;/a>;，可以从文字，嗡嗡声，图像或视频或音乐伴奏到唱歌中制作音乐。 &lt;/li>; &lt;li>; Duet AI，我们的AI驱动的合作者，在使用Google Workspace和Google Cloud时为用户提供帮助。 &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai&quot;>; Google Workspace中的二重奏AI &lt;/a>;并总结电子邮件和聊天消息，并总结会议。 &lt;a href=&quot;https://cloud.google.com/blog/products/application-modernization/introducing-duet-duet-duet-for-google-cloud-cloud&quot;>; Google Cloud中的Duet AI &lt;/a>; ，扩展和监控应用程序，并识别和加速对网络安全威胁的分辨率。 &lt;/li>; &lt;li>;和许多&lt;a href=&quot;https://blog.google/technology/developers/google-io-2023-100-anncements/&quot;>;其他发展&lt;/a>;。 &lt;/li>; &lt;/ul>; &lt;p>;在去年发布我们的文本形象生成模型之后，6月&lt;a href=&quot;https://imagen.research.google/&quot;>; imagen &lt;/a>;，我们发布了&lt;a href=&quot;https://blog.research.google/2023/06/imagen-editor-and-editor-and-editbench-advancench-advancing.html&quot;>; Imagen Editor &lt;/a>;，该编辑器提供了使用区域面具和面具和区域面具的能力自然语言提示交互编辑生成图像，以提供对模型输出的更精确的控制。 &lt;/p>; &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfNnxCCKKLZgjajw30Ml1CiPruUIScAPwpa77LQ8Auqkf_KJh9rlJWhYRnQf1g4L0qhVDUJNHabkpL_ZW60FJs8XUWV1kT5M32YU-6oYBC5383noYqno-cYzUboAAOgXvDlWtqwu-zl94M2r02Fsid0jjgLBJl3JCVR4lc8ZVw7-c7q9OlHjzmrRXz5i5H/s1261/ image4.png&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;306&quot; data-original-width=&quot;1261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfNnxCCKKLZgjajw30Ml1CiPruUIScAPwpa77LQ8Auqkf_KJh9rlJWhYRnQf1g4L0qhVDUJNHabkpL_ZW60FJs8XUWV1kT5M32YU -6oybc5383noyqno-cyzuboxvdlwtqwu-Zl94M2R02FSID0JJGLBJLBJLBJLBJLBJCVR4LC8ZVW7-C7-C7C7Q9OLHJZMRRXZMRRXZMRRRXZ5I5I5I5H/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>;基于人类的专业图像美学模型对良好照明，框架，曝光和清晰度等品质的偏好。 &lt;/p>; &lt;p>; 10月，我们启动了一个功能，该功能&lt;a href =“ https://blog.research.google/2023/10/google-search-can-can-now-now-now-help-with-with-english-speaking--练习.html“>;帮助人们练习说话和提高语言技能&lt;/a>;。启用该功能的关键技术是与Google翻译团队合作开发的一种新颖的深度学习模型，称为Deep Aligner。与基于基于&lt;a href =“ https://aclanthology.org/aclanthology.org/aclanthology.org/aclanthology.org/c96-”的对齐方式相比，这个单一的新模型导致所有测试语言对的对齐质量的急剧提高，将平均对准错误率从25％降低到5％。 2141/“>;隐藏的马尔可夫模型&lt;/a>;（HMMS）。 &lt;/p>; &lt;p>; 11月，与&lt;a href=&quot;https://blog.youtube/inside-youtube/ai-and-music-experiment/&quot;>; youtube &lt;/a>;，我们宣布&lt;a href =“ https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/”>; lylia &lt;/a>;，我们迄今为止最先进的AI音乐生成模型。我们发布了两个实验，旨在为创造力，DreamTrack和Music AI工具打开一个新的操场，并与&lt;a href =“ https://blog.youtube/inside-youtube/inside-youtube/partnering-with-the-music-industry-instry-on” -ai/“>; YouTube与音乐行业合作的AI Technology &lt;/a>;的原则。 &lt;/p>; &lt;p>;然后在12月，我们启动了&lt;a href=&quot;https://blog.google/technology/technology/google-google-gemini-ai/&quot;>; gemini &lt;/a>;，我们最有能力，最有能力，最一般的AI模型。双子座的建造是从文本，音频，图像和视频的地面上构成的。我们最初的双子座模型家族有三种不同尺寸的Nano，Pro和Ultra。纳米模型是我们最小，最有效的模型，用于为像素等产品中的设备体验供电。 Pro模型具有高度的能力，最适合在各种任务中扩展。 Ultra模型是我们用于高度复杂任务的最大，功能最强大的模型。 &lt;/p>; &lt;br />; &lt;div class =“ saparator” style =“ clear：clear; text-align：center;“>; &lt;a href =” https：//www.youtube.com/watch?v=jv1vkhv4zq8 “>; &lt;iframelo面路allyfullscreen =” class =“ blog_video_class” height =“ 360” src =“ https://www.youtube.com/embed/embed/jv1v1vkhv4zq8” width width width =“ 640 &lt;/iframe>; &lt;/a>; &lt;/div>; &lt;br />; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/gemini/gemini/gemini_1_report.pdf&quot;>;技术报告&lt; /a>;关于&lt;a href=&quot;https://deepmind.google/technologies/gemini&quot;>; gemini型号&lt;/a>;，我们表明Gemini Ultra的性能超过了3232的30次最新最新的结果。 LLM研发中使用的广泛使用的学术基准。 Gemini Ultra的得分为90.04％，是第一个在&lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;>; mmlu &lt;/a>;上超过人类专家的模型，并获得了一个州立大学的现实。 - 新的&lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;>; mmmu &lt;/a>;基准的ART分数为59.4％。 &lt;/p>; &lt;p>;在&lt;a href=&quot;https://deepmind.google/discover/discover/blog/competitival-programming-with-alphacode/&quot;>; alphaCode上建造&lt;/a>;我们&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/alphacode2/alphacode2/alphacode2_tech_report.pdf&quot;>;引入了Alphacode 2 &lt;/a>;由专用版本的Geminii版本启用， 。当在与原始字母的同一平台上进行评估时，我们发现字母2求解了1.7倍的问题，并且同时进行了比85％的竞争参与者&lt;/p>; &lt;p>;，&lt;a href =&#39;https：https：https：https： //blog.google/products/bard/google-bard-try-gemini-ai/&quot;>; bard使用Gemini Pro模型进行了最大的升级&lt;/a>;总结，推理，编码和计划。在八个基准测试中，Gemini Pro的六个基准优于GPT-3.5，包括MMLU，这是测量大型AI模型的关键标准之一，&lt;a href=&quot;https://huggingface.co/datasetsetset.co/datasetsets/gsm8k&quot;>; gsm8k &lt;>; gsm8k &lt;>; gsm8k &lt; /a>;，可以测量小学数学推理。 Gemini Ultra将于明年年初通过Bard Advanced（一种新的尖端AI体验）来到Bard。 &lt;/p>; &lt;p>; gemini pro也可以在&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/gemini-support-on-vertex-ai&quot;>; vertex AI &lt;/a>;，Google Cloud的端到端AI平台，该平台使开发人员能够构建可以在文本，代码，图像和视频中处理信息的应用程序。 &lt;a href=&quot;https://blog.google/technology/ai/gemini-api-developers-cloud/&quot;>; gemini pro在12月的AI工作室也提供了&lt;/a>;。 &lt;/p>; &lt;p>;为了最好地说明双子座的某些功能，我们制作了一个&lt;a href=&quot;https://deepmind.google/technologies/gemini/gemini/#hands-on&quot;>;一系列简短视频&lt;/a>;双子座如何可以：&lt;/p>; &lt;ul>; &lt;li>; &lt;a href=&quot;https://www.youtube.com/watch?v=spiop_cb54a&quot;>;解锁科学文学的见解&lt;/a>; &lt;/li >; &lt;li>; &lt;a href=&quot;https://www.youtube.com/watch?v=lvggmvmhv69s＆amp;t = 1s&quot;>;在竞争性编程中出色&lt;/a>; &lt;/li>; &lt;li>; &lt;a>; &lt;li>; &lt;a a href = https://www.youtube.com/watch?v=d64qd7swr3s&quot;>; process and prococess and Checeals and probocess &lt;/a>; &lt;/a>; &lt;/li>; &lt;li>; &lt;a href =“ https://www.youtube.com/watch？ v = k4px1vaxaai”>;在数学和物理学中解释推理&lt;/a>; &lt;/li>; &lt;li>; &lt;a href=&quot;https://www.youtube.com/watch?v=v5trc_5trc_5trc_5-8g4&quot;>;有关用户意图的原因要产生定制体验&lt;/a>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>; ml/ai研究&lt;/h2>; &lt;p>;除了我们在产品和技术方面的进步外，我们还做了一个机器学习和AI研究更广泛领域的重要进步。 &lt;/p>; &lt;p>;最高级ML模型的核心是变压器模型体系结构，&lt;a href =“ https://blog.research.google/2017/08/transformer-novel-neural-network.html “>;由Google研究人员在2017年开发&lt;/a>;。它最初是为语言开发的，它已被证明在范围内有用，如&lt;a href=&quot;https://blog.research.google/2020/12/transformers-for-image-image-recognition-at.html&quot;>;计算机视觉&lt;/ a>;，&lt;a href=&quot;https://deepmind.google/discover/discover/blog/transforming-the-future-of-music-creation/&quot;>;音频&lt;/a>;，&lt;a href =“ https：// deepmind 。 Google/Technologies/alphafold/“>;蛋白质折叠&lt;/a>;等等。今年，我们在&lt;a href=&quot;https://blog.research.google/2023/03/scaling-vision-vision-vision-transformers-to-22.html&quot;>;缩放视觉变压器&lt;/a>; ART在各种视觉任务中的结果，并且在构建&lt;a href =“ https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.htman.html”中也很有用>;更有能力的机器人&lt;/a>;。 &lt;/p>; &lt;p>; &lt;/p>; &lt;p>;扩展模型的多功能性需要执行高级和多步推理的能力。今年，我们根据几个研究轨道实现了这个目标。例如，&lt;a href=&quot;https://blog.search.google/2023/08/teaching-language-models-models-models-to-reason.html&quot;>;算法提示&lt;/a>;是一种新方法，是一种教授语言模型的新方法通过演示一系列算法步骤，然后可以在新的上下文中应用该步骤。这种方法将一种中学数学基准的准确性从25.9％提高到61.1％。 &lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;>; &lt;tbody>;>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMgWRH7DtSwqbAImqRRsW26oyKnDiinTNvtkUuvASZJSaChsNXG1-4EeDkTr22E7xjRzwcFdWCZSKFuuBoLfsZiH27pZ1d6XMef8ns6RGx619oZnHdeCVZb7EOPWigNqbGsmu4FrU2Xgampr0HIASv7ks8ha9DE8L3hmAhKU8_Aps8L_1evceD2MKyG23/s1200/image5.gif&quot; style=&quot;边距左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-Original-height =“ 166” data-Original-width =“ 1200” src =“ https：//blogger.googleusercercontent。 com/img/b/R29vZ2xl/AVvXsEhqMgWRH7DtSwqbAImqRRsW26oyKnDiinTNvtkUuvASZJSaChsNXG1-4EeDkTr22E7xjRzwcFdWCZSKFuuBoLfsZiH27pZ1d6XMef8ns6RGx619oZnHdeCVZb7EOPWigNqbGsmu4FrU2Xgampr0HIASv7ks8ha9DE8L3hmAhKU8_Aps8L_1evceD2MKyG23/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：center;“>; &lt;em style =” text-align：left;“>;通过提供算法提示，我们可以通过context学习来教授算法的规则。&lt;/em>; &lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;在视觉问题的域中，在与加州大学伯克利分校的研究人员的合作中，我们展示了如何&lt;a href =&#39;https://blog.research.google/2023/07 /模式 -  visual-Question-andwering-via.html&quot;>; better回答复杂的视觉问题&lt;/a>;（“马匹的马车？通过合成程序来执行多步推理的问题。 &lt;/p>; &lt;p>;我们现在正在使用&lt;a href=&quot;https://blog.research.google/2023/05/large-sequence-models-models-models-for-software.html&quot;>;一般模型软件开发生命周期&lt;/a>;要自动生成代码审核评论，响应代码审核评论，对代码部分提出绩效改进建议（通过在其他情况下从过去的更改中学习），请修复代码以响应编译错误等等。 &lt;/p>; &lt;p>;在与Google Maps团队进行的多年研究合作中，我们能够扩展逆增强学习，并将其应用于&lt;a href =“ https://blog.research.google/2023/2023/ 09/世界规模inverse-Reinforcression.html“>;改善路线建议的世界规模问题&lt;/a>; &lt;/a>;对于超过10亿用户。我们的工作最终达到了16-24％的全球路线匹配率相对改善，有助于确保路线与用户偏好更好地保持一致。 &lt;/p>; &lt;p>;我们还继续致力于提高机器学习模型的推理性能。在&lt;a href=&quot;https://blog.research.google/2023/08/neural-network-pruning-with.html&quot;>; computationally-fromely友好的方法中，用于修剪神经网络中的连接&lt;/a>;能够将一种近似算法设计到计算上棘手的最佳选项选择问题，该问题能够从图像分类模型中修复70％的边缘，并且仍然保留了原始原始的几乎所有精度。 &lt;/p>; &lt;p>;在&lt;a href=&quot;https://blog.reachearch.google/2023/06/speed-is-is-is-ry-you-need-on-device.html&quot;>;加速eve eviceing empererating on-deviceing上扩散模型&lt;/a>;，我们还能够将各种优化应用于注意机制，卷积内核和操作融合，以使运行高质量图像生成模型在设备上运行高质量的模型；例如，在智能手机上只能在12秒内生成“可爱的小狗，可爱的小狗的感性和高分辨率图像”。 &lt;/p>; &lt;br />; &lt;div class =“ saparator” style =“ clear：clear; clex-align：center;“>; &lt;a href =” https：//blogger.googleusercortent.com/img/img/r29vz2xxl /AVvXsEhM5NuphyphenhyphenlH7_H5PfABZrn1a-CuFJm8XyEpAlodQqpcrTvYj3XNWarRULjbSgKqY1trCsC0hbVvHTU9l4pUyn3vFupsfyLDVgdMgsTYHtE1b8AWQwZWUgXGAAJ_F12rcOqVDjbe1q5OX0TdYEQBOt-FpKIqvfYivuAcPU3bVTZcYxUdFOdtaW5JE6Fii7ej/s522/image7.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original- width=&quot;270&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM5NuphyphenhyphenlH7_H5PfABZrn1a-CuFJm8XyEpAlodQqpcrTvYj3XNWarRULjbSgKqY1trCsC0hbVvHTU9l4pUyn3vFupsfyLDVgdMgsTYHtE1b8AWQwZWUgXGAAJ_F12rcOqVDjbe1q5OX0TdYEQBOt-FpKIqvfYivuAcPU3bVTZcYxUdFOdtaW5JE6Fii7ej/w208-h400/image7.gif&quot; width=&quot;208&quot; />; &lt;/a>; &lt;/div>; &lt;br />; &lt;p>;能够强大的语言和多模型模型的进步也使我们的机器人研究工作受益。我们将单独训练的语言，视觉和机器人控制模型组合到&lt;a href=&quot;https://blog.research.google/2023/03/palm-e-e-embodied-multimodal-language.html&quot;>; palm-e &lt;/ A>;，一种用于机器人技术的体现多模式模型，&lt;a href =“ https://deepmind.google/discover/discover/blog/rt-new-new-model-model-translates-vision-vision-vision-vision-vision-vision-language-into-into-into-action/ “>;机器人变压器2 &lt;/a>;（RT-2），一种新颖的视觉 - 语言 - 动作（VLA）模型，该模型&lt;a href =“ https://deepmind.google/discover/discover/blog/robobocat-a-a-a-self--self--self------从Web和Robotics数据中提高Robotic-Agent/“>;“学习&lt;/a>;”，并将这些知识转化为机器人控制的通用指令。&lt;/p>; &lt;p>; &lt;/p>; &lt;/p>; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ margin-left：auto; margin-right：auto;”>; &lt;tbody>; &lt;trbody>; &lt;tr>; &lt;tr>; &lt;td style =“ text-align：text-align： center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJL6HUJ6swdYZlAdRsPttHH9EmdE7TpGlK92U9hxHu29ANiHMQLC3QB1PX8HFWwatiJ6V-rjeImQ67oRUGtQMR4TDXB7mOVqsz-BouN1y29vbUU7rc-nTkj2H-V0V3VNCTMujhLSyNM3duUEVOYhm0nf8wBVrIghfEdpRsKtHn_gg2dWVxzzSXTk6ROTb/s616/image8.jpg&quot; style=&quot;margin-left: auto;边缘权利：自动;“>; &lt;img border =“ 0” data-original-height =“ 559” data-eriginal-width =“ 616” src =“ https://blogger.googleusercontent.com/img/img/b/b/ R29vZ2xl/AVvXsEijJL6HUJ6swdYZlAdRsPttHH9EmdE7TpGlK92U9hxHu29ANiHMQLC3QB1PX8HFWwatiJ6V-rjeImQ67oRUGtQMR4TDXB7mOVqsz-BouN1y29vbUU7rc-nTkj2H-V0V3VNCTMujhLSyNM3duUEVOYhm0nf8wBVrIghfEdpRsKtHn_gg2dWVxzzSXTk6ROTb/s16000/image8.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;RT-2 architecture and training: We co-fine-tune a pre-trained vision-language model on robotics and web data. The resulting model takes in robot camera images and directly predicts actions for a robot to perform.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Furthermore, we showed how &lt;a href=&quot;https:// blog.research.google/2023/08/saytap-language-to-quadrupedal.html&quot;>;language can also be used to control the gait of quadrupedal robots&lt;/a>; and explored the &lt;a href=&quot;https://blog .research.google/2023/08/language-to-rewards-for-robotic-skill.html&quot;>;use of language to help formulate more explicit reward functions&lt;/a>; to bridge the gap between human language and robotic actions. Then, in &lt;a href=&quot;https://blog.research.google/2023/05/barkour-benchmarking-animal-level.html&quot;>;Barkour&lt;/a>; we benchmarked the agility limits of quadrupedal robots.&lt;/p>; &lt;br />; &lt;h2>;Algorithms &amp;amp; optimization&lt;/h2>; &lt;p>; Designing efficient, robust, and scalable algorithms remains a high priority. This year, our work included: applied and scalable algorithms, market algorithms, system efficiency and optimization, and privacy. &lt;/p>; &lt;p>; We introduced &lt;a href=&quot;https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/&quot;>;AlphaDev&lt;/a>;, an AI system that uses reinforcement learning to discover enhanced computer science algorithms. AlphaDev uncovered a faster algorithm for sorting, a method for ordering data, which led to improvements in the LLVM libc++ sorting library that were up to 70% faster for shorter sequences and about 1.7% faster for sequences exceeding 250,000 elements. &lt;/p>; &lt;p>; We developed a novel model to &lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;predict the properties of large graphs&lt;/a>;, enabling estimation of performance for large programs. We released a new dataset, &lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;TPUGraphs&lt;/a>;, to accelerate &lt;a href=&quot;https://www.kaggle.com/competitions/predict-ai-model-runtime&quot;>;open research in this area&lt;/a>;, and showed how we can use &lt;a href=&quot;https://blog.research.google/2023/12/advancements-in-machine-learning-for.html&quot;>;modern ML to improve ML efficiency&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC00uLmFXRBzNXoaXirQMkAV7j91dBVwgvY4BEFuOLP4V9MquLuKt9imKFHBHsc7laEKUIuXUe_-v0DJyCauIQMrOzUaSOKl15hxBeAbgLGPWNYehM7Z8seK3P9JcjyMmeSZNyXWMYNME84KZwIygF1deRSpaZ1oBeK-uFnxEqCJcm6M0z4hA18JVGew-H/s1223/image11.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1042&quot; data-original-width=&quot;1223&quot; height=&quot;341&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC00uLmFXRBzNXoaXirQMkAV7j91dBVwgvY4BEFuOLP4V9MquLuKt9imKFHBHsc7laEKUIuXUe_-v0DJyCauIQMrOzUaSOKl15hxBeAbgLGPWNYehM7Z8seK3P9JcjyMmeSZNyXWMYNME84KZwIygF1deRSpaZ1oBeK-uFnxEqCJcm6M0z4hA18JVGew-H/w400-h341/image11.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;The TPUGraphs dataset has 44 million graphs for ML program optimization.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;We developed a new &lt;a href=&quot;https://en.wikipedia.org/wiki/Load_balancing_(computing)&quot;>;load balancing&lt;/a>; algorithm for distributing queries to a server, called &lt;a href=&quot;https://arxiv.org/abs/2312.10172&quot;>;Prequal&lt;/a>;, which minimizes a combination of requests-in-flight and estimates the latency. Deployments across several systems have saved CPU, latency, and RAM significantly. We also designed a new &lt;a href=&quot;https://arxiv.org/abs/2305.02508&quot;>;analysis framework&lt;/a>; for the classical caching problem with capacity reservations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRpdeenR8s32zMYfc6hmCq1OKu_Fk8NnhymDBWweQky1o9OIqARGqtzA-SUPAX1UZVQsrvPDXXbr20ZBz70RNBS3njSwCtkGYmBbWYOV7J87xFTMXCDRoiOh4EtGqf_aKBtegTJrbyru3ompVpfYzMx6EKWX26xHs_POZJ2dd3lbvDX-JggUoXFDn-HDJa/s1896/image12.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;422&quot; data-original-width=&quot;1896&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRpdeenR8s32zMYfc6hmCq1OKu_Fk8NnhymDBWweQky1o9OIqARGqtzA-SUPAX1UZVQsrvPDXXbr20ZBz70RNBS3njSwCtkGYmBbWYOV7J87xFTMXCDRoiOh4EtGqf_aKBtegTJrbyru3ompVpfYzMx6EKWX26xHs_POZJ2dd3lbvDX-JggUoXFDn-HDJa/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Heatmaps of normalized CPU usage transitioning to&amp;nbsp;&lt;a href=&quot;Prequal&quot;>;Prequal&lt;/a>;&amp;nbsp;at 08:00.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We improved state-of-the-art in clustering and &lt;a href=&quot;https://en. wikipedia.org/wiki/Graph_neural_network&quot;>;graph algorithms&lt;/a>; by developing new techniques for&lt;a href=&quot;https://arxiv.org/abs/2106.05513&quot;>; computing minimum-cut&lt;/a>;, &lt;a href =&quot;https://arxiv.org/abs/2309.17243&quot;>;approximating correlation clustering&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2308.00503&quot;>;massively parallel graph clustering&lt;/a>; 。 Additionally, we introduced&lt;a href=&quot;https://arxiv.org/abs/2308.03578&quot;>; TeraHAC&lt;/a>;, a novel hierarchical clustering algorithm for trillion-edge graphs, designed a &lt;a href=&quot;https://blog.research.google/2023/11/best-of-both-worlds-achieving.html&quot;>;text clustering algorithm&lt;/a>; for better scalability while maintaining quality, and designed the most efficient &lt;a href=&quot;https://arxiv.org/abs/2307.03043&quot;>;algorithm for approximating the Chamfer Distance&lt;/a>;, the standard similarity function for multi-embedding models, offering &amp;gt;50× speedups over highly-optimized exact algorithms and scaling to billions of points.&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; We continued optimizing Google&#39;s large embedding models (LEMs), which power many of our core products and recommender systems. Some new techniques include &lt;a href=&quot;https://arxiv.org/abs/2305.12102&quot;>;Unified Embedding&lt;/a>; for battle-tested feature representations in web-scale ML systems and &lt;a href=&quot;https://arxiv.org/abs/2209.14881&quot;>;Sequential Attention&lt;/a>;, which uses attention mechanisms to discover high-quality sparse model architectures during training. &lt;/p>; &lt;!--&lt;p>; This year, we also continued our research in market algorithms to design computationally efficient marketplaces and causal inference. First, we remain committed to advancing the rapidly growing interest in ads automation for which our recent work &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3543507.3583416&quot;>;explains the adoption of autobidding mechanisms&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3580507.3597725&quot;>;examines the effect of different auction formats on the incentives of advertisers&lt;/a>;. In the multi-channel setting, our findings shed light on how the choice between local and global optimizations affects the design of multi-channel &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3580507.3597707&quot;>;auction systems&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3618408.3618709&quot;>;bidding systems&lt;/a>;. &lt;/p>;-->; &lt;p>; Beyond auto-bidding systems, we also studied auction design in other complex settings, such as &lt;a href=&quot;https://arxiv.org/abs/2204.01962&quot;>;buy-many mechanisms&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2207.09429&quot;>;auctions for heterogeneous bidders&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2309.10766&quot;>;contract designs&lt;/a>;, and innovated &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3618408.3618478&quot;>;robust online bidding algorithms&lt;/a>;. Motivated by the application of generative AI in collaborative creation (eg, joint ad for advertisers), we proposed &lt;a href=&quot;https://arxiv.org/abs/2310.10826&quot;>;a novel token auction model &lt;/a>;where LLMs bid for influence in the collaborative AI creation. Finally, we show how to &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3580507.3597702&quot;>;mitigate personalization effects in experimental design&lt;/a>;, which, for example, may cause recommendations to drift over time. &lt;/p>; &lt;p>; The Chrome Privacy Sandbox, a multi-year collaboration between Google Research and Chrome, has publicly launched several APIs, including for &lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#protected-audience&quot;>;Protected Audience&lt;/a>;, &lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#topics&quot;>;Topics&lt;/a>;, and &lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#attribution-reporting&quot;>;Attribution Reporting&lt;/a>;. This is a major step in protecting user privacy while supporting the open and free web ecosystem. These efforts have been facilitated by fundamental research on &lt;a href=&quot;https://arxiv.org/abs/2304.07210&quot;>;re-identification risk&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2301.05605&quot;>;private streaming computation&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/12/summary-report-optimization-in-privacy.html&quot;>;optimization&lt;/a>; of privacy caps and budgets, &lt;a href=&quot;https://arxiv.org/pdf/2308.13510.pdf&quot;>;hierarchical aggregation&lt;/a>;, and training models with &lt;a href=&quot;https://arxiv.org/pdf/2312.05659.pdf&quot;>;label privacy&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Science and society&lt;/h2>; &lt;p>; In the not too distant future, there is a very real possibility that AI applied to scientific problems can accelerate the rate of discovery in certain domains by 10× or 100×, or more, and lead to major advances in diverse areas including bioengineering, &lt;a href=&quot;https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning&quot;>;materials science&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/&quot;>;weather prediction&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;climate forecasting&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/09/google-research-embarks-on-effort-to.html&quot;>;neuroscience&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/04/an-ml-based-approach-to-better.html&quot;>;genetic medicine&lt;/a>;, and &lt;a href=&quot;https://health.google/health-research/publications/&quot;>;healthcare&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Sustainability and climate change &lt;/h3>; &lt;p>; In &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-reduce-greenhouse-emissions-project-greenlight/&quot;>;Project Green Light&lt;/a>;, we partnered with 13 cities around the world to help improve traffic flow at intersections and reduce stop-and-go emissions. Early numbers from these partnerships indicate a potential for up to 30% reduction in stops and up to 10% reduction in emissions. &lt;/p>; &lt;p>; In our &lt;a href=&quot;https://sites.research.google/contrails/&quot;>;contrails work&lt;/a>;, we analyzed large-scale weather data, historical satellite images, and past flights 。 We &lt;a href=&quot;https://blog.google/technology/ai/ai-airlines-contrails-climate-change/&quot;>;trained an AI model&lt;/a>; to predict where contrails form and reroute airplanes accordingly. In partnership with American Airlines and Breakthrough Energy, we used this system to demonstrate contrail reduction by 54%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnU7wUIVSDG3HicS_tHszwAD_RlhU_SXJO0quz5CUJ0RLT03erljh8ckLW8NLAlYtOPpX6lzsohacC7x2X-_2abBGDGWGpIN0KZpYJ0YH3FOzRDhq-zVTeXne4LjpKGPoDtNtljKkee2R4hE3ju3wYhltF5q8oaPZ1I9R39eB2uYBnFfRxjka1vCg8H5Vv/s957/image14.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;957&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnU7wUIVSDG3HicS_tHszwAD_RlhU_SXJO0quz5CUJ0RLT03erljh8ckLW8NLAlYtOPpX6lzsohacC7x2X-_2abBGDGWGpIN0KZpYJ0YH3FOzRDhq-zVTeXne4LjpKGPoDtNtljKkee2R4hE3ju3wYhltF5q8oaPZ1I9R39eB2uYBnFfRxjka1vCg8H5Vv/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Contrails detected over the United States using AI and GOES-16 satellite imagery.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We are also developing novel technology-driven approaches to &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;help communities with the effects of climate change&lt;/a>;. For example, we have &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;expanded our flood forecasting coverage to 80 countries&lt;/a>;, which directly impacts more than 460 million people. We have initiated a &lt;a href=&quot;https://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html&quot;>;number of research efforts&lt;/a>; to help mitigate the increasing danger of wildfires, including &lt;a href=&quot;https://blog.research.google/2023/02/real-time-tracking-of-wildfire.html&quot;>;real-time tracking of wildfire boundaries&lt;/a>; using satellite imagery, and work that &lt;a href=&quot;https://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html&quot;>;improves emergency evacuation plans&lt;/a>; for communities at risk to rapidly-spreading wildfires. Our &lt;a href=&quot;https://www.americanforests.org/article/american-forests-unveils-updates-for-tree-equity-score-tool-to-address-climate-justice/&quot;>;partnership&lt;/a>; with American Forests puts data from our &lt;a href=&quot;https://insights.sustainability.google/places/ChIJVTPokywQkFQRmtVEaUZlJRA/trees?hl=en-US&quot;>;Tree Canopy&lt;/a>; project to work in their &lt;a href=&quot;https://treeequityscore.org/&quot;>;Tree Equity Score&lt;/a>; platform, helping communities identify and address unequal access to trees.&lt;/p>; &lt;p>; Finally, we continued to develop better models for weather prediction at longer time horizons. Improving on &lt;a href=&quot;https://blog.research.google/2020/03/a-neural-weather-model-for-eight-hour.html&quot;>;MetNet&lt;/a>; and &lt;a href=&quot;https ://blog.research.google/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;MetNet-2&lt;/a>;, in this year&#39;s work on &lt;a href=&quot;https: //blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html&quot;>;MetNet-3&lt;/a>;, we now outperform traditional numerical weather simulations up to twenty-four hours 。 In the area of medium-term, global weather forecasting, our work on &lt;a href=&quot;https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/&quot;>;GraphCast&lt;/a>; showed significantly better prediction accuracy for up to 10 days compared to &lt;a href=&quot;https://en.wikipedia.org/wiki/Integrated_Forecast_System&quot;>;HRES&lt;/a>;, the most accurate operational deterministic forecast, produced by the &lt;a href=&quot;https://www.ecmwf.int/&quot;>;European Centre for Medium-Range Weather Forecasts&lt;/a>; (ECMWF). In collaboration with ECMWF, we released &lt;a href=&quot;https://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html&quot;>;WeatherBench-2&lt;/a>;, a benchmark for evaluating the accuracy of weather forecasts in a common framework. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/Q6fOlW-Y_Ss&quot; width=&quot;640&quot; youtube-src-id=&quot;Q6fOlW-Y_Ss&quot;>;&lt;/iframe>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A selection of GraphCast&#39;s predictions rolling across 10 days showing specific humidity at 700 hectopascals (about 3 km above surface), surface temperature, and surface wind speed.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Health and the life sciences&lt;/h3>; &lt;p>; The potential of AI to dramatically improve processes in healthcare is significant. Our initial &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06291-2&quot;>;Med-PaLM&lt;/a>; model was the first model capable of achieving a passing score on the US medical licensing exam. Our more recent &lt;a href=&quot;https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/&quot;>;Med-PaLM 2 model&lt;/a>; improved by a further 19%, achieving an expert-level accuracy of 86.5%. These &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM models&lt;/a>; are language-based, enable clinicians to ask questions and have a dialogue about complex medical conditions, and are &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/introducing-medlm-for-the-healthcare-industry&quot;>;available&lt;/a>; to healthcare organizations as part of &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/medlm/overview&quot;>;MedLM&lt;/a>; through Google Cloud. &lt;/p>; &lt;p>; In the same way our general language models are evolving to handle multiple modalities, we have recently shown research on a &lt;a href=&quot;https://blog.research.google/2023/08/multimodal-medical-ai.html&quot;>;multimodal version of Med-PaLM&lt;/a>; capable of interpreting medical images, textual data, and other modalities, &lt;a href=&quot;https://arxiv.org/abs/2307.14334&quot;>;describing a path&lt;/a>; for how we can realize the exciting potential of AI models to help advance real-world clinical care. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIHNosDcouStVqnHjoNr_b7YQQGCEXsxlCOy1ltKotOv5GQfl6JA9We90L6Ej3TZ2tiutOrASoon-BPt__Fh9jN2NiXY1W6z5emcW63JkkS7sS1LrsMz7mINkzvSuD_i4NR3GdRbsQFlVrNrC3cv1bNHETISJ_Ml0n7ddXbtHmO_AzfSd8EPq7-ud7iBNH/s1600/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIHNosDcouStVqnHjoNr_b7YQQGCEXsxlCOy1ltKotOv5GQfl6JA9We90L6Ej3TZ2tiutOrASoon-BPt__Fh9jN2NiXY1W6z5emcW63JkkS7sS1LrsMz7mINkzvSuD_i4NR3GdRbsQFlVrNrC3cv1bNHETISJ_Ml0n7ddXbtHmO_AzfSd8EPq7-ud7iBNH/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same model weights.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;We have also been working on &lt;a href=&quot;https://deepmind.google/discover/blog/codoc-developing-reliable-ai-tools-for-healthcare/&quot;>;how best to harness AI models in clinical workflows&lt;/a>;. We have shown that &lt;a href=&quot;https://blog.research.google/2023/03/learning-from-deep-learning-case-study.html&quot;>;coupling deep learning with interpretability methods&lt;/a>; can yield new insights for clinicians. We have also shown that self-supervised learning, with careful consideration of privacy, safety, fairness and ethics, &lt;a href=&quot;https://blog.research.google/2023/04/robust-and-efficient-medical-imaging.html&quot;>;can reduce the amount of de-identified data needed&lt;/a>; to train clinically relevant medical imaging models by 3×–100×, reducing the barriers to adoption of models in real clinical settings. We also released an &lt;a href=&quot;https://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html&quot;>;open source mobile data collection platform&lt;/a>; for people with chronic disease to provide tools to the community to build their own studies.&lt;/p>; &lt;p>; AI systems can also discover completely new signals and biomarkers in existing forms of medical data. In work on &lt;a href=&quot;https://blog.research.google/2023/03/detecting-novel-systemic-biomarkers-in.html&quot;>;novel biomarkers discovered in retinal images&lt;/a>;, we demonstrated that a number of systemic biomarkers spanning several organ systems (eg, kidney, blood, liver) can be predicted from external eye photos. In other work, we showed that combining &lt;a href=&quot;https://blog.research.google/2023/04/developing-aging-clock-using-deep.html&quot;>;retinal images and genomic information&lt;/a>; helps identify some underlying factors of aging. &lt;/p>; &lt;p>; In the genomics space, we worked with 119 scientists across 60 institutions to create a &lt;a href=&quot;https://blog.research.google/2023/05/building-better-pangenomes-to-improve.html&quot;>;new map of the human genome&lt;/a>;, or pangenome. This more equitable pangenome better represents the genomic diversity of global populations. Building on our ground-breaking &lt;a href=&quot;https://www.nature.com/articles/s41586-021-03819-2&quot;>;AlphaFold&lt;/a>; work, our work on &lt;a href=&quot;https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/&quot;>;AlphaMissense&lt;/a>; this year provides a catalog of predictions for 89% of all 71 million possible &lt;a href=&quot;https://en.wikipedia.org/wiki/Missense_mutation&quot;>;missense variants&lt;/a>; as either likely pathogenic or likely benign. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHNPYsNIjVTRyfPZODcVpp-XnQAtz2b0HcKsa8F9GyJXoKfp-9PH8N_IcXO_lJ7WfuTZ2ezeAVYCDPnqeyu-qeXahqu1lwJXb6Zq00Mt7M-WMyFff9eUL67L_QZBwK95DNlVAcFpd9cr1GW5gxqNb0mOJszQphyphenhyphen6Yi_QelNzeYnvZ1XNKZqNU2EP_x8MjW/s1070/image1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;491&quot; data-original-width=&quot;1070&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHNPYsNIjVTRyfPZODcVpp-XnQAtz2b0HcKsa8F9GyJXoKfp-9PH8N_IcXO_lJ7WfuTZ2ezeAVYCDPnqeyu-qeXahqu1lwJXb6Zq00Mt7M-WMyFff9eUL67L_QZBwK95DNlVAcFpd9cr1GW5gxqNb0mOJszQphyphenhyphen6Yi_QelNzeYnvZ1XNKZqNU2EP_x8MjW/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Examples of AlphaMissense predictions overlaid on AlphaFold predicted structures (red – predicted as pathogenic; blue – predicted as benign; grey – uncertain). Red dots represent known pathogenic missense variants, blue dots represent known benign variants.&amp;nbsp;&lt;strong>;Left:&lt;/strong>;&amp;nbsp;HBB protein. Variants in this protein can cause sickle cell anaemia.&amp;nbsp;&lt;strong>;Right:&lt;/strong>;&amp;nbsp;CFTR protein. Variants in this protein can cause cystic fibrosis.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also shared &lt;a href=&quot;https://deepmind.google/discover/blog/a-glimpse-of-the-next-generation-of-alphafold/&quot;>;an update&lt;/a>; on progress towards the next generation of AlphaFold. Our latest model can now generate predictions for nearly all molecules in the &lt;a href=&quot;https://www.wwpdb.org/&quot;>;Protein Data Bank&lt;/a>; (PDB), frequently reaching atomic accuracy. This unlocks new understanding and significantly improves accuracy in multiple key biomolecule classes, including ligands (small molecules), proteins, nucleic acids (DNA and RNA), and those containing post-translational modifications (PTMs).&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; On the neuroscience front, we &lt;a href=&quot;https://blog.research.google/2023/09/google-research-embarks-on-effort-to.html&quot;>;announced a new collaboration&lt;/a>; with Harvard, Princeton, the NIH, and others to map an entire mouse brain at synaptic resolution, beginning with a first phase that will focus on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hippocampal_formation&quot;>;hippocampal formation&lt;/a>; — the area of the brain responsible for memory formation, spatial navigation, and other important functions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Quantum computing&lt;/h3>; &lt;p>; Quantum computers have the potential to solve big, real-world problems across science and industry. But to realize that potential, they must be significantly larger than they are today, and they must reliably perform tasks that cannot be performed on classical computers. &lt;/p>; &lt;p>; This year, we took an important step towards the development of a large-scale, useful quantum computer. Our breakthrough is the first demonstration of &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;quantum error correction&lt;/a>;, showing that it&#39;s possible to reduce errors while also increasing the number of qubits. To enable real-world applications, these qubit building blocks must perform more reliably, lowering the error rate from ~1 in 10&lt;sup>;3&lt;/sup>; typically seen today, to ~1 in 10&lt;sup>;8&lt;/sup>; 。 &lt;/p>; &lt;br />; &lt;h2>;Responsible AI research&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Design for Responsibility &lt;/h3>; &lt;p>; Generative AI is having a transformative impact in a wide range of fields including healthcare, education, security, energy, transportation, manufacturing, and entertainment. Given these advances, the importance of designing technologies consistent with our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; remains a top priority. We also recently published case studies of &lt;a href=&quot;https://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot;>;emerging practices in society-centered AI&lt;/a>; 。 And in our annual &lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf&quot; target=&quot;_blank&quot;>;AI Principles Progress Update&lt;/a>;, we offer details on how our Responsible AI research is integrated into products and risk management processes. &lt;/p>; &lt;p>; Proactive design for Responsible AI begins with identifying and documenting potential harms. For example, we recently &lt;a href=&quot;https://deepmind.google/discover/blog/evaluating-social-and-ethical-risks-from-generative-ai/&quot;>;introduced&lt;/a>; a &lt;a href=&quot;https://arxiv.org/abs/2310.11986&quot;>;three-layered&lt;/a>; context-based framework for comprehensively evaluating the social and ethical risks of AI systems. During model design, harms can be mitigated with the use of &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot;>;responsible datasets&lt;/a>; 。 &lt;/p>; &lt;p>; We are &lt;a href=&quot;https://blog.google/technology/research/project-elevate-black-voices-google-research/&quot;>;partnering with Howard University&lt;/a>; to build high quality African-American English (AAE) datasets to improve our products and make them work well for more people. Our research on &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3593013.3594016&quot;>;globally inclusive cultural representation&lt;/a>; and our publication of the &lt;a href=&quot;https://skintone.google/&quot;>;Monk Skin Tone scale&lt;/a>; furthers our commitments to equitable representation of all people. The insights we gain and techniques we develop not only help us improve our own models, they also power &lt;a href=&quot;https://blog.google/intl/en-in/company-news/using-ai-to-study-demographic-representation-in-indian-tv/&quot;>;large-scale studies of representation in popular media&lt;/a>; to inform and inspire more inclusive content creation around the world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYAo6ClPM9zD8ctnoTvdyhnwW1VdVT2p677EMKGrN0oULjyK9TdS05z1OzhTTuyxp0kfuUUbHEieZXYRW6hUe3XlJM6HYOS68rXneSGQeLTy_Bo_SCvbxnjHdG2CB_8aLCz5pP-B_dciLKZYlIo9j8bEyUdKtZQhg7DukG9pJudJKU-o0DRuM5XrbvkBI/s1196/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;158&quot; data-original-width=&quot;1196&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYAo6ClPM9zD8ctnoTvdyhnwW1VdVT2p677EMKGrN0oULjyK9TdS05z1OzhTTuyxp0kfuUUbHEieZXYRW6hUe3XlJM6HYOS68rXneSGQeLTy_Bo_SCvbxnjHdG2CB_8aLCz5pP-B_dciLKZYlIo9j8bEyUdKtZQhg7DukG9pJudJKU-o0DRuM5XrbvkBI/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Monk Skin Tone (MST) Scale. See more at&amp;nbsp;&lt;a href=&quot;http://skintone.google/&quot;>;skintone.google&lt;/a>;.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With advances in generative image models, &lt;a href=&quot;https://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot;>;fair and inclusive representation of people&lt;/a>; remains是重中之重。 In the development pipeline, we are working to &lt;a href=&quot;https://blog.research.google/2023/07/using-societal-context-knowledge-to.html&quot;>;amplify underrepresented voices and to better integrate social context knowledge&lt;/a>;. We proactively address potential harms and bias using &lt;a href=&quot;https://arxiv.org/pdf/2306.06135.pdf&quot;>;classifiers and filters&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2311.17259.pdf&quot;>;careful dataset analysis&lt;/a>;, and in-model mitigations such as fine-tuning, &lt;a href=&quot;https://arxiv.org/abs/2310.16523&quot;>;reasoning&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2306.14308&quot;>;few-shot prompting&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2310.16959&quot;>;data augmentation&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2310.17022&quot;>;controlled decoding&lt;/a>;, and our research showed that generative AI enables &lt;a href=&quot;https://arxiv.org/abs/2302.06541&quot;>;higher quality safety classifiers&lt;/a>; to be developed with far less data. We also released &lt;a href=&quot;https://developers.googleblog.com/2023/10/make-with-makersuite-part-2-tuning-llms.html&quot;>;a powerful way to better tune models with less data&lt;/a>; giving developers more control of responsibility challenges in generative AI.&lt;/p>; &lt;p>; We have developed new&lt;a href=&quot;https://arxiv.org/abs/2303.08114&quot;>; state-of-the-art explainability methods&lt;/a>; to identify the role of training data on model behaviors. By &lt;a href=&quot;https://arxiv.org/abs/2302.06598&quot;>;combining training data attribution methods with agile classifiers&lt;/a>;, we found that we can identify mislabelled training examples. This makes it possible to reduce the noise in training data, leading to significant improvements in model accuracy. &lt;/p>; &lt;p>; We initiated several efforts to improve safety and transparency about online content. For example, we introduced &lt;a href=&quot;https://deepmind.google/discover/blog/identifying-ai-generated-images-with-synthid/&quot;>;SynthID&lt;/a>;, a tool for watermarking and identifying AI-generated images. SynthID is imperceptible to the human eye, doesn&#39;t compromise image quality, and allows the watermark to remain detectable, even after modifications like adding filters, changing colors, and saving with various lossy compression schemes. &lt;/p>; &lt;p>; We also launched &lt;a href=&quot;https://blog.google/products/search/google-search-new-fact-checking-features/&quot;>;About This Image&lt;/a>; to help people assess the credibility of images, showing information like an image&#39;s history, how it&#39;s used on other pages, and available metadata about an image. And we &lt;a href=&quot;https://arxiv.org/abs/2210.03535&quot;>;explored safety methods&lt;/a>; that have been developed in other fields, learning from established situations where there is low-risk tolerance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBYUNXlxnFiBnnB_-EFKhdc-1W8MwG-VZKrhyKtWFmfAcqlrbSdPl7TAslOAMaH1Zon0TvGKpj23nlO7XZyg2ovFuNHpgXbsyUUPrxzf1RtJFlBPzR5Hh9KAus1l79qrBFP5JJDScQgn_5cq3ZVf7T0VuPiNLLJ-PsENlba_BA0nORQkofZYY7K1DhJGXt/s616/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;346&quot; data-original-width=&quot;616&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBYUNXlxnFiBnnB_-EFKhdc-1W8MwG-VZKrhyKtWFmfAcqlrbSdPl7TAslOAMaH1Zon0TvGKpj23nlO7XZyg2ovFuNHpgXbsyUUPrxzf1RtJFlBPzR5Hh9KAus1l79qrBFP5JJDScQgn_5cq3ZVf7T0VuPiNLLJ-PsENlba_BA0nORQkofZYY7K1DhJGXt/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;SynthID generates an imperceptible digital watermark for AI-generated images.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Privacy remains an essential aspect of our commitment to Responsible AI. We continued improving our state-of-the-art privacy preserving learning algorithm &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;, developed the DP-Alternating Minimization algorithm (&lt;a href=&quot;https://arxiv.org/pdf/2310.15454.pdf&quot;>;DP-AM&lt;/a>;) to enable personalized recommendations with rigorous privacy protection, and defined a new &lt;a href=&quot;https://blog.research.google/2023/09/differentially-private-median-and-more.html&quot;>;general paradigm&lt;/a>; to reduce the privacy costs for many aggregation and learning tasks. We also proposed a scheme for &lt;a href=&quot;https://openreview.net/pdf?id=q15zG9CHi8&quot;>;auditing differentially private machine learning systems&lt;/a>;.&lt;/p>; &lt;p>; On the applications front we demonstrated that &lt;a href=&quot;https://arxiv.org/pdf/2308.10888.pdf&quot;>;DP-SGD offers a practical solution&lt;/a>; in the large model fine-tuning regime and showed that images generated by DP diffusion models are &lt;a href=&quot;https://arxiv.org/pdf/2302.13861.pdf&quot;>;useful for a range of downstream tasks&lt;/a>;. We &lt;a href=&quot;https://blog.research.google/2023/12/sparsity-preserving-differentially.html&quot;>;proposed&lt;/a>; a new algorithm for DP training of large embedding models that provides efficient training on TPUs without compromising accuracy. &lt;/p>; &lt;p>; We also teamed up with a broad group of academic and industrial researchers to organize the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>; to address the scenario in which training images are forgotten to protect the privacy or rights of individuals. We shared a mechanism for &lt;a href=&quot;https://arxiv.org/pdf/2311.17035.pdf&quot;>;extractable memorization&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2302.03874&quot;>;participatory systems&lt;/a>; that give users more control over their sensitive data. &lt;/p>; &lt;p>; We continued to expand the world&#39;s largest corpus of atypical speech recordings to &amp;gt;1M utterances in &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;>;Project Euphonia&lt;/a>;, which enabled us to train a &lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; to &lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;better recognize atypical speech by 37%&lt;/a>; on real-world benchmarks. &lt;/p>; &lt;p>; We also built an &lt;a href=&quot;https://blog.research.google/2023/08/study-socially-aware-temporally-causal.html&quot;>;audiobook recommendation system&lt;/a>; for students with reading disabilities such as dyslexia. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Adversarial testing&lt;/h3>; &lt;p>; Our work in adversarial testing &lt;a href=&quot;https://blog.research.google/2023/03/responsible-ai-at-google-research.html&quot;>;engaged community voices&lt;/a>; from historically marginalized communities. We partnered with groups such as the &lt;a href=&quot;https://arxiv.org/abs/2303.08177&quot;>;Equitable AI Research Round Table&lt;/a>; (EARR) to ensure we represent the diverse communities who use our models and &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;engage with external users&lt;/a>; to identify potential harms in generative model outputs. &lt;/p>; &lt;p>; We &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot;>;established a dedicated Google AI Red Team&lt;/a>; focused on testing AI models and products for security, privacy, and abuse risks. We showed that attacks such as “&lt;a href=&quot;https://arxiv.org/pdf/2302.10149.pdf?isApp=1&quot;>;poisoning&lt;/a>;” or &lt;a href=&quot;https://arxiv.org/pdf/2306.15447.pdf&quot;>;adversarial examples&lt;/a>; can be applied to production models and surface additional risks such as memorization in both &lt;a href=&quot;https://www.usenix.org/system/files/usenixsecurity23-carlini.pdf&quot;>;image&lt;/a>; and &lt;a href=&quot;https://arxiv.org/pdf/2311.17035.pdf&quot;>;text generative models&lt;/a>;. We also demonstrated that defending against such attacks can be challenging, as merely applying defenses can cause other &lt;a href=&quot;https://arxiv.org/pdf/2309.05610.pdf&quot;>;security and privacy leakages&lt;/a>;. We also introduced model evaluation for &lt;a href=&quot;https://arxiv.org/abs/2305.15324&quot;>;extreme risks&lt;/a>;, such as offensive cyber capabilities or strong manipulation skills. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Democratizing AI though tools and education&lt;/h3>; &lt;p>; As we advance the state-of-the-art in ML and AI, we also want to ensure people can understand and apply AI to specific problems. We released &lt;a href=&quot;https://makersuite.google.com/&quot;>;MakerSuite&lt;/a>; (now &lt;a href=&quot;https://makersuite.google.com&quot;>;Google AI Studio&lt;/a>;), a web-based tool that enables AI developers to quickly iterate and build lightweight AI-powered apps. To help AI engineers better understand and debug AI, we released &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;LIT 1.0&lt;/a>;, a state-of-the-art, open-source debugger for machine learning models. &lt;/p>; &lt;p>; &lt;a href=&quot;https://colab.google/&quot;>;Colab&lt;/a>;, our tool that helps developers and students access powerful computing resources right in their web browser, reached over 10 million users 。 We&#39;ve just added &lt;a href=&quot;https://blog.google/technology/ai/democratizing-access-to-ai-enabled-coding-with-colab/&quot;>;AI-powered code assistance&lt;/a>; to all users at no cost — making Colab an even more helpful and integrated experience in data and ML workflows. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy5OhqeP7Rf5gTbCP-gYsmTFyc9ztUMXrcv_M8Lya5zLPzzRVrHmldGiR-rf1PnggxP_rlG2mo6YJ9NhnNnFHEbtn8f-FJk_cxOTtO9hL0ZhxV9hSGd0LW0-RymxkPVBbqXKDk4nRV7mJE6heVnn-6tYcLdpofuhqKPHnqxDTZf1hjifTg3k7K4VNcS2S6/s844/image10.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;470&quot; data-original-width=&quot;844&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy5OhqeP7Rf5gTbCP-gYsmTFyc9ztUMXrcv_M8Lya5zLPzzRVrHmldGiR-rf1PnggxP_rlG2mo6YJ9NhnNnFHEbtn8f-FJk_cxOTtO9hL0ZhxV9hSGd0LW0-RymxkPVBbqXKDk4nRV7mJE6heVnn-6tYcLdpofuhqKPHnqxDTZf1hjifTg3k7K4VNcS2S6/s16000/image10.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;One of the most used features is “Explain error” — whenever the user encounters an execution error in Colab, the code assistance model provides an explanation along with a potential fix.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To ensure AI produces accurate knowledge when put to use, we also recently introduced &lt;a href=&quot;https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/&quot;>;FunSearch&lt;/a>;, a new approach that generates verifiably true knowledge in mathematical sciences using evolutionary methods and large language models.&lt;/p>; &lt;p>; For AI engineers and product designers, we&#39;re updating the &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;People + AI Guidebook&lt;/a>; with generative AI best practices, and we continue to design &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;AI Explorables&lt;/a>;, which includes &lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;how and why models sometimes make incorrect predictions confidently&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Community engagement&lt;/h3>; &lt;p>; We continue to advance the fields of AI and computer science by publishing much of our work and participating in and organizing conferences. We have published more than 500 papers so far this year, and have strong presences at conferences like ICML (see the &lt;a href=&quot;https://blog.research.google/2023/07/google-at-icml-2023.html&quot;>;Google Research&lt;/a>; and &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-research-at-icml-2023/&quot;>;Google DeepMind&lt;/a>; posts), ICLR (&lt;a href=&quot;https://blog.research.google/2023/04/google-at-iclr-2023.html&quot;>;Google Research&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/deepminds-latest-research-at-iclr-2023/&quot;>;Google DeepMind&lt;/a>;), NeurIPS (&lt;a href=&quot;https://blog.research.google/2023/12/google-at-neurips-2023.html&quot;>;Google Research&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023/&quot;>;Google DeepMind&lt;/a>;), &lt;a href=&quot;https://blog.research.google/2023/10/google-at-iccv-2023.html&quot;>;ICCV&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/06/google-at-cvpr-2023.html&quot;>;CVPR&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/07/google-at-acl-2023.html&quot;>;ACL&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/04/google-at-chi-2023.html&quot;>;CHI&lt;/a>;, and &lt;a href=&quot;https://blog.research.google/2023/08/google-at-interspeech-2023.html&quot;>;Interspeech&lt;/a>;. We are also working to support researchers around the world, participating in events like the &lt;a href=&quot;https://deeplearningindaba.com/2023/google-outreach-mentorship-programme/&quot;>;Deep Learning Indaba&lt;/a>;, &lt;a href=&quot;https://khipu.ai/khipu2023/khipu-2023-speakers2023/&quot;>;Khipu&lt;/a>;, supporting &lt;a href=&quot;https://blog.google/around-the-globe/google-latin-america/phd-fellowship-research-latin-america/&quot;>;PhD Fellowships in Latin America&lt;/a>;, and more. We also worked with partners from 33 academic labs to pool data from 22 different robot types and create the &lt;a href=&quot;https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/&quot;>;Open X-Embodiment dataset and RT-X model&lt;/a>; to better advance responsible AI development. &lt;/p>; &lt;p>; Google has spearheaded an industry-wide effort to develop &lt;a href=&quot;https://mlcommons.org/working-groups/ai-safety/ai-safety/&quot;>;AI safety benchmarks&lt;/a >; under the &lt;a href=&quot;https://mlcommons.org/&quot;>;MLCommons&lt;/a>; standards organization with participation from several major players in the generative AI space including OpenAI, Anthropic, Microsoft, Meta, Hugging Face, and more 。 Along with others in the industry we also &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/&quot;>;co-founded&lt;/a>; the &lt;a href=&quot;https://www.frontiermodelforum.org/&quot;>;Frontier Model Forum&lt;/a>; (FMF), which is focused on ensuring safe and responsible development of frontier AI models. With our FMF partners and other philanthropic organizations, we launched a $10 million &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-anthropic-open-ai-frontier-model-forum-executive-director/&quot;>;AI Safety Fund&lt;/a>; to advance research into the ongoing development of the tools for society to effectively test and evaluate the most capable AI models. &lt;/p>; &lt;p>; In close partnership with &lt;a href=&quot;http://Google.org&quot;>;Google.org&lt;/a>;, we &lt;a href=&quot;https://blog.google/technology/ai/google-ai-data-un-global-goals/&quot;>;worked with the United Nations&lt;/a>; to build the &lt;a href=&quot;https://unstats.un.org/UNSDWebsite/undatacommons/sdgs&quot;>;UN Data Commons for the Sustainable Development Goals&lt;/a>;, a tool that tracks metrics across the 17 &lt;a href=&quot;https://sdgs.un.org/goals&quot;>;Sustainable Development Goals&lt;/a>;, and &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/supported-organizations&quot;>;supported projects&lt;/a>; from NGOs, academic institutions, and social enterprises on &lt;a href=&quot;https://blog.google/outreach-initiatives/google-org/httpsbloggoogleoutreach-initiativesgoogle-orgunited-nations-global-goals-google-ai-/&quot;>;using AI to accelerate progress on the SDGs&lt;/a>;. &lt;/p>; &lt;p>; The items highlighted in this post are a small fraction of the research work we have done throughout the last year. Find out more at the &lt;a href=&quot;https://blog.research.google/&quot;>;Google Research&lt;/a>; and &lt;a href=&quot;https://deepmind.google/discover/blog/&quot;>;Google DeepMind&lt;/a>; blogs, and our &lt;a href=&quot;https://research.google/pubs/&quot;>;list of publications&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Future vision&lt;/h2>; &lt;p>; As multimodal models become even more capable, they will empower people to make incredible progress in areas from science to education to entirely new areas of knowledge. &lt;/p>; &lt;p>; Progress continues apace, and as the year advances, and our products and research advance as well, people will find more and interesting creative uses for AI. &lt;/p>; &lt;p>; Ending this Year-in-Review where we began, as we say in &lt;em>;&lt;a href=&quot;https://ai.google/static/documents/google-why-we-focus-on-ai.pdf&quot;>;Why We Focus on AI (and to what end)&lt;/a>;&lt;/em>;: &lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; If pursued boldly and responsibly, we believe that AI can be a foundational technology that transforms the lives of people everywhere — this is what excites us! &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small ;&quot;>;This Year-in-Review is cross-posted on both the &lt;a href=&quot;https://blog.research.google/2023/12/2023-year-of-groundbreaking-advances-in.html&quot;>; Google Research Blo&lt;/a>;g and the &lt;a href=&quot;https://deepmind.google/discover/blog/2023-a-year-of-groundbreaking-advances-in-ai-and-computing/&quot;>;Google DeepMind Blog&lt;/a>;.&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8816183473385638131/comments/default&quot; rel=&quot;replies&quot; title=&quot; Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/2023-year-of-groundbreaking-advances-in.html#comment-form &quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8816183473385638131&quot; rel=&quot;edit &quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8816183473385638131&quot; rel=&quot;self&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/2023-year-of-groundbreaking-advances-in.html&quot; rel=&quot;alternate&quot; title=&quot;2023: A year of groundbreaking advances in AI and computing&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiU12G_p2DZO4gQ-P95aP2IFvtKRHRG5Oik9VzJ4WtT_VznggjUrL4tTzqto-C8yx6ULgvugKgH9usiZxnKGk97pOFyHWnu-1S4sSbR2Jjq5T36tQRbZucTAP7gmXkGw77xN6s39IKxPaxbo5tw_Cq52ZfPWOCBkWL-2XTuzsIh6viRIqgGcdUdNq-pHjzl/s72-c/year_in_review-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/ media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5516200703494636207&lt;/id>;&lt;published>;2023- 12-19T13:08:00.000-08:00&lt;/发布>;&lt;更新>;2024-01-12T11:04:04.629-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/ atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;video&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term= &quot;Vision Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VideoPoet: A large language model for zero-shot video generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot; >;Posted by Dan Kondratyuk and David Ross, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFG8pxLk-uvJVPesDUxU7Ox7Tb0VR4lB4jNygxiiIl9gyeX-rgtCb5jhWbPDKGad4d5GkPFr7-uzdZOngFtnPbmV6IurKEd5vHTiLesCvx8RDzBy_5u31e6C93kuirOG8pKcwEBkBrw4TBTzh3WIoX1TZYzYM3M4aj4zYD2r_p5FflI7ntpqoD9fsGQhwO/s1600/videopoetpreview .gif&quot; style=&quot;display: none;&quot; />; &lt;p>; A recent wave of video generation models has burst onto the scene, in many cases showcasing stunning picturesque quality. One of the current bottlenecks in video generation is in the ability to produce coherent large motions. In many cases, even the current leading models either generate small motion or, when producing larger motions, exhibit noticeable artifacts. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了探索语言模型在视频生成中的应用，我们引入了 VideoPoet (&lt;a href=&quot;http://sites.research.google/videopoet &quot;>;网站&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2312.14125&quot;>;研究论文&lt;/a>;），一个能够处理各种视频的大型语言模型（LLM）生成任务，包括文本到视频、图像到视频、视频风格化、视频&lt;a href=&quot;https://en.wikipedia.org/wiki/Inpainting&quot;>;修复&lt;/a>;和&lt;a href= “https://paperswithcode.com/task/image-outpainting&quot;>;outpainting&lt;/a>;，以及视频转音频。 One notable observation is that the leading video generation models are almost exclusively diffusion-based (for one example, see &lt;a href=&quot;https://imagen.research.google/video/&quot;>;Imagen Video&lt;/a>;). On the other hand, LLMs are widely recognized as the &lt;em>;de facto&lt;/em>; standard due to their exceptional learning capabilities across various modalities, including language, code, and audio (eg, &lt;a href=&quot;https://google-research.github.io/seanet/audiopalm/examples/&quot;>;AudioPaLM&lt;/a>;). In contrast to alternative models in this space, our approach seamlessly integrates many video generation capabilities within a single LLM, rather than relying on separately trained components that specialize on each task. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overview&lt;/h2>; &lt;p>; The diagram below illustrates VideoPoet&#39;s capabilities. Input images can be animated to produce motion, and (optionally cropped or masked) video can be edited for inpainting or outpainting. For stylization, the model takes in a video representing the depth and optical flow, which represent the motion, and paints contents on top to produce the text-guided style. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfgEjHPIukR1pHUT7VBU8DecJaayp8sIV1WC4HM6EOW-K1-E_Xu9-qE2hrTBrtgrks3awr8IiT9NhxVMDOR0qoFZ8nDQT_Si3LY60CWKySkaybXRW5Uf6EwZIiDRy7qkQGuoLUxoysR-fjEr0NTMqAGP2Cm8cyUQHVOOlS7MysnwHwqhdM-eA63PXy5XsV/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;682&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfgEjHPIukR1pHUT7VBU8DecJaayp8sIV1WC4HM6EOW-K1-E_Xu9-qE2hrTBrtgrks3awr8IiT9NhxVMDOR0qoFZ8nDQT_Si3LY60CWKySkaybXRW5Uf6EwZIiDRy7qkQGuoLUxoysR-fjEr0NTMqAGP2Cm8cyUQHVOOlS7MysnwHwqhdM-eA63PXy5XsV/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of VideoPoet, capable of multitasking on a variety of video-centric inputs and outputs. The LLM can optionally take text as input to guide generation for text-to-video, image-to-video, video-to-audio, stylization, and outpainting tasks. Resources used: &lt;a href=&quot;https://commons.wikimedia.org/wiki/Main_Page&quot;>;Wikimedia Commons&lt;/a>; and &lt;a href=&quot;https://davischallenge.org/&quot;>;DAVIS&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Language models as video generators&lt;/h2>; &lt;p>; One key advantage of using LLMs for training is that one can reuse many of the scalable efficiency improvements that have been introduced in existing LLM training infrastructure. However, LLMs operate on discrete tokens, which can make video generation challenging. Fortunately, there exist &lt;a href=&quot;https://magvit.cs.cmu.edu/v2/&quot;>;video&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2107.03312&quot;>;audio&lt;/a>; tokenizers, which serve to encode video and audio clips as sequences of discrete tokens (ie, integer indices), and which can also be converted back into the original representation. &lt;/p>; &lt;p>; VideoPoet trains an &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive language model&lt;/a>; to learn across video, image, audio, and text modalities through the use of multiple tokenizers (&lt;a href=&quot;https://magvit.cs.cmu.edu/v2/&quot;>;MAGVIT V2&lt;/a>; for video and image and &lt;a href=&quot;https://arxiv.org/abs/2107.03312&quot;>;SoundStream&lt;/a>; for audio). Once the model generates tokens conditioned on some context, these can be converted back into a viewable representation with the tokenizer decoders. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s2680/image21.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;824&quot; data-original-width=&quot;2680&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s16000/image21.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A detailed look at the VideoPoet task design, showing the training and inference inputs and outputs of various tasks. Modalities are converted to and from tokens using tokenizer encoder and decoders. Each modality is surrounded by boundary tokens, and a task token indicates the type of task to perform.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Examples generated by VideoPoet&lt;/h2>; &lt;p>; Some examples generated by our model are shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoUJFpPd377ceVnh3Yi0Y1oM6pVPL_FSEwugxBVKfEwHV8VA-1ZPmddz1VRBtqESjjEP83EJ4HOLLSBmXOVLEODZVFZYUuDiCRyMUIvSaRsxieR-58iAwHPBf7SNeSBU2a3Pm80JOFivTSjZsqlerHxAGg_Ko_8gCDLWWtNAQffyGCNJrw2G5PPG-vSYtz/s1100/image18.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;963&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoUJFpPd377ceVnh3Yi0Y1oM6pVPL_FSEwugxBVKfEwHV8VA-1ZPmddz1VRBtqESjjEP83EJ4HOLLSBmXOVLEODZVFZYUuDiCRyMUIvSaRsxieR-58iAwHPBf7SNeSBU2a3Pm80JOFivTSjZsqlerHxAGg_Ko_8gCDLWWtNAQffyGCNJrw2G5PPG-vSYtz/s16000/image18.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Videos generated by VideoPoet from various text prompts. For specific text prompts refer to &lt;a href=&quot;http://sites.research.google/videopoet&quot;>;the website&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For text-to-video, video outputs are variable length and can apply a range of motions and styles depending on the text content. To ensure responsible practices, we reference artworks and styles in the public domain eg, Van Gogh&#39;s “Starry Night”.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;Text Input&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“A Raccoon dancing in Times Square”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“A horse galloping through Van-Gogh&#39;s &#39;Starry Night&#39;”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“Two pandas playing cards”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“A large blob of exploding splashing rainbow paint, with an apple emerging, 8k”&lt;/em>;&lt;/td>; &lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;Video Output&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFNvzU_hzVGT7XCvA4AdBprfzwpOV_b8xs6Q54No72B88fgtHFK7eHFr3UJ9Ac0tXIemkOUR6VxNC2I8HQWznWs2x4IsB8biOEe2DXBrMxW6HveNhuiae40mF8asd3jYh2WqZMTTObSKiHb0RJIFsK_C7VzBq685OPUY_uqPC5NJGXqKs3AWJOG-Gqgk0V/s448/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFNvzU_hzVGT7XCvA4AdBprfzwpOV_b8xs6Q54No72B88fgtHFK7eHFr3UJ9Ac0tXIemkOUR6VxNC2I8HQWznWs2x4IsB8biOEe2DXBrMxW6HveNhuiae40mF8asd3jYh2WqZMTTObSKiHb0RJIFsK_C7VzBq685OPUY_uqPC5NJGXqKs3AWJOG-Gqgk0V/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0FAaXVgkTSmoK90XYZq3sbW1fgRUmvOLZUpZ4dubLjJOFYID6w_q5GGIU1hy1E8B4J7hF01OOAupDxJWyD2OBMJEs6AIAbJEzH0qrx7TovnikAUXZyN_MQBu33QYe17CTtI95oI6x91qJhSSPyXNGlmkFKbWX8xwd6nT7Esd9RNE8tjiSbWwWQTKoAfjg/s448/image11.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0FAaXVgkTSmoK90XYZq3sbW1fgRUmvOLZUpZ4dubLjJOFYID6w_q5GGIU1hy1E8B4J7hF01OOAupDxJWyD2OBMJEs6AIAbJEzH0qrx7TovnikAUXZyN_MQBu33QYe17CTtI95oI6x91qJhSSPyXNGlmkFKbWX8xwd6nT7Esd9RNE8tjiSbWwWQTKoAfjg/s16000/image11.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4yqm_nd4mtu-3d_AdfxkKAX453hyphenhyphenXE9e7ohBzZC9RvboIZWxF_5fLw4XaCU3x1aUtW4WRckKzfqB-yzHdV_uzPiCAAwv6zgv__7MZtxdwiWsMiQ59NDH3axwd0UOIFA8C2aq0XNSgcV1ieCN1fc9MXFVIszTG8z7gpcAkgqTn5HOBBJ-pks8I1tOudAR8/s448/image12.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4yqm_nd4mtu-3d_AdfxkKAX453hyphenhyphenXE9e7ohBzZC9RvboIZWxF_5fLw4XaCU3x1aUtW4WRckKzfqB-yzHdV_uzPiCAAwv6zgv__7MZtxdwiWsMiQ59NDH3axwd0UOIFA8C2aq0XNSgcV1ieCN1fc9MXFVIszTG8z7gpcAkgqTn5HOBBJ-pks8I1tOudAR8/s16000/image12.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2huAujTA8vjmb5rlgj6vXF7uIHr0N7KrnLFiTXyjuN0l6kxSp7gQRPES5hD30ZzN-XTzUZSC-ROT19x0wE5cjQA_FOovY-Zox_68e0Dl4Dxanqvyuos3S1TExRdg3WZANucc-DXtKUsnim9Kh0GwU6LyFhpCJgHal0bI0UbpBYrXtlNGBYWuT8XVNIt6u/s448/image17.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEg2huAujTA8vjmb5rlgj6vXF7uIHr0N7KrnLFiTXyjuN0l6kxSp7gQRPES5hD30ZzN-XTzUZSC-ROT19x0wE5cjQA_FOovY-Zox_68e0Dl4Dxanqvyuos3S1TExRdg3WZANucc-DXtKUsnim9Kh0GwU6LyFhpCJgHal0bI0UbpBYrXtlNGBYWuT8XVNIt6u/s16000/image17.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;For image-to-video, VideoPoet can take the input image and animate it with a prompt.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5kAQCs7GJoJR0m_8hmYsq-Wd-KsjuF782YuV5D33BlPE8f3-AU1iTKwOVrpxnnBDHa-5AXgkXNBNil61r5eVhXa2v16VUraEt6DAa-4_v-xHJq6lJfwkJ9ATQZTGdxKsbvnfielzv_6iJyLrubPyrGhle_BUecTVJkGo_7S8sM-3yl6vVVtqNNg4nf0mw/s1536/image13.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5kAQCs7GJoJR0m_8hmYsq-Wd-KsjuF782YuV5D33BlPE8f3-AU1iTKwOVrpxnnBDHa-5AXgkXNBNil61r5eVhXa2v16VUraEt6DAa-4_v-xHJq6lJfwkJ9ATQZTGdxKsbvnfielzv_6iJyLrubPyrGhle_BUecTVJkGo_7S8sM-3yl6vVVtqNNg4nf0mw/s16000/image13.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of image-to-video with text prompts to guide the motion. Each video is paired with an image to its left. &lt;strong>;Left&lt;/strong>;: “A ship navigating the rough seas, thunderstorm and lightning, animated oil on canvas”. &lt;strong>;Middle&lt;/strong>;: “Flying through a nebula with many twinkling stars”. &lt;strong>;Right&lt;/strong>;: “A wanderer on a cliff with a cane looking down at the swirling sea fog below on a windy day”. Reference: &lt;a href=&quot;https://commons.wikimedia.org/wiki/Main_Page&quot;>;Wikimedia Commons&lt;/a>;, public domain**.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For video stylization, we predict the optical flow and depth information before feeding into VideoPoet with some additional input text. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ2eDU0pNOfQJF8cKPOV5QkKthIO3-98jbqyZJ7lFLk7cckq8Gg4FXrro6oikQRxDVDKKz8rg9CU6wihsfU68RnnLkHGxJUeFNGBobjsbJ4VHGFtUg-nerlt2rPiJ9bu8i2VkkXX5yEK650t4ay8F7K2zSW5-TjjkbR61TskVhsaQCw1-8lkP1gMDg96i1/s1536/image16.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ2eDU0pNOfQJF8cKPOV5QkKthIO3-98jbqyZJ7lFLk7cckq8Gg4FXrro6oikQRxDVDKKz8rg9CU6wihsfU68RnnLkHGxJUeFNGBobjsbJ4VHGFtUg-nerlt2rPiJ9bu8i2VkkXX5yEK650t4ay8F7K2zSW5-TjjkbR61TskVhsaQCw1-8lkP1gMDg96i1/s16000/image16.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of video stylization on top of VideoPoet text-to-video generated videos with text prompts, depth, and optical flow used as conditioning. The left video in each pair is the input video, the right is the stylized output. &lt;b>;Left&lt;/b>;: “Wombat wearing sunglasses holding a beach ball on a sunny beach.” &lt;b>;Middle&lt;/b>;: “Teddy bears ice skating on a crystal clear frozen lake.” &lt;b>;Right&lt;/b>;: “A metal lion roaring in the light of a forge.”&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; VideoPoet is also capable of generating audio. Here we first generate 2-second clips from the model and then try to predict the audio without any text guidance. This enables generation of video and audio from a single model.&lt;/p>; &lt;br />; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;&lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/105_drums_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/107_cat_piano_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/108_train_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/104_dog_popcorn_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of video-to-audio, generating audio from a video example without任何文本输入。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 默认情况下，VideoPoet 模型会生成纵向视频，以针对短格式内容定制其输出。为了展示其功能，我们制作了一部由 VideoPoet 生成的许多短片组成的短片。对于剧本，我们请 &lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>; 写一个关于旅行的短篇故事浣熊，具有逐个场景的细分和随附的提示列表。然后，我们为每个提示生成视频剪辑，并将所有生成的剪辑拼接在一起以生成下面的最终视频。&lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: 两者； text-align: center;&quot;>; &lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/70wZKfx6Ylk&quot; width=&quot;640 &quot; youtube-src-id=&quot;70wZKfx6Ylk&quot;>;&lt;/iframe>; &lt;/div>; &lt;br />; &lt;p>; 当我们开发 VideoPoet 时，我们注意到模型功能的一些不错的属性，我们在下面重点介绍。&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;长视频&lt;/h3>; &lt;p>; 我们可以通过调节最后 1 秒来生成更长的视频视频并预测接下来的 1 秒。通过重复链接，我们表明该模型不仅可以很好地扩展视频，而且即使在多次迭代中也能忠实地保留所有对象的外观。&lt;/p>; &lt;p>; 这里有两个例子VideoPoet 从文本输入生成长视频： &lt;br />; &lt;br />; &lt;/p>;&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;>;&lt;tbody >; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;文本输入&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;tdalign=&quot;center&quot; width= &quot;35%&quot;>;&lt;em>;“一名宇航员开始在火星上跳舞。然后五彩缤纷的烟花在背景中爆炸。”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;tdalign=&quot;center&quot; width=&quot;35%&quot;>;&lt;em>;“丛林中非常锋利的精灵石头城市，有明亮的蓝色河流、瀑布和大而陡峭的垂直悬崖面。”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;视频输出&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaT8uErAOI-bKPCwsVpEQ_SSxqdgjVB9ai-Db3M9YXhGM3X9N0Jwt-UcDb6X8n2V_4-Tf76xkwlSS4ftV8TvAI V6ZbjeXK5JPtQr8Mb_ZcPKiIvOdwFXJOBEfDk1Gp1hzRBkoYwmoH3bAu6NVNBo-ficSneXgvhDF7fwMGVqMik0KWGwv_GLf7clQ2l5e5/ s448/image14.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaT8uErAOI-bKPCwsVpEQ_SSxqdgjVB9ai-Db3M9YXhGM3X9N0Jwt-UcDb6X8n2V_4-Tf76xkwlSS4ftV8TvAIV6ZbjeXK5JPtQr8Mb_ZcPK iIvOdwFXJOBEfDk1Gp1hzRBkoYwmoH3bAu6NVNBo-ficSneXgvhDF7fwMGVqMik0KWGwv_GLf7clQ2l5e5/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>; &amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDbroT8i5N-AdcRsTLjkLWoqzif1nJj-z1bRxcZwM_-1213gK6Or85VxKIFBMsnvAF_KLAmXWWLeDPxA5ZqW tNI5nqfp_6wzgKnqACckJMjBU2eNy8ySgvWjuFOPNarVcBwx9ZlrAdyMrWCdt29xDE7dPHhlwcxbRSyO7-ZllKs1SRGDgVm5XUc21I3Ddv/ s448/image9.gif&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDbroT8i5N-AdcRsTLjkLWoqzif1nJj-z1bRxcZwM_-1213gK6Or85VxKIFBMsnvAF_KLAmXWWLeDPxA5ZqWtNI5nqfp_6wzgKnqACckJMjBU2eNy8ySgvWjuFOPNarVcBwx9ZlrAdyMrWCdt29xDE7dPHhlwcxbRSyO7-ZllKs1SRGDgVm5XUc21I3Ddv/s16000/image9.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; It is also possible to interactively edit existing video clips generated by VideoPoet. If we supply an input video, we can change the motion of objects to perform different actions. The object manipulation can be centered at the first frame or the middle frames, which allow for a high degree of editing control. &lt;/p>; &lt;p>; For example, we can randomly generate some clips from the input video and select the desired next clip. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2NFzPadmS-8v2ShkxFaqage2MkmSopCm17wtoYnVCFufD5GKZHzM9ZUeL4EvCtVLZGMJYiUA1NVJhplymInJr4_K-G9s9263JAVRMxPb9_15zipLZIwHcmYpwyZmGRwgtbFwpONB9CrOqnDmG9bJBvr7pXNbEqLtms_7QNMbSYSspRefkisKLzeWXAIW9/s1280/image10.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2NFzPadmS-8v2ShkxFaqage2MkmSopCm17wtoYnVCFufD5GKZHzM9ZUeL4EvCtVLZGMJYiUA1NVJhplymInJr4_K-G9s9263JAVRMxPb9_15zipLZIwHcmYpwyZmGRwgtbFwpONB9CrOqnDmG9bJBvr7pXNbEqLtms_7QNMbSYSspRefkisKLzeWXAIW9/s16000/image10.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An input video on the left is used as conditioning to generate four choices given the initial prompt: “Closeup of an adorable rusty broken-down steampunk robot covered in moss moist and budding vegetation, surrounded by tall grass”. For the first three outputs we show what would happen for unprompted motions. For the last video in the list below, we add to the prompt, “powering up with smoke in the background” to guide the action.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Image to video control&lt;/h3>; &lt;p>; Similarly, we can apply motion to an input image to edit its contents towards the desired state, conditioned on a text prompt. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlvHv0EU7YHj88knDA8pnCs5VDEsHI8OQeWDx8-dhNXcVKteNqMMrMczg9k0j-T8kevUiQua-Eet5BzT9xJ5CR-lpmFjMYyOQFZcAfXu-rlgTmes9_4-GONo7NFHYAw1q-Ivk6MVj34iyjj6KGpQ8dp6OwJH1SKGyxA2BDPvvEUUIJB6UBkvu1c1ILCgdr/s512/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlvHv0EU7YHj88knDA8pnCs5VDEsHI8OQeWDx8-dhNXcVKteNqMMrMczg9k0j-T8kevUiQua-Eet5BzT9xJ5CR-lpmFjMYyOQFZcAfXu-rlgTmes9_4-GONo7NFHYAw1q-Ivk6MVj34iyjj6KGpQ8dp6OwJH1SKGyxA2BDPvvEUUIJB6UBkvu1c1ILCgdr/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animating a painting with different prompts. &lt;b>;Left&lt;/b>;: “A woman turning to look at the camera.” &lt;b>;Right&lt;/b>;: “A woman yawning.” **&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Camera motion&lt;/h3>; &lt;p>; We can also accurately control camera movements by appending the type of desired camera motion to the text prompt. As an example, we generated an image by our model with the prompt, &lt;em>;“Adventure game concept art of a sunrise over a snowy mountain by a crystal clear river”&lt;/em>;. The examples below append the given text suffix to apply the desired motion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4w_RIRlokn88vR5u6GdRE32zOr5wKLUphlx4BwiZDQL0J6i-yhyphenhyphensc4wCsJ07izsk_MkLVT-TAfRIqU9sJ4E_cYVRszJ1bw-Ha4jYsv1oBgSMjAENhCPHIvg2aXBIULeH4UzR-0K5AHPixzxBYD7rP11xf4m2s7e1WFUyRHLv-RkKhAC9whQvwUovphkgy/s1536/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4w_RIRlokn88vR5u6GdRE32zOr5wKLUphlx4BwiZDQL0J6i-yhyphenhyphensc4wCsJ07izsk_MkLVT-TAfRIqU9sJ4E_cYVRszJ1bw-Ha4jYsv1oBgSMjAENhCPHIvg2aXBIULeH4UzR-0K5AHPixzxBYD7rP11xf4m2s7e1WFUyRHLv-RkKhAC9whQvwUovphkgy/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Prompts from left to right: “Zoom out”, “Dolly zoom”, “Pan left”, “Arc shot”, “Crane shot”, “FPV drone shot”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation results&lt;/h2>; &lt;p>; We evaluate VideoPoet on text-to-video generation with a variety of benchmarks to compare the results to other approaches. To ensure a neutral evaluation, we ran all models on a wide variation of prompts without cherry-picking examples and asked people to rate their preferences. The figure below highlights the percentage of the time VideoPoet was chosen as the preferred option in green for the following questions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Text fidelity&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjotT5lhyphenhyphenreDLFlq_hZRSAKKI2jWx9Pp2y1xvBTQflO-H7EQ3VZYmsRTiaTV1creTpMo0It1-IfiFh313zzhhjDPeSxSW3nnRWsQC1toPBSsRlQD0T6UmzFqSNGaeQ0CGBKmn6xyAJXTG3NaF9o0icxag6f_eRzjTvu71gNRB3lOLN4xK8iQWA7dT5FKo2F/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjotT5lhyphenhyphenreDLFlq_hZRSAKKI2jWx9Pp2y1xvBTQflO-H7EQ3VZYmsRTiaTV1creTpMo0It1-IfiFh313zzhhjDPeSxSW3nnRWsQC1toPBSsRlQD0T6UmzFqSNGaeQ0CGBKmn6xyAJXTG3NaF9o0icxag6f_eRzjTvu71gNRB3lOLN4xK8iQWA7dT5FKo2F/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User preference ratings for text fidelity, ie, what percentage of videos are preferred in terms of accurately following a prompt.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Motion interestingness&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIWoXGSpe80GopYbQLJcyISxwM7DVtB-OFr2gqCUC33D3J6aLCS9sE4LKuoXhA89YNZE9yg_VmvUwJg2N1_nKt9m5z2NJgWiM2Ylqs2_Y2nAULojUuwpNmLv7LhYv4aGs4WgffyECcQtKM3Z83bmosuuXvHw4DeekkzAIpCkF2LlN6jExQysy68Ovgmgk1/s1999/image15.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;797&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIWoXGSpe80GopYbQLJcyISxwM7DVtB-OFr2gqCUC33D3J6aLCS9sE4LKuoXhA89YNZE9yg_VmvUwJg2N1_nKt9m5z2NJgWiM2Ylqs2_Y2nAULojUuwpNmLv7LhYv4aGs4WgffyECcQtKM3Z83bmosuuXvHw4DeekkzAIpCkF2LlN6jExQysy68Ovgmgk1/s16000/image15.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User preference ratings for motion interestingness, ie, what percentage of videos are preferred in terms of producing interesting motion.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Based on the above, on average people selected 24–35% of examples from VideoPoet as following prompts better than a competing model vs. 8–11% for competing models. Raters also preferred 41–54% of examples from VideoPoet for more interesting motion than 11–21% for other models. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Through VideoPoet, we have demonstrated LLMs&#39; highly-competitive video generation quality across a wide variety of tasks, especially in producing interesting and high quality motions within videos. Our results suggest the promising potential of LLMs in the field of video generation. For future directions, our framework should be able to support “any-to-any” generation, eg, extending to text-to-audio, audio-to-video, and video captioning should be possible, among many others. &lt;/p>; &lt;p>; To view more examples in original quality, see the &lt;a href=&quot;http://sites.research.google/videopoet&quot;>;website demo&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research has been supported by a large body of contributors, including Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan Seybold, and Lu Jiang 。我们还要感谢Aren Jansen，Marco Tagliasacchi，Neil Zeghidour，John Hershey的音频令牌和处理，Angad Singh在“新秀The Raccoon”中为Storyboarding，Cordelia Schmid，Cordelia Schmid，用于研究讨论支持，Jay Yagnik是初始概念的建筑师。&lt;/em>; &lt;/p>; &lt;br />; &lt;p>; &lt;em>; ** &lt;/em>; &lt;br />; &lt;br />; &lt;em>;（a）&lt;a href = “ https://commons.wikimedia.org/wiki/file：rembrandt_christ_in_the_storm_on_the_storm_the_lake_lake_lake_f_galilee.jpg&quot;>; Galilee海上的风暴（b）&lt;a href=&quot;https://commons.wikimedia.org/wiki/wiki/file:pillars_of_creation_2014_hst_wfc3-wfc3-wfc3-uvis_fell-res.jpg&quot;>; Creation的支柱&lt;br />; &lt;em>;（c）&lt;a href=&quot;https://commons.wikimedia.org/wiki/wiki/file：caspar_david_fried_friedrich_-_wanderer_bove_bove_the_sea_sea_sea_sea_sea_of_fog.jpeg.jpeg.jpeg.jpeg.jpeg &quot;>; fog &lt;/a>; wanderer wanderer wander of fog &lt;/a>;弗里德里希（Friedrich），1818年，公共领域&lt;/em>; &lt;br />; &lt;em>;（d）&lt;a href=&quot;https://commons.wikimedia.org/wiki/wiki/file：Mona_lisa_lisa_by_leonardo_leonardo_leonardo_da_da_da_da_vinda_vinda_from_from_from_c2rmf_c2rmf_cretected.jpaqa &lt;/a>;，作者：莱昂纳多·达·芬奇（Leonardo da Vinci），1503年，公共领域。&lt;/em>; &lt;/p>; &lt;/penter>; &lt;link href =“ http://blog.research.google/feeds/551620070703494636207/comments/comments/comments/comments/default/default/default” rel =“回复” title =“ post注释” type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2023/2023/12/videopoet-lange-lange-language-language-model-model-for -Zero.html＃comment-form“ rel =” reply =“ title =” 0注释“ type” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/8474926263314520262626/posts/ default/5516200703494636207&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5516200703494636207&quot; rel=&quot;self&quot; type =“ application/application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2023/12/videopoet-large-lange-language-language-model-model-for-zero.html” =“ videopoet：零摄影视频生成的大型语言模型” type =“ text/html”/>; &lt;under>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://wwwww.blogger.com/profile /12098626514775266161&lt;/uri>;&lt; ：//img1.blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail height =“ 72” url =“ https：//blogdger 。 E6C93KUIROG8PKCWEBKBRW4TBTZH3WIOX1TZYZYM3M4AJ4ZYD2R_P5FFLI7NTPQOD9FSGQHWO/S72-C/vIDEOPOETOETOETPOEETPREVIEW.GIF.gif.gif.gif.gif.gif >; &lt;/媒体：缩略图>; &lt;thr：总计>; 0 &lt;/thr：thr>; &lt;/entry>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-158796530372727576226 &lt;/id &lt;/id &lt;/id &lt;/id >; 2023-12-19T08：01：00.000-08：00 &lt;/publined>; &lt;更新>; 2023-12-19T08：15：57.101-08：00 &lt;/updateed &lt;/dipfated>; &lt;category schemion =“ http：///www.bloggergerger .com/atom/ns＃“ term =“ for cocorial good”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” >; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ google Maps”>; &lt;/category>; &lt;/category>; &lt;title type =“ text”>;模拟点亮了事后交通流的路径&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由Yechen Li和Neha Arora发布，软件工程师，Google Research &lt;/span>; &lt;/span>; &lt;img src =“ https：//blognger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEj35HCKLS0slKFX6LVrCCK0crWWJ-YwNVxNkDqze9UpfuVTWfD7URw9CyRwyFwAdIT-CHG59vl19KgfrGWEtoufCS6SQOzLuV1n7SYpun6EOAML0MfdxwjGhDKyKrfIz2t6Ivv_T07YVCPlA7uQMmPr8LYrXPSdE3V1WAwOwU0DYR_8wMWMJZh7MLeshXeP/s600/TrafficFlow.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Fifteen minutes. That&#39;s &lt;a href=&quot;https://colosseum.tours/interesting-facts#:~:text=THE%20COLOSSEUM%20COULD%20BE%20FILLED,in%20a%20matter%20of%20minutes.&quot;>;how long it took to empty the Colosseum&lt;/a>;, an engineering marvel that&#39;s still standing as the largest amphitheater in the world. Two thousand years later, this design continues to work well to move enormous crowds out of sporting and entertainment venues. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; But of course, exiting the arena is only the first step. Next, people must navigate the traffic that builds up in the surrounding streets. This is an age-old problem that remains unsolved to this day. In Rome, they addressed the issue by prohibiting private traffic on the street that passes directly by the Colosseum. This policy worked there, but what if you&#39;re not in Rome? What if you&#39;re at the Superbowl? Or at a Taylor Swift concert? &lt;/p>; &lt;p>; An approach to addressing this problem is to use simulation models, sometimes called &quot;digital twins&quot;, which are virtual replicas of real-world transportation networks that attempt to capture every detail from the layout of streets and intersections to the flow of vehicles. These models allow traffic experts to mitigate congestion, reduce accidents, and improve the experience of drivers, riders, and walkers alike. Previously, our team used these models to &lt;a href=&quot;https://arxiv.org/abs/2111.03426&quot;>;quantify sustainability impact of routing&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html&quot;>;test evacuation plans&lt;/a>; and show simulated traffic in &lt;a href=&quot;https://blog.google/products/maps/google-maps-immersive-view-routes/&quot;>;Maps Immersive View&lt;/a>;. &lt;/p>; &lt;p>; Calibrating high-resolution traffic simulations to match the specific dynamics of a particular setting is a longstanding challenge in the field. The availability of aggregate mobility data, detailed Google Maps road network data, advances in transportation science (such as understanding the relationship &lt;a href=&quot;https://tristan2022.org/Papers/TRISTAN_2022_paper_3704.pdf&quot;>;between segment demands and speeds&lt;/a>; for road segments with traffic signals), and &lt;a href=&quot;https://research.google/pubs/pub52679/&quot;>;calibration techniques&lt;/a>; which make use of speed data in physics-informed traffic models are paving the way for compute-efficient optimization at a global scale. &lt;/p>; &lt;p>; To test this technology in the real world, Google Research partnered with the Seattle Department of Transportation (SDOT) to develop simulation-based traffic guidance plans. Our goal is to help thousands of attendees of major sports and entertainment events leave the stadium area quickly and safely. The proposed plan reduced average trip travel times by 7 minutes for vehicles leaving the stadium region during large events. We deployed it in collaboration with SDOT using Dynamic Message Signs (DMS) and verified impact over multiple events between August and November, 2023. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_Re7-eWmfiWXL9-7r7r-w6IztNjfP3VPyfkvQc2TkBVfrCovR1enYO45bnwd5f04jM8WjwwmAEYWQwtSsp3bkY1PmiRfzkAFLSSskHaUy5PVsLvlPNV48GhlUoh9o70zJI0GWilCDCFUFQabAjQHj35bFR4IpDMHzEU2my_ayCGeLuMSV7Ml2wkrqgLV/s1200/PreImplementation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;518&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_Re7-eWmfiWXL9-7r7r-w6IztNjfP3VPyfkvQc2TkBVfrCovR1enYO45bnwd5f04jM8WjwwmAEYWQwtSsp3bkY1PmiRfzkAFLSSskHaUy5PVsLvlPNV48GhlUoh9o70zJI0GWilCDCFUFQabAjQHj35bFR4IpDMHzEU2my_ayCGeLuMSV7Ml2wkrqgLV/s16000/PreImplementation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfOuJeVqGFMNIcVEAWUPtxWJWEjMrxQ-5lP9ytEbGd8yzq13b8s6m1YdIxviZWyBZRsM5DR6ro0O4qmeCxXYcxGf5dvjGxAkpwVRz6AMq_RHvxsdovmiVFifWxe66vGRnIpu1M-bsML2NTqlM4s8TKuwjilDCn0tuvDKNty9lzZFMirDoiHqR2ktBA3wSR/s1200/PostImplementation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;519&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfOuJeVqGFMNIcVEAWUPtxWJWEjMrxQ-5lP9ytEbGd8yzq13b8s6m1YdIxviZWyBZRsM5DR6ro0O4qmeCxXYcxGf5dvjGxAkpwVRz6AMq_RHvxsdovmiVFifWxe66vGRnIpu1M-bsML2NTqlM4s8TKuwjilDCn0tuvDKNty9lzZFMirDoiHqR2ktBA3wSR/s16000/PostImplementation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;One policy recommendation we made was to divert traffic from S Spokane St, a major thoroughfare that connects the area to highways I-5 and SR 99, and is often congested after events. Suggested changes improved the flow of traffic through highways and arterial streets near the stadium, and reduced the length of vehicle queues that formed behind traffic signals. (Note that vehicles are larger than reality in this clip for demonstration.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Simulation model&lt;/h2>; &lt;p>; For this project, we created a new simulation model of the area around Seattle&#39;s stadiums. The intent for this model is to replay each traffic situation for a specified day as closely as possible. We use an open-source simulation software, &lt;a href=&quot;https://www.eclipse.org/sumo/&quot;>;Simulation of Urban MObility&lt;/a>; (SUMO). SUMO&#39;s behavioral models help us describe traffic dynamics, for instance, how drivers make decisions, like car-following, lane-changing and speed limit compliance. We also use insights from Google Maps to define the network&#39;s structure and various static segment attributes (eg, number of lanes, speed limit, presence of traffic lights). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNDnNe6WYClfVTVeD4lwsBtcDbo595yieDfOLEjPxH9e7gp9KRmYpp52c-oRiEaaILGodzScZE0E_IpianSL6pump2hGcB9-Cdj0QgfSDvJ_k8UIuvt9iraOEPCesgw3aeYoKvc5cgSF4H__xspISt4OJulVnhJZ1NEkKQBC_4dGirle3zd9ALtqnyBW2d/s1601/SUMO.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;643&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNDnNe6WYClfVTVeD4lwsBtcDbo595yieDfOLEjPxH9e7gp9KRmYpp52c-oRiEaaILGodzScZE0E_IpianSL6pump2hGcB9-Cdj0QgfSDvJ_k8UIuvt9iraOEPCesgw3aeYoKvc5cgSF4H__xspISt4OJulVnhJZ1NEkKQBC_4dGirle3zd9ALtqnyBW2d/s16000/SUMO.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the Simulation framework.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Travel demand is an important simulator input. To compute it, we first decompose the road network of a given metropolitan area into zones, specifically level 13 &lt;a href=&quot;http://s2geometry.io/devguide/s2cell_hierarchy.html&quot;>;S2 cells&lt;/a>; with 1.27 km&lt;sup>;2 &lt;/sup>;area per cell. From there, we define the travel demand as the expected number of trips that travel from an origin zone to a destination zone in a given time period. The demand is represented as aggregated origin–destination (OD) matrices. &lt;/p>; &lt;p>; To get the initial expected number of trips between an origin zone and a destination zone, we use aggregated and anonymized mobility statistics. Then we solve the OD calibration problem by combining initial demand with observed traffic statistics, like segment speeds, travel times and vehicular counts, to reproduce event scenarios. &lt;/p>; &lt;p>; We model the traffic around multiple past events in Seattle&#39;s T-Mobile Park and Lumen Field and evaluate the accuracy by computing aggregated and anonymized traffic statistics. Analyzing these event scenarios helps us understand the effect of different routing policies on congestion in the region. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjipEowD8pS0yHrySDZtCCOvA_tvr-dty0jq4kBZLaGdN5U_9zcdH67bW8xkOK1Wd6n3zlwjnm4gyeYkVQD3dannZZ877CNTOfnCt8LqCbWGkWmw8IM_CqMLdg6odPan_uKoQlbAzlkvfk__nPGURcq6SqpGdsUIaJKBX1-fv3M7VFD-kQYT6THUKkRyjF4/s1601/TrafficHeatMap.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;655&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjipEowD8pS0yHrySDZtCCOvA_tvr-dty0jq4kBZLaGdN5U_9zcdH67bW8xkOK1Wd6n3zlwjnm4gyeYkVQD3dannZZ877CNTOfnCt8LqCbWGkWmw8IM_CqMLdg6odPan_uKoQlbAzlkvfk__nPGURcq6SqpGdsUIaJKBX1-fv3M7VFD-kQYT6THUKkRyjF4/s16000/TrafficHeatMap.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Heatmaps demonstrate a substantial increase in numbers of trips in the region after a game as compared to the same time on a non-game day.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiF10JbnM2SHjvrnedtjNB45oN_yi8jN6wEeGyV4zRVnI7eV53aGVdAojwUge-mDG2zFRC_o4RjROXggnY49mtkYWemI11FS9dF4hi73RhAHQKXUYkozX6MXA3o04JkoZ0WYXSyeXu-lgBwbHQIOOMRnI6UpTZoKIBiNlDhk3YxB4ksNQtrD6uaXPgQot0M/s1999/TrafficFlow.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;931&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiF10JbnM2SHjvrnedtjNB45oN_yi8jN6wEeGyV4zRVnI7eV53aGVdAojwUge-mDG2zFRC_o4RjROXggnY49mtkYWemI11FS9dF4hi73RhAHQKXUYkozX6MXA3o04JkoZ0WYXSyeXu-lgBwbHQIOOMRnI6UpTZoKIBiNlDhk3YxB4ksNQtrD6uaXPgQot0M/s16000/TrafficFlow.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;The graph shows observed segment speeds on the x-axis and simulated speeds on the y-axis for a modeled event. The concentration of data points along the red x=y line demonstrates the ability of the simulation to reproduce realistic traffic conditions.&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Routing policies&lt;/h2>; &lt;p>; SDOT and the Seattle Police Department&#39;s (SPD) local knowledge helped us determine the most congested routes that needed improvement: &lt; /p>; &lt;ul>; &lt;li>;Traffic from T-Mobile Park stadium parking lot&#39;s Edgar Martinez Dr. S exit to eastbound I-5 highway / westbound SR 99 highway &lt;/li>; &lt;li>;Traffic through Lumen Field stadium parking lot to northbound Cherry St. I-5 on-ramp &lt;/li>; &lt;li>;Traffic going southbound through Seattle&#39;s SODO neighborhood to S Spokane St. &lt;/li>; &lt;/ul>; &lt;p>; We developed routing policies and evaluated them using the simulation模型。 To disperse traffic faster, we tried policies that would route northbound/southbound traffic from the nearest ramps to further highway ramps, to shorten the wait times. We also experimented with opening HOV lanes to event traffic, recommending alternate routes (eg, SR 99), or load sharing between different lanes to get to the nearest stadium ramps. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_UIXMPB7t5xkh_obQI7KsQe8MXF0RcWk8fmU3g-wh8q5sasXBOhh3L6nB2pgixz6JinYIdCetv0215Xz-GjfLJ3SGTcgVYTALQ5raMDjeIIR-MXXbnly6CNDplcn0vDcqkLG91B1TKRyOHzFQWrZ3K5aMb87EPPrA1PhGmommkDKaKDGuJPDi4Lru9K1x/s1600/NorthboundCherry.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;840&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_UIXMPB7t5xkh_obQI7KsQe8MXF0RcWk8fmU3g-wh8q5sasXBOhh3L6nB2pgixz6JinYIdCetv0215Xz-GjfLJ3SGTcgVYTALQ5raMDjeIIR-MXXbnly6CNDplcn0vDcqkLG91B1TKRyOHzFQWrZ3K5aMb87EPPrA1PhGmommkDKaKDGuJPDi4Lru9K1x/s16000/NorthboundCherry.gif&quot; />;&lt;/a>; &lt;br />; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2TMCOc-7sYPAHOoFvwEcRlAACKZjelEvcYajm7EgoPS4il1TxabdxuOTjNZ81N0bXurNwpq_w9i-U0wupOjHnES3A0uRPwiTO1KpmI1PDffBOZt4rxagHmwra3hweXVtBx3NRPxZ7VSw75NsygwD2ijowkTSUUATiOUhlEAIVJ2W3EmoYKGE1LmPIdHds/s1600/SouthboundSpokane.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;837&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2TMCOc-7sYPAHOoFvwEcRlAACKZjelEvcYajm7EgoPS4il1TxabdxuOTjNZ81N0bXurNwpq_w9i-U0wupOjHnES3A0uRPwiTO1KpmI1PDffBOZt4rxagHmwra3hweXVtBx3NRPxZ7VSw75NsygwD2ijowkTSUUATiOUhlEAIVJ2W3EmoYKGE1LmPIdHds/s16000/SouthboundSpokane.gif&quot; />;&lt;/a>;&lt;/div>; &lt;h2>;Evaluation results&lt;/h2>; &lt;p>; We model multiple events with different traffic conditions, event times, and attendee counts. For each policy, the simulation reproduces post-game traffic and reports the travel time for vehicles, from departing the stadium to reaching their destination or leaving the Seattle SODO area. The time savings are computed as the difference of travel time before/after the policy, and are shown in the below table, per policy, for small and large events. We apply each policy to a percentage of traffic, and re-estimate the travel times. Results are shown if 10%, 30%, or 50% of vehicles are affected by a policy. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbtXIUDzRNBrnUuMOh1uGJsM85brY5Z5q37x87YVaJngDk-5hY5YAGduh7x-K1suIbpEc1E1CKzvo67pdIRgP1pCGL1iGQlCuOiVT2zRcMI-ab0ABBhI2-3tABYfpfjcD6ai2XjsUjKusOqAFHSMO7iT7XLgFEsdheSL2lbtEpvToeC23gv6oLzidPT6X3/s1252/TrafficImprovement.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;916&quot; data-original-width=&quot;1252&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbtXIUDzRNBrnUuMOh1uGJsM85brY5Z5q37x87YVaJngDk-5hY5YAGduh7x-K1suIbpEc1E1CKzvo67pdIRgP1pCGL1iGQlCuOiVT2zRcMI-ab0ABBhI2-3tABYfpfjcD6ai2XjsUjKusOqAFHSMO7iT7XLgFEsdheSL2lbtEpvToeC23gv6oLzidPT6X3/s16000/TrafficImprovement.png&quot; />;&lt;/a>;&lt;/div>; &lt;p>;Based on these simulation results, the feasibility of implementation, and other considerations, SDOT has decided to implement the “Northbound Cherry St ramp” and “Southbound S Spokane St ramp” policies using DMS during large events. The signs suggest drivers take alternative routes to reach their destinations. The combination of these two policies leads to an average of 7 minutes of travel time savings per vehicle, based on rerouting 30% of traffic during large events. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; This work demonstrates the power of simulations to model, identify, and quantify the effect of proposed traffic guidance policies. Simulations allow network planners to identify underused segments and evaluate the effects of different routing policies, leading to a better spatial distribution of traffic. The offline modeling and online testing show that our approach can reduce total travel time. Further improvements can be made by adding more traffic management strategies, such as optimizing traffic lights. Simulation models have been historically time consuming and hence affordable only for the largest cities and high stake projects. By investing in more scalable techniques, we hope to bring these models to more cities and use cases around the world. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;In collaboration with Alex Shashko, Andrew Tomkins, Ashley Carrick, Carolina Osorio, Chao Zhang, Damien Pierce, Iveel Tsogsuren, Sheila de Guia, and Yi-fan Chen. Visual design by John Guilyard. We would like to thank our SDOT partners Carter Danne, Chun Kwan, Ethan Bancroft, Jason Cambridge, Laura Wojcicki, Michael Minor, Mohammed Said, Trevor Partap, and SPD partners Lt. Bryan Clenna and Sgt. Brian Kokesh.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1587965303727576226/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/simulations-illuminate-path-to-post.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1587965303727576226&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1587965303727576226&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/simulations-illuminate-path-to-post.html&quot; rel=&quot;alternate&quot; title=&quot;Simulations illuminate the path to post-event traffic flow&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj35HCKLS0slKFX6LVrCCK0crWWJ-YwNVxNkDqze9UpfuVTWfD7URw9CyRwyFwAdIT-CHG59vl19KgfrGWEtoufCS6SQOzLuV1n7SYpun6EOAML0MfdxwjGhDKyKrfIz2t6Ivv_T07YVCPlA7uQMmPr8LYrXPSdE3V1WAwOwU0DYR_8wMWMJZh7MLeshXeP/s72-c/TrafficFlow.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6088118107306075362&lt;/id>;&lt;published>;2023-12-15T14:34:00.000-08:00&lt;/published>;&lt;updated>;2023-12-15T14:34:10.381-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Kaggle&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TPU&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Advancements in machine learning for machine learning&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Phitchaya Mangpo Phothilimthana, Staff Research Scientist, Google DeepMind, and Bryan Perozzi, Senior Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8VzlEwBVhUqzyMz9pok3Vx0ft_8X_IZyvjpiFtX7PewhMGRzI6UmfEtUKSQ_kiO7CW8-KQ1OmDujYmHpwLvRndQFqNnmQJnEgnMo5Gj5tJi5aVnscmlo7vbHFFhS8l-yMoIICJLm80V9EPpyIObG-07Cw_VYjzWNfaZ0E_vp4f8v_WGpn1lU8F3LsJHIx/s1600/Screenshot%202023-12-15%20at%202.33.10%E2%80%AFPM.png&quot; style=&quot;display: none;&quot; />; &lt;p>; With the recent and accelerated advances in machine learning (ML), machines can &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf&quot;>;understand natural language&lt;/a>;, &lt;a href=&quot;https://blog.google/technology/ai/lamda/&quot;>;engage in conversations&lt;/a>;, &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview&quot;>;draw images&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;>;create videos&lt;/a>; and more. Modern ML models are programmed and trained using ML programming frameworks, such as &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;, &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;, &lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>;, among many others. These libraries provide high-level instructions to ML practitioners, such as linear algebra operations (eg, matrix multiplication, convolution, etc.) and neural network layers (eg, &lt;a href=&quot;https://keras.io/api/layers/convolution_layers/convolution2d/&quot;>;2D convolution layers&lt;/a>;, &lt;a href=&quot;https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/&quot;>;transformer layers&lt;/a>;). Importantly, practitioners need not worry about how to make their models run efficiently on hardware because an ML framework will automatically optimize the user&#39;s model through an underlying &lt;em>;compiler&lt;/em>;. The efficiency of the ML workload, thus, depends on how good the compiler is. A compiler typically relies on heuristics to solve complex optimization problems, often resulting in suboptimal performance. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this blog post, we present exciting advancements in ML for ML. In particular, we show how we use ML to improve efficiency of ML workloads! Prior works, both internal and external, have shown that we can use ML to improve performance of ML programs by selecting better ML compiler decisions. Although there exist a few datasets for program performance prediction, they target small sub-programs, such as basic blocks or kernels. We introduce “&lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs&lt;/a>;” (presented at &lt;a href=&quot;https://nips.cc/Conferences/2023&quot;>;NeurIPS 2023&lt;/a>;), which we recently released to fuel more research in ML for program optimization. We hosted a &lt;a href=&quot;https://www.kaggle.com/competitions/predict-ai-model-runtime/overview&quot;>;Kaggle competition&lt;/a>; on the dataset, which recently completed with 792 participants on 616 teams from 66 countries. Furthermore, in “&lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;Learning Large Graph Property Prediction via Graph Segment Training&lt;/a>;”, we cover a novel method to scale &lt;a href=&quot;https://arxiv.org/abs/2005.03675&quot;>;graph neural network&lt;/a>; (GNN) training to handle large programs represented as graphs. The technique both enables training arbitrarily large graphs on a device with limited memory capacity and improves generalization of the model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ML compilers&lt;/h2>; &lt;p>; ML compilers are software routines that convert user-written programs (here, mathematical instructions provided by libraries such as TensorFlow) to executables (instructions to execute on the actual hardware). An ML program can be represented as a computation graph, where a node represents a tensor operation (such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;matrix multiplication&lt;/a>;), and an edge represents a tensor flowing from one node to another. ML compilers have to solve many complex optimization problems, including &lt;em>;graph-level &lt;/em>;and &lt;em>;kernel-level&lt;/em>; optimizations. A graph-level optimization requires the context of the entire graph to make optimal decisions and transforms the entire graph accordingly. A kernel-level optimization transforms one kernel (a fused subgraph) at a time, independently of other kernels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;465&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Important optimizations in ML compilers include graph-level and kernel-level optimizations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To provide a concrete example, imagine a &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_(mathematics)&quot;>;matrix&lt;/a>; (2D tensor): &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;290&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; It can be stored in computer memory as [ABC abc] or [A a B b C c], known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Row-_and_column-major_order&quot;>;row- and column-major memory layout&lt;/a>;, respectively. One important ML compiler optimization is to assign memory layouts to all intermediate tensors in the program. The figure below shows two different layout configurations for the same program. Let&#39;s assume that on the left-hand side, the assigned layouts (in red) are the most efficient option for each individual operator. However, this layout configuration requires the compiler to insert a &lt;em>;copy&lt;/em>; operation to transform the memory layout between the &lt;em>;add&lt;/em>; and &lt;em>;convolution&lt;/em>;运营。 On the other hand, the right-hand side configuration might be less efficient for each individual operator, but it doesn&#39;t require the additional memory transformation. The layout assignment optimization has to trade off between local computation efficiency and layout transformation overhead. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;343&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A node represents a tensor operator, annotated with its output tensor shape [&lt;em>;n&lt;sub>;0&lt;/sub>;&lt;/em>;, &lt;em>;n&lt;sub>;1&lt;/sub>;&lt;/em>;, ...], where &lt;em>;n&lt;sub>;i &lt;/sub>;&lt;/em>;is the size of dimension &lt;em>;i&lt;/em>;. Layout {&lt;em>;d&lt;sub>;0&lt;/sub>;&lt;/em>;, &lt;em>;d&lt;sub>;1&lt;/sub>;&lt;/em>;, ...} represents minor-to-major ordering in memory. Applied configurations are highlighted in red, and other valid configurations are highlighted in blue. A layout configuration specifies the layouts of inputs and outputs of influential operators (ie, convolution and reshape). A copy operator is inserted when there is a layout mismatch.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; If the compiler makes optimal choices, significant speedups can be made. For example, we have seen &lt;a href=&quot;https://ieeexplore.ieee.org/document/9563030&quot;>;up to a 32% speedup&lt;/a>; when choosing an optimal layout configuration over the default compiler&#39;s configuration in the &lt;a href=&quot;https://www.tensorflow.org/xla&quot;>;XLA&lt;/a>; benchmark suite. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TpuGraphs dataset&lt;/h2>; &lt;p>; Given the above, we aim to improve ML model efficiency by improving the ML compiler. Specifically, it can be very effective to equip the compiler&lt;strong>; &lt;/strong>;with a &lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;learned cost model&lt;/a>;&lt;strong>; &lt;/strong>;that takes in an input program and compiler configuration and then outputs the predicted runtime of the program. &lt;/p>; &lt;p>; With this motivation, we &lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;release TpuGraphs&lt;/a>;, a dataset for learning cost models for programs running on Google&#39;s custom &lt;a href=&quot;https://cloud.google.com/tpu/docs/intro-to-tpu&quot;>;Tensor Processing Units&lt;/a>; (TPUs). The dataset targets two XLA compiler configurations: &lt;em>;layout&lt;/em>; (generalization of row- and column-major ordering, from matrices, to higher dimension tensors) and &lt;em>;tiling&lt;/em>; (configurations of tile sizes) 。 We provide download instructions and starter code on the &lt;a href=&quot;https://github.com/google-research-datasets/tpu_graphs&quot;>;TpuGraphs GitHub&lt;/a>;.数据集中的每个示例都包含ML工作负载的计算图，编译配置以及与配置编译时图的执行时间。数据集中的图是从开源ML程序中收集的，具有流行的模型体系结构，例如，&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>; resnet &lt;/a>;，&lt;a href =“ https://arxiv.org/abs/1905.11946&quot;>; effficitynet &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>; mask r-cnnn &lt;/a>;，和&lt;a hreff =“ https://arxiv.org/abs/1706.03762”>;变压器&lt;/a>;。该数据集提供的图形比最大的（早期）属性预测数据集（具有可比的图形大小）多25×，并且与ML程序上的现有性能预测数据集相比，图的平均值为770×。通过这种大大扩展的规模，我们首次可以在大图上探索图形级别的预测任务，这受到挑战，例如可扩展性，训练效率和模型质量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s2868/image18.png “ style =”边距 - 左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Original-height =“ 1546” data-Original-width =“ 2868” src =“ https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s16000/image18.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-字幕“样式=”文本align：中心;“>;与其他图形属性预测数据集相比。我们的数据集（如下所示的架构）。我们的基线模型基于GNN，因为输入程序表示为图形。节点特征，以下面的蓝色显示，由两个部分组成。第一部分是&lt;em>; opcode ID &lt;/em>;，这是节点的最重要信息，它指示张量操作的类型。因此，我们的基线模型通过嵌入查找表将OpCode ID映射到&lt;em>; opcode嵌入&lt;/em>;。然后将OpCode嵌入与第二部分（将其余的节点特征）串联为GNN的输入。我们结合了GNN生成的节点嵌入，以使用简单的图池降低（即，总和和均值）创建图形的固定尺寸嵌入。然后，将所得的图形嵌入线性通过前馈层线性转换为最终标量输出。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s2284/image20.png&quot; imageanchor =“ 1” style =“边距左：auto; margin-right：auto;“>; &lt;img border =“ 0” data-eriginal-height =“ 1106” data-Original-width =“ 2284” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s16000/image20.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：center;“>;我们的基线学习成本模型使用GNN，因为程序可以自然表示为图形。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;此外，我们提出&lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;图形段训练&lt;/a>;（GST），一种用于缩放GNN训练以在设备上处理大图的方法预测任务在整个图表上（即图形级预测）的情况。与节点或边缘级预测的缩放训练不同，图形预测的缩放量表已研究，但对我们的域至关重要，因为计算图可以包含数十万个节点。在典型的GNN训练（下面左侧的“完整图形训练”）中，使用整个图进行训练，这意味着该图的所有节点和边缘都用于计算梯度。对于大图，这可能在计算上是不可行的。在GST中，将每个大图分区分为较小的段，并选择一个随机子集以更新模型。其余部分的嵌入是生产的，而不会保存其中间激活（以避免消耗记忆）。然后将所有片段的嵌入组合在一起以生成原始大图的嵌入，然后用于预测。此外，我们介绍了历史嵌入式表，以有效地获得图形段的嵌入和段辍学，以减轻历史嵌入的陈旧性。一起，我们的完整方法将端到端的训练时间加快了3倍。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s790/image2.png&quot; style=&quot;边距 - 左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Original-height =“ 434” data-Original-width =“ 790” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>;比较完整的图形训练（典型方法）vs图段训练（我们提出的方法）。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =” line-height：40％ ;“>; &lt;br />; &lt;/div>; &lt;h2>; kaggle竞赛&lt;/h2>; &lt;p>;最后，我们运行了“ &lt;a href =” https://kaggle.com/competitions/competitions/predict-ai-ai-model--运行时“>;快速还是慢？预测AI模型运行时&lt;/a>;”竞争TPUGRAPH数据集。这场比赛以616支球队的792名参与者结束。我们有来自66个国家 /地区的10507份意见书。对于153位用户（包括前100名中的47位），这是他们的第一次比赛。我们学到了参加参与团队采用的许多有趣的新技术，例如：&lt;/p>; &lt;ul>; &lt;li>; &lt;em>;图形修剪/压缩&lt;/em>;：而不是使用GST方法，许多团队以不同的方式进行了实验。要压缩大图（例如，仅保留包括可配置节点及其直接邻居的子图）。 &lt;/li>; &lt;li>; &lt;em>;特征填充值&lt;/em>;：某些团队观察到默认填充值0是有问题的，因为0与有效的特征值发生冲突，因此使用-1的填充值可以改善模型的准确性显着。 &lt;/li>; &lt;li>; &lt;em>;节点功能&lt;/em>;：某些团队观察到其他节点功能（例如&lt;a href=&quot;https://www.tensorflow.org/xla/xla/operation_semantics#dot&quot;>; dot将军的承包维度&lt;/a>;）很重要。一些团队发现，节点功能的不同编码也很重要。 &lt;/li>; &lt;li>; &lt;em>;交叉构造注意力&lt;/em>;：一个获胜的团队设计了一个简单的层，允许模型明确地“比较”配置相互对抗。该技术显示出比让每个配置分别推断的模型要好得多。 &lt;/li>; &lt;/ul>; &lt;p>;我们将在&lt;a href=&quot;https://mlforsystems.org/&quot;>; ML用于Systems Workshop &lt;/a>;的竞赛中汇报比赛并预览竞赛的获奖解决方案。在2023年12月16日的Neurips上。最后，祝贺所有获奖者，并感谢您为推进ML的系统研究所做的贡献！ &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; Neurips expo &lt;/h2>; &lt;p>;如果您对有关结构化数据和人造的更多研究感兴趣情报，我们主持了神经博览会面板&lt;a href=&quot;https://nips.cc/expo/conferences/2023/2023/talk%20panel/78252&quot;>;图形学习符合人工智能&lt;/a>; 12月9日学习的成本模型等等！ &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;p>; &lt;em>; sami abu-el-el-haija（Google Research）为这项工作和写作做出了重大贡献。本文中的研究描述了与Mike Burrows，Kaidi Cao，Bahare Fatemi，Jure Leskovec，Charith Mendis，Charith Mendis，Dustin Zelle和Yanqi Zhou。 =“ http://blog.research.google/feeds/6088118107307306075362/comments/comments/default/default” rel =“ requeies” title =“ post comment” type =“ application/atom+xml” /blog.research.google/2023/12/advancements-in-machine-learning-for.html#comment-form“ rel =” re =“ reversies” title =“ 0注释” type =“ text/html”/>; &lt;link link href href href =“ http://www.blogger.com/feeds/84749263331452026626/posts/posts/default/60881181181073075362” rel =“ rel =” rel =“ edit” .com/feeds/847492633145202626/posts/default/6088118107306075362“ rel =” rel =“ self” type =“ application/application/application/application/atom+xml”/>; &lt;link href = href href =“机载式学习for.html“ rel =“替代” title =“机器学习中的进步机器学习” type =“ text/html”/>; &lt;ause>; &lt;nuce>; &lt;names>; google ai &lt;/name &lt;/name>; &lt;uri>;>; http://www.blogger.com/profile/1209862651477775266161&lt;/uri>; /g/2005＃缩略图“ src =” https://img1.blogblog.com/img/b16-rounded.gif.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/rutig>; &lt;/rution>; &lt;媒体>; &lt;媒体：缩略图高度= &quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8VzlEwBVhUqzyMz9pok3Vx0ft_8X_IZyvjpiFtX7PewhMGRzI6UmfEtUKSQ_kiO7CW8-KQ1OmDujYmHpwLvRndQFqNnmQJnEgnMo5Gj5tJi5aVnscmlo7vbHFFhS8l-yMoIICJLm80V9EPpyIObG-07Cw_VYjzWNfaZ0E_vp4f8v_WGpn1lU8F3LsJHIx/s72-c/Screenshot%202023-12-15%20at%202.33.10%E2% 80％afpm.png“ width =” 72“ xmlns：媒体=” http://search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr>; &lt;thr：thr：thr>; 0 &lt;/thr：thr>; thr>; thr>; &lt;/thr>; &lt;/thr条目>; &lt;enter>; &lt;id>;标签：Blogger.com，1999：Blog-8474926331452026626.POST-6479608460460479432217 &lt;/id>; &lt;/id>; &lt;Publined>; 2023-12-15T11：40：40：00.000-08：00.000-08：00.000-08：008：00 &lt;/出版>; &lt;更新>; &lt;更新>; &lt;更新>; &lt;更新>; 2023-12-15T12：18：41.742-08：00 &lt;/updated>; &lt;category scheme =“ http://www.blogger.com/atom/atom/ns#” scheme =“ http://www.blogger.com/atom/ns#” term =“ neurips”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term = “样式传输”>; &lt;/category>; &lt;title type =“ text”>; StyledRop：任何样式中的文本到图像生成&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>; Posted by Kihyuk Sohn and Dilip Krishnan, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bqg9UcqVUFC-uKMbsFvxRmRWkokKR4VinfUZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9tO_P8/s1600/StyleDrop% 20 Hero.gif“ style =” display：none;” />; &lt;p>;经过大量图像文本对训练的文本对图像模型已使创建丰富而多样的图像，包括许多流派和主题。此外，当添加到输入文本提示符中时，诸如“动漫”或“ Steampunk”之类的流行样式可能会转化为特定的视觉输出。尽管许多努力已被纳入&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/prompt_engineering&quot;>;及时工程&lt;/a>;，由于这种样式很难以文本形式描述，因此很难描述配色方案，照明和其他特征的细微差别。例如，“水彩画”可能是指各种样式，并且使用简单地说“水彩绘画样式”的文本提示可能会导致一种特定的风格或几种无法预测的组合。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ Margin-Left：auto ; margin-right：auto;“>; &lt;tbody>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https：//blogger.googleusercercercercercercercorcercontent.com/img/r29vz2x2xxl/avz2xl/avvxssejni6hi6hhhwwwwwtzqf5zqf5zqufdzqudprhm qudprhy n1pvvvijagzsekbsltocf6qm2m2m2m0lgrtnkqxn -7QJC7ISZ1KEA8PM4_ADYUZYWPXLB3H7W4H5P5X3F7Y_A21ULNQWCEAZQ-EX6YOLCDZACL0UMK-EHQZVMUZVMUZVMUZ0AZHGMUZHG9NVXXXXXXXXXXXXB52GW-ZKKZ/SERON-SEROL-SERON-SERON-nire Marrogin-nife and and.f.f.iff：nargor.ft.f.iff： -right：auto;“>; &lt;img border =“ 0” data-original-height = &quot;403&quot; data-original-width=&quot;959&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjNi6hwWTqFdzKvJ9qxXf5ppyfhixq1qERPyXZqBmdrPVP4ObR94N1pVVIJAGZseKbsLtOcf6qm2M0LgRtNKqXN-7qjC7Isz1keA8Pm4_ADYuzywpXLb3h7W4H5p5X3f7Y_A21uLNQWceazq-Ex6YOLCDzacl0Umk-EhqZvMUz0aZHG9nvxxb52gw-zKz/s16000/image5.gif&quot; />;&lt; /a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;当我们指的是“水彩绘画样式”时，我们的意思是什么？ StyleDrop不用指定自然语言的样式，而是通过参考样式参考图像&lt;sup ID =“ fnref1”>; &lt;a href=&quot;#fn1&quot; rel=&quot; rel=&quot; footnote&quot;>; &lt; SPAN style =“ font-size：x-small;”>;*&lt;/span>; &lt;/a>; &lt;/sup>;。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;在此博客中我们介绍“ &lt;a href=&quot;https://arxiv.org/abs/2306.00983&quot;>; StyledLop：以任何样式的text-to-to-image生成&lt;/a>; &lt;/a>; &lt;/a>;”，该工具允许具有明显更高的风格化文本 - 图像合成。 Styledrop并没有寻求文本提示来描述样式，而是使用一种或多种样式&lt;em>;参考图像&lt;/em>;来描述文本到图像生成的样式。通过这样做，StyledRop可以以与参考相一致的样式产生图像，同时有效地避免了文本及时工程的负担。这是通过有效地通过&lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;>; Adapter Tuning &lt;/a>;在几种样式上通过&lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf/1902.00751.pdf/1902.00751.参考图像。此外，通过在其生成的一组图像上迭代微调StyleDrop，它可以从文本提示中实现样式符合图像的生成。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;方法概述&lt;/h2>; &lt;p>; StyledRop是一种文本到图像生成模型，允许生成视觉样式与用户提供的样式参考图像一致的图像。这是通过预先训练的文本对图像生成模型的参数有效微调的几次迭代来实现的。具体来说，我们在&lt;a href=&quot;https://muse-model.github.io/&quot;>; muse &lt;/a>;上构建了StyleDrop。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h3>;缪斯：文本到图像生成视觉变压器&lt;/h3>; &lt;p>; &lt;p>; &lt;a href =&#39; https://muse-model.github.io/&quot;>; muse &lt;/a>;是一种基于掩盖的生成图像变压器的state-of-the-tex-art Text-to-image生成模型（&lt;a href =“ https：https：https：https：https：https：https： //openaccess.thecvf.com/content/cvpr2022/papers/chang_maskgit_masked_generative_generative_image_image_transformer_cvpr_2022_2022_paper.pdf&quot;>; maskgit &lt;/a>;）。与扩散模型不同，例如&lt;a href=&quot;https://imagen.research.google/&quot;>; imagen &lt;/a>;或&lt;a href=&quot;https://github.com/compvis/compvis/stable-diffusion&quot;>;稳定扩散&lt;/a>;，缪斯将图像代表为一系列离散令牌，并使用变压器体系结构对其分布进行建模。与扩散模型相比，众所周知，缪斯在达到竞争性发电质量的同时更快。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h3>;参数效率适配器调整&lt;/h3>; &lt;p>; styledrop是通过微调pre-pre-pre-构建的受过一些样式参考图像及其相应文本提示的训练有素的缪斯模型。关于变压器的参数效率微调有许多作品，包括&lt;a href=&quot;https://arxiv.org/abs/2104.08691&quot;>;提示调谐&lt;/a>;和&lt;a href =“ https：// https：/// arxiv.org/abs/2106.09685&quot;>; low-rank适应&lt;/a>;（lora）大语言模型。其中，我们选择适配器调整，该调整可有效地对&lt;a href=&quot;https://arxiv.org/pdf/pdf/1902.00751.pdf&quot;>;语言&lt;/a>;和语言&lt;/a>;和&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf&quot;>;image generation&lt;/a>; tasks in a parameter-efficient manner.例如，它引入了少于100万个可训练的参数来微调3B参数的缪斯群，并且仅需要1000个培训步骤即可进行融合。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6so7Abf52d3Kyu-77Di6YvRncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s1632/image2.png “ ImageAnchor =” 1“样式=” Margin-Left：Auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 1152” data-Original-width =“ 1632” SRC = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6so7Abf52d3Kyu-77Di6YvRncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Parameter-efficient adapter tuning of Muse.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height :40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Iterative training with feedback&lt;/h3>; &lt;p>; While StyleDrop is effective at learning styles from a few style reference images, it is still challenging to learn from a single style reference image. This is because the model may not effectively disentangle the &lt;em>;content &lt;/em>;(ie, what is in the image) and the &lt;em>;style&lt;/em>; (ie, how it is being presented), leading to reduced &lt;em>;text controllability&lt;/em>; in generation. For example, as shown below in Step 1 and 2, a generated image of a chihuahua from StyleDrop trained from a single style reference image shows a leakage of content (ie, the house) from the style reference image. Furthermore, a generated image of a temple looks too similar to the house in the reference image (concept collapse). &lt;/p>; &lt;p>; We address this issue by training a new StyleDrop model on a subset of synthetic images, chosen by the user or by image-text alignment models (eg, &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;), whose images are generated by the first round of the StyleDrop model trained on a single image. By training on multiple synthetic image-text aligned images, the model can easily disentangle the style from the content, thus achieving improved image-text alignment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw416CwaV4R3GvkMSa3CVubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s1295/image3.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;825&quot; data-original-width=&quot;1295&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw416CwaV4R3GvkMSa3CVubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Iterative training with feedback&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>;. The first round of StyleDrop may result in reduced text controllability, such as a content leakage or concept collapse, due to the difficulty of content-style disentanglement. Iterative training using synthetic images, generated by the previous rounds of StyleDrop models and chosen by human or image-text alignment models, improves the text adherence of stylized text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;StyleDrop gallery&lt;/h3>; &lt;p>; We show the effectiveness of StyleDrop by running experiments on 24 distinct style reference images. As shown below, the images generated by StyleDrop are highly consistent in style with each other and with the style reference image, while depicting various contexts, such as a baby penguin, banana, piano, etc. Moreover, the model can render alphabet images with一致的风格。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2ldPyRTLwUFp8tDyQgt3EQTl8g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/s1632/image6.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1632&quot; data-original-width=&quot;1536&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2ldPyRTLwUFp8tDyQgt3EQTl8g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/w602-h640/image6.gif&quot; width=&quot;602&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stylized text-to-image generation. Style reference images&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>; are on the left inside the yellow box. Text prompts used are:&lt;br>; First row: a baby penguin, a banana, a bench.&lt;br>; Second row: a butterfly, an F1 race car, a Christmas tree.&lt;br>; Third row: a coffee maker, a hat, a moose.&lt;br>; Fourth row: a robot, a towel, a wood cabin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpAyPPdjBT2pS6Bgq2CAppCGBU-sw4QmUpDrN-1lrlpmtxCBRIN3AQN07IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s1526/image1.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;767&quot; data-original-width=&quot;1526&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpAyPPdjBT2pS6Bgq2CAppCGBU-sw4QmUpDrN-1lrlpmtxCBRIN3AQN07IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stylized visual character generation. Style reference images&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>; are on the left inside the yellow box. Text prompts used are: (first row) letter &#39;A&#39;, letter &#39;B&#39;, letter &#39;C&#39;, (second row) letter &#39;E&#39;, letter &#39;F&#39;, letter &#39;G&#39;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Generating images of my object in my style&lt;/h3>; &lt;p>; Below we show generated images by sampling from two personalized generation distributions, one for an object and another for the style. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lrXSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurRp8-rmsqoLkj-oXWqi4xe4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s1006/image8.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;412&quot; data-original-width=&quot;1006&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lrXSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurRp8-rmsqoLkj-oXWqi4xe4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s16000/image8.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images at the top in the blue border are object reference images from the DreamBooth dataset (teapot, vase, dog and cat), and the image on the left at the bottom in the red border is the style reference image*. Images in the purple border (ie the four lower right images) are generated from the style image of the specific object.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Quantitative results&lt;/h3>; &lt;p>; For the quantitative evaluation, we synthesize images from a subset of &lt;a href=&quot;https://github.com/google-research/parti/blob/main/PartiPrompts.tsv&quot;>;Parti prompts&lt;/a>; and measure the &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;image-to-image CLIP score&lt;/a>; for style consistency and &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;image-to-text CLIP score&lt;/a>; for text consistency. We study non–fine-tuned models of Muse and Imagen. Among fine-tuned models, we make a comparison to &lt;a href=&quot;https://dreambooth.github.io/&quot;>;DreamBooth&lt;/a>; on &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, state-of-the-art personalized text-to-image method for subjects. We show two versions of StyleDrop, one trained from a single style reference image, and another, “StyleDrop (HF)”, that is trained iteratively using synthetic images with human feedback as described above. As shown below, StyleDrop (HF) shows significantly improved style consistency score over its non–fine-tuned counterpart (0.694 vs. 0.556), as well as DreamBooth on Imagen (0.694 vs. 0.644). We observe an improved text consistency score with StyleDrop (HF) over StyleDrop (0.322 vs. 0.313). In addition, in a human preference study between DreamBooth on Imagen and StyleDrop on Muse, we found that 86% of the human raters preferred StyleDrop on Muse over DreamBooth on Imagen in terms of consistency to the style reference image. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiID8cTIDI32_6aRRoVhwxHDBJnPrnk_etBx904nBw0kFBDD6z99ttJqzz2574I-irFJ4_5dg0xHPqHPdpgeFi68Tl1gu4pciiChyphenhyphenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCIcKNpmhiJNvYDvI4ux5/s2980/image16.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;2980&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiID8cTIDI32_6aRRoVhwxHDBJnPrnk_etBx904nBw0kFBDD6z99ttJqzz2574I-irFJ4_5dg0xHPqHPdpgeFi68Tl1gu4pciiChyphenhyphenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCIcKNpmhiJNvYDvI4ux5/s16000/image16.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; StyleDrop achieves style consistency at text-to-image generation using a few style reference images. Google&#39;s AI Principles guided our development of Style Drop, and we urge the responsible use of the technology. StyleDrop was adapted to &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/image/fine-tune-style&quot;>;create a custom style model in Vertex AI&lt;/a>;, and we believe it could be a helpful tool for art directors and graphic designers — who might want to brainstorm or prototype visual assets in their own styles, to improve their productivity and boost their creativity — or businesses that want to generate new media assets that reflect a particular brand. As with other generative AI capabilities, we recommend that practitioners ensure they align with copyrights of any media assets they use. More results are found on our &lt;a href=&quot;https://styledrop.github.io/&quot;>;project website&lt;/a>; and &lt;a href=&quot;https://youtu.be/gNHD0_hkJ9A&quot;>;YouTube video&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip Krishnan. &lt;/em>;We thank owners of images used in our experiments (&lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/main/images/assets/data.md&quot;>;links&lt;/a>; for attribution) for sharing their valuable assets. &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt; sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;*&lt;/b>;&lt;/a>;&lt;/sup>;See &lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/ main/images/assets/data.md&quot;>;image sources&lt;/a>;&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt; /p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6479608460479432217/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://blog.research.google/2023/12/styledrop-text-to-image-generation-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// blog.research.google/2023/12/styledrop-text-to-image-generation-in.html&quot; rel=&quot;alternate&quot; title=&quot;StyleDrop: Text-to-image generation in any style&quot; type=&quot;text/html &quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd: image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16 &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bqg9UcqVUFC-uKMbsFvxRmRWkokKR4VinfUZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9tO_P8/s72- c/StyleDrop%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8223613466421639113&lt;/id>;&lt;published>;2023-12-10T06:11:00.000-08:00&lt;/published>;&lt; updated>;2023-12-13T14:11:22.885-08:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Google at NeurIPS 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline- author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s1100/google_research_sticker-hero.jpg “样式=“显示：无；” />; &lt;p>; This week the 37th annual &lt;a href=&quot;https://neurips.cc/Conferences/2023&quot;>;Conference on Neural Information Processing Systems&lt;/a>; (NeurIPS 2023), the biggest machine learning conference of the year, kicks off in New Orleans, LA. Google is proud to be a &lt;a href=&quot;https://neurips.cc/Conferences/2023/Sponsors&quot;>;Diamond Level sponsor&lt;/a>; of NeurIPS this year and will have a strong presence with &amp;gt;170 accepted papers, two keynote talks, and additional contributions to the broader research community through organizational support and involvement in &amp;gt;20 workshops and tutorials. Google is also proud to be a Platinum Sponsor for both the &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66602&quot;>;Women in Machine Learning&lt;/a>; and &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66607&quot;>;LatinX in AI&lt;/a>; workshops. We look forward to sharing some of our extensive ML research and expanding our partnership with the broader ML research community. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Attending for NeurIPS 2023 in person? Come visit the Google Research booth to learn more about the exciting work we&#39;re doing to solve some of the field&#39;s most interesting challenges. Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions). &lt;/p>; &lt;p>; You can learn more about our latest cutting edge work being presented at the conference in the list below (Google affiliations highlighted in &lt;strong>;bold&lt;/strong>;). And see &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023&quot;>;Google DeepMind&#39;s blog&lt;/a>; to learn more about their participation at NeurIPS 2023. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board &amp;amp; Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; NeurIPS Board: &lt;strong>;Corinna Cortes&lt;/strong>;&lt;br />; Advisory Board: &lt;strong>;John C. Platt&lt;/strong>;&lt;br />; Senior Area Chair: &lt;strong>;Inderjit S. Dhillon&lt;/strong>;&lt;br />; Creative AI Chair: &lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; Program Chair: &lt;strong>;Amir Globerson&lt;/strong>;&lt;br />; Datasets and Benchmarks Chair:&lt;b>; Remi Denton&lt;/b>;&lt;br />;Expo Chair: &lt;b>;Wenming Ye&lt;/b>;&lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research Booth Demo/Q&amp;amp;A Schedule&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;h4>;This schedule is subject to change. Please visit the Google booth (#215) for more information.&lt;/h4>; &lt;p>; What You See is What You Read? Improving Text-Image Alignment Evaluation&lt;br />; Presenter: &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Monday, Dec 11 | &lt;em>;12:15PM - 1:45PM&lt;/em>; &lt;/p>; &lt;p>; Talk like a Graph: Encoding Graphs for Large Language Models&lt;br />; Presenters: &lt;strong>;Bahar Fatemi&lt;/strong>;,&lt;strong>; Jonathan Halcrow&lt;/strong>;,&lt;strong>; Bryan Perozzi&lt;/strong>;&lt;br />; Monday, Dec 11 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use&lt;br />; Presenter: &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Monday, Dec 11 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; MLCommons Croissant&lt;br />; Presenters: &lt;strong>;Omar Benjelloun&lt;/strong>;,&lt;strong>; Meg Risdal&lt;/strong>;, &lt;strong>;Lora Aroyo&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model&lt;br />; Presenter: &lt;strong>;Xiuye Gu&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Embedding Large Graphs&lt;br />; Presenters: &lt;strong>;Bryan Perozzi&lt;/strong>;, &lt;strong>;Anton Tsitsulin&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; Correlated Noise Provably Beats Independent Noise for Differentially Private Learning&lt;br />; Presenter: &lt;strong>;Krishna Pillutla&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; Med-PaLM&lt;br />; Presenter: &lt;strong>;Tao Tu&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; StyleDrop: Text-to-Image Generation in Any Style&lt;br />; Presenters: &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; DICES Dataset: Diversity in Conversational AI Evaluation for Safety&lt;br />; Presenters: &lt;strong>;Lora Aroyo&lt;/strong>;, &lt;strong>;Alicia Parrish&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; Resonator: Scalable Game-Based Evaluation of Large Models&lt;br />; Presenters: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Michal Todorovic&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Adversarial Nibbler&lt;br />; Presenter: &lt;strong>;Lora Aroyo&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Towards Generalist Biomedical AI&lt;br />; Presenter: &lt;strong>;Tao Tu&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; Conditional Adaptors&lt;br />; Presenter: &lt;strong>;Junwen Bai&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; Patient Assistance with Multimodal RAG&lt;br />; Presenters: &lt;strong>;Ryan Knuffman&lt;/strong>;, &lt;strong>;Milica Cvetkovic&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;p>; How Hessian Structure Explains Mysteries in Sharpness Regularization&lt;br />; Presenter: &lt;strong>;Hossein Mobahi&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Keynote Speakers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73993&quot;>;The Many Faces of Responsible AI&lt;/a>;&lt;br />; Speaker: &lt;strong>;Lora Aroyo&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73987&quot;>;Sketching: Core Tools, Learning-Augmentation, and Adaptive Robustness&lt;/a>;&lt;br />; Speaker: &lt;strong>;Jelani Nelson&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Affinity Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66602&quot;>;Women in ML&lt;/a>;&lt;br />; &lt;strong>;Google Sponsored - Platinum&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66607&quot;>;LatinX in AI&lt;/a>;&lt;br />; &lt;strong>;Google Sponsored - Platinum&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66605&quot;>;New in ML&lt;/a>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66541&quot;>;AI for Accelerated Materials Design&lt;/a>; (AI4Mat-2023)&lt;br />; Fireside Chat: &lt;strong>;Gowoon Cheon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66524&quot;>;Associative Memory &amp;amp; Hopfield Networks in 2023&lt;/a>;&lt;br />; Panelist: &lt;strong>;Blaise Agüera y Arcas&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66535&quot;>;Information-Theoretic Principles in Cognitive Systems&lt;/a>; (InfoCog)&lt;br />; Speaker: &lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66518&quot;>;Machine Learning and the Physical Sciences&lt;/a>;&lt;br />; Speaker: &lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66494&quot;>;UniReps: Unifying Representations in Neural Models&lt;/a>;&lt;br />; Organizer: &lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66517&quot;>;Robustness of Zero/Few-shot Learning in Foundation Models&lt;/a>; (R0-FoMo)&lt;br />; Speaker: &lt;strong>;Partha Talukdar&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Ananth Balashankar&lt;/strong>;, &lt;strong>;Yao Qin&lt;/strong>;, &lt;strong>;Ahmad Beirami&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66539&quot;>;Workshop on Diffusion Models&lt;/a>;&lt;br />; Speaker: &lt;strong>;Tali Dekel&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66502&quot;>;Algorithmic Fairness through the Lens of Time&lt;/a>;&lt;br />; Roundtable Lead: &lt;strong>;Stephen Pfohl&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Golnoosh Farnadi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66550&quot;>;Backdoors in Deep Learning: The Good, the Bad, and the Ugly&lt;/a>;&lt;br />; Organizer: &lt;strong>;Eugene Bagdasaryan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66549&quot;>;OPT 2023: Optimization for Machine Learning&lt;/a>;&lt;br />; Organizer: &lt;strong>;Cristóbal Guzmán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66545&quot;>;Machine Learning for Creativity and Design&lt;/a>;&lt;br />; Speaker: &lt;strong>;Aleksander Holynski&lt;/strong>;, &lt;strong>;Alexander Mordvintsev&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66511&quot;>;Robot Learning Workshop: Pretraining, Fine-Tuning, and Generalization with Large Scale Models&lt;/a>;&lt;br />; Speaker: &lt;strong>;Matt Barnes&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66516&quot;>;Machine Learning for Audio&lt;/a>;&lt;br />; Organizer: &lt;strong>;Shrikanth Narayanan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66531&quot;>;Federated Learning in the Age of Foundation Models&lt;/a>; (FL@FM-NeurIPS&#39;23)&lt;br />; Speaker: &lt;strong>;Cho-Jui Hsieh&lt;/strong>;, &lt;strong>;Zheng Xu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66526&quot;>;Socially Responsible Language Modelling Research&lt;/a>; (SoLaR)&lt;br />; Panelist: &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66506&quot;>;I Can&#39;t Believe It&#39;s Not Better (ICBINB): Failure Modes in the Age of Foundation Models&lt;/a>;&lt;br />; Advisory Board: &lt;strong>;Javier Antorán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66501&quot;>;Machine Learning for Systems&lt;/a>;&lt;br />; Organizer: &lt;strong>;Yawen Wang&lt;/strong>;&lt;br />; Competition Committee: &lt;strong>;Bryan Perozzi&lt;/strong>;, &lt;strong>;Sami Abu-el-haija&lt;/strong>;&lt;br />; Steering Committee: &lt;strong>;Milad Hashemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66514&quot;>;Self-Supervised Learning: Theory and Practice&lt;/a>;&lt;br />; Organizer: &lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66496&quot;>;Attributing Model Behavior at Scale&lt;/a>; (ATTRIB 2023)&lt;br />; Organizers: Kevin Guu*, Tolga Bolukbasi* &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Competitions&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/competition/66581&quot;>;NeurIPS 2023 Machine Unlearning Competition&lt;/a>;&lt;br />; Organizer: &lt;b>;Isabelle Guyon&lt;/b>;, &lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/competition/66593&quot;>;Lux AI Challenge Season 2 NeurIPS Edition&lt;/a>;&lt;br />; Organizer: &lt;strong>;Bovard Doerschuk-Tiberi&lt;/strong>;, &lt;strong>;Addison Howard&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/tutorial/73949&quot;>;Data-Centric AI for Reliable and Responsible AI: From Theory to Practice&lt;/a>;&lt;br />; &lt;strong>;Isabelle Guyon&lt;/strong>;, Nabeel Seedat, Mihaela va der Schaar &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Creative AI Track&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Creative AI Performances 1 &amp;amp; 2&lt;br />; Speaker: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74093&quot;>;Performance 1&lt;/a>;: Mon, Dec 11 | 6:30PM - 8:30PM, Lobby Stage&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74094&quot;>;Performance 2&lt;/a>;: 12 月 14 日，星期四 | 7:00PM - 9:00PM, Lobby Stage&lt;/em>; &lt;/p>; &lt;p>; Creative AI Sessions 1 – 3&lt;br />; Speaker: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74090&quot;>;Session 1&lt;/a>;: Tue, Dec 12 | 3:05PM - 3:40PM, Hall D2&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74091&quot;>;Session 2&lt;/a>;: Wed, Dec 13 | 10:45AM - 2:15PM, Hall D2&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74092&quot;>;Session 3&lt;/a>;: 12 月 14 日，星期四 | 10:45 AM - 2:15PM, Hall D2&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/session/75889&quot;>;Creative AI Videos&lt;/a>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Expo Talks&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78252&quot;>;Graph Learning Meets Artificial Intelligence&lt;/a>;&lt;br />; Speaker: &lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78243&quot;>;Resonator: Music Space&lt;/a>;&lt;br />; Speakers: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Michal Todorovic&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78237&quot;>;Empirical Rigor in ML as a Massively Parallelizable Challenge&lt;/a>;&lt;br />; Speaker: &lt;strong>;Megan Risdal (Kaggle)&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Oral Talks&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sW8yGZ4uVJ&quot;>;Ordering-based Conditions for Global Convergence of Policy Gradient Methods&lt;/a>;&lt;br />; Jincheng Mei, Bo Dai, &lt;strong>;Alekh Agarwal&lt;/strong>;,&lt;strong>; &lt;/strong>;Mohammad Ghavamzadeh*, Csaba Szepesvari, Dale Schuurmans &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=y8UAQQHVTX&quot;>;Private Everlasting Prediction&lt;/a>;&lt;br />; Moni Naor, Kobbi Nissim, &lt;strong>;Uri Stemmer&lt;/strong>;, Chao Yan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PITeSdYQkv&quot;>;User-Level Differential Privacy With Few Examples Per User&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, Raghu Meka, &lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dVaWCDMBof&quot;>;DataComp: In Search of the Next Generation of Multimodal Datasets&lt;/a>;&lt;br />; Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, &lt;strong>;Pang Wei Koh&lt;/strong>;, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=w116w62fxH&quot;>;Optimal Learners for Realizable Regression: PAC Learning and Online Learning&lt;/a>;&lt;br />; Idan Attias, Steve Hanneke, Alkis Kalavasis, &lt;strong>;Amin Karbasi&lt;/strong>;, Grigoris Velegkas &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jDIlzSU8wJ&quot;>;The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation&lt;/a>;&lt;br />; Saurabh Saxena, &lt;strong>;Charles Herrmann&lt;/strong>;,&lt;strong>; Junhwa Hur&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;,&lt;strong>; &lt;/strong>;Mohammad Norouzi*, &lt;strong>;Deqing Sun&lt;/strong>;,&lt;strong>; &lt;/strong>;David J. Fleet &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Journal Track&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://jmlr.org/papers/v24/20-998.html&quot;>;Graph Clustering with Graph Neural Networks&lt;/a>;&lt;br />; &lt;strong>;Anton Tsitsulin&lt;/strong>;, &lt;strong>;John Palowitch&lt;/strong>;,&lt;strong>; Bryan Perozzi&lt;/strong>;, Emmanuel Müller &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Spotlight Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1p6teT6F73&quot;>;Alternating Updates for Efficient Transformers&lt;/a>; (see blog post)&lt;br />; &lt;strong>;Cenk Baykal&lt;/strong>;, &lt;strong>;Dylan Cutler&lt;/strong>;, &lt;strong>;Nishanth Dikkala&lt;/strong>;, Nikhil Ghosh*, &lt;strong>;Rina Panigrahy&lt;/strong>;, &lt;strong>;Xin Wang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EldbUlZtbd&quot;>;Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models&lt;/a>;&lt;br />; &lt;strong>;Peter Hase&lt;/strong>;, Mohit Bansal, &lt;strong>;Been Kim&lt;/strong>;,&lt;strong>; Asma Ghandeharioun&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jR2FkqW6GB&quot;>;Is Learning in Games Good for the Learners?&lt;/a>;&lt;br />; William Brown,&lt;strong>; Jon Schneider&lt;/strong>;,&lt;strong>; Kiran Vodrahalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Bj1QSgiBPP&quot;>;Participatory Personalization in Classification&lt;/a>;&lt;br />; Hailey Joren, &lt;strong>;Chirag Nagpal&lt;/strong>;,&lt;strong>; Katherine Heller&lt;/strong>;,&lt;strong>; &lt;/strong>;Berk Ustun &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=c2eedxSlPJ&quot;>;Tight Risk Bounds for Gradient Descent on Separable Data&lt;/a>;&lt;br />; Matan Schliserman, &lt;strong>;Tomer Koren&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=67o9UQgTD0&quot;>;Counterfactual Memorization in Neural Language Models&lt;/a>;&lt;br />; &lt;strong>;Chiyuan Zhang&lt;/strong>;,&lt;strong>; &lt;/strong>;Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, Nicholas Carlini &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5NxJuc0T1P&quot;>;Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models&lt;/a>;&lt;br />; &lt;strong>;Zhong Yi Wan&lt;/strong>;, Ricardo Baptista,&lt;strong>; Anudhyan Boral&lt;/strong>;, &lt;strong>;Yi-Fan Chen&lt;/strong>;,&lt;strong>; John Anderson&lt;/strong>;,&lt;strong>; Fei Sha&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Leonardo Zepeda-Nunez&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=vTug54Uunq&quot;>;Faster Margin Maximization Rates for Generic Optimization Methods&lt;/a>;&lt;br />; Guanghui Wang, Zihao Hu, Vidya Muthukumar, &lt;strong>;Jacob Abernethy&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3PjCt4kmRx&quot;>;From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces&lt;/a>;&lt;br />; Peter Shaw, Mandar Joshi, &lt;strong>;James Cohan&lt;/strong>;,&lt;strong>; &lt;/strong>;Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, Kristina N Toutanova &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5Gw9YkJkFF&quot;>;PAC Learning Linear Thresholds from Label Proportions&lt;/a>;&lt;br />; &lt;strong>;Anand Brahmbhatt&lt;/strong>;,&lt;strong>; Rishi Saket&lt;/strong>;,&lt;strong>; Aravindan Raghuveer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CXPUg86A1D&quot;>;SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs&lt;/a>;&lt;br />; Lijun Yu*, &lt;strong>;Yong Cheng&lt;/strong>;,&lt;strong>; &lt;/strong>;Zhiruo Wang, &lt;strong>;Vivek Kumar&lt;/strong>;,&lt;strong>; Wolfgang Macherey&lt;/strong>;, &lt;strong>;Yanping Huang&lt;/strong>;,&lt;strong>; David Ross&lt;/strong>;,&lt;strong>; Irfan Essa&lt;/strong>;,&lt;strong>; &lt;/strong>;Yonatan Bisk,&lt;strong>; Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Kevin Murphy&lt;/strong>;,&lt;strong>; &lt;/strong>;Alexander Hauptmann,&lt;strong>; Lu Jiang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QatZNssk7T&quot;>;Adaptive Data Analysis in a Balanced Adversarial Model&lt;/a>;&lt;br />; Kobbi Nissim,&lt;strong>; Uri Stemmer&lt;/strong>;,&lt;strong>; &lt;/strong>;Eliad Tsfadia &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NiQTy0NW1L&quot;>;Lexinvariant Language Models&lt;/a>;&lt;br />; Qian Huang, Eric Zelikman, Sarah Chen,&lt;strong>; Yuhuai Wu&lt;/strong>;, Gregory Valiant, Percy Liang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=HF6bnhfSqH&quot;>;On Quantum Backpropagation, Information Reuse, and Cheating Measurement Collapse&lt;/a>;&lt;br />; &lt;strong>;Amira Abbas&lt;/strong>;,&lt;strong>; &lt;/strong>;Robbie King, Hsin-Yuan Huang, &lt;strong>;William J. Huggins&lt;/strong>;, &lt;strong>;Ramis Movassagh&lt;/strong>;,&lt;strong>; Dar Gilboa&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Jarrod McClean&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Pya0kCEpDk&quot;>;Private Estimation Algorithms for Stochastic Block Models and Mixture Models&lt;/a>;&lt;br />; Hongjie Chen,&lt;strong>; Vincent Cohen-Addad&lt;/strong>;,&lt;strong>; &lt;/strong>;Tommaso d&#39;Orsi,&lt;strong>; Alessandro Epasto&lt;/strong>;, Jacob Imola, David Steurer, Stefan Tiegel &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DBz9E5aZey&quot;>;Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation&lt;/a>;&lt;br />; &lt;strong>;Aniket Das&lt;/strong>;, &lt;strong>;Dheeraj Nagaraj&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=e0pRF9tOtm&quot;>;Private (Stochastic) Non-Convex Optimization Revisited: Second-Order Stationary Points and Excess Risks&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/strong>;,&lt;strong>; &lt;/strong>;Daogao Liu*, Sewoong Oh, Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=bKqrWLCMrX&quot;>;Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts&lt;/a>;&lt;br />; Pritam Sarkar, &lt;strong>;Ahmad Beirami&lt;/strong>;,&lt;strong>; Ali Etemad&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ikkdTD3hQJ&quot;>;AIMS: All-Inclusive Multi-Level Segmentation for Anything&lt;/a>;&lt;br />; Lu Qi, Jason Kuen, Weidong Guo, Jiuxiang Gu, Zhe Lin, Bo Du, Yu Xu, &lt;strong>;Ming-Hsuan Yang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rheCTpRrxI&quot;>;DreamHuman: Animatable 3D Avatars from Text&lt;/a>;&lt;br />; &lt;strong>;Nikos Kolotouros&lt;/strong>;,&lt;strong>; Thiemo Alldieck&lt;/strong>;,&lt;strong>; Andrei Zanfir&lt;/strong>;, &lt;strong>;Eduard Gabriel Bazavan&lt;/strong>;, &lt;strong>;Mihai Fieraru&lt;/strong>;, &lt;strong>;Cristian Sminchisescu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=oaCDiKoJ2w&quot;>;Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts&lt;/a>;&lt;br />; Chaoqi Wang, Ziyu Ye, &lt;strong>;Zhe Feng&lt;/strong>;, &lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;, Haifeng Xu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=m21rQusNgb&quot;>;Learning List-Level Domain-Invariant Representations for Ranking&lt;/a>;&lt;br />; Ruicheng Xian*, &lt;strong>;Honglei Zhuang&lt;/strong>;, &lt;strong>;Zhen Qin&lt;/strong>;, Hamed Zamani*, &lt;strong>;Jing Lu&lt;/strong>;,&lt;strong>; Ji Ma&lt;/strong>;, &lt;strong>;Kai Hui&lt;/strong>;, Han Zhao, &lt;strong>;Xuanhui Wang&lt;/strong>;, &lt;strong>;Michael Bendersky&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=hCdqDkA25J&quot;>;Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization&lt;/a>;&lt;br />; Liang Zhang, Junchi Yang, &lt;strong>;Amin Karbasi&lt;/strong>;, Niao He &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=hJzEoQHfCe&quot;>;Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems&lt;/a>;&lt;br />; Benjamin Coleman, Wang-Cheng Kang, &lt;strong>;Matthew Fahrbach&lt;/strong>;,&lt;strong>; &lt;/strong>;Ruoxi Wang, Lichan Hong, Ed Chi, Derek Cheng &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xOJUmwwlJc&quot;>;Proximity-Informed Calibration for Deep Neural Networks&lt;/a>;&lt;br />; Miao Xiong, Ailin Deng, &lt;strong>;Pang Wei Koh&lt;/strong>;, Jiaying Wu, Shen Li, Jianqing Xu, Bryan Hooi &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=WfsWy59bX2&quot;>;Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization&lt;/a>;&lt;br />; &lt;strong>;Adel Javanmard&lt;/strong>;,&lt;strong>; Vahab Mirrokni&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EUiIbwV379&quot;>;Better Private Linear Regression Through Better Private Feature Selection&lt;/a>;&lt;br />; &lt;strong>;Travis Dick&lt;/strong>;, Jennifer Gillenwater*, &lt;strong>;Matthew Joseph&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=XAyPlfmWpu&quot;>;Binarized Neural Machine Translation&lt;/a>;&lt;br />; Yichi Zhang, Ankush Garg, Yuan Cao, &lt;strong>;Łukasz Lew&lt;/strong>;, Behrooz Ghorbani*, Zhiru Zhang, Orhan Firat &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/poster/73665&quot;>;BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information&lt;/a>;&lt;br />; &lt;strong>;Mehran Kazemi&lt;/strong>;,&lt;strong>; Quan Yuan&lt;/strong>;, &lt;strong>;Deepti Bhatia&lt;/strong>;, &lt;strong>;Najoung Kim&lt;/strong>;, &lt;strong>;Xin Xu&lt;/strong>;, &lt;strong>;Vaiva Imbrasaite&lt;/strong>;, &lt;strong>;Deepak Ramachandran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1vvsIJtnnr&quot;>;Boosting with Tempered Exponential Measures&lt;/a>;&lt;br />; &lt;strong>;Richard Nock&lt;/strong>;,&lt;strong>; &lt;/strong>;Ehsan Amid,&lt;strong>; Manfred Warmuth&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SGlrCuwdsB&quot;>;Concept Algebra for (Score-Based) Text-Controlled Generative Models&lt;/a>;&lt;br />; Zihao Wang, Lin Gui, Jeffrey Negrea, &lt;strong>;Victor Veitch&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q8mH2d6uw2&quot;>;Deep Contract Design via Discontinuous Networks&lt;/a>;&lt;br />; Tonghan Wang, &lt;strong>;Paul Dütting&lt;/strong>;, Dmitry Ivanov, Inbal Talgam-Cohen, David C. Parkes &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=YoghyvSG0H&quot;>;Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection&lt;/a>;&lt;br />; Cheng-Ju Ho, Chen-Hsuan Tai, Yen-Yu Lin, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;,&lt;strong>; Yi-Hsuan Tsai&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PYASzxr2OP&quot;>;Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback&lt;/a>;&lt;br />; Han Shao, Lee Cohen, Avrim Blum, &lt;strong>;Yishay Mansour&lt;/strong>;, Aadirupa Saha, Matthew Walter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qCglMj6A4z&quot;>;Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy&lt;/a>;&lt;br />; Anastasia Koloskova*, &lt;strong>;Ryan McKenna&lt;/strong>;, &lt;strong>;Zachary Charles&lt;/strong>;, &lt;strong>;J Keith Rush&lt;/strong>;, &lt;strong>;Hugh Brendan McMahan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCwToX315b&quot;>;Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products&lt;/a>;&lt;br />; &lt;strong>;Tamas Sarlos&lt;/strong>;,&lt;strong>; &lt;/strong>;Xingyou Song, David P. Woodruff, Qiuyi (Richard) Zhang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=JhQP33aMx2&quot;>;Module-wise Adaptive Distillation for Multimodality Foundation Models&lt;/a>;&lt;br />;&lt;br />; Chen Liang, &lt;strong>;Jiahui Yu&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Matthew Brown&lt;/strong>;, Yin Cui, Tuo Zhao, &lt;strong>;Boqing Gong&lt;/strong>;, Tianyi Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zGRWp7yRqd&quot;>;Multi-Swap k-Means++&lt;/a>;&lt;br />; Lorenzo Beretta, &lt;strong>;Vincent Cohen-Addad&lt;/strong>;, &lt;strong>;Silvio Lattanzi&lt;/strong>;,&lt;strong>; Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8vuDHCxrmy&quot;>;OpenMask3D: Open-Vocabulary 3D Instance Segmentation&lt;/a>;&lt;br />; Ayça Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys,&lt;strong>; Federico Tombari&lt;/strong>;, &lt;strong>;Francis Engelmann&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7RMGI4slcb&quot;>;Order Matters in the Presence of Dataset Imbalance for Multilingual Learning&lt;/a>;&lt;br />; Dami Choi*, &lt;strong>;Derrick Xin&lt;/strong>;, &lt;strong>;Hamid Dadkhahi&lt;/strong>;, Justin Gilmer, Ankush Garg, Orhan Firat, Chih-Kuan Yeh, Andrew M. Dai, Behrooz Ghorbani &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=yEf8NSqTPu&quot;>;PopSign ASL v1.0: An Isolated American Sign Language Dataset Collected via Smartphones&lt;/a>;&lt;br />; Thad Starner, Sean Forbes, Matthew So, David Martin, Rohit Sridhar, Gururaj Deshpande, &lt;strong>;Sam Sepah&lt;/strong>;, Sahir Shahryar, Khushi Bhardwaj, Tyler Kwok, Daksh Sehgal, Saad Hassan, Bill Neubauer, Sofia Vempala, Alec Tan, Jocelyn Heath, Unnathi Kumar, Priyanka Mosur, Tavenner Hall, Rajandeep Singh, Christopher Cui, &lt;strong>;Glenn Cameron&lt;/strong>;, &lt;strong>;Sohier Dane&lt;/strong>;, &lt;strong>;Garrett Tanzer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gaktiSjatl&quot;>;Semi-Implicit Denoising Diffusion Models (SIDDMs)&lt;/a>;&lt;br />; Yanwu Xu*, Mingming Gong, Shaoan Xie, &lt;strong>;Wei Wei&lt;/strong>;,&lt;strong>; Matthias Grundmann&lt;/strong>;,&lt;strong>; &lt;/strong>;Kayhan Batmanghelich, &lt;strong>;Tingbo Hou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xGz0wAIJrS&quot;>;State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding&lt;/a>;&lt;br />; Devleena Das, Sonia Chernova, &lt;strong>;Been Kim&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=AwhpBEqmyo&quot;>;StoryBench: A Multifaceted Benchmark for Continuous Story Visualization&lt;/a>;&lt;br />; Emanuele Bugliarello*, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan,&lt;strong>; Vittorio Ferrari&lt;/strong>;, Pieter-Jan Kindermans, &lt;strong>;Paul Voigtlaender&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=wv3bHyQbX7&quot;>;Subject-driven Text-to-Image Generation via Apprenticeship Learning&lt;/a>;&lt;br />; Wenhu Chen, Hexiang Hu, &lt;strong>;Yandong Li&lt;/strong>;, &lt;strong>;Nataniel Ruiz&lt;/strong>;, &lt;strong>;Xuhui Jia&lt;/strong>;, Ming-Wei Chang, William W. Cohen &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=plAix1NxhU&quot;>;TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs&lt;/a>;&lt;br />; &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;, &lt;strong>;Sami Abu-El-Haija&lt;/strong>;, Kaidi Cao*, &lt;strong>;Bahare Fatemi&lt;/strong>;, &lt;strong>;Mike Burrows&lt;/strong>;, Charith Mendis*, &lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=a147pIS2Co&quot;>;Training Chain-of-Thought via Latent-Variable Inference&lt;/a>;&lt;br />; &lt;strong>;Du Phan&lt;/strong>;, &lt;strong>;Matthew D. Hoffman&lt;/strong>;, David Dohan*, Sholto Douglas,&lt;strong>; Tuan Anh Le&lt;/strong>;, Aaron Parisi, &lt;strong>;Pavel Sountsov&lt;/strong>;, Charles Sutton, Sharad Vikram,&lt;strong>; Rif A. Saurous&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1ZzG6td0el&quot;>;Unified Lower Bounds for Interactive High-dimensional Estimation under Information Constraints&lt;/a>;&lt;br />; Jayadev Acharya, Clement L. Canonne, &lt;strong>;Ziteng Sun&lt;/strong>;, Himanshu Tyagi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=j5AoleAIru&quot;>;What You See is What You Read? Improving Text-Image Alignment Evaluation&lt;/a>;&lt;br />; &lt;strong>;Michal Yarom&lt;/strong>;, &lt;strong>;Yonatan Bitton&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Roee Aharoni&lt;/strong>;, &lt;strong>;Jonathan Herzig&lt;/strong>;,&lt;strong>; Oran Lang&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Eran Ofek&lt;/strong>;, &lt;strong>;Idan Szpektor&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=4KZhZJSPYU&quot;>;When Does Confidence-Based Cascade Deferral Suffice?&lt;/a>;&lt;br />; &lt;strong>;Wittawat Jitkrittum&lt;/strong>;, &lt;strong>;Neha Gupta&lt;/strong>;, &lt;strong>;Aditya Krishna Menon&lt;/strong>;, &lt;strong>;Harikrishna Narasimhan&lt;/strong>;, &lt;strong>;Ankit Singh Rawat&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=A18PgVSUgf&quot;>;Accelerating Molecular Graph Neural Networks via Knowledge Distillation&lt;/a>;&lt;br />; Filip Ekström Kelvinius, Dimitar Georgiev, Artur Petrov Toshev, &lt;strong>;Johannes Gasteiger&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7EMphtUgCI&quot;>;AVIS: Autonomous Visual Information Seeking with Large Language Model Agent&lt;/a>;&lt;br />; Ziniu Hu*, &lt;strong>;Ahmet Iscen&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, Kai-Wei Chang, Yizhou Sun, &lt;strong>;David Ross&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=9mJXDcr17V&quot;>;Beyond Invariance: Test-Time Label-Shift Adaptation for Addressing &quot;Spurious&quot; Correlations&lt;/a>;&lt;br />; Qingyao Sun, Kevin Patrick Murphy, &lt;strong>;Sayna Ebrahimi&lt;/strong>;, Alexander D&#39;Amour &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0tEjORCGFD&quot;>;Collaborative Score Distillation for Consistent Visual Editing&lt;/a>;&lt;br />; Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, &lt;strong>;Kihyuk Sohn&lt;/strong>;, Jinwoo Shin &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1SF2tiopYJ&quot;>;CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs&lt;/a>;&lt;br />; Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di, &lt;strong>;Federico Tombari&lt;/strong>;, Nassir Navab, Benjamin Busam &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=65aDEXIhih&quot;>;Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy&lt;/a>;&lt;br />; &lt;strong>;Amit Daniely&lt;/strong>;, Nathan Srebro, Gal Vardi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BopG5dhH7L&quot;>;A Computationally Efficient Sparsified Online Newton Method&lt;/a>;&lt;br />; Fnu Devvrit*, Sai Surya Duvvuri,&lt;strong>; &lt;/strong>;Rohan Anil, &lt;strong>;Vineet Gupta&lt;/strong>;, &lt;strong>;Cho-Jui Hsieh&lt;/strong>;, &lt;strong>;Inderjit S Dhillon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6EDHfVHicP&quot;>;DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field&lt;/a>;&lt;br />; Chenyangguang Zhang, Yan Di, Ruida Zhang, Guangyao Zhai, &lt;strong>;Fabian Manhardt&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;, Xiangyang Ji &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=2nTpPxJ5Bs&quot;>;Double Auctions with Two-sided Bandit Feedback&lt;/a>;&lt;br />; &lt;strong>;Soumya Basu&lt;/strong>;, Abishek Sankararaman &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2305.19234&quot;>;Grammar Prompting for Domain-Specific Language Generation with Large Language Models&lt;/a>;&lt;br />; Bailin Wang, Zi Wang, Xuezhi Wang, &lt;strong>;Yuan Cao&lt;/strong>;, Rif A. Saurous, Yoon Kim &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5SIz31OGFV&quot;>;Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training&lt;/a>;&lt;br />; Rie Johnson, Tong Zhang* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3YDukx2cpr&quot;>;Large Graph Property Prediction via Graph Segment Training&lt;/a>;&lt;br />; Kaidi Cao*, &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;, &lt;strong>;Sami Abu-El-Haija&lt;/strong>;, &lt;strong>;Dustin Zelle&lt;/strong>;, &lt;strong>;Yanqi Zhou&lt;/strong>;,&lt;strong>; &lt;/strong>;Charith Mendis*, Jure Leskovec, &lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1GxKVprbwM&quot;>;On Computing Pairwise Statistics with Local Differential Privacy&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Adam Sealfon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7UdVPRmpif&quot;>;On Student-teacher Deviations in Distillation: Does it Pay to Disobey?&lt;/a>;&lt;br />; &lt;strong>;Vaishnavh Nagarajan&lt;/strong>;, &lt;strong>;Aditya Krishna Menon&lt;/strong>;, &lt;strong>;Srinadh Bhojanapalli&lt;/strong>;, &lt;strong>;Hossein Mobahi&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CzkOzKWpMa&quot;>;Optimal Cross-learning for Contextual Bandits with Unknown Context Distributions&lt;/a>;&lt;br />; &lt;strong>;Jon Schneider&lt;/strong>;, &lt;strong>;Julian Zimmert&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8XRMbNAP6Z&quot;>;Near-Optimal k-Clustering in the Sliding Window Model&lt;/a>;&lt;br />; David Woodruff, &lt;strong>;Peilin Zhong&lt;/strong>;, Samson Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3H37XciUEv&quot;>;Post Hoc Explanations of Language Models Can Improve Language Models&lt;/a>;&lt;br />; Satyapriya Krishna, Jiaqi Ma, Dylan Z Slack, &lt;strong>;Asma Ghandeharioun&lt;/strong>;, Sameer Singh, Himabindu Lakkaraju &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BJ0fQUU32w&quot;>;Recommender Systems with Generative Retrieval&lt;/a>;&lt;br />; Shashank Rajput*&lt;strong>;, &lt;/strong>;Nikhil Mehta, Anima Singh, &lt;strong>;Raghunandan Hulikal Keshavan&lt;/strong>;, &lt;strong>;Trung Vu, Lukasz Heldt&lt;/strong>;,&lt;strong>; &lt;/strong>;Lichan Hong, Yi Tay&lt;strong>;, Vinh Q. Tran&lt;/strong>;, &lt;strong>;Jonah Samost&lt;/strong>;, Maciej Kula, Ed H. Chi, Maheswaran Sathiamoorthy &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8OTPepXzeh&quot;>;Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models&lt;/a>;&lt;br />; &lt;strong>;Ying Fan&lt;/strong>;, Olivia Watkins, Yuqing Du, Hao Liu, &lt;strong>;Moonkyung Ryu&lt;/strong>;,&lt;strong>; Craig Boutilier&lt;/strong>;, Pieter Abbeel, Mohammad Ghavamzadeh*, Kangwook Lee, Kimin Lee* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5VQFAvUHcd&quot;>;Replicable Clustering&lt;/a>;&lt;br />; &lt;strong>;Hossein Esfandiari&lt;/strong>;, &lt;strong>;Amin Karbasi&lt;/strong>;, &lt;strong>;Vahab Mirrokni&lt;/strong>;, Grigoris Velegkas, Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5cPz5hrjy6&quot;>;Replicability in Reinforcement Learning&lt;/a>;&lt;br />; &lt;strong>;Amin Karbasi&lt;/strong>;, Grigoris Velegkas, Lin Yang, Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=szFqlNRxeS&quot;>;Riemannian Projection-free Online Learning&lt;/a>;&lt;br />; Zihao Hu, Guanghui Wang, &lt;strong>;Jacob Abernethy&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=29WbraPk8U&quot;>;Sharpness-Aware Minimization Leads to Low-Rank Features&lt;/a>;&lt;br />; Maksym Andriushchenko, &lt;strong>;Dara Bahri&lt;/strong>;, &lt;strong>;Hossein Mobahi&lt;/strong>;, Nicolas Flammarion &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=2hQ7MBQApp&quot;>;What is the Inductive Bias of Flatness Regularization? A Study of Deep Matrix Factorization Models&lt;/a>;&lt;br />; Khashayar Gatmiry, Zhiyuan Li, Ching-Yao Chuang, &lt;strong>;Sashank Reddi&lt;/strong>;, Tengyu Ma, Stefanie Jegelka &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=JzQlGqBm8d&quot;>;Block Low-Rank Preconditioner with Shared Basis for Stochastic Optimization&lt;/a>;&lt;br />; Jui-Nan Yen, Sai Surya Duvvuri, &lt;strong>;Inderjit S Dhillon&lt;/strong>;, &lt;strong>;Cho-Jui Hsieh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Ntd6X7uWYF&quot;>;Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints&lt;/a>;&lt;br />; &lt;strong>;Soumyabrata Pal&lt;/strong>;, &lt;strong>;Arun Sai Suggala&lt;/strong>;, &lt;strong>;Karthikeyan Shanmugam&lt;/strong>;, &lt;strong>;Prateek Jain&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NIrTSCiIZ7&quot;>;Boundary Guided Learning-Free Semantic Control with Diffusion Models&lt;/a>;&lt;br />; Ye Zhu, Yu Wu, &lt;strong>;Zhiwei Deng&lt;/strong>;, Olga Russakovsky, Yan Yan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=IyYyKov0Aj&quot;>;Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference&lt;/a>;&lt;br />; Tao Lei, &lt;strong>;Junwen Bai&lt;/strong>;, Siddhartha Brahma,&lt;strong>; Joshua Ainslie&lt;/strong>;, Kenton Lee, Yanqi Zhou, Nan Du*, &lt;strong>;Vincent Y. Zhao&lt;/strong>;, &lt;strong>;Yuexin Wu&lt;/strong>;, Bo Li,&lt;strong>; Yu Zhang&lt;/strong>;, Ming-Wei Chang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=KTRwpWCMsC&quot;>;Conformal Prediction for Time Series with Modern Hopfield Networks&lt;/a>;&lt;br />; Andreas Auer, &lt;strong>;Martin Gauch&lt;/strong>;, Daniel Klotz, Sepp Hochreiter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PzYAMXmIT3&quot;>;Does Visual Pretraining Help End-to-End Reasoning?&lt;/a>;&lt;br />; &lt;strong>;Chen Sun&lt;/strong>;, Calvin Luo, &lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PAYXfIUKWY&quot;>;Effective Robustness Against Natural Distribution Shifts for Models with Different Training Data&lt;/a>;&lt;br />; Zhouxing Shi*, &lt;strong>;Nicholas Carlini&lt;/strong>;, &lt;strong>;Ananth Balashankar&lt;/strong>;, Ludwig Schmidt, &lt;strong>;Cho-Jui Hsieh&lt;/strong>;, Alex Beutel*, &lt;strong>;Yao Qin&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Nh5dp6Uuvx&quot;>;Improving Neural Network Representations Using Human Similarity Judgments&lt;/a>;&lt;br />; Lukas Muttenthaler*, Lorenz Linhardt, Jonas Dippel, Robert A. Vandermeulen, Katherine Hermann, Andrew K. Lampinen, Simon Kornblith &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=N6FhEMnxCU&quot;>;Label Robust and Differentially Private Linear Regression: Computational and Statistical Efficiency&lt;/a>;&lt;br />; Xiyang Liu, &lt;strong>;Prateek Jain&lt;/strong>;, &lt;strong>;Weihao Kong&lt;/strong>;, &lt;strong>;Sewoong Oh&lt;/strong>;, &lt;strong>;Arun Sai Suggala&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Fdfyga5i0A&quot;>;Mnemosyne: Learning to Train Transformers with Transformers&lt;/a>;&lt;br />; Deepali Jain, Krzysztof Choromanski, &lt;strong>;Avinava Dubey&lt;/strong>;, Sumeet Singh, Vikas Sindhwani, Tingnan Zhang, Jie Tan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=MCkUS1P3Sh&quot;>;Nash Regret Guarantees for Linear Bandits&lt;/a>;&lt;br />; Ayush Sawarni, &lt;strong>;Soumyabrata Pal&lt;/strong>;, Siddharth Barman &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2307.03043&quot;>;A Near-Linear Time Algorithm for the Chamfer Distance&lt;/a>;&lt;br />; Ainesh Bakshi, Piotr Indyk, &lt;strong>;Rajesh Jayaram&lt;/strong>;, Sandeep Silwal, Erik Waingarten. &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=FviF8vuz5B&quot;>;On Differentially Private Sampling from Gaussian and Product Distributions&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, Xiao Hu*, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LelK6Mfoey&quot;>;On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes&lt;/a>;&lt;br />; Jia Lin Hau, Erick Delage, Mohammad Ghavamzadeh*, Marek Petrik &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=HFQFAyNucq&quot;>;ResMem: Learn What You Can and Memorize the Rest&lt;/a>;&lt;br />; Zitong Yang, &lt;strong>;Michal Lukasik&lt;/strong>;,&lt;strong>; Vaishnavh Nagarajan&lt;/strong>;, &lt;strong>;Zonglin Li&lt;/strong>;, &lt;strong>;Ankit Singh Rawat&lt;/strong>;, &lt;strong>;Manzil Zaheer, &lt;/strong>;&lt;strong>;Aditya Krishna Menon&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PcNpL9Q39p&quot;>;Responsible AI (RAI) Games and Ensembles&lt;/a>;&lt;br />; Yash Gupta, Runtian Zhai, &lt;strong>;Arun Suggala&lt;/strong>;, Pradeep Ravikumar &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DVlawv2rSI&quot;>;RoboCLIP: One Demonstration Is Enough to Learn Robot Policies&lt;/a>;&lt;br />; Sumedh A Sontakke, Jesse Zhang, &lt;strong>;Sébastien MR Arnold&lt;/strong>;, Karl Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, Laurent Itti &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=I6aOjhpcNQ&quot;>;Robust Concept Erasure via Kernelized Rate-Distortion Maximization&lt;/a>;&lt;br />; Somnath Basu Roy Chowdhury, Nicholas Monath, &lt;strong>;Kumar Avinava Dubey&lt;/strong>;, &lt;strong>;Amr Ahmed&lt;/strong>;, Snigdha Chaturvedi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=FmZVRe0gn8&quot;>;Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms&lt;/a>;&lt;br />; Alexander Bukharin, Yan Li, Yue Yu, Qingru Zhang, &lt;strong>;Zhehui Chen&lt;/strong>;, Simiao Zuo, Chao Zhang, Songan Zhang, Tuo Zhao &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PTvxck0QDE&quot;>;Simplicity Bias in 1-Hidden Layer Neural Networks&lt;/a>;&lt;br />; Depen Morwani*, Jatin Batra, &lt;strong>;Prateek Jain&lt;/strong>;,&lt;strong>; Praneeth Netrapalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2302.03806&quot;>;SLaM: Student-Label Mixing for Distillation with Unlabeled Examples&lt;/a>;&lt;br />; Vasilis Kontonis,&lt;strong>; Fotis Iliopoulos&lt;/strong>;, &lt;strong>;Khoa Trinh&lt;/strong>;, &lt;strong>;Cenk Baykal&lt;/strong>;, &lt;strong>;Gaurav Menghani&lt;/strong>;, &lt;strong>;Erik Vee&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCHmP68Gtj&quot;>;SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding&lt;/a>;&lt;br />; Paul-Edouard Sarlin*, &lt;strong>;Eduard Trulls&lt;/strong>;, Marc Pollefeys, &lt;strong>;Jan Hosang&lt;/strong>;, &lt;strong>;Simon Lynen&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QvIvWMaQdX&quot;>;SOAR: Improved Indexing for Approximate Nearest Neighbor Search&lt;/a>;&lt;br />; &lt;strong>;Philip Sun&lt;/strong>;, &lt;strong>;David Simcha&lt;/strong>;, &lt;strong>;Dave Dopson&lt;/strong>;, &lt;strong>;Ruiqi Guo&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=KoaFh16uOc&quot;>;StyleDrop: Text-to-Image Synthesis of Any Style&lt;/a>;&lt;br />; &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Jarred Barber&lt;/strong>;, Kimin Lee*, &lt;strong>;Nataniel Ruiz&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>;, Huiwen Chang*,&lt;strong>; Yuanzhen Li&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Glenn Entis&lt;/strong>;, &lt;strong>;Irina Blok&lt;/strong>;, &lt;strong>;Daniel Castro Chin&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LSYQB4CwD3&quot;>;Three Towers: Flexible Contrastive Learning with Pretrained Image Models&lt;/a>;&lt;br />; Jannik Kossen*,&lt;strong>; Mark Collier&lt;/strong>;, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, &lt;strong>;Jesse Berent&lt;/strong>;,&lt;strong>; &lt;/strong>;Rodolphe Jenatton,&lt;strong>; Efi Kokiopoulou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=GIlsH0T4b2&quot;>;Two-Stage Learning to Defer with Multiple Experts&lt;/a>;&lt;br />; Anqi Mao, Christopher Mohri, &lt;strong>;Mehryar Mohri&lt;/strong>;, Yutao Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ZBzYWP2Gpl&quot;>;AdANNS: A Framework for Adaptive Semantic Search&lt;/a>;&lt;br />; Aniket Rege, &lt;strong>;Aditya Kusupati&lt;/strong>;, Sharan Ranjit S, Alan Fan, Qingqing Cao, Sham Kakade, &lt;strong>;Prateek Jain&lt;/strong>;, Ali Farhadi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Srt1hhQgqa&quot;>;Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer&lt;/a>;&lt;br />; Bowen Tan*, &lt;strong>;Yun Zhu&lt;/strong>;, &lt;strong>;Lijuan Liu&lt;/strong>;, Eric Xing, Zhiting Hu, &lt;strong>;Jindong Chen&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dJZ3MvDw86&quot;>;Causal-structure Driven Augmentations for Text OOD Generalization&lt;/a>;&lt;br />; &lt;strong>;Amir Feder&lt;/strong>;, Yoav Wald, Claudia Shi, Suchi Saria, David Blei &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=S0xrBMFihS&quot;>;Dense-Exponential Random Features: Sharp Positive Estimators of the Gaussian Kernel&lt;/a>;&lt;br />; Valerii Likhosherstov, Krzysztof Choromanski, &lt;strong>;Avinava Dubey&lt;/strong>;, &lt;strong>;Frederick Liu&lt;/strong>;, &lt;strong>;Tamas Sarlos&lt;/strong>;, Adrian Weller &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Vm1zeYqwdc&quot;>;Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence&lt;/a>;&lt;br />; Grace Luo, Lisa Dunlap, Dong Huk Park, &lt;strong>;Aleksander Holynski&lt;/strong>;, Trevor Darrell &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qgv56R2YJ7&quot;>;Diffusion Self-Guidance for Controllable Image Generation&lt;/a>;&lt;br />; &lt;strong>;Dave Epstein&lt;/strong>;, Allan Jabri, &lt;strong>;Ben Poole&lt;/strong>;, Alexei A Efros,&lt;strong>; Aleksander Holynski&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dnGEPkmnzO&quot;>;Fully Dynamic k-Clustering in Õ(k) Update Time&lt;/a>;&lt;br />; Sayan Bhattacharya, Martin Nicolas Costa, &lt;strong>;Silvio Lattanzi&lt;/strong>;, &lt;strong>;Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SVjDiiVySh&quot;>;Improving CLIP Training with Language Rewrites&lt;/a>;&lt;br />; &lt;strong>;Lijie Fan&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>;, Phillip Isola, Dina Katabi, &lt;strong>;Yonglong Tian&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=UzUhiKACmS&quot;>;k-Means Clustering with Distance-Based Privacy&lt;/a>;&lt;br />; &lt;strong>;Alessandro Epasto&lt;/strong>;, &lt;strong>;Vahab Mirrokni&lt;/strong>;, Shyam Narayanan, &lt;strong>;Peilin Zhong&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Xu8aG5Q8M3&quot;>;LayoutGPT: Compositional Visual Planning and Generation with Large Language Models&lt;/a>;&lt;br />; Weixi Feng, Wanrong Zhu, Tsu-Jui Fu,&lt;strong>; Varun Jampani&lt;/strong>;, &lt;strong>;Arjun Reddy Akula&lt;/strong>;, Xuehai He, &lt;strong>;Sugato Basu&lt;/strong>;, Xin Eric Wang, William Yang Wang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SxXN3kNTsV&quot;>;Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management&lt;/a>;&lt;br />; Dhawal Gupta*, &lt;strong>;Yinlam Chow&lt;/strong>;,&lt;strong>; Azamat Tulepbergenov&lt;/strong>;, Mohammad Ghavamzadeh*, &lt;strong>;Craig Boutilier&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=aG6xOP9QY7&quot;>;Optimal Unbiased Randomizers for Regression with Label Differential Privacy&lt;/a>;&lt;br />; &lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;, &lt;strong>;Badih Ghazi&lt;/strong>;, &lt;strong>;Pritish Kamath&lt;/strong>;,&lt;strong>; Ravi Kumar&lt;/strong>;, &lt;strong>;Ethan Jacob Leeman&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Avinash V Varadarajan&lt;/strong>;, &lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=WbFhFvjjKj&quot;>;Paraphrasing Evades Detectors of AI-generated Text, but Retrieval Is an Effective Defense&lt;/a>;&lt;br />; &lt;strong>;Kalpesh Krishna&lt;/strong>;, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SaMrN9tnxE&quot;>;ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation&lt;/a>;&lt;br />; Shuyang Sun*, &lt;strong>;Weijun Wang&lt;/strong>;, Qihang Yu*, &lt;strong>;Andrew Howard&lt;/strong>;, Philip Torr, Liang-Chieh Chen* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SouroWC5Un&quot;>;Robust and Actively Secure Serverless Collaborative Learning&lt;/a>;&lt;br />; Nicholas Franzese, Adam Dziedzic, &lt;strong>;Christopher A. Choquette-Choo&lt;/strong>;, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, &lt;strong>;Somesh Jha&lt;/strong>;, Nicolas Papernot, Xiao Wang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SdYHLTCC5J&quot;>;SpecTr: Fast Speculative Decoding via Optimal Transport&lt;/a>;&lt;br />; &lt;strong>;Ziteng Sun&lt;/strong>;, &lt;strong>;Ananda Theertha Suresh&lt;/strong>;, &lt;strong>;Jae Hun Ro&lt;/strong>;,&lt;strong>; Ahmad Beirami&lt;/strong>;, &lt;strong>;Himanshu Jain&lt;/strong>;,&lt;strong>; Felix Yu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=YZ7ip645Ra&quot;>;Structured Prediction with Stronger Consistency Guarantees&lt;/a>;&lt;br />; Anqi Mao, &lt;strong>;Mehryar Mohri&lt;/strong>;, Yutao Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qgiG7WZohZ&quot;>;Affinity-Aware Graph Networks&lt;/a>;&lt;br />; &lt;strong>;Ameya Velingker&lt;/strong>;,&lt;strong>; Ali Kemal Sinop&lt;/strong>;, Ira Ktena, Petar Veličković, &lt;strong>;Sreenivas Gollapudi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rJc5Lsn5QU&quot;>;ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections&lt;/a>;&lt;br />; Chun-Han Yao*, &lt;strong>;Amit Raj&lt;/strong>;, Wei-Chih Hung,&lt;strong>; Yuanzhen Li&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Varun Jampani&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=eoDNaH3pfB&quot;>;Black-Box Differential Privacy for Interactive ML&lt;/a>;&lt;br />; &lt;strong>;Haim Kaplan&lt;/strong>;, &lt;strong>;Yishay Mansour&lt;/strong>;, &lt;strong>;Shay Moran&lt;/strong>;, Kobbi Nissim, &lt;strong>;Uri Stemmer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=orh4e0AO9R&quot;>;Bypassing the Simulator: Near-Optimal Adversarial Linear Contextual Bandits&lt;/a>;&lt;br />; Haolin Liu, Chen-Yu Wei, &lt;strong>;Julian Zimmert&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kXOXrVnwbb&quot;>;DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model&lt;/a>;&lt;br />; &lt;strong>;Xiuye Gu&lt;/strong>;, Yin Cui*, &lt;strong>;Jonathan Huang&lt;/strong>;, &lt;strong>;Abdullah Rashwan&lt;/strong>;, &lt;strong>;Xuan Yang&lt;/strong>;, &lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;, &lt;strong>;Huizhong Chen&lt;/strong>;, Liang-Chieh Chen*, &lt;strong>;David Ross&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kqBUgrkm1c&quot;>;Easy Learning from Label Proportions&lt;/a>;&lt;br />; &lt;strong>;Robert Busa-Fekete&lt;/strong>;, Heejin Choi*, &lt;strong>;Travis Dick&lt;/strong>;, &lt;strong>;Claudio Gentile&lt;/strong>;, &lt;strong>;Andres Munoz Medina&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q3fCWoC9l0&quot;>;Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks&lt;/a>;&lt;br />; Eeshaan Jain, Tushar Nandy, &lt;strong>;Gaurav Aggarwal&lt;/strong>;, &lt;strong>;Ashish Tendulkar&lt;/strong>;, Rishabh Iyer, Abir De &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=h2lkx9SQCD&quot;>;Faster Differentially Private Convex Optimization via Second-Order Methods&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/strong>;, Mahdi Haghifam*, Thomas Steinke, Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gdVcFOvxT3&quot;>;Finding Safe Zones of Markov Decision Processes Policies&lt;/a>;&lt;br />; Lee Cohen, &lt;strong>;Yishay Mansour&lt;/strong>;, Michal Moshkovitz &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=s1FjXzJ0jy&quot;>;Focused Transformer: Contrastive Training for Context Scaling&lt;/a>;&lt;br />; Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu*, Henryk Michalewski, Piotr Miłoś &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=h3kuB4z2G9&quot;>;Front-door Adjustment Beyond Markov Equivalence with Limited Graph Knowledge&lt;/a>;&lt;br />; Abhin Shah, &lt;strong>;Karthikeyan Shanmugam&lt;/strong>;, Murat Kocaoglu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nI7EmXq2PL&quot;>;H-Consistency Bounds: Characterization and Extensions&lt;/a>;&lt;br />; Anqi Mao, &lt;strong>;Mehryar Mohri&lt;/strong>;, Yutao Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kjMGHTo8Cs&quot;>;Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation&lt;/a>;&lt;br />; David Brandfonbrener, &lt;strong>;Ofir Nachum&lt;/strong>;, Joan Bruna &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pvPujuvjQd&quot;>;Most Neural Networks Are Almost Learnable&lt;/a>;&lt;br />; &lt;strong>;Amit Daniely&lt;/strong>;, Nathan Srebro, Gal Vardi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nQ84YY9Iut&quot;>;Multiclass Boosting: Simple and Intuitive Weak Learning Criteria&lt;/a>;&lt;br />; Nataly Brukhim, &lt;strong>;Amit Daniely&lt;/strong>;, &lt;strong>;Yishay Mansour&lt;/strong>;, &lt;strong>;Shay Moran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gJHAT79cZU&quot;>;NeRF Revisited: Fixing Quadrature Instability in Volume Rendering&lt;/a>;&lt;br />; Mikaela Angelina Uy, Kiyohiro Nakayama, Guandao Yang, Rahul Krishna Thomas, Leonidas Guibas,&lt;strong>; Ke Li&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=izNfcaHJk0&quot;>;Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation&lt;/a>;&lt;br />; Wei-Ning Chen, Dan Song, Ayfer Ozgur, &lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rzDBoh1tBh&quot;>;Private Federated Frequency Estimation: Adapting to the Hardness of the Instance&lt;/a>;&lt;br />; Jingfeng Wu*, &lt;strong>;Wennan Zhu&lt;/strong>;, &lt;strong>;Peter Kairouz&lt;/strong>;, Vladimir Braverman &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pVlC0reMKq&quot;>;RETVec: Resilient and Efficient Text Vectorizer&lt;/a>;&lt;br />; &lt;strong>;Elie Bursztein&lt;/strong>;, &lt;strong>;Marina Zhang&lt;/strong>;, &lt;strong>;Owen Skipper Vallis&lt;/strong>;, &lt;strong>;Xinyu Jia&lt;/strong>;, &lt;strong>;Alexey Kurakin&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ne6zeqLFCZ&quot;>;Symbolic Discovery of Optimization Algorithms&lt;/a>;&lt;br />; Xiangning Chen*, &lt;strong>;Chen Liang&lt;/strong>;, &lt;strong>;Da Huang&lt;/strong>;, &lt;strong>;Esteban Real&lt;/strong>;, &lt;strong>;Kaiyuan Wang&lt;/strong>;, &lt;strong>;Hieu Pham&lt;/strong>;, &lt;strong>;Xuanyi Dong&lt;/strong>;, &lt;strong>;Thang Luong&lt;/strong>;, Cho-Jui Hsieh, &lt;strong>;Yifeng Lu&lt;/strong>;, &lt;strong>;Quoc V. Le&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=lds9D17HRd&quot;>;A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence&lt;/a>;&lt;br />; Junyi Zhang, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Junhwa Hur&lt;/strong>;, &lt;strong>;Luisa F. Polania&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;,&lt;strong>; Ming-Hsuan Yang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=iSd8g75QvP&quot;>;A Trichotomy for Transductive Online Learning&lt;/a>;&lt;br />; Steve Hanneke, &lt;strong>;Shay Moran&lt;/strong>;, Jonathan Shafer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=i2H2sEiq2T&quot;>;A Unified Fast Gradient Clipping Framework for DP-SGD&lt;/a>;&lt;br />; &lt;strong>;William Kong&lt;/strong>;, &lt;strong>;Andres Munoz Medina&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=mlbes5TAAg&quot;>;Unleashing the Power of Randomization in Auditing Differentially Private ML&lt;/a>;&lt;br />; &lt;strong>;Krishna Pillutla&lt;/strong>;, &lt;strong>;Galen Andrew&lt;/strong>;, &lt;strong>;Peter Kairouz&lt;/strong>;, &lt;strong>;H. Brendan McMahan&lt;/strong>;, &lt;strong>;Alina Oprea&lt;/strong>;, &lt;strong>;Sewoong Oh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zEm6hF97Pz&quot;>;(Amplified) Banded Matrix Factorization: A unified approach to private training&lt;/a>;&lt;br />; Christopher A Choquette-Choo,&lt;strong>; Arun Ganesh&lt;/strong>;, &lt;strong>;Ryan McKenna&lt;/strong>;, &lt;strong>;H Brendan McMahan&lt;/strong>;, &lt;strong>;Keith Rush&lt;/strong>;,&lt;strong>; &lt;/strong>;Abhradeep Guha Thakurta, &lt;strong>;Zheng Xu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xcGhx9FdxM&quot;>;Adversarial Resilience in Sequential Prediction via Abstention&lt;/a>;&lt;br />; Surbhi Goel, Steve Hanneke, &lt;strong>;Shay Moran&lt;/strong>;, Abhishek Shetty &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uTlKUAm68H&quot;>;Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception&lt;/a>;&lt;br />; &lt;strong>;Hassan Akbari&lt;/strong>;, &lt;strong>;Dan Kondratyuk&lt;/strong>;, &lt;strong>;Yin Cui&lt;/strong>;, &lt;strong>;Rachel Hornung&lt;/strong>;, &lt;strong>;Huisheng Wang&lt;/strong>;, &lt;strong>;Hartwig Adam&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf/0083ab45a583fe1992b3c4f9222081c50b9d30c3.pdf&quot;>;Android in the Wild: A Large-Scale Dataset for Android Device Control&lt;/a>;&lt;br />; &lt;strong>;Christopher Rawles&lt;/strong>;, &lt;strong>;Alice Li&lt;/strong>;, &lt;strong>;Daniel Rodriguez&lt;/strong>;, &lt;strong>;Oriana Riva&lt;/strong>;, Timothy Lillicrap &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CiRHWaRbp0&quot;>;Benchmarking Robustness to Adversarial Image Obfuscations&lt;/a>;&lt;br />; Florian Stimberg, &lt;strong>;Ayan Chakrabarti&lt;/strong>;, &lt;strong>;Chun-Ta Lu&lt;/strong>;, &lt;strong>;Hussein Hazimeh&lt;/strong>;, &lt;strong>;Otilia Stretcu&lt;/strong>;, &lt;strong>;Wei Qiao&lt;/strong>;,&lt;strong>; Yintao Liu&lt;/strong>;, &lt;strong>;Merve Kaya&lt;/strong>;, &lt;strong>;Cyrus Rashtchian&lt;/strong>;, &lt;strong>;Ariel Fuxman&lt;/strong>;, &lt;strong>;Mehmet Tek&lt;/strong>;, Sven Gowal &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uIj1jDc8k6&quot;>;Building Socio-culturally Inclusive Stereotype Resources with Community Engagement&lt;/a>;&lt;br />; &lt;strong>;Sunipa Dev&lt;/strong>;, Jaya Goyal, &lt;strong>;Dinesh Tewari&lt;/strong>;, &lt;strong>;Shachi Dave&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=L9I9FhHfS3&quot;>;Consensus and Subjectivity of Skin Tone Annotation for ML Fairness&lt;/a>;&lt;br />; &lt;strong>;Candice Schumann&lt;/strong>;, &lt;strong>;Gbolahan O Olanubi&lt;/strong>;, &lt;strong>;Auriel Wright&lt;/strong>;, Ellis Monk Jr*, &lt;strong>;Courtney Heldreth&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Susanna Ricco&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zdli6OxpWd&quot;>;Counting Distinct Elements Under Person-Level Differential Privacy&lt;/a>;&lt;br />; &lt;strong>;Alexander Knop&lt;/strong>;, Thomas Steinke &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=GjNvvswoUL&quot;>;DICES Dataset: Diversity in Conversational AI Evaluation for Safety&lt;/a>;&lt;br />; &lt;strong>;Lora Aroyo&lt;/strong>;, Alex S. Taylor, &lt;strong>;Mark Diaz&lt;/strong>;, &lt;strong>;Christopher M. Homan&lt;/strong>;, &lt;strong>;Alicia Parrish&lt;/strong>;, Greg Serapio-García, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;, &lt;strong>;Ding Wang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SS3CK3yx5Z&quot;>;Does Progress on ImageNet Transfer to Real-world Datasets?&lt;/a>;&lt;br />; Alex Fang, &lt;strong>;Simon Kornblith&lt;/strong>;, Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=AA2uO0HHmr&quot;>;Estimating Generic 3D Room Structures from 2D Annotations&lt;/a>;&lt;br />; Denys Rozumnyi*, &lt;strong>;Stefan Popov&lt;/strong>;, &lt;strong>;Kevis-kokitsi Maninis&lt;/strong>;, Matthias Nießner, &lt;strong>;Vittorio Ferrari&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6hZIfAY9GD&quot;>;Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias&lt;/a>;&lt;br />; Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, &lt;strong>;Jiaming Shen&lt;/strong>;,&lt;strong>; &lt;/strong>;Chao Zhang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Y45ZCxslFx&quot;>;MADLAD-400: A Multilingual And Document-Level Large Audited Dataset&lt;/a>;&lt;br />; Sneha Kudugunta, &lt;strong>;Isaac Caswell&lt;/strong>;, Biao Zhang, Xavier Garcia, Derrick Xin, &lt;strong>;Aditya Kusupati&lt;/strong>;, Romi Stella, Ankur Bapna, Orhan Firat &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uhKtQMn21D&quot;>;Mechanic: A Learning Rate Tuner&lt;/a>;&lt;br />; Ashok Cutkosky, Aaron Defazio, &lt;strong>;Harsh Mehta&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8TMhs2pIfG&quot;>;NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations&lt;/a>;&lt;br />; &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Kevis-kokitsi Maninis&lt;/strong>;, &lt;strong>;Andreas Engelhardt&lt;/strong>;, &lt;strong>;Arjun Karpur&lt;/strong>;, &lt;strong>;Karen Truong&lt;/strong>;, &lt;strong>;Kyle Sargent&lt;/strong>;, &lt;strong>;Stefan Popov&lt;/strong>;, &lt;strong>;Andre Araujo&lt;/strong>;, &lt;strong>;Ricardo Martin Brualla&lt;/strong>;, &lt;strong>;Kaushal Patel&lt;/strong>;, &lt;strong>;Daniel Vlasic&lt;/strong>;,&lt;strong>; Vittorio Ferrari&lt;/strong>;, &lt;strong>;Ameesh Makadia&lt;/strong>;, Ce Liu*, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Howard Zhou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=x6cOcxRnxG&quot;>;Neural Ideal Large Eddy Simulation: Modeling Turbulence with Neural Stochastic Differential Equations&lt;/a>;&lt;br />; &lt;strong>;Anudhyan Boral&lt;/strong>;, &lt;strong>;Zhong Yi Wan&lt;/strong>;, &lt;strong>;Leonardo Zepeda-Nunez&lt;/strong>;, &lt;strong>;James Lottes&lt;/strong>;, &lt;strong>;Qing Wang&lt;/strong>;, &lt;strong>;Yi-Fan Chen&lt;/strong>;,&lt;strong>; John Roberts Anderson&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=wFuemocyHZ&quot;>;Restart Sampling for Improving Generative Processes&lt;/a>;&lt;br />; Yilun Xu, Mingyang Deng, Xiang Cheng, &lt;strong>;Yonglong Tian&lt;/strong>;, Ziming Liu, Tommi Jaakkola &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xUyBP16Q5J&quot;>;Rethinking Incentives in Recommender Systems: Are Monotone Rewards Always Beneficial?&lt;/a>;&lt;br />; Fan Yao, Chuanhao Li, Karthik Abinav Sankararaman, Yiming Liao, &lt;strong>;Yan Zhu&lt;/strong>;, Qifan Wang, Hongning Wang, Haifeng Xu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jGyMUum1Lq&quot;>;Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union&lt;/a>;&lt;br />; Zifu Wang, &lt;strong>;Maxim Berman&lt;/strong>;, Amal Rannen-Triki, Philip Torr, Devis Tuia, Tinne Tuytelaars, Luc Van Gool, Jiaqian Yu, Matthew B. Blaschko &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0H5fRQcpQ7&quot;>;RoboHive: A Unified Framework for Robot Learning&lt;/a>;&lt;br />; &lt;strong>;Vikash Kumar&lt;/strong>;, Rutav Shah, Gaoyue Zhou, Vincent Moens, Vittorio Caggiano, Abhishek Gupta, &lt;strong>;Aravind Rajeswaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Vn5qZGxGj3&quot;>;SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data&lt;/a>;&lt;br />; Mélisande Teng, Amna Elmustafa, Benjamin Akera, Yoshua Bengio, Hager Radi, &lt;strong>;Hugo Larochelle&lt;/strong>;, David Rolnick &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sqTcCXkG4P&quot;>;Sparsity-Preserving Differentially Private Training of Large Embedding Models&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, Yangsibo Huang*, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Amer Sinha&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xpjsOQtKqx&quot;>;StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners&lt;/a>;&lt;br />; &lt;strong>;Yonglong Tian&lt;/strong>;, &lt;strong>;Lijie Fan&lt;/strong>;, Phillip Isola, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EPz1DcdPVE&quot;>;Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning&lt;/a>;&lt;br />; &lt;strong>;Zachary Charles&lt;/strong>;, &lt;strong>;Nicole Mitchell&lt;/strong>;, &lt;strong>;Krishna Pillutla&lt;/strong>;, &lt;strong>;Michael Reneer&lt;/strong>;, &lt;strong>;Zachary Garrett&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zWxKYyW9ik&quot;>;Universality and Limitations of Prompt Tuning&lt;/a>;&lt;br />; Yihan Wang, Jatin Chauhan, Wei Wang, &lt;strong>;Cho-Jui Hsieh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sovxUzPzLN&quot;>;Unsupervised Semantic Correspondence Using Stable Diffusion&lt;/a>;&lt;br />; Eric Hedlin, Gopal Sharma, Shweta Mahajan, &lt;strong>;Hossam Isack&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;,&lt;strong>; &lt;/strong>;Kwang Moo Yi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QEDjXv9OyY&quot;>;YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus&lt;/a>;&lt;br />; &lt;strong>;Dave Uthus&lt;/strong>;, &lt;strong>;Garrett Tanzer&lt;/strong>;, &lt;strong>;Manfred Georg&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=swNtr6vGqg&quot;>;The Noise Level in Linear Regression with Dependent Data&lt;/a>;&lt;br />; Ingvar Ziemann, &lt;strong>;Stephen Tu&lt;/strong>;, George J. Pappas, Nikolai Matni &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8223613466421639113/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-neurips-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8223613466421639113&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8223613466421639113&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-neurips-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at NeurIPS 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s72-c/google_research_sticker-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6514019106482066697&lt;/id>;&lt;published>;2023-12-08T10:34:00.000-08:00&lt;/published>;&lt;updated>;2024-01-08T07:50:55.303-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Sparsity-preserving differentially private training&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yangsibo Huang, Research Intern, and Chiyuan Zhang, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s1600/Sparse%20DP-SGD.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;em>;Large embedding models&lt;/em>; have emerged as a fundamental tool for various applications in recommendation systems [&lt;a href=&quot;https://research.google/pubs/pub45530/&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.05158&quot;>;2&lt;/a>;] and natural language processing [&lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;>;3&lt;/a>;, &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;>;4&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;5&lt;/a>;]. Such models enable the integration of non-numerical data into deep learning models by mapping categorical or &lt;a href=&quot;https://en.wikipedia.org/wiki/String_(computer_science)&quot;>;string&lt;/a>;-valued input attributes with large vocabularies to fixed-length representation vectors using embedding layers. These models are widely deployed in personalized recommendation systems and achieve state-of-the-art performance in language tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;>;language modeling&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;sentiment analysis&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;question answering&lt;/a>;. In many such scenarios, privacy is an equally important feature when deploying those models. As a result, various techniques have been proposed to enable private data analysis. Among those, &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>; (DP) is a widely adopted definition that limits exposure of individual user information while still allowing for the analysis of population-level patterns. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; For training deep neural networks with DP guarantees, the most widely used algorithm is &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>; (DP &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>;). One key component of DP-SGD is adding &lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_noise&quot;>;Gaussian noise&lt;/a>; to every coordinate of the gradient vectors during training. However, this creates scalability challenges when applied to &lt;em>;large embedding models&lt;/em>;, because they rely on gradient sparsity for efficient training, but adding noise to all the coordinates destroys sparsity. &lt;/p>; &lt;p>; To mitigate this gradient sparsity problem, in “&lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;Sparsity-Preserving Differentially Private Training of Large Embedding Models&lt;/a>;” (to be presented at &lt;a href=&quot;https://nips.cc/Conferences/2023&quot;>;NeurIPS 2023&lt;/a>;), we propose a new algorithm called &lt;em>;adaptive filtering-enabled sparse training&lt;/em>; (DP-AdaFEST). At a high level, the algorithm maintains the sparsity of the gradient by selecting only a subset of feature rows to which noise is added at each iteration. The key is to make such selections differentially private so that a three-way balance is achieved among the privacy cost, the training efficiency, and the model utility. Our empirical evaluation shows that DP-AdaFEST achieves a substantially sparser gradient, with a reduction in gradient size of over 10&lt;sup>;5&lt;/sup>;X compared to the dense gradient produced by standard DP-SGD, while maintaining comparable levels of accuracy 。 This gradient size reduction could translate into 20X wall-clock time improvement. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overview&lt;/h2>; &lt;p>; To better understand the challenges and our solutions to the gradient sparsity problem, let us start with an overview of how DP-SGD works during training. As illustrated by the figure below, DP-SGD operates by clipping the gradient contribution from each example in the current random subset of samples (called a mini-batch), and adding coordinate-wise &lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_noise&quot;>;Gaussian noise&lt;/a>; to the average gradient during each iteration of &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>; (SGD). DP-SGD has demonstrated its effectiveness in protecting user privacy while maintaining model utility in a variety of applications [&lt;a href=&quot;https://arxiv.org/pdf/2204.13650.pdf&quot;>;6&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2110.06500.pdf&quot;>;7&lt;/a>;]. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of how DP-SGD works. During each training step, a mini-batch of examples is sampled, and used to compute the per-example gradients. Those gradients are processed through clipping, aggregation and summation of Gaussian noise to produce the final privatized gradients.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The challenges of applying DP-SGD to large embedding models mainly come from 1) the non-numerical feature fields like user/product IDs and categories, and 2) words and tokens that are transformed into dense vectors through an embedding layer. Due to the vocabulary sizes of those features, the process requires large embedding tables with a substantial number of parameters. In contrast to the number of parameters, the gradient updates are usually extremely sparse because each mini-batch of examples only activates a tiny fraction of embedding rows (the figure below visualizes the ratio of zero-valued coordinates, ie, the sparsity, of the gradients under various batch sizes). This sparsity is heavily leveraged for industrial applications that efficiently handle the training of large-scale embeddings. For example, &lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Google Cloud TPUs&lt;/a>;, custom-designed AI accelerators that are optimized for training and inference of large AI models, have &lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/building-large-scale-recommenders-using-cloud-tpus&quot;>;dedicated APIs&lt;/a>; to handle large embeddings with sparse updates. This leads to &lt;a href=&quot;https://eng.snap.com/training-models-with-tpus&quot;>;significantly improved training throughput&lt;/a>; compared to training on GPUs, which at this time did not have specialized optimization for sparse embedding lookups. On the other hand, DP-SGD completely destroys the gradient sparsity because it requires adding independent Gaussian noise to &lt;em>;all&lt;/em>; the coordinates. This creates a road block for private training of large embedding models as the training efficiency would be significantly reduced compared to non-private training. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s1814/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1058&quot; data-original-width=&quot;1814&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Embedding gradient sparsity (the fraction of zero-value gradient coordinates) in the &lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset&quot;>;Criteo pCTR&lt;/a>; model (see below). The figure reports the gradient sparsity, averaged over 50 update steps, of the top five categorical features (out of a total of 26) with the highest number of buckets, as well as the sparsity of all categorical features. The sprasity decreases with the batch size as more examples hit more rows in the embedding table, creating non-zero gradients. However, the sparsity is above 0.97 even for very large batch sizes. This pattern is consistently observed for all the five features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Algorithm&lt;/h2>; &lt;p>; Our algorithm is built by extending standard DP-SGD with an extra mechanism at each iteration to privately select the “hot features”, which are the features that are activated by multiple training examples in the current mini-batch. As illustrated below, the mechanism works in a few steps: &lt;/p>; &lt;ol>; &lt;li>;Compute how many examples contributed to each feature bucket (we call each of the possible values of a categorical feature a “bucket”). &lt;/li>;&lt;li>;Restrict the total contribution from each example by clipping their counts. &lt;/li>;&lt;li>;Add Gaussian noise to the contribution count of each feature bucket. &lt;/li>;&lt;li>;Select only the features to be included in the gradient update that have a count above a given threshold (a sparsity-controlling parameter), thus maintaining sparsity. This mechanism is differentially private, and the privacy cost can be easily computed by composing it with the standard DP-SGD iterations. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s1600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;743&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the process of the algorithm on a synthetic categorical feature that has 20 buckets. We compute the number of examples contributing to each bucket, adjust the value based on per-example total contributions (including those to other features), add Gaussian noise, and retain only those buckets with a noisy contribution exceeding the threshold for (noisy) gradient update.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Theoretical motivation&lt;/h2>; &lt;p>; We provide the theoretical motivation that underlies DP-AdaFEST by viewing it as optimization using stochastic &lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_complexity_(optimization)&quot;>;gradient oracles&lt;/a>;. Standard analysis of stochastic gradient descent in a theoretical setting decomposes the test error of the model into &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&quot;>;“bias” and “variance” terms&lt;/a>;. The advantage of DP-AdaFEST can be viewed as reducing variance at the cost of slightly increasing the bias. This is because DP-AdaFEST adds noise to a smaller set of coordinates compared to DP-SGD, which adds noise to all the coordinates. On the other hand, DP-AdaFEST introduces some bias to the gradients since the gradient on the embedding features are dropped with some probability. We refer the interested reader to Section 3.4 of the &lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We evaluate the effectiveness of our algorithm with large embedding model applications, on public datasets, including one ad prediction dataset (&lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset&quot;>;Criteo-Kaggle&lt;/a>;) and one language understanding dataset (&lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;SST-2&lt;/a>;). We use &lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;DP-SGD with exponential selection&lt;/a>; as a baseline comparison. &lt;/p>; &lt;p>; The effectiveness of DP-AdaFEST is evident in the figure below, where it achieves significantly higher gradient size reduction (ie, gradient sparsity) than the baseline while maintaining the same level of utility (ie, only minimal performance降解）。 &lt;/p>; &lt;p>; Specifically, on the Criteo-Kaggle dataset, DP-AdaFEST reduces the gradient computation cost of regular DP-SGD by more than 5x10&lt;sup>;5&lt;/sup>; times while maintaining a comparable &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;>;AUC&lt;/a>; (which we define as a loss of less than 0.005). This reduction translates into a more efficient and cost-effective training process. In comparison, as shown by the green line below, the baseline method is not able to achieve reasonable cost reduction within such a small utility loss threshold. &lt;/p>; &lt;p>; In language tasks, there isn&#39;t as much potential for reducing the size of gradients, because the vocabulary used is often smaller and already quite compact (shown on the right below). However, the adoption of sparsity-preserving DP-SGD effectively obviates the dense gradient computation. Furthermore, in line with the bias-variance trade-off presented in the theoretical analysis, we note that DP-AdaFEST occasionally exhibits superior utility compared to DP-SGD when the reduction in gradient size is minimal. Conversely, when incorporating sparsity, the baseline algorithm faces challenges in maintaining utility. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s1860/image6.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1860&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A comparison of the best gradient size reduction (the ratio of the non-zero gradient value counts between regular DP-SGD and sparsity-preserving algorithms) achieved under ε =1.0 by DP -AdaFEST (our algorithm) and the baseline algorithm (&lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;DP-SGD with exponential selection&lt;/a>;) compared to DP-SGD at different thresholds for utility不同之处。 A higher curve indicates a better utility/efficiency trade-off.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In practice, most ad prediction models are being continuously trained and evaluated. To simulate this online learning setup, we also evaluate with time-series data, which are notoriously challenging due to being non-stationary. Our evaluation uses the &lt;a href=&quot;https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/&quot;>;Criteo-1TB&lt;/a>; dataset, which comprises real-world user-click data collected over 24 days. Consistently, DP-AdaFEST reduces the gradient computation cost of regular DP-SGD by more than 10&lt;sup>;4&lt;/sup>; times while maintaining a comparable AUC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s2500/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;891&quot; data-original-width=&quot;2500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A comparison of the best gradient size reduction achieved under ε =1.0 by DP-AdaFEST (our algorithm) and DP-SGD with exponential selection (a previous algorithm) compared to DP-SGD at different thresholds for utility difference. A higher curve indicates a better utility/efficiency trade-off. DP-AdaFEST consistently outperforms the previous method.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present a new algorithm, DP-AdaFEST, for preserving gradient sparsity in differentially private training — particularly in applications involving large embedding models, a fundamental tool for various applications in recommendation systems and natural language processing. Our algorithm achieves significant reductions in gradient size while maintaining accuracy on real-world benchmark datasets. Moreover, it offers flexible options for balancing utility and efficiency via sparsity-controlling parameters, while our proposals offer much better privacy-utility loss. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was a collaboration with Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi and Amer Sinha.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6514019106482066697/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html&quot; rel=&quot;alternate&quot; title=&quot;Sparsity-preserving differentially private training&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s72-c/Sparse%20DP-SGD.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5912569868305742102&lt;/id>;&lt;published>;2023-12-07T09:51:00.000-08:00&lt;/published>;&lt;updated>;2023-12-07T09:53:23.920-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Virtual Reality&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VALID: A perceptually validated virtual avatar library for inclusion and diversity&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Mar Gonzalez-Franco, Research Scientist, Google AR &amp;amp; VR&lt;/span>;&lt;p>; As virtual reality (VR) and augmented reality (AR) technologies continue to grow in popularity, virtual avatars are becoming an increasingly important part of our digital interactions. In particular, virtual avatars are at the center of many social VR and AR interactions, as they are key to representing remote participants and facilitating collaboration. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In the last decade, interdisciplinary scientists have dedicated a significant amount of effort to better understand the use of avatars, and have made many interesting observations, including the capacity of the users to &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frvir.2020.575943/full&quot;>;embody their avatar&lt;/a>; (ie, the illusion that the avatar body is their own) and the &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9089510&quot;>;self-avatar follower effect&lt;/a>;, which creates a binding between the actions of the avatar and the user strong enough that the avatar can actually affect user behavior. &lt;/p>; &lt;p>; The use of avatars in experiments isn&#39;t just about how users will interact and behave in VR spaces, but also about discovering the limits of human perception and neuroscience. In fact, some VR social experiments often rely on recreating scenarios that can&#39;t be reproduced easily in the real world, such as bar crawls to &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0052766&quot;>;explore ingroup vs. outgroup effects&lt;/a>;, or deception experiments, such as the &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0209704&quot;>;Milgram obedience to authority inside virtual reality&lt;/a>;. Other studies try to explore deep neuroscientific phenomena, like the &lt;a href=&quot;https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2021.0453&quot;>;human mechanisms for motor control&lt;/a>;. This perhaps follows the trail of the &lt;a href=&quot;https://www.nature.com/articles/35784&quot;>;rubber hand illusion&lt;/a>; on brain plasticity, where a person can start feeling as if they own a rubber hand while their real hand is hidden behind a curtain. There is also an increased number of possible therapies for psychiatric treatment using personalized &lt;a href=&quot;https://doi.org/10.1186/s13063-022-06683-1&quot;>;avatars&lt;/a>;. In these cases, VR becomes an &lt;a href=&quot;https://en.wikipedia.org/wiki/Ecological_validity&quot;>;ecologically valid&lt;/a>; tool that allows scientists to explore or treat human behavior and perception. &lt;/p>; &lt;p>; None of these experiments and therapies could exist without good access to research tools and libraries that can enable easy experimentation. As such, multiple systems and open source tools have been released around avatar creation and animation over recent years. However, existing avatar libraries have not been validated systematically on the diversity spectrum. Societal &lt;a href=&quot;https://doi.org/10.1016/j.concog.2013.04.016&quot;>;bias and dynamics&lt;/a>; also transfer to VR/AR when interacting with avatars, which could lead to incomplete conclusions for studies on human behavior inside VR/AR. &lt;/p>; &lt;p>; To partially overcome this problem, we partnered with the University of Central Florida to create and release the open-source &lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;Virtual Avatar Library for Inclusion and Diversity&lt;/a>; (VALID). Described in &lt;a href=&quot;https://doi.org/10.3389/frvir.2023.1248915&quot;>;our recent paper&lt;/a>;, published in &lt;a href=&quot;https://www.frontiersin.org/journals/virtual -reality&quot;>;&lt;i>;Frontiers in Virtual Reality&lt;/i>;&lt;/a>;, this library of avatars is readily available for usage in VR/AR experiments and includes 210 avatars of seven different races and ethnicities recognized by the US Census Bureau 。 The avatars have been perceptually validated and designed to advance diversity and inclusion in virtual avatar research. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s1717/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1413&quot; data-original-width=&quot;1717&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Headshots of all 42 base avatars available on the VALID library were created in extensive interaction with members of the 7 ethnic and racial groups from the &lt;a href=&quot;https://www.federalregister.gov/documents/2023/01/27/2023-01635/initial-proposals-for-updating-ombs-race-and-ethnicity-statistical-standards&quot;>;Federal Register&lt;/a>;, which include (AIAN, Asian, Black, Hispanic, MENA, NHPI and White).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Creation and validation of the library&lt;/h2>; &lt;p>; Our initial selection of races and ethnicities for the diverse avatar library follows the most recent guidelines of the &lt;a href=&quot;https://www.npr.org/2023/01/26/1151608403/mena-race-categories-us-census-middle-eastern-latino-hispanic&quot;>;US Census Bureau&lt;/a>; that as of 2023 recommended the use of 7 ethnic and racial groups representing a large demographic of the US society, which can also be extrapolated to the global population. These groups include &lt;em>;Hispanic or Latino&lt;/em>;, &lt;em>;American Indian or Alaska Native (AIAN), Asian,&lt;/em>; &lt;em>;Black or African American,&lt;/em>; &lt;em>;Native Hawaiian or Other Pacific Islander (NHPI),&lt;/em>; &lt;em>;White, Middle East or North Africa&lt;/em>; (MENA). We envision the library will continue to evolve to bring even more diversity and representation with future additions of avatars. &lt;/p>; &lt;p>; The avatars were hand modeled and created using a process that combined average facial features with extensive collaboration with representative stakeholders from each racial group, where their feedback was used to artistically modify the facial mesh of the avatars. Then we conducted an online study with participants from 33 countries to determine whether the race and gender of each avatar in the library are recognizable. In addition to the avatars, we also provide labels statistically validated through observation of users for the race and gender of all 42 base avatars (see below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V/s1999/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;736&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the headshots of a Black/African American avatar presented to participants during the validation of the library.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We found that all Asian, Black, and White avatars were universally identified as their modeled race by all participants, while our American Indian or Native Alaskan (AIAN), Hispanic, and Middle Eastern or North African (MENA) avatars were typically only identified by participants of the same race. This also indicates that participant race can improve identification of a virtual avatar of the same race. The paper accompanying the library release highlights how this ingroup familiarity should also be taken into account when studying avatar behavior in VR. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s1999/image1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1509&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Confusion matrix heatmap of agreement rates for the 42 base avatars separated by other-race participants and same-race participants. One interesting aspect visible in this matrix, is that participants were significantly better at identifying the avatars of their own race than other races.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset details&lt;/h2>; &lt;p>; Our models are available in &lt;a href=&quot;https://www.autodesk.com/products/fbx/overview&quot;>;FBX format&lt;/a>;, are compatible with previous avatar libraries like the commonly used &lt;a href=&quot;https://github.com/microsoft/Microsoft-Rocketbox&quot;>;Rocketbox&lt;/a>;, and can be easily integrated into most game engines such as &lt;a href=&quot;https://unity.com/&quot;>;Unity&lt;/a>; and &lt;a href=&quot;https://www.unrealengine.com/&quot;>;Unreal&lt;/a>;. Additionally, the avatars come with 69 bones and 65 facial blendshapes to enable researchers and developers to easily create and apply dynamic facial expressions and animations. The avatars were intentionally made to be partially cartoonish to avoid extreme look-a-like scenarios in which a person could be impersonated, but still representative enough to be able to run reliable user studies and social experiments. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s1999/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;986&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images of the skeleton rigging (bones that allow for animation) and some facial blend shapes included with the VALID avatars.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The avatars can be further combined with variations of casual attires and five professional attires, including medical, military, worker and business. This is an intentional improvement from prior libraries that in some cases reproduced stereotypical gender and racial bias into the avatar attires, and provided very limited diversity to certain professional avatars. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s1537/image5.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1023&quot; data-original-width=&quot;1537&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images of some sample attire included with the VALID avatars.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Get started with VALID&lt;/h2>; &lt;p>; We believe that the &lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;Virtual Avatar Library for Inclusion and Diversity&lt;/a>; (VALID) will be a valuable resource for researchers and developers working on VR/AR applications. We hope it will help to create more inclusive and equitable virtual experiences. To this end, we invite you to explore the avatar library, which we have released under the open source &lt;a href=&quot;https://en.wikipedia.org/wiki/MIT_License&quot;>;MIT license&lt;/a>;. You can download the avatars and use them in a variety of settings at no charge. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This library of avatars was born out of a collaboration with Tiffany D. Do, Steve Zelenty and Prof. Ryan P McMahan from the University of Central Florida.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5912569868305742102/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html&quot; rel=&quot;alternate&quot; title=&quot;VALID: A perceptually validated virtual avatar library for inclusion and diversity&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s72-c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8414954450937764241&lt;/id>;&lt;published>;2023-12-05T17:32:00.000-08:00&lt;/published>;&lt;updated>;2024-01-03T14:16:52.565-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;EMNLP&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at EMNLP 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s320/EMNLP%202023.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Google is proud to be a &lt;a href=&quot;https://2023.emnlp.org/sponsors/&quot;>;Diamond Sponsor&lt;/a>; of &lt;a href=&quot;https://2023.emnlp.org/&quot;>;Empirical Methods in Natural Language Processing&lt;/a>; (EMNLP 2023), a premier annual conference, which is being held this week in Sentosa, Singapore. Google has a strong presence at this year&#39;s conference with over 65 accepted papers and active involvement in 11 workshops and tutorials. Google is also happy to be a &lt;a href=&quot;https://www.winlp.org/winlp-2023-workshop/winlp-2023-sponsors/&quot;>;Major Sponsor&lt;/a>; for the &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/winlp-2023-workshop/&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1701403043253017&amp;amp;usg=AOvVaw2oPPnFe0AEcdP1iSblNV91&quot;>;Widening NLP&lt;/a>; workshop (WiNLP), which aims to highlight global representations of people, perspectives, and cultures in AI and ML. We look forward to sharing some of our extensive NLP research and expanding our partnership with the broader research community. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We hope you&#39;ll visit the Google booth to chat with researchers who are actively pursuing the latest innovations in NLP, and check out some of the scheduled booth activities (eg, demos and Q&amp;amp;A sessions listed below). Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) and &lt;a href=&quot;https://www.linkedin.com/showcase/googleresearch/?viewAsMember=true&quot;>;LinkedIn&lt;/a>; accounts to find out more about the Google booth activities at EMNLP 2023. &lt;/p>; &lt;p>; Take a look below to learn more about the Google research being presented at EMNLP 2023 (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board &amp;amp; Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Sponsorship Chair: &lt;strong>;&lt;em>;Shyam Upadyay&lt;/em>;&lt;/strong>; &lt;br />; Industry Track Chair: &lt;strong>;&lt;em>;Imed Zitouni&lt;/em>;&lt;/strong>; &lt;br />; Senior Program Committee: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Annie Louis&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brian Roark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03291.pdf&quot;>;SynJax: Structured Probability Distributions for JAX&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Miloš Stanojević&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Laurent Sartran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.11077.pdf&quot;>;Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning&lt;/a>; &lt;br />; &lt;em>;Clifton Poth&lt;/em>;, &lt;em>;Hannah Sterz&lt;/em>;, &lt;em>;Indraneil Paul&lt;/em>;, &lt;em>;Sukannya Purkayastha&lt;/em>;, &lt;em>;Leon Engländer&lt;/em>;,&lt;em>; Timo Imhof&lt;/em>;, &lt;em>;Ivan Vulić&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;Iryna Gurevych&lt;/em>;, &lt;strong>;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.08937.pdf&quot;>;DocumentNet: Bridging the Data Gap in Document Pre-training&lt;/a>; &lt;br />; &lt;em>;Lijun Yu&lt;/em>;, &lt;strong>;&lt;em>;Jin Miao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xiaoyu Sun&lt;/em>;&lt;/strong>;, &lt;em>;Jiayi Chen&lt;/em>;, &lt;em>;Alexander Hauptmann&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wei Wei&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.08592.pdf&quot;>;AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-Powered Applications&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Bhaktipriya Radharapu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kevin Robinson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.15239.pdf&quot;>;CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks&lt;/a>; &lt;br />; &lt;em>;Mete Ismayilzada&lt;/em>;, &lt;em>;Debjit Paul&lt;/em>;, &lt;em>;Syrielle Montariol&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;, &lt;em>;Antoine Bosselut&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11610.pdf&quot;>;Large Language Models Can Self-Improve&lt;/a>; &lt;br />; &lt;em>;Jiaxin Huang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Shixiang Shane Gu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuexin Wu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuezhi Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hongkun Yu&lt;/em>;&lt;/strong>;, &lt;em>;Jiawei Han&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14767.pdf&quot;>;Dissecting Recall of Factual Associations in Auto-Regressive Language Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Katja Filippova&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10160.pdf&quot;>;Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Omer Goldman&lt;/em>;, &lt;em>;Yoav Goldberg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.16391.pdf&quot;>;Selective Labeling: How to Radically Lower Data-Labeling Costs for Document Extraction Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yichao Zhou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;James Bradley Wendt&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Navneet Potti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jing Xie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sandeep Tata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2112.12870.pdf&quot;>;Measuring Attribution in Natural Language Generation Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Hannah Rashkin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vitaly Nikolaev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthew Lamm&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Collins&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dipanjan&lt;/em>; &lt;em>;Das&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Slav Petrov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gaurav Singh Tomar&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Iulia Turc&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.02011.pdf&quot;>;Inverse Scaling Can Become U-Shaped&lt;/a>; &lt;br />; &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;, &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Quoc Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14282.pdf&quot;>;INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback&lt;/a>; &lt;br />; &lt;em>;Wenda Xu&lt;/em>;, &lt;em>;Danqing Wang&lt;/em>;, &lt;em>;Liangming Pan&lt;/em>;, &lt;em>;Zhenqiao Song&lt;/em>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;, &lt;em>;William Yang Wang&lt;/em>;, &lt;em>;Lei Li&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.14796.pdf&quot;>;On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-Based Method&lt;/a>; &lt;br />; &lt;em>;Zorik Gekhman&lt;/em>;, &lt;em>;Nadav Oved&lt;/em>;,&lt;strong>; &lt;em>;Orgad Keller&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roi Reichart&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.04347.pdf&quot;>;Investigating Efficiently Extending Transformers for Long-Input Summarization&lt;/a>; &lt;br />; &lt;em>;Jason Phang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yao Zhao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Peter J Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09744.pdf&quot;>;DSI++: Updating Transformer Memory with New Documents&lt;/a>; &lt;br />; &lt;em>;Sanket Vaibhav Mehta&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jinfeng Rao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Marc Najork&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Emma Strubell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Donald Metzler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.12029.pdf&quot;>;MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup&lt;/a>; &lt;br />; &lt;em>;Hua Shen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Vicky Zayats&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Johann C Rocholl&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel David Walker&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dirk Padfield&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14318.pdf&quot;>;q2d: Turning Questions into Dialogs to Teach Models How to Search&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shlomi Cohen-Ganor&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Ido Hakimi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yoad Lewenberg&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Enav Weinreb&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02171.pdf&quot;>;Emergence of Abstract State Representations in Embodied Sequence Modeling&lt;/a>; &lt;br />; &lt;em>;Tian Yun&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Zilai Zeng&lt;/em>;, &lt;em>;Kunal Handa&lt;/em>;, &lt;strong>;&lt;em>;Ashish V Thapliyal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bo Pang&lt;/em>;&lt;/strong>;, &lt;em>;Ellie Pavlick&lt;/em>;, &lt;em>;Chen Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14332.pdf&quot;>;Evaluating and Modeling Attribution for Cross-Lingual Question Answering&lt;/a>; &lt;br />; &lt;em>;Benjamin Muller&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2305.14281.pdf&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701762357021319&amp;amp;usg=AOvVaw2Q_4he8TIMmr8URRV1w4uX&quot;>;Weakly-Supervised Learning of Visual Relations in Multimodal Pre-training&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Emanuele Bugliarello&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lisa Anne Hendricks&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13286.pdf&quot;>;How Do Languages Influence Each Other? Studying Cross-Lingual Data Sharing During LM Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni&lt;/em>;, &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>;, &lt;em>;Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14214.pdf&quot;>;CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models&lt;/a>; &lt;br>; &lt;em>;Benjamin Minixhofer&lt;/em>;, &lt;strong>;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>;, &lt;em>;Ivan Vulić&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.01328.pdf&quot;>;IC3: Image Captioning by Committee Consensus&lt;/a>; &lt;br />; &lt;em>;David Chan&lt;/em>;, &lt;strong>;&lt;em>;Austin Myers&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sudheendra Vijayanarasimhan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David A Ross&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; John Canny&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11877.pdf&quot;>;The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;, &lt;em>;Omer Goldman&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Ido Dagan&lt;/em>;, &lt;em>;Shauli Ravfogel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14542.pdf&quot;>;Evaluating Large Language Models on Controlled Generation Tasks&lt;/a>; &lt;br />; &lt;em>;Jiao Sun&lt;/em>;, &lt;em>;Yufei Tian&lt;/em>;, &lt;em>;Wangchunshu Zhou&lt;/em>;, &lt;em>;Nan Xu&lt;/em>;, &lt;em>;Qian Hu&lt;/em>;, &lt;em>;Rahul Gupta&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;em>;Nanyun Peng&lt;/em>;, &lt;em>;Xuezhe Ma&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14324.pdf&quot;>;Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Daniel Deutsch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;George Foster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11399.pdf&quot;>;Transcending Scaling Laws with 0.1% Extra Compute&lt;/a>; &lt;br />; &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; Hyung Won Chung&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;, &lt;em>;David R. So&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Siamak Shakeri&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Huaixiu Steven Zheng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jinfeng Rao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Donald&lt;/em>; &lt;em>;Metzler&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Slav Petrov,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Quoc V. Le&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.09006.pdf&quot;>;Data Similarity is Not Enough to Explain Language Model Performance&lt;/a>; &lt;br />; &lt;em>;Gregory Yauney&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Emily Reif&lt;/em>;&lt;/strong>;, &lt;em>;David Mimno&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.00913.pdf&quot;>;Self-Influence Guided Data Reweighting for Language Model Pre-training&lt;/a>; &lt;br />; &lt;em>;Megh Thakkar&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Tolga Bolukbasi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sriram Ganapathy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shikhar Vashishth&lt;/em>;&lt;/strong>;, &lt;em>; Sarath Chandar&lt;/em>;, &lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11826.pdf&quot;>;ReTAG: Reasoning Aware Table to Analytic Text Generation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Deepanway Ghosal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Preksha Nema&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aravindan Raghuveer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15265v1.pdf&quot;>;GATITOS: Using a New Multilingual Lexicon for Low-Resource Machine Translation&lt;/a>; &lt;br />; &lt;em>;Alex Jones&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ishank Saxena&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.20201v1.pdf&quot;>;Video-Helpful Multimodal Machine Translation&lt;/a>; &lt;br />; &lt;em>;Yihang Li, Shuichiro Shimizu&lt;/em>;, &lt;em>;Chenhui Chu&lt;/em>;, &lt;em>;Sadao Kurohashi&lt;/em>;, &lt;strong>;&lt;em>;Wei Li&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.08298.pdf&quot;>;Symbol Tuning Improves In-Context Learning in Language Models&lt;/a>; &lt;br />; &lt;em>;Jerry Wei&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Andrew Kyle Lampinen&lt;/strong>;&lt;/em>;, &lt;em>;Xiangning Chen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Da Huang&lt;/em>;&lt;/strong>;, &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Xinyun&lt;/em>; &lt;em>;Chen&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yifeng Lu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;Tengyu Ma&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Quoc V Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14755.pdf&quot;>;&quot;Don&#39;t Take This Out of Context!&quot; On the Need for Contextual Models and Evaluations for Stylistic Rewriting&lt;/a>; &lt;br />; &lt;em>;Akhila Yerukola&lt;/em>;, &lt;em>;Xuhui Zhou&lt;/em>;, &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>;, &lt;em>;Maarten Sap&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08264.pdf&quot;>;QAmeleon: Multilingual QA with Only 5 Examples&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Priyanka Agrawal&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Chris Alberti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fantine Huot&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Joshua Maynez&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ji Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kuzman Ganchev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mirella Lapata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03540.pdf&quot;>;Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Eugene Kharitonov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Damien Vincent&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zalán Borsos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raphaël Marinier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sertan Girgin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Matt Sharifi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Marco Tagliasacchi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Zeghidour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09939.pdf&quot;>;AnyTOD: A Programmable Task-Oriented Dialog System&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jeffrey Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuan Cao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghav Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Harrison Lee&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mingqiu Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hagen Soltau&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Izhak Shafran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yonghui Wu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14613.pdf&quot;>;Selectively Answering Ambiguous Questions&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jeremy R. Cole&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael JQ Zhang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Gillick&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Julian Martin Eisenschlos&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bhuwan Dhingra&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jacob Eisenstein&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08954.pdf&quot;>;PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs&lt;/a>; (see &lt;a href=&quot;https://blog.research.google/2023/03/presto-multilingual-dataset-for-parsing.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;strong>;&lt;em>;Rahul Goel&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Waleed Ammar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aditya Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Siddharth Vashishtha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Motoki Sano&lt;/em>;&lt;/strong>;,&lt;em>; Faiz Surani&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Max Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;HyunJeong Choe&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;David Greene&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Chuan He&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rattima Nitisaroj&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anna&lt;/em>; &lt;em>;Trukhina&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shachi Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pararth Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rushin Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhou Yu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13281.pdf&quot;>;LM vs LM: Detecting Factual Errors via Cross Examination&lt;/a>; &lt;br />; &lt;em>;Roi Cohen&lt;/em>;, &lt;em>;May Hamri&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.03668.pdf&quot;>;A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding&lt;/a>; &lt;br />; &lt;em>;Andrea Burns&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Krishna Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Geoff Brown&lt;/em>;&lt;/strong>;, &lt;em>;Bryan A. Plummer&lt;/em>;, &lt;em>;Kate&lt;/em>; &lt;em>;Saenko&lt;/em>;, &lt;strong>;&lt;em>;Jianmo Ni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mandy Guo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2302.08956.pdf&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701763216883702&amp;amp;usg=AOvVaw1QisabqC-Gy-U4ou1jbHAY&quot;>;AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages&lt;/a>; &lt;br />; &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;, &lt;em>;Idris Abdulmumin&lt;/em>;, &lt;em>;Abinew Ali Ayele&lt;/em>;, &lt;em>;Nedjma Ousidhoum&lt;/em>;, &lt;em>;David Ifeoluwa Adelani&lt;/em>;, &lt;em>;Seid Muhie Yimam&lt;/em>;, &lt;em>;Ibrahim Said Ahmad&lt;/em>;, &lt;em>;Meriem Beloucif&lt;/em>;, &lt;em>;Saif M.&lt;/em>; &lt;em>;Mohammad&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;Oumaima Hourrane&lt;/em>;, &lt;em>;Alipio Jorge&lt;/em>;, &lt;em>;Pavel Brazdil&lt;/em>;, &lt;em>;Felermino D. M&lt;/em>;. &lt;em>;A. Ali&lt;/em>;, &lt;em>;Davis David&lt;/em>;, &lt;em>;Salomey Osei&lt;/em>;, &lt;em>;Bello Shehu-Bello&lt;/em>;, &lt;em>;Falalu Ibrahim Lawan&lt;/em>;, &lt;em>;Tajuddeen&lt;/em>; &lt;em>;Gwadabe&lt;/em>;, &lt;em>;Samuel Rutunda&lt;/em>;, &lt;em>;Tadesse Destaw Belay&lt;/em>;, &lt;em>;Wendimu Baye Messelle&lt;/em>;,&lt;em>; Hailu Beshada&lt;/em>; &lt;em>;Balcha&lt;/em>;, &lt;em>;Sisay Adugna Chala&lt;/em>;, &lt;em>;Hagos Tesfahun Gebremichael&lt;/em>;,&lt;em>; Bernard Opoku&lt;/em>;, &lt;em>;Stephen Arthur&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13682.pdf&quot;>;Optimizing Retrieval-Augmented Reader Models via Token Elimination&lt;/a>; &lt;br />; &lt;em>;Moshe Berchansky&lt;/em>;, &lt;em>;Peter Izsak&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;,&lt;em>; Ido Dagan&lt;/em>;, &lt;em>;Moshe Wasserblat&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13194.pdf&quot;>;SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Maynez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vitaly Nikolaev&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Thibault Sellam&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aditya Siddhant&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13245.pdf&quot;>;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;James Lee-Thorp&lt;/em>;&lt;/strong>;, &lt;em>;Michiel de Jong&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Federico Lebron&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09752.pdf&quot;>;CoLT5: Faster Long-Range Transformers with Conditional Computation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tao Lei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michiel de Jong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Santiago Ontanon&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Siddhartha Brahma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Uthus&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mandy Guo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;James Lee-Thorp&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yun-Hsuan Sung&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16523.pdf&quot;>;Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nicholas Blumm&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiao Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghavendra Kotikalapudi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sahitya Potluri&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Qijun Tan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hansa Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;, &lt;em>;Alex Beutel&lt;/em>;, &lt;strong>;&lt;em>;Jilin Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14926.pdf&quot;>;Universal Self-Adaptive Prompting&lt;/a>; (see &lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Xingchen Wan&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; &lt;strong>;Ruoxi Sun, Hootan Nakhost&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Julian Martin Eisenschlos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sercan O. Arik&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11171.pdf&quot;>;TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zorik Gekhman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Chen Elkind&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.07871.pdf&quot;>;Hierarchical Pre-training on Multimodal Electronic Health Records&lt;/a>; &lt;br />; &lt;em>;Xiaochen Wang&lt;/em>;, &lt;em>;Junyu Luo&lt;/em>;, &lt;em>;Jiaqi Wang&lt;/em>;, &lt;em>;Ziyi Yin&lt;/em>;, &lt;em>;Suhan Cui&lt;/em>;, &lt;em>;Yuan Zhong&lt;/em>;, &lt;strong>;&lt;em>;Yaqing Wang&lt;/em>;&lt;/strong>;, &lt;em>;Fenglong Ma&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14499.pdf&quot;>;NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Gillick&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremy R. Cole&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11841.pdf&quot;>;How Does Generative Retrieval Scale to Millions of Passages?&lt;/a>; &lt;br />; &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Kai Hui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Honglei Zhuang&lt;/em>;&lt;/strong>;, &lt;em>;Jimmy Lin&lt;/em>;, &lt;strong>;&lt;em>;Donald Metzler&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.13959.pdf&quot;>;Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets&lt;/a>; &lt;br />; &lt;em>;Irina Bejan&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Artem Sokolov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Katja Filippova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Findings of EMNLP&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11689.pdf&quot;>;Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs&lt;/a>; &lt;br />; &lt;em>;Jiefeng Chen&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Jinsung Yoon&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sayna Ebrahimi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sercan O Arik&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Somesh Jha&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.10062.pdf&quot;>;A Comprehensive Evaluation of Tool-Assisted Generation Strategies&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16568.pdf&quot;>;1-PAGER: One Pass Answer Generation and Evidence Retrieval&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05401.pdf&quot;>;MaXM: Towards Multilingual Visual Question Answering&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Linting Xue, Michal Yarom&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ashish V. Thapliyal&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Idan Szpektor&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Julien&lt;/em>; &lt;em>;Amelot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xi Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.18431v1.pdf&quot;>;SDOH-NLI: A Dataset for Inferring Social Determinants of Health from Clinical Notes&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;, &lt;em>;Eric Loreaux&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Tal Schuster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Jun Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alvin Rajkomar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14815.pdf&quot;>;Machine Reading Comprehension Using Case-based Reasoning&lt;/a>; &lt;br />; &lt;em>;Dung Ngoc Thai&lt;/em>;, &lt;em>;Dhruv Agarwal&lt;/em>;, &lt;em>;Mudit Chaudhary&lt;/em>;, &lt;em>;Wenlong Zhao&lt;/em>;, &lt;em>;Rajarshi Das&lt;/em>;,&lt;em>; Jay-Yoon Lee&lt;/em>;, &lt;em>;Hannaneh Hajishirzi&lt;/em>;, &lt;strong>;&lt;em>;Manzil Zaheer&lt;/em>;&lt;/strong>;, &lt;em>;Andrew McCallum&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.06897.pdf&quot;>;Cross-lingual Open-Retrieval Question Answering for African Languages&lt;/a>; &lt;br />; &lt;em>;Odunayo Ogundepo&lt;/em>;, &lt;em>;Tajuddeen Gwadabe&lt;/em>;, &lt;strong>;&lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;David Ifeoluwa Adelani&lt;/em>;, &lt;em>;Bonaventure FP Dossou&lt;/em>;, &lt;em>;Abdou Aziz DIOP&lt;/em>;, &lt;em>;Claytone Sikasote&lt;/em>;, &lt;em>;Gilles HACHEME&lt;/em>;, &lt;em>;Happy Buzaaba&lt;/em>;,&lt;em>; Ignatius Ezeani&lt;/em>;, &lt;em>;Rooweither Mabuya&lt;/em>;, &lt;em>;Salomey Osei&lt;/em>;, &lt;em>;Chris&lt;/em>; &lt;em>;Chinenye Emezue&lt;/em>;, &lt;em>;Albert Kahira&lt;/em>;, &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;, &lt;em>;Akintunde Oladipo&lt;/em>;, &lt;em>;Abraham Toluwase Owodunni&lt;/em>;, &lt;em>;Atnafu Lambebo Tonja&lt;/em>;, &lt;em>;Iyanuoluwa Shode&lt;/em>;, &lt;em>;Akari Asai&lt;/em>;, &lt;em>;Anuoluwapo Aremu&lt;/em>;, &lt;em>;Ayodele Awokoya&lt;/em>;, &lt;em>;Bernard Opoku&lt;/em>;, &lt;em>;Chiamaka Ijeoma Chukwuneke&lt;/em>;, &lt;em>;Christine Mwase&lt;/em>;, &lt;em>;Clemencia Siro&lt;/em>;, &lt;em>;Stephen Arthur&lt;/em>;, &lt;em>;Tunde Oluwaseyi Ajayi&lt;/em>;, &lt;em>;Verrah Akinyi Otiende&lt;/em>;, &lt;em>;Andre Niyongabo Rubungo&lt;/em>;, &lt;em>;Boyd Sinkala&lt;/em>;, &lt;em>;Daniel Ajisafe&lt;/em>;, &lt;em>;Emeka Felix Onwuegbuzia&lt;/em>;, &lt;em>;Falalu Ibrahim Lawan&lt;/em>;, &lt;em>;Ibrahim Said Ahmad&lt;/em>;, &lt;em>;Jesujoba Oluwadara Alabi&lt;/em>;, &lt;em>;CHINEDU EMMANUEL&lt;/em>; &lt;em>;MBONU&lt;/em>;, &lt;em>;Mofetoluwa Adeyemi&lt;/em>;, &lt;em>;Mofya Phiri&lt;/em>;, &lt;em>;Orevaoghene Ahia&lt;/em>;, &lt;em>;Ruqayya Nasir Iro&lt;/em>;, &lt;em>;Sonia Adhiambo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.08653.pdf&quot;>;On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Polina Zablotskaia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Du Phan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Maynez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shashi Narayan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeremiah Zhe Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.09860.pdf&quot;>;Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;, &lt;em>;Behrooz Ghorbani&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Patrick Fernandes&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14552.pdf&quot;>;Sources of Hallucination by Large Language Models on Inference Tasks&lt;/a>; &lt;br />; &lt;em>;Nick McKenna&lt;/em>;, &lt;em>;Tianyi Li&lt;/em>;, &lt;em>;Liang Cheng&lt;/em>;, &lt;strong>;&lt;em>;Mohammad Javad Hosseini&lt;/em>;&lt;/strong>;, &lt;em>;Mark Johnson&lt;/em>;, &lt;em>;Mark Steedman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.09017v2.pdf&quot;>;Don&#39;t Add, Don&#39;t Miss: Effective Content Preserving Generation from Pre-selected Text Spans&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Eran Hirsch&lt;/em>;, &lt;em>;Ido Dagan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07686.pdf&quot;>;What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study&lt;/a>; &lt;br />; &lt;em>;Aman Madaan&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; &lt;strong>;Katherine Hermann&lt;/strong>;&lt;/em>;, &lt;strong>;&lt; em>;Amir Yazdanbakhsh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03945.pdf&quot;>;Understanding HTML with Large Language Models&lt;/a>; &lt; br />; &lt;strong>;&lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Ofir Nachum&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yingjie Miao&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Mustafa Safdari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Austin Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha&lt;/em>; &lt; em>;Chowdhery&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Noah Fiedel&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Aleksandra Faust&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09928.pdf&quot;>;Improving the Robustness of Summarization Models by Detecting and Removing Input Noise&lt;/a>; &lt;br />; &lt;em>;Kundan Krishna&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yao Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jiaming Luo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mohammad&lt;/em>; &lt;em>;Saleh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter J. Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://arxiv.org/pdf/2310.15916.pdf&quot;>;In-Context Learning Creates Task Vectors&lt;/a>; &lt;br />; &lt;em>;Roee Hendel&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10544.pdf&quot;>;Pre -training Without Attention&lt;/a>; &lt;br />; &lt;em>;Junxiong Wang&lt;/em>;, &lt;em>;Jing Nathan Yan&lt;/em>;, &lt;strong>;&lt;em>;Albert Gu&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Alexander M Rush&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12441.pdf&quot;>;MUX-PLMs: Data Multiplexing for High-Throughput Language Models&lt;/a>; &lt;br />; &lt;em>;Vishvak Murahari&lt;/em>;, &lt;em>;Ameet Deshpande&lt;/em>;, &lt;em>;Carlos E Jimenez&lt;/em>;,&lt;em>; &lt;strong >;Izhak Shafran&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Mingqiu Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yuan&lt;/em>; &lt;em>;Cao&lt;/em>;&lt;/ strong>;, &lt;em>;Karthik R Narasimhan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14408.pdf&quot;>;PaRaDe: Passage Ranking Using Demonstrations with LLMs&lt;/ a>; &lt;br />; &lt;em>;Andrew Drozdov&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Honglei Zhuang&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Zhuyun Dai&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhen Qin&lt;/em>;&lt;/strong>;, &lt;em>;Razieh Rahimi&lt;/em>;, &lt;strong>;&lt;em>;Xuanhui Wang&lt;/em>;&lt;/strong >;,&lt;strong>; &lt;em>;Dana Alon&lt;/em>;&lt;/strong>;, &lt;em>;Mohit Iyyer&lt;/em>;, &lt;em>;Andrew McCallum&lt;/em>;, &lt;em>;Donald Metzler&lt;sup>;*&lt;/ sup>;&lt;/em>;,&lt;strong>; &lt;em>;Kai Hui&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13678.pdf&quot;>; Long-Form Speech Translation Through Segmentation with Finite-State Decoding Constraints on Large Language Models&lt;/a>; &lt;br />; &lt;em>;Arya D. McCarthy&lt;/em>;, &lt;strong>;&lt;em>;Hao Zhang&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shankar Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Felix Stahlberg&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ke Wu&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07496.pdf&quot;>;Unsupervised Opinion Summarization Using Approximate Geodesics&lt;/a>; &lt;br />; &lt;em>;Somnath Basu Roy Chowdhury&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Nicholas Monath&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kumar Avinava Dubey&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Amr Ahmed&lt;/em>;&lt;/strong>;, &lt;em>;Snigdha Chaturvedi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02883. pdf&quot;>;SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sercan O . Arik&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rajarishi Sinha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hootan Nakhost&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em >;Hanjun Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pengcheng Yin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://zi-lin.com/pdf/EMNLP_2023_retrieval.pdf&quot;>;Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty&lt;/a>; &lt;br />; &lt;em>;Zi Lin&lt; /em>;, &lt;strong>;&lt;em>;Quan Yuan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Panupong Pasupat&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeremiah Zhe Liu&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Jingbo Shang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.08740.pdf&quot;>;A Zero-Shot Language Agent for Computer Control with Structured Reflection&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Tao Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Li&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Zhiwei Deng&lt;/em>;&lt;/strong>;, &lt;em>;Bryan Wang&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Yang Li&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08371.pdf&quot;>;Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches&lt;/a>; &lt;br / >; &lt;em>;Daniel Fried&lt;/em>;, &lt;em>;Nicholas Tomlin&lt;/em>;, &lt;em>;Jennifer Hu&lt;/em>;,&lt;strong>; &lt;em>;Roma Patel&lt;/em>;&lt;/strong>;, &lt;strong >;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13535.pdf&quot;>;Improving Classifier Robustness Through Active Generation of Pairwise Counterfactuals &lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ananth Balashankar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuezhi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yao Qin &lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nithum Thain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jilin Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ed H.&lt;/em>; &lt;em>;Chi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Beutel&lt;/em>;&lt;/ strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14224.pdf&quot;>;mmT5: Modular Multilingual Pre-training Solves Source Language Hallucinations&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Francesco Piccinno&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Massimo Nicosia&lt;/em>;&lt;/strong>;, &lt; strong>;&lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Machel Reid&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian&lt;/em>; &lt;em>;Ruder&lt;/ em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.10551.pdf&quot;>;Scaling Laws vs Model Architectures: How Does Inductive Bias Influence Scaling?&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yi Tay&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Samira Abnar&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Hyung Won Chung&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; William Fedus&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jinfeng Rao&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dani Yogatama&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Donald Metzler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00142. pdf&quot;>;TaTA: A Multilingual Table-to-Text Dataset for African Languages&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt; /em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vitaly Nikolaev,&lt;/em>; &lt;em>;Jan A. Botha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael Chavinda&lt;/em>;&lt; /strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2305.11938.pdf&quot;>;XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Ruder ,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alexander Gutkin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mihir Kale&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Min Ma&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Massimo Nicosia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Parker Riley&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jean Michel Amath Sarr&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;John Frederick&lt;/em>; &lt;em>;Wieting&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nitish Gupta&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anna Katanova&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christo Kirov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dana L Dickinson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Roark&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bidisha Samanta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Connie Tao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Ifeoluwa Adelani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vera Axelrod&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Isaac Rayburn&lt;/em>; &lt;em>;Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dan Garrette&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Reeve Ingle&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Melvin Johnson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dmitry Panteleev&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.00693.pdf&quot;>;On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval&lt;/a>; &lt;br />; &lt;em>;Jiayi Chen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt; /em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bo Dai&lt;/em>;&lt;/strong>;, &lt;em>;Aidong Zhang&lt;/em>;, &lt;em>;Wei Wei&lt;sup>;*&lt;/sup>;&lt;/ em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px ;&quot;>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701752203060961&amp;amp;usg =AOvVaw3sRozMNVYuwvyXeLluOgKI&quot;>;The Seventh Widening NLP Workshop&lt;/a>; (WiNLP) &lt;br />; Major Sponsor &lt;br />; Organizers: &lt;strong>;&lt;em>;Sunipa Dev&lt;/em>;&lt;/strong>; &lt;br />; Panelist: &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/crac2023/?pli=1&quot;>; The Sixth Workshop on Computational Models of Reference, Anaphora and Coreference&lt;/a>; (CRAC) &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://nlposs.github.io/2023/index.html&quot;>;The 3rd Workshop for Natural Language Processing Open Source Software&lt;/a>; (NLP-OSS) &lt;br />; Organizer: &lt;strong>;&lt; em>;Geeticka Chauhan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://splu-robonlp-2023.github.io/&quot;>;Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics&lt;/a>; (SpLU-RoboNLP) &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gem -benchmark.com/workshop&quot;>;Natural Language Generation, Evaluation, and Metric&lt;/a>; (GEM) &lt;br />; Organizer: &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://arabicnlp2023.sigarab.org/&quot;>;The First Arabic Natural Language Processing Conference&lt;/a>; (ArabicNLP) &lt;br />; Organizer: &lt;strong>;&lt;em>;Imed Zitouni&lt;/em >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.bigpictureworkshop.com/&quot;>;The Big Picture: Crafting a Research Narrative&lt;/a>; (BigPicture) &lt;br />; Organizer: &lt;strong>;&lt;em>;Nora Kassner&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://blackboxnlp .github.io/&quot;>;BlackboxNLP 2023: The 6th Workshop on Analysing and Interpreting Neural Networks for NLP&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br / >; Panelist: &lt;strong>;&lt;em>;Neel Nanda&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.conll.org/2023&quot;>;The SIGNLL Conference on Computational Natural Language Learning&lt;/a>; (CoNLL) &lt;br />; Co-Chair: &lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt;/strong>; &lt;br />; Areas and ACs: &lt;strong>;&lt;em>;Kyle Gorman&lt; /em>;&lt;/strong>; (Speech and Phonology), &lt;strong>;&lt;em>;Fei Liu &lt;/em>;&lt;/strong>;(Natural Language Generation) &lt;/p>; &lt;p>; &lt;a href=&quot;https:// sigtyp.github.io/ws2023-mrl.html&quot;>;The Third Workshop on Multi-lingual Representation Learning&lt;/a>; (MRL) &lt;br />; Organizer: &lt;strong>;&lt;em>;Omer Goldman&lt;/em>;&lt;/strong >;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot; https://emnlp2023-creative-nlg.github.io/&quot;>;Creative Natural Language Generation&lt;/a>; &lt;br />; Organizer: &lt;em>;Tuhin Chakrabarty&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;/p >; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;p>; &lt;i>;This schedule is subject to change 。 Please visit the Google booth for more information.&lt;/i>;&lt;/p>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Developing and Utilizing Evaluation Metrics for Machine Translation &amp;amp; Improving Multilingual NLP &lt;br />; Presenter: &lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dan Deutch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jan-Thorsten Peter&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Vilar Torres&lt;/em>;&lt;/strong>; &lt;br />; Fri, Dec 8 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Differentiable Search Indexes &amp;amp; Generative Retrieval &lt;br />; Presenter: &lt;strong>;&lt;em>;Sanket Vaibhav Mehta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Tran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>; Kai Hui&lt;/em>;&lt;/strong>;, &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;br />; Fri, Dec 8 | 3:30PM -4:00PM SST &lt;/p>; &lt;p>; Retrieval and Generation in a single pass &lt;br />; Presenter: &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Amplifying Adversarial Attacks &lt;br />; Presenter:&lt;strong>; &lt;em>;Anu Sinha&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 12:30PM -1:45PM SST &lt;/p>; &lt;p>; Automate prompt design: Universal Self-Adaptive Prompting (see &lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;blog post&lt;/a>;) &lt;br />; Presenter: &lt;strong>;&lt;em>;Xingchen Wan&lt;sup>;*&lt;/sup>;&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 3:30PM -4:00PM SST &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8414954450937764241/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at EMNLP 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s72-c/EMNLP%202023.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3665865722098768988&lt;/id>;&lt;published>;2023-12-04T14:41:00.000-08:00&lt;/published>;&lt;updated>;2023-12-04T14:46:06.688-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A new quantum algorithm for classical mechanics with an exponential speedup&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Robin Kothari and Rolando Somma, Research Scientists, Google Research, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s1100/glued-trees.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Quantum computers promise to solve some problems exponentially faster than classical computers, but there are only a handful of examples with such a dramatic speedup, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Shor%27s_algorithm&quot;>;Shor&#39;s factoring algorithm&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2023/10/developing-industrial-use-cases-for.html&quot;>;quantum simulation&lt;/a>;. Of those few examples, the majority of them involve simulating physical systems that are inherently quantum mechanical — a natural application for quantum computers. But what about simulating systems that are not inherently quantum? Can quantum computers offer an exponential advantage for this? &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://link.aps.org/doi/10.1103/PhysRevX.13.041041&quot;>;Exponential quantum speedup in simulating coupled classical oscillators&lt;/a>;”, published in &lt;a href=&quot;https://journals.aps.org/prx/&quot;>;Physical Review X&lt;/a>; (PRX) and presented at the &lt;a href=&quot;https://focs.computer.org/2023/&quot;>;Symposium on Foundations of Computer Science&lt;/a>; (FOCS 2023), we report on the discovery of a new quantum algorithm that offers an exponential advantage for simulating coupled &lt;a href=&quot;https://en.wikipedia.org/wiki/Harmonic_oscillator&quot;>;classical harmonic oscillators&lt;/a>;. These are some of the most fundamental, ubiquitous systems in nature and can describe the physics of countless natural systems, from electrical circuits to molecular vibrations to the mechanics of bridges. In collaboration with Dominic Berry of Macquarie University and Nathan Wiebe of the University of Toronto, we found a mapping that can transform any system involving coupled oscillators into a problem describing the time evolution of a quantum system. Given certain constraints, this problem can be solved with a quantum computer exponentially faster than it can with a classical computer. Further, we use this mapping to prove that any problem efficiently solvable by a quantum algorithm can be recast as a problem involving a network of coupled oscillators, albeit exponentially many of them. In addition to unlocking previously unknown applications of quantum computers, this result provides a new method of designing new quantum algorithms by reasoning purely about classical systems. &lt;/p>; &lt;br />; &lt;h2>;Simulating coupled oscillators&lt;/h2>; &lt;p>; The systems we consider consist of classical harmonic oscillators. An example of a single harmonic oscillator is a mass (such as a ball) attached to a spring. If you displace the mass from its rest position, then the spring will induce a restoring force, pushing or pulling the mass in the opposite direction. This restoring force causes the mass to oscillate back and forth. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/s359/oscillator.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;116&quot; data-original-width=&quot;359&quot; height=&quot;129&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/w400-h129/oscillator.gif&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A simple example of a harmonic oscillator is a mass connected to a wall by a spring. [Image Source: &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Simple_harmonic_oscillator.gif&quot;>;Wikimedia&lt;/a>;]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Now consider &lt;em>;coupled &lt;/em>;harmonic oscillators, where &lt;em>;multiple&lt;/em>; masses are attached to one another through springs. Displace one mass, and it will induce a wave of oscillations to pulse through the system. As one might expect, simulating the oscillations of a large number of masses on a classical computer gets increasingly difficult. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/s1129/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;832&quot; data-original-width=&quot;1129&quot; height=&quot;295&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/w400-h295/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example system of masses connected by springs that can be simulated with the quantum algorithm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To enable the simulation of a large number of coupled harmonic oscillators, we came up with a mapping that encodes the positions and velocities of all masses and springs into the quantum wavefunction of a system of qubits. Since the number of parameters describing the wavefunction of a system of qubits grows exponentially with the number of qubits, we can encode the information of &lt;em>;N&lt;/em>; balls into a quantum mechanical system of only about log(&lt;em>;N&lt;/em>;) qubits. As long as there is a compact description of the system (ie, the properties of the masses and the springs), we can evolve the wavefunction to learn coordinates of the balls and springs at a later time with far fewer resources than if we had used a naïve classical approach to simulate the balls and springs. &lt;/p>; &lt;p>; We showed that a certain class of coupled-classical oscillator systems can be efficiently simulated on a quantum computer. But this alone does not rule out the possibility that there exists some as-yet-unknown clever classical algorithm that is similarly efficient in its use of resources. To show that our quantum algorithm achieves an exponential speedup over &lt;em>;any&lt;/em>; possible classical algorithm, we provide two additional pieces of evidence. &lt;/p>; &lt;br />; &lt;h2>;The glued-trees problem and the quantum oracle&lt;/h2>; &lt;p>; For the first piece of evidence, we use our mapping to show that the quantum algorithm can efficiently solve a famous problem about graphs known to be difficult to solve classically, called the &lt;a href=&quot;https://arxiv.org/abs/quant-ph/0209131&quot;>;glued-trees problem&lt;/a>;. The problem takes two branching trees — a graph whose nodes each branch to two more nodes, resembling the branching paths of a tree — and glues their branches together through a random set of edges, as shown in the figure below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s930/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;930&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visual representation of the glued trees problem. Here we start at the node labeled ENTRANCE and are allowed to locally explore the graph, which is obtained by randomly gluing together two binary trees. The goal is to find the node labeled EXIT.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The goal of the glued-trees problem is to find the exit node — the “root” of the second tree — as efficiently as possible. But the exact configuration of the nodes and edges of the glued trees are initially hidden from us. To learn about the system, we must query an &lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_machine&quot;>;oracle&lt;/a>;, which can answer specific questions about the setup. This oracle allows us to explore the trees, but only locally. Decades ago, &lt;a href=&quot;https://doi.org/10.1145/780542.780552&quot;>;it was shown&lt;/a>; that the number of queries required to find the exit node on a classical computer is proportional to a polynomial factor of &lt;em>;N&lt;/em>;, the total number of nodes. &lt;/p>; &lt;p>; But recasting this as a problem with balls and springs, we can imagine each node as a ball and each connection between two nodes as a spring. Pluck the entrance node (the root of the first tree), and the oscillations will pulse through the trees. It only takes a time that scales with the &lt;em>;depth&lt;/em>; of the tree — which is exponentially smaller than &lt;em>;N&lt;/em>; — to reach the exit node. So, by mapping the glued-trees ball-and-spring system to a quantum system and evolving it for that time, we can detect the vibrations of the exit node and determine it exponentially faster than we could using a classical computer. &lt;/p>; &lt;br />; &lt;h2>;BQP-completeness&lt;/h2>; &lt;p>; The second and strongest piece of evidence that our algorithm is exponentially more efficient than any possible classical algorithm is revealed by examination of the set of problems a quantum computer can solve efficiently (ie, solvable in &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time&quot;>;polynomial time&lt;/a>;), referred to as &lt;a href=&quot;https://en.wikipedia.org/wiki/BQP&quot;>;bounded-error quantum polynomial time&lt;/a>; or BQP. The hardest problems in BQP are called “BQP-complete”. &lt;/p>; &lt;p>; While it is generally accepted that there exist some problems that a quantum algorithm can solve efficiently and a classical algorithm cannot, this has not yet been proven. So, the best evidence we can provide is that our problem is BQP-complete, that is, it is among the hardest problems in BQP. If someone were to find an efficient classical algorithm for solving our problem, then every problem solved by a quantum computer efficiently would be classically solvable! Not even the &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_factorization&quot;>;factoring problem&lt;/a>; (finding the prime factors of a given large number), which forms the basis of &lt;a href=&quot;https://en.wikipedia.org/wiki/RSA_(cryptosystem)&quot;>;modern encryption&lt;/a>; and was famously solved by Shor&#39;s algorithm, is expected to be BQP-complete. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s574/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;563&quot; data-original-width=&quot;574&quot; height=&quot;314&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s320/image1.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A diagram showing the believed relationships of the classes BPP and BQP, which are the set of problems that can be efficiently solved on a classical computer and quantum computer, respectively. BQP-complete problems are the hardest problems in BQP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To show that our problem of simulating balls and springs is indeed BQP-complete, we start with a standard BQP-complete problem of simulating universal quantum circuits, and show that every quantum circuit can be expressed as a system of many balls coupled with springs. Therefore, our problem is also BQP-complete. &lt;/p>; &lt;br />; &lt;h2>;Implications and future work&lt;/h2>; &lt;p>; This effort also sheds light on work from 2002, when theoretical computer scientist Lov K. Grover and his colleague, Anirvan M. Sengupta, used an &lt;a href=&quot;https://journals.aps.org/pra/abstract/10.1103/PhysRevA.65.032319&quot;>;analogy to coupled&lt;/a>; pendulums to illustrate how Grover&#39;s famous quantum &lt;a href=&quot;https://en.wikipedia.org/wiki/Grover%27s_algorithm&quot;>;search algorithm&lt;/a>; could find the correct element in an unsorted database quadratically faster than could be done classically. With the proper setup and initial conditions, it would be possible to tell whether one of &lt;em>;N&lt;/em>; pendulums was different from the others — the analogue of finding the correct element in a database — after the system had evolved for time that was only ~√(&lt;em>;N)&lt;/em>;. While this hints at a connection between certain classical oscillating systems and quantum algorithms, it falls short of explaining why Grover&#39;s quantum algorithm achieves a quantum advantage. &lt;/p>; &lt;p>; Our results make that connection precise. We showed that the dynamics of any classical system of harmonic oscillators can indeed be equivalently understood as the dynamics of a corresponding quantum system of exponentially smaller size. In this way we can simulate Grover and Sengupta&#39;s system of pendulums on a quantum computer of log(&lt;em>;N&lt;/em>;) qubits, and find a different quantum algorithm that can find the correct element in time ~√(&lt;em>;N&lt;/em>;). The analogy we discovered between classical and quantum systems can be used to construct other quantum algorithms offering exponential speedups, where the reason for the speedups is now more evident from the way that classical waves propagate. &lt;/p>; &lt;p>; Our work also reveals that every quantum algorithm can be equivalently understood as the propagation of a classical wave in a system of coupled oscillators. This would imply that, for example, we can in principle build a classical system that solves the factoring problem after it has evolved for time that is exponentially smaller than the runtime of any known classical algorithm that solves factoring. This may look like an efficient classical algorithm for factoring, but the catch is that the number of oscillators is exponentially large, making it an impractical way to solve factoring. &lt;/p>; &lt;p>; Coupled harmonic oscillators are ubiquitous in nature, describing a broad range of systems from electrical circuits to chains of molecules to structures such as bridges. While our work here focuses on the fundamental complexity of this broad class of problems, we expect that it will guide us in searching for real-world examples of harmonic oscillator problems in which a quantum computer could offer an exponential advantage. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our Quantum Computing Science Communicator, Katie McCormick, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3665865722098768988/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html&quot; rel=&quot;alternate&quot; title=&quot;A new quantum algorithm for classical mechanics with an exponential speedup&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s72-c/glued-trees.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-68038970217457115&lt;/id>;&lt;published>;2023-12-04T10:00:00.000-08:00&lt;/published>;&lt;updated>;2023-12-04T11:29:40.224-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ads&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Summary report optimization in the Privacy Sandbox Attribution Reporting API&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hidayet Aksu, Software Engineer, and Adam Sealfon, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN-hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, the &lt;a href=&quot;https://privacysandbox.com/&quot;>;Privacy Sandbox&lt;/a>; initiative was launched to explore responsible ways for advertisers to measure the effectiveness of their campaigns, by aiming to &lt;a href=&quot;https://blog.chromium.org/2020/01/building-more-private-web-path-towards.html&quot;>;deprecate third-party cookies&lt;/a>; (subject to &lt;a href=&quot;https://www.gov.uk/cma-cases/investigation-into-googles-privacy-sandbox-browser-changes&quot;>;resolving any competition concerns with the UK&#39;s Competition and Markets Authority&lt;/a>;). &lt;a href=&quot;https://en.wikipedia.org/wiki/HTTP_cookie#Third-party_cookie&quot;>;Cookies&lt;/a>; are small pieces of data containing user preferences that websites store on a user&#39;s device; they can be used to provide a better browsing experience (eg, allowing users to automatically sign in) and to serve relevant content or ads. The Privacy Sandbox attempts to address concerns around the use of cookies for tracking browsing data across the web by providing a privacy-preserving alternative. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Many browsers use &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>; (DP) to provide privacy-preserving APIs, such as the &lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/attribution-reporting&quot;>;Attribution Reporting API&lt;/a>; (ARA), that don&#39;t rely on cookies for ad conversion measurement. ARA encrypts individual user actions and collects them in an aggregated &lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/&quot;>;summary report&lt;/a>;, which estimates measurement goals like the number and value of conversions (useful actions on a website, such as making a purchase or signing up for a mailing list) attributed to ad campaigns. &lt;/p>; &lt;p>; The task of configuring API parameters, eg, allocating a contribution budget across different conversions, is important for maximizing the utility of the summary reports. In “&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;Summary Report Optimization in the Privacy Sandbox Attribution Reporting API&lt;/a>;”, we introduce a formal mathematical framework for modeling summary reports. Then, we formulate the problem of maximizing the utility of summary reports as an optimization problem to obtain the optimal ARA parameters. Finally, we evaluate the method using real and synthetic datasets, and demonstrate significantly improved utility compared to baseline non-optimized summary reports. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ARA summary reports&lt;/h2>; &lt;p>; We use the following example to illustrate our notation. Imagine a fictional gift shop called &lt;em>;Du &amp;amp; Penc&lt;/em>; that uses digital advertising to reach its customers. The table below captures their holiday sales, where each record contains impression features with (i) an impression ID, (ii) the campaign, and (iii) the city in which the ad was shown, as well as conversion features with (i) the number of items purchased and (ii) the total dollar value of those items. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqrhtaPxDlj8o5sbubBQLdCq_RRnT2ZPmhxuvEIC_9SvLfhKkXh1t_elS4eRCQyHv2pYX4YkAX1UkEX3c3BdbB5PejqLF0uq_MAXuXKVA1YPKVnZ5tqertr02eNDFjJ0nnoeD_rGdDFWxLrTsCNGXOE9LrGSsuPhrtZ6SgqRomWRjpun1JdrwWDfnfJF0n/s1035/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;397&quot; data-original-width=&quot;1035&quot; height=&quot;245&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqrhtaPxDlj8o5sbubBQLdCq_RRnT2ZPmhxuvEIC_9SvLfhKkXh1t_elS4eRCQyHv2pYX4YkAX1UkEX3c3BdbB5PejqLF0uq_MAXuXKVA1YPKVnZ5tqertr02eNDFjJ0nnoeD_rGdDFWxLrTsCNGXOE9LrGSsuPhrtZ6SgqRomWRjpun1JdrwWDfnfJF0n/w640-h245/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Impression and conversion feature logs for Du &amp;amp; Penc.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Mathematical model&lt;/h3>; &lt;p>; ARA summary reports can be modeled by four algorithms: (1) Contribution Vector, (2) &lt;a href=&quot;https://github.com/WICG/attribution-reporting-api/blob/main/AGGREGATE.md#contribution-bounding-and-budgeting&quot;>;Contribution Bounding&lt;/a>;, (3) &lt;a href=&quot;https://github.com/WICG/attribution-reporting-api/blob/main/AGGREGATION_SERVICE_TEE.md?&quot;>;Summary Reports&lt;/a>;, and (4) Reconstruct Values. Contribution Bounding and Summary Reports are performed by the ARA, while Contribution Vector and Reconstruct Values are performed by an AdTech provider — tools and systems that enable businesses to buy and sell digital advertising. The objective of this work is to assist AdTechs in optimizing summary report algorithms. &lt;/p>; &lt;p>; The Contribution Vector algorithm converts measurements into an ARA format that is discretized and scaled. Scaling needs to account for the overall contribution limit per impression. Here we propose a method that clips and performs randomized rounding. The outcome of the algorithm is a histogram of aggregatable keys and values. &lt;/p>; &lt;p>; Next, the Contribution Bounding algorithm runs on client devices and enforces the contribution bound on attributed reports where any further contributions exceeding the limit are dropped. The output is a histogram of attributed conversions. &lt;/p>; &lt;p>; The Summary Reports algorithm runs on the server side inside a &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot;>;trusted execution environment&lt;/a>; and returns noisy aggregate results that satisfy DP. Noise is sampled from the discrete &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplace_distribution&quot;>;Laplace distribution&lt;/a>;, and to enforce privacy budgeting, a report may be queried only once. &lt;/p>; &lt;p>; Finally, the Reconstruct Values algorithm converts measurements back to the original scale. Reconstruct Values and Contribution Vector Algorithms are designed by the AdTech, and both impact the utility received from the summary report. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrative usage of ARA summary reports, which include Contribution Vector (Algorithm A), Contribution Bounding (Algorithm C), Summary Reports (Algorithm S), and Reconstruct Values (Algorithm R). Algorithms C and S are fixed in the API. The AdTech designs A and R.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Error metrics&lt;/h2>; &lt;p>; There are several factors to consider when selecting an error metric for evaluating the quality of an approximation. To choose a particular metric, we considered the desirable properties of an error metric that further can be used as an objective function. Considering desired properties, we have chosen &lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/design-decisions/#rmsre&quot;>;𝜏-truncated root mean square relative error&lt;/a>; (RMSRE&lt;sub>;𝜏&lt;/sub>;) as our error metric for its properties. See the &lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;paper&lt;/a>; for a detailed discussion and comparison to other possible metrics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Optimization&lt;/h2>; &lt;p>; To optimize utility as measured by RMSRE&lt;sub>;𝜏&lt;/sub>;, we choose a capping parameter, &lt;em>;C&lt;/em>;, and privacy budget, 𝛼, for each slice. The combination of both determines how an actual measurement (such as two conversions with a total value of $3) is encoded on the AdTech side and then passed to the ARA for Contribution Bounding algorithm processing. RMSRE&lt;sub>;𝜏&lt;/sub>; can be computed exactly, since it can be expressed in terms of the bias from clipping and the variance of the noise distribution. Following those steps we find out that RMSRE&lt;sub>;𝜏&lt;/sub>; for a fixed privacy budget, 𝛼,&lt;sub>; &lt;/sub>;or a capping parameter, C, is &lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_optimization&quot;>;convex&lt;/a>; (so the error-minimizing value for the other parameter can be obtained efficiently), while for joint variables (C, 𝛼) it becomes non-convex (so we may not always be able to select the best possible parameters). In any case, any off-the-shelf optimizer can be used to select privacy budgets and capping parameters. In our experiments, we use the &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html&quot;>;SLSQP&lt;/a>; minimizer from the &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html&quot;>;scipy.optimize&lt;/a>; library. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Synthetic data&lt;/h2>; &lt;p>; Different ARA configurations can be evaluated empirically by testing them on a conversion dataset. However, access to such data can be restricted or slow due to privacy concerns, or simply unavailable. One way to address these limitations is to use synthetic data that replicates the characteristics of real data. &lt;/p>; &lt;p>; We present a method for generating synthetic data responsibly through statistical modeling of real-world conversion datasets. We first perform an empirical analysis of real conversion datasets to uncover relevant characteristics for ARA. We then design a pipeline that uses this distribution knowledge to create a realistic synthetic dataset that can be customized via input parameters. &lt;/p>; &lt;p>; The pipeline first generates impressions drawn from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Power_law&quot;>;power-law distribution&lt;/a>; (step 1), then for each impression it generates conversions drawn from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_distribution&quot;>;Poisson distribution&lt;/a>; (step 2) and finally, for each conversion, it generates conversion values drawn from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-normal_distribution&quot;>;log-normal distribution&lt;/a>; (step 3). With dataset-dependent parameters, we find that these distributions closely match ad-dataset characteristics. Thus, one can learn parameters from historical or public datasets and generate synthetic datasets for experimentation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s841/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;841&quot; data-original-width=&quot;840&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall dataset generation steps with features for illustration.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>; Experimental evaluation &lt;/h2>; &lt;p>; We evaluate our algorithms on three real-world datasets (&lt;a href=&quot;https://ailab.criteo.com/criteo-sponsored-search-conversion-log-dataset/&quot;>;Criteo&lt;/a>;, AdTech Real Estate, and AdTech Travel) and three synthetic datasets. Criteo consists of 15M clicks, Real Estate consists of 100K conversions, and Travel consists of 30K conversions. Each dataset is partitioned into a training set and a test set. The training set is used to choose contribution budgets, clipping threshold parameters, and the conversion count limit (the real-world datasets have only one conversion per click), and the error is evaluated on the test set. Each dataset is partitioned into slices using impression features. For real-world datasets, we consider three queries for each slice; for synthetic datasets, we consider two queries for each slice. &lt;/p>; &lt;p>; For each query we choose the RMSRE&lt;sub>;𝝉&lt;/sub>; 𝜏 value to be five times the median value of the query on the training dataset. This ensures invariance of the error metric to data rescaling, and allows us to combine the errors from features of different scales by using 𝝉 per each feature. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s1971/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1028&quot; data-original-width=&quot;1971&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Scatter plots of real-world datasets illustrating the probability of observing a conversion value. The fitted curves represent best log-normal distribution models that effectively capture the underlying patterns in the data.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Results&lt;/h3>; &lt;p>; We compare our optimization-based algorithm to a simple baseline approach. For each query, the baseline uses an equal contribution budget and a fixed quantile of the training data to choose the clipping threshold. Our algorithms produce substantially lower error than baselines on both real-world and synthetic datasets. Our optimization-based approach adapts to the privacy budget and data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1730&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RMSRE&lt;sub>;τ&lt;/sub>; for privacy budgets {1, 2, 4, 8, 16, 32, 64} for our algorithms and baselines on three real-world and three synthetic datasets. Our optimization-based approach consistently achieves lower error than baselines that use a fixed quantile for the clipping threshold and split the contribution budget equally among the queries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We study the optimization of summary reports in the ARA, which is currently deployed on hundreds of millions of Chrome browsers. We present a rigorous formulation of the contribution budgeting optimization problem for ARA with the goal of equipping researchers with a robust abstraction that facilitates practical improvements. &lt;/p>; &lt;p>; Our recipe, which leverages historical data to bound and scale the contributions of future data under differential privacy, is quite general and applicable to settings beyond advertising. One approach based on this work is to use past data to learn the parameters of the data distribution, and then to apply synthetic data derived from this distribution for privacy budgeting for queries on future data. Please see the &lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;paper&lt;/a>; and &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/ara_optimization&quot;>;accompanying code&lt;/a>; for detailed algorithms and proofs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was done in collaboration with Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, and Avinash Varadarajan. We thank Akash Nadan for his help.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/68038970217457115/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/summary-report-optimization-in-privacy.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/68038970217457115&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/68038970217457115&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/summary-report-optimization-in-privacy.html&quot; rel=&quot;alternate&quot; title=&quot;Summary report optimization in the Privacy Sandbox Attribution Reporting API&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN-hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8288223977991952319&lt;/id>;&lt;published>;2023-12-01T09:19:00.000-08:00&lt;/published>;&lt;updated>;2023-12-05T09:38:21.187-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Translation&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Translate&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Unsupervised speech-to-speech translation from monolingual data&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eliya Nachmani, Research Scientist, and Michelle Tadmor Ramanovich, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1-3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s1600/T3.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Speech-to-speech translation (S2ST) is a type of &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;>;machine translation&lt;/a>; that converts spoken language from one language to another. This technology has the potential to break down language barriers and facilitate communication between people from different cultures and backgrounds. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Previously, we introduced &lt;a href=&quot;https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html&quot;>;Translatotron 1&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html&quot;>;Translatotron 2&lt;/a>;, the first ever models that were able to directly translate speech between two languages. However they were trained in supervised settings with parallel speech data. The scarcity of parallel speech data is a major challenge in this field, so much that most public datasets are semi- or fully-synthesized from text. This adds additional hurdles to learning translation and reconstruction of speech attributes that are not represented in the text and are thus not reflected in the synthesized training data. &lt;/p>; &lt;p>; Here we present &lt;a href=&quot;https://arxiv.org/abs/2305.17547&quot;>;Translatotron 3&lt;/a>;, a novel unsupervised speech-to-speech translation architecture. In Translatotron 3, we show that it is possible to learn a speech-to-speech translation task from monolingual data alone. This method opens the door not only to translation between more language pairs but also towards translation of the non-textual speech attributes such as pauses, speaking rates, and speaker identity. Our method does not include any direct supervision to target languages and therefore we believe it is the right direction for paralinguistic characteristics (eg, such as tone, emotion) of the source speech to be preserved across translation. To enable speech-to-speech translation, we use &lt;a href=&quot;https://arxiv.org/abs/1511.06709&quot;>;back-translation&lt;/a>;, which is a technique from unsupervised machine translation (UMT) where a synthetic translation of the source language is used to &lt;a href=&quot;https://arxiv.org/abs/1710.11041&quot;>;translate texts without bilingual text datasets&lt;/a>;. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Translatotron 3 &lt;/h2>; &lt;p>; Translatotron 3 addresses the problem of unsupervised S2ST, which can eliminate the requirement for bilingual speech datasets. To do this, Translatotron 3&#39;s design incorporates three key aspects: &lt;/p>; &lt;ol>; &lt;li>;Pre-training the entire model as a &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html&quot;>;masked autoencoder&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;>;SpecAugment&lt;/a>;, a simple data augmentation method for speech recognition that operates on the logarithmic &lt;a href=&quot;https://en.wikipedia.org/wiki/Mel-frequency_cepstrum&quot;>;mel spectogram&lt;/a>; of the input audio (instead of the raw audio itself) and is shown to effectively improve the generalization capabilities of the encoder. &lt;/li>;&lt;li>;Unsupervised embedding mapping based on &lt;a href=&quot;https://arxiv.org/abs/1710.04087&quot;>;multilingual unsupervised embeddings&lt;/a>; (MUSE), which is trained on unpaired languages but allows the model to learn an embedding space that is shared between the source and target languages. &lt;/li>;&lt;li>;A reconstruction loss based on back-translation, to train an encoder-decoder direct S2ST model in a fully unsupervised manner. &lt;/li>; &lt;/ol>; &lt;p>; The model is trained using a combination of the unsupervised MUSE embedding loss, reconstruction loss, and S2S back-translation loss. During inference, the shared encoder is utilized to encode the input into a multilingual embedding space, which is subsequently decoded by the target language decoder. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Architecture&lt;/h3>; &lt;p>; Translatotron 3 employs a shared encoder to encode both the source and target语言。 The decoder is composed of a linguistic decoder, an acoustic synthesizer (responsible for acoustic generation of the translation speech), and a singular attention module, like Translatotron 2. However, for Translatotron 3 there are two decoders, one for the source language and another for the target language. During training, we use monolingual speech-text datasets (ie, these data are made up of speech-text pairs; they are &lt;em>;not&lt;/em>; translations). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Encoder&lt;/h3>; &lt;p>; The encoder has the same architecture as the speech encoder in the Translatotron 2. The output of the encoder is split into two parts: the first part incorporates semantic information whereas the second part incorporates acoustic information. By using the MUSE loss, the first half of the output is trained to be the MUSE embeddings of the text of the input speech spectrogram. The latter half is updated without the MUSE loss. It is important to note that the same encoder is shared between source and target languages. Furthermore, the MUSE embedding is multilingual in nature. As a result, the encoder is able to learn a multilingual embedding space across source and target languages. This allows a more efficient and effective encoding of the input, as the encoder is able to encode speech from both languages into a common embedding space, rather than maintaining a separate embedding space for each language. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Decoder&lt;/h3>; &lt;p>; Like Translatotron 2, the decoder is composed of three distinct components, namely the linguistic decoder, the acoustic synthesizer, and the attention module. To effectively handle the different properties of the source and target languages, however, Translatotron 3 has two separate decoders, for the source and target languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Two part training&lt;/h3>; &lt;p>; The training methodology consists of two parts: (1) auto-encoding with reconstruction and (2) a back-translation term. In the first part, the network is trained to auto-encode the input to a multilingual embedding space using the MUSE loss and the reconstruction loss. This phase aims to ensure that the network generates meaningful multilingual representations. In the second part, the network is further trained to translate the input spectrogram by utilizing the back-translation loss. To mitigate the issue of &lt;a href=&quot;https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=Catastrophic%20interference%2C%20also%20known%20as,information%20upon%20learning%20new%20information.&quot;>;catastrophic forgetting&lt;/a>; and enforcing the latent space to be multilingual, the MUSE loss and the reconstruction loss are also applied in this second part of training. To ensure that the encoder learns meaningful properties of the input, rather than simply reconstructing the input, we apply SpecAugment to encoder input at both phases. It has been shown to effectively improve the generalization capabilities of the encoder by augmenting the input data. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Training objective&lt;/h3>; &lt;p>; During the back-translation training phase (illustrated in the section below), the network is trained to translate the input spectrogram to the target language and then back to the source language. The goal of back-translation is to enforce the latent space to be multilingual. To achieve this, the following losses are applied: &lt;/p>; &lt;ul>; &lt;li>;MUSE loss: The MUSE loss measures the similarity between the multilingual embedding of the input spectrogram and the multilingual embedding of the back-translated spectrogram. &lt;/li>;&lt;li>;Reconstruction loss: The reconstruction loss measures the similarity between the input spectrogram and the back-translated spectrogram. &lt;/li>; &lt;/ul>; &lt;p>; In addition to these losses, SpecAugment is applied to the encoder input at both phases. Before the back-translation training phase, the network is trained to auto-encode the input to a multilingual embedding space using the MUSE loss and reconstruction loss. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MUSE loss&lt;/h3>; &lt;p>; To ensure that the encoder generates multilingual representations that are meaningful for both decoders, we employ a MUSE loss during training. The MUSE loss forces the encoder to generate such a representation by using pre-trained MUSE embeddings. During the training process, given an input text transcript, we extract the corresponding MUSE embeddings from the embeddings of the input language. The error between MUSE embeddings and the output vectors of the encoder is then minimized. Note that the encoder is indifferent to the language of the input during inference due to the multilingual nature of the embeddings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s1999 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;999&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The training and inference in Translatotron 3. Training includes the reconstruction loss via the auto-encoding path and employs the reconstruction loss via back-translation. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Audio samples&lt;/h2>; &lt;p>; Following are examples of direct speech-to-speech translation from Translatotron 3: &lt;/p>; &lt;h3>; Spanish-to-English (on Conversational dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left ： 汽车; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 Synthesized dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS reference (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance &lt;/h2>; &lt;p>; To empirically evaluate the performance of the proposed approach, we conducted experiments on English and Spanish using various datasets, including the &lt;a href=&quot;https://aclanthology.org/2020.lrec-1.520/&quot;>;Common Voice 11&lt;/a>; dataset, as well as two synthesized datasets derived from the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;Conversational&lt;/a>; and Common Voice 11 datasets. &lt;/p>; &lt;p>; The translation quality was measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; (higher is better) on ASR (automatic speech recognition) transcriptions from the translated speech, compared to the corresponding reference translation text. Whereas, the speech quality is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>; score (higher is better). Furthermore, the speaker similarity is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;average cosine similarity&lt;/a>; (higher is better). &lt;/p>; &lt;p>; Because Translatotron 3 is an &lt;em>;unsupervised&lt;/em>; method, as a baseline we used a cascaded S2ST system that is combined from ASR, unsupervised machine translation (UMT), and TTS (text-to -演讲）。 Specifically, we employ UMT that uses the nearest neighbor in the embedding space in order to create the translation. &lt;/p>; &lt;p>; Translatotron 3 outperforms the baseline by large margins in every aspect we measured: translation quality, speaker similarity, and speech quality. It particularly excelled on the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;conversational corpus&lt;/a>;. Moreover, Translatotron 3 achieves speech naturalness similar to that of the ground truth audio samples (measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>;, higher is better). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Translation quality (measured by BLEU, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Speech similarity (measured by average cosine similarity between input speaker and output speaker, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Mean-opinion-score (measured by average MOS metric, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; As future work, we would like to extend the work to more languages and investigate whether zero-shot S2ST can be applied with the back-translation technique. We would also like to examine the use of back-translation with different types of speech data, such as noisy speech and low-resource languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The direct contributors to this work include Eliya Nachmani, Alon Levkovitch, Yifan Ding, Chulayutsh Asawaroengchai, Heiga Zhen, and Michelle Tadmor Ramanovich. We also thank Yu Zhang, Yuma Koizumi, Soroosh Mariooryad, RJ Skerry-Ryan, Neil Zeghidour, Christian Frank, Marco Tagliasacchi, Nadav Bar, Benny Schlesinger and Yonghui Wu.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8288223977991952319/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html&quot; rel=&quot;alternate&quot; title=&quot;Unsupervised speech-to-speech translation from monolingual data&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1-3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s72-c/T3.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1055595481112584523&lt;/id>;&lt;published>;2023-11-22T08:03:00.000-08:00&lt;/published>;&lt;updated>;2023-11-30T13:46:35.804-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;High-Performance Computing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Weather&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Improving simulations of clouds and their effects on climate&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Tapio Schneider, Visiting Researcher, and Yi-fan Chen, Engineering Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Today&#39;s climate models successfully capture broad global warming trends. However, because of uncertainties about processes that are &lt;a href=&quot;https://physicstoday.scitation.org/doi/abs/10.1063/PT.3.4772&quot;>;small in scale yet globally important&lt;/a>;, such as &lt;a href=&quot;http://rdcu.be/ohot&quot;>;clouds&lt;/a>; and &lt;a href=&quot;https://doi.org/10.3389/fmars.2019.00065&quot;>;ocean turbulence&lt;/a>;, these models&#39; predictions of upcoming climate changes are not very accurate in detail. For example, predictions of the time by which the global mean surface temperature of Earth will have warmed 2℃, relative to preindustrial times, &lt;a href=&quot;https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_TS.pdf&quot;>;vary by 40–50 years&lt;/a>; (a full human generation) among today&#39;s models. As a result, we do not have the &lt;a href=&quot;https://www.nature.com/articles/s41558-020-00984-6&quot;>;accurate and geographically granular predictions&lt;/a>; we need to plan resilient infrastructure, adapt supply chains to climate disruption, and assess the risks of climate-related hazards to vulnerable communities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In large part this is because clouds dominate errors and uncertainties in climate predictions for the coming decades [&lt;a href=&quot;https://doi.org/10.1029/2005GL023851&quot;>;1&lt;/a>;, &lt;a href=&quot;https://link.springer.com/article/10.1007/s00382-013-1725-9&quot;>;2&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1038/nclimate3402&quot;>;3&lt;/a>;]. Clouds reflect sunlight and exert a &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenhouse_effect&quot;>;greenhouse effect&lt;/a>;, making them crucial for regulating Earth&#39;s energy balance and mediating the response of the climate system to changes in greenhouse gas concentrations. However, they are too small in scale to be directly resolvable in today&#39;s climate models. Current climate models resolve motions at scales of tens to a hundred kilometers, with a &lt;a href=&quot;https://link.springer.com/article/10.1186/s40645-019-0304-z&quot;>;few pushing toward&lt;/a>; the &lt;a href=&quot;https://journals.ametsoc.org/view/journals/bams/101/5/bams-d-18-0167.1.xml&quot;>;kilometer-scale&lt;/a>;. However, the turbulent air motions that sustain, for example, the low clouds that cover large swaths of tropical oceans have scales of meters to tens of meters. Because of this wide difference in scale, climate models use empirical parameterizations of clouds, rather than simulating them directly, which result in large errors and uncertainties. &lt;/p>; &lt;p>; While clouds cannot be directly resolved in global climate models, their turbulent dynamics can be simulated in limited areas by using high-resolution &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_eddy_simulation&quot;>;large eddy simulations&lt;/a>; (LES). However, the high computational cost of simulating clouds with LES has inhibited broad and systematic numerical experimentation, and it has held back the generation of large datasets for training parameterization schemes to represent clouds in coarser-resolution global climate models. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;Accelerating Large-Eddy Simulations of Clouds with Tensor Processing Units&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/journal/19422466&quot;>;Journal of Advances in Modeling Earth Systems&lt;/a>;&lt;/em>; (JAMES), and in collaboration with a &lt;a href=&quot;https://clima.caltech.edu/&quot;>;Climate Modeling Alliance&lt;/a>; (CliMA) lead who is a visiting researcher at Google, we demonstrate that &lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Tensor Processing Units&lt;/a>; (TPUs) — application-specific integrated circuits that were originally developed for machine learning (ML) applications — can be effectively used to perform LES of clouds. We show that TPUs, in conjunction with tailored software implementations, can be used to simulate particularly computationally challenging &lt;a href=&quot;https://en.wikipedia.org/wiki/Marine_stratocumulus&quot;>;marine stratocumulus clouds&lt;/a>; in the conditions observed during the &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;Dynamics and Chemistry of Marine Stratocumulus&lt;/a>; (DYCOMS) field study. This successful TPU-based LES code reveals the utility of TPUs, with their large computational resources and tight interconnects, for cloud simulations. &lt;/p>; &lt;p>; Climate model accuracy for critical metrics, like precipitation or the energy balance at the top of the atmosphere, has improved roughly 10% per decade in the last 20 years. Our goal is for this research to enable a 50% reduction in climate model errors by improving their representation of clouds. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large-eddy simulations on TPUs&lt;/h2>; &lt;p>; In this work, we focus on stratocumulus clouds, which cover ~20% of the tropical oceans and are the most prevalent cloud type on earth. Current climate models are not yet able to reproduce stratocumulus cloud behavior correctly, which has been one of the largest sources of errors in these models. Our work will provide a much more accurate ground truth for large-scale climate models. &lt;/p>; &lt;p>; Our simulations of clouds on TPUs exhibit unprecedented computational throughput and scaling, making it possible, for example, to simulate stratocumulus clouds with 10× speedup over real-time evolution across areas up to about 35 × 54 km&lt;sup>;2&lt;/sup>;. Such domain sizes are close to the cross-sectional area of typical global climate model grid boxes. Our results open up new avenues for computational experiments, and for substantially enlarging the sample of LES available to train parameterizations of clouds for global climate models.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;8&quot; cellspacing=&quot;4&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s540/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s540/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of the cloud evolution from a simulation of a 285 x 285 x 2 km&lt;sup>;3&lt;/sup>; stratocumulus cloud sheet. This is the largest cloud sheet of its kind ever simulated. &lt;strong>;Left&lt;/strong>;: An oblique view of the cloud field with the camera cruising. &lt;strong>;Right&lt;/strong>;: Top view of the cloud field with the camera gradually pulled away.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The LES code is written in TensorFlow, an open-source software platform developed by Google for ML applications. The code takes advantage of TensorFlow&#39;s graph computation and &lt;a href=&quot;https://www.tensorflow.org/xla&quot;>;Accelerated Linear Algebra&lt;/a>; (XLA) optimizations, which enable the full exploitation of TPU hardware, including the high-speed, low-latency &lt;a href=&quot;https://patents.google.com/patent/US9372800&quot;>;inter-chip interconnects&lt;/a>; (ICI) that helped us achieve this unprecedented performance. At the same time, the TensorFlow code makes it easy to incorporate ML components directly within the physics-based fluid solver. &lt;/p>; &lt;p>; We validated the code by simulating canonical test cases for atmospheric flow solvers, such as a buoyant bubble that rises in neutral stratification, and a negatively buoyant bubble that sinks and impinges on the surface. These test cases show that the TPU-based code faithfully simulates the flows, with increasingly fine turbulent details emerging as the resolution increases. The validation tests culminate in simulations of the conditions during the DYCOMS field campaign. The TPU-based code reliably reproduces the cloud fields and turbulence characteristics observed by aircraft during a field campaign — a feat that is &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;notoriously difficult to achieve for LES&lt;/a>; because of the &lt;a href=&quot;https://doi.org/10.1029/2018MS001312&quot;>;rapid changes in temperature and other thermodynamic properties&lt;/a>; at the top of the stratocumulus decks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/s1023/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;815&quot; data-original-width=&quot;1023&quot; height=&quot;510&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/w640-h510/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;One of the test cases used to validate our TPU Cloud simulator. The fine structures from the density current generated by the negatively buoyant bubble impinging on the surface are much better resolved with a high resolution grid (10m, bottom row) compared to a low resolution grid (200 m, top row).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Outlook&lt;/h2>; &lt;p>; With this foundation established, our next goal is to substantially enlarge existing &lt;a href=&quot;https://doi.org/10.1029/2021MS002631&quot;>;databases&lt;/a>; of high-resolution cloud simulations that researchers building climate models can use to develop better cloud parameterizations — whether these are for physics-based models, ML models, or hybrids of the two. This requires additional physical processes beyond that described in the &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;paper&lt;/a>;; for example, the need to integrate radiative transfer processes into the code. Our goal is to generate data across a variety of cloud types, eg, thunderstorm clouds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s540/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;303&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of a thunderstorm simulation using the same simulator as the stratocumulus simulation work. Rainfall can also be observed near the ground.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; This work illustrates how advances in hardware for ML can be surprisingly effective when repurposed in other research areas — in this case, climate modeling. These simulations provide detailed training data for processes such as in-cloud turbulence, which are not directly observable, yet are crucially important for climate modeling and prediction. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank the co-authors of the paper: Sheide Chammas, Qing Wang, Matthias Ihme, and John Anderson. We&#39;d also like to thank Carla Bromberg, Rob Carver, Fei Sha, and Tyler Russell for their insights and contributions to the work.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1055595481112584523/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and.html&quot; rel=&quot;alternate&quot; title=&quot;Improving simulations of clouds and their effects on climate&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3970135078050750650&lt;/id>;&lt;published>;2023-11-21T10:09:00.000-08:00&lt;/published>;&lt;updated>;2023-11-21T10:09:33.541-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Open sourcing Project Guideline: A platform for computer vision accessibility technology&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dave Hawkey, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Two years ago we &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;announced Project Guideline&lt;/a>;, a collaboration between Google Research and &lt;a href=&quot;https://www.guidingeyes.org/&quot;>;Guiding Eyes for the Blind&lt;/a>; that enabled people with visual impairments (eg, blindness and low-vision) to walk, jog, and run independently. Using only a Google Pixel phone and headphones, Project Guideline leverages on-device machine learning (ML) to navigate users along outdoor paths marked with a painted line. The technology has been &lt;a href=&quot;https://projectguidelinejp.withgoogle.com/intl/en/&quot;>;tested all over the world&lt;/a>; and even demonstrated during the &lt;a href=&quot;https://www.youtube.com/live/2cW1-plwqeQ?si=MTIX2uJkyWuLluht&amp;amp;t=7334&quot;>;opening ceremony at the Tokyo 2020 Paralympic Games&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Since the original announcement, we set out to improve Project Guideline by embedding new features, such as obstacle detection and advanced path planning, to safely and reliably navigate users through more complex scenarios (such as sharp turns and nearby pedestrians). The early version featured a simple frame-by-frame image segmentation that detected the position of the path line relative to the image frame. This was sufficient for orienting the user to the line, but provided limited information about the surrounding environment. Improving the navigation signals, such as alerts for obstacles and upcoming turns, required a much better understanding and mapping of the users&#39; environment. To solve these challenges, we built a platform that can be utilized for a variety of spatially-aware applications in the accessibility space and beyond. &lt;/p>; &lt;p>; Today, we announce the &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;open source release of Project Guideline&lt;/a>;, making it available for anyone to use to improve upon and build new accessibility experiences. The release includes &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;source code&lt;/a>; for the core platform, an &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/android&quot;>;Android application&lt;/a>;, pre-trained &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/vision/models&quot;>;ML models&lt;/a>;, and a &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/unreal&quot;>;3D simulation framework&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;System design&lt;/h2>; &lt;p>; The primary use-case is an Android application, however we wanted to be able to run, test, and debug the core logic in a variety of environments in a reproducible way. This led us to design and build the system using C++ for close integration with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; and other core libraries, while still being able to integrate with Android using the &lt;a href=&quot;https://developer.android.com/ndk&quot;>;Android NDK&lt;/a>;. &lt;/p>; &lt;p>; Under the hood, Project Guideline uses &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; to estimate the position and orientation of the user as they navigate the课程。 A segmentation model, built on the &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLabV3+&lt;/a>; framework, processes each camera frame to generate a binary mask of the guideline (see the &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;previous blog post&lt;/a>; for more details). Points on the segmented guideline are then projected from image-space coordinates onto a world-space ground plane using the camera pose and lens parameters (intrinsics) provided by ARCore. Since each frame contributes a different view of the line, the world-space points are aggregated over multiple frames to build a virtual mapping of the real-world guideline. The system performs piecewise curve approximation of the guideline world-space coordinates to build a spatio-temporally consistent trajectory. This allows refinement of the estimated line as the user progresses along the path. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;355&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Project Guideline builds a 2D map of the guideline, aggregating detected points in each frame (&lt;strong>;red&lt;/strong>;) to build a stateful representation (&lt;strong>;blue&lt;/strong>;) as the runner progresses along the path.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A control system dynamically selects a target point on the line some distance ahead based on the user&#39;s current position, velocity, and direction. An audio feedback signal is then given to the user to adjust their heading to coincide with the upcoming line segment. By using the runner&#39;s velocity vector instead of camera orientation to compute the navigation signal, we eliminate noise caused by irregular camera movements common during running. We can even navigate the user back to the line while it&#39;s out of camera view, for example if the user overshot a turn. This is possible because ARCore continues to track the pose of the camera, which can be compared to the stateful line map inferred from previous camera images. &lt;/p>; &lt;p>; Project Guideline also includes obstacle detection and avoidance features. An ML model is used to estimate depth from single images. To train this monocular depth model, we used &lt;a href=&quot;https://blog.research.google/2023/10/sanpo-scene-understanding-accessibility.html&quot;>;SANPO&lt;/a>;, a large dataset of outdoor imagery from urban, park, and suburban environments that was curated in-house. The model is capable of detecting the depth of various obstacles, including people, vehicles, posts, and more. The depth maps are converted into 3D point clouds, similar to the line segmentation process, and used to detect the presence of obstacles along the user&#39;s path and then alert the user through an audio signal. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/s800/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; height=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/w640-h480/image2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using a monocular depth ML model, Project Guideline constructs a 3D point cloud of the environment to detect and alert the user of potential obstacles along the path.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A low-latency audio system based on the &lt;a href=&quot;https://developer.android.com/ndk/guides/audio/aaudio/aaudio&quot;>;AAudio API&lt;/a>; was implemented to provide the navigational sounds and cues to the user. Several &lt;em>;sound packs&lt;/em>; are available in Project Guideline, including a spatial sound implementation using the &lt;a href=&quot;https://resonance-audio.github.io/resonance-audio/&quot;>;Resonance Audio API&lt; /a>;. The sound packs were developed by a team of sound researchers and engineers at Google who designed and tested many different sound models. The sounds use a combination of panning, pitch, and spatialization to guide the user along the line. For example, a user veering to the right may hear a beeping sound in the left ear to indicate the line is to the left, with increasing frequency for a larger course correction. If the user veers further, a high-pitched warning sound may be heard to indicate the edge of the path is approaching. In addition, a clear “stop” audio cue is always available in the event the user veers too far from the line, an anomaly is detected, or the system fails to provide a navigational signal. &lt;/p>; &lt;p>; Project Guideline has been built specifically for Google Pixel phones with the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Google Tensor&lt;/a>; chip. The Google Tensor chip enables the optimized ML models to run on-device with higher performance and lower power consumption. This is critical for providing real-time navigation instructions to the user with minimal delay. On a Pixel 8 there is a 28x latency improvement when running the depth model on the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Tensor Processing Unit&lt;/a>; (TPU) instead of CPU, and 9x improvement compared to GPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s1124/image3.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;748&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Testing and simulation&lt;/h2>; &lt;p>; Project Guideline includes a simulator that enables rapid testing and prototyping of the system in a virtual environment. Everything from the ML models to the audio feedback system runs natively within the simulator, giving the full Project Guideline experience without needing all the hardware and physical environment set up. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/s1200/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1200&quot; height=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/w640-h480/image4.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Screenshot of Project Guideline simulator.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; To launch the technology forward, &lt;a href=&quot;https://www.wear.works&quot;>;WearWorks&lt;/a>; has become an early adopter and teamed up with Project Guideline to integrate their patented haptic navigation experience, utilizing haptic feedback in addition to sound to guide runners. WearWorks has been developing haptics for over 8 years, and previously empowered the first blind marathon runner to complete the NYC Marathon without sighted assistance. We hope that integrations like these will lead to new innovations and make the world a more accessible place.&lt;br />; &lt;/p>; &lt;p>; The Project Guideline team is also working towards removing the painted line completely, using the latest advancements in mobile ML technology, such as the &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;ARCore Scene Semantics API&lt;/a>;, which can identify sidewalks, buildings, and other objects in outdoor scenes. We invite the accessibility community to build upon and improve this technology while exploring new use cases in other fields. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of Project Guideline and the technologies behind it. We&#39;d like to thank Project Guideline team members: Dror Avalon, Phil Bayer, Ryan Burke, Lori Dooley, Song Chun Fan, Matt Hall, Amélie Jean-aimée, Dave Hawkey, Amit Pitaru, Alvin Shi, Mikhail Sirotenko, Sagar Waghmare, John Watkinson, Kimberly Wilber, Matthew Willson, Xuan Yang, Mark Zarich, Steven Clark, Jim Coursey, Josh Ellis, Tom Hoddes, Dick Lyon, Chris Mitchell, Satoru Arao, Yoojin Chung, Joe Fry, Kazuto Furuichi, Ikumi Kobayashi, Kathy Maruyama, Minh Nguyen, Alto Okamura, Yosuke Suzuki, and Bryan Tanaka. Thanks to ARCore contributors: Ryan DuToit, Abhishek Kar, and Eric Turner. Thanks to Alec Go, Jing Li, Liviu Panait, Stefano Pellegrini, Abdullah Rashwan, Lu Wang, Qifei Wang, and Fan Yang for providing ML platform support. We&#39;d also like to thank Hartwig Adam, Tomas Izo, Rahul Sukthankar, Blaise Aguera y Arcas, and Huisheng Wang for their leadership support. Special thanks to our partners Guiding Eyes for the Blind and Achilles International.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3970135078050750650/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html&quot; rel=&quot;alternate&quot; title=&quot;Open sourcing Project Guideline: A platform for computer vision accessibility technology&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6563276834599281497&lt;/id>;&lt;published>;2023-11-17T11:36:00.000-08:00&lt;/published>;&lt;updated>;2023-11-17T11:36:32.036-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research Awards&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;University Relations&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Emerging practices for Society-Centered AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Anoop Sinha, Research Director, Technology &amp;amp; Society, and Yossi Matias, Vice President, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The first of &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google&#39;s AI Principles&lt;/a>; is to “Be socially beneficial.” As AI practitioners, we&#39;re inspired by the transformative potential of AI technologies to benefit society and our shared environment at a scale and swiftness that wasn&#39;t possible before. From &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;helping address the climate crisis&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/how-were-using-ai-to-help-transform-healthcare/&quot;>;helping transform healthcare&lt;/a>;, to &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/global-accessibility-awareness-day-google-product-update/&quot;>;making the digital world more accessible&lt;/a>;, our goal is to apply AI responsibly to be helpful to more people around the globe. Achieving global scale requires researchers and communities to think ahead — and act — collectively across the AI ecosystem. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We call this approach Society-Centered AI. It is both an extension and an expansion of &lt;a href=&quot;https://hcil.umd.edu/human-centered-ai/&quot;>;Human-Centered AI, &lt;/a>;focusing on the aggregate needs of society that are still informed by the needs of individual users, specifically within the context of the larger, shared human experience. Recent AI advances offer unprecedented, societal-level capabilities, and we can now methodically address those needs — if we apply collective, multi-disciplinary AI research to society-level, shared challenges, from forecasting hunger to predicting diseases to improving productivity. &lt;/p>; &lt;p>; The &lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/AI_Opportunity_Agenda.pdf&quot;>;opportunity for AI&lt;/a>; to benefit society increases each天。 We took a look at our work in these areas and at the research projects we have supported. Recently, Google &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;announced that 70 professors were selected&lt;/a>; for the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;2023 Award for Inclusion Research Program&lt;/a>;, which supports academic research that addresses the needs of historically marginalized groups globally. Through evaluation of this work, we identified a few emerging practices for Society-Centered AI: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Understand society&#39;s needs&lt;/strong>; &lt;br />;Listening to communities and partners is crucial to understanding major issues deeply and identifying priority challenges to address. As an emerging general purpose technology, AI has the potential to address major global societal issues that can significantly impact people&#39;s lives (eg, educating workers, improving healthcare, and improving productivity). We have found the key to impact is to be centered on society&#39;s needs. For this, we focus our efforts on goals society has agreed should be prioritized, such as the United Nations&#39; &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/globalgoals&quot;>;17 Sustainable Development Goals&lt;/a>;, a set of interconnected goals jointly developed by more than 190 countries to address global challenges. &lt;/li>;&lt;li>;&lt;strong>;Collective efforts to address those needs &lt;br />;&lt;/strong>;Collective efforts bring stakeholders (eg, local and academic communities, NGOs, private-public collaborations) into a joint process of design, development, implementation, and evaluation of AI technologies as they are being developed and deployed to address societal needs. &lt;/li>;&lt;li>;&lt;strong>;Measuring success by how well the effort addresses society&#39;s needs &lt;br />;&lt;/strong>;It is important and challenging to measure how well AI solutions address society&#39;s needs. In each of our cases, we identified primary and secondary indicators of impact that we optimized through our collaborations with stakeholders. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Why is Society-Centered AI important?&lt;/h2>; &lt;p>; The case examples described below show how the Society-Centered AI approach has led to impact across topics, such as accessibility, health, and climate. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Understanding the needs of individuals with non-standard speech&lt;/h3>; &lt;p>; There are &lt;a href=&quot;https://www.nidcd.nih.gov/health/statistics/quick-statistics-voice-speech-language&quot;>;millions of people&lt;/a>; with non-standard speech (eg, impaired articulation, &lt;a href=&quot;https://en.wikipedia.org/wiki/Dysarthria&quot;>;dysarthria&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Spasmodic_dysphonia&quot;>;dysphonia&lt;/a>;)仅在美国。 In &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/impaired-speech-recognition/&quot;>;2019&lt;/a>;, Google Research launched &lt;a href=&quot;https://blog.research.google/2019/08/project-euphonias-personalized-speech.html&quot;>;Project Euphonia&lt;/a>;, a methodology that allows individual users with non-standard speech to train personalized speech recognition models. Our success began with the impact we had on each individual who is now able to use voice dictation on their mobile device. &lt;/p>; &lt;p>; Euphonia started with a Society-Centered AI approach, including collective efforts with the non-profit organizations &lt;a href=&quot;http://als.net/&quot;>;ALS Therapy Development Institute&lt;/a>; and &lt;a href=&quot;http://www.alsri.org/&quot;>;ALS Residence Initiative&lt;/a>; to understand the needs of individuals with &lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;amyotrophic lateral sclerosis&lt;/a>; (ALS) and their ability to use automatic speech recognition systems. Later, we developed &lt;a href=&quot;https://blog.research.google/2021/09/personalized-asr-models-from-large-and.html&quot;>;the world&#39;s largest corpus&lt;/a>; of non-standard speech recordings, which enabled us to train a &lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; to &lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;better recognize disordered speech by 37%&lt;/a>; on real conversation &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;word error rate&lt;/a>; (WER) measurement. This also led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/speech-accessibility-project/&quot;>;2022&lt;/a>; collaboration between the University of Illinois Urbana-Champaign, Alphabet, Apple, Meta, Microsoft, and Amazon to begin the &lt;a href=&quot;https://speechaccessibilityproject.beckman.illinois.edu/&quot;>;Speech Accessibility Project&lt;/a>;, an ongoing initiative to create a publicly available dataset of disordered speech samples to improve products and make speech recognition more inclusive of diverse speech patterns. Other technologies that use AI to help remove barriers of modality and languages, include &lt;a href=&quot;https://about.google/stories/making-conversation-more-accessible-with-live-transcribe/&quot;>;live transcribe&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/first-time-i-was-able-call-my-23-year-old-son/&quot;>;live caption&lt;/a>; and &lt;a href=&quot;https://blog.google/intl/en-in/products/explore-communicate/easier-access-to-web-pages-let/&quot;>;read aloud&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Focusing on society&#39;s health needs&lt;/h3>; &lt;p>; Access to timely maternal health information can save lives globally: &lt;a href=&quot;https://www.who.int/news/item/23-02-2023-a-woman-dies-every-two-minutes-due-to-pregnancy-or-childbirth--un-agencies&quot;>;every two minutes a woman dies during pregnancy or childbirth&lt;/a>; and &lt;a href=&quot;https://data.unicef.org/topic/child-survival/under-five-mortality/&quot;>;1 in 26 children die before reaching age five&lt;/a>;. In rural India, the education of expectant and new mothers around key health issues pertaining to pregnancy and infancy required scalable, low-cost technology solutions. Together with &lt;a href=&quot;https://armman.org/&quot;>;ARMMAN&lt;/a>;, Google Research supported &lt;a href=&quot;https://blog.research.google/2022/08/using-ml-to-boost-engagement-with.html&quot;>;a program&lt;/a>; that uses mobile messaging and machine learning (ML) algorithms to predict when women might benefit from receiving interventions (ie, targeted preventative care information) and encourages them to engage with the &lt;a href=&quot;https://armman.org/mmitra/&quot;>;mMitra&lt;/a>; free voice call program. Within a year, the mMitra program has shown a 17% increase in infants with tripled birth weight and a 36% increase in women understanding the importance of taking iron tablets during pregnancy. Over 175K mothers and growing have been reached through this automated solution, which public health workers use to improve the quality of information delivery. &lt;/p>; &lt;p>; These efforts have been successful in improving health due to the close collective partnership among the community and those building the AI technology. We have adopted this same approach via collaborations with caregivers to address a variety of medical needs. Some examples include: the use of the &lt;a href=&quot;https://health.google/caregivers/arda/&quot;>;Automated Retinal Disease Assessment&lt;/a>; (ARDA) to &lt;a href=&quot;https://blog.google/technology/health/5-myths-about-medical-ai-debunked/&quot;>;help screen for diabetic retinopathy&lt;/a>; in 250,000 patients in clinics around the world; our partnership with &lt;a href=&quot;https://www.icadmed.com/&quot;>;iCAD&lt;/a>; to bring our &lt;a href=&quot;https://blog.google/technology/ai/icad-partnership-breast-cancer-screening/&quot;>;mammography&lt;/a>; AI models to clinical settings to aid in breast cancer detection; and the development of &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>;, a medical large language model that is now being &lt;a href=&quot;https: //cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model&quot;>;tested with Cloud partners&lt;/a>; to help doctors provide better病人护理。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Compounding impact from sustained efforts for crisis response&lt;/h3>; &lt;p>; Google Research&#39;s flood prediction efforts began in 2018 with &lt;a href=&quot;https://www.blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;flood forecasting&lt;/a>; in India and &lt;a href=&quot;https://www.undp.org/bangladesh/blog/climate-change-google-and-bangladesh-floods&quot;>;expanded to Bangladesh&lt;/a>; to help combat the catastrophic damage from yearly floods. The initial efforts began with partnerships with &lt;a href=&quot;https://cwc.gov.in/&quot;>;India&#39;s Central Water Commission&lt;/a>;, local governments and communities. The implementation of these efforts used &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>; on Search and Maps, and, more recently, broadly expanded access via &lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>;. Continued &lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;collaborations&lt;/a>; and advancing an AI-based global flood forecasting model allowed us to &lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;expand this capability&lt;/a>; to &lt;a href=&quot;https://blog .google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;over 80 countries&lt;/a>; across Africa, the Asia-Pacific region, Europe, and South, Central, and North美国。 We also partnered with networks of community volunteers to further amplify flood alerts. By working with governments and communities to measure the impact of these efforts on society, we refined our approach and algorithms each year. &lt;/p>; &lt;p>; We were able to leverage those methodologies and some of the underlying technology, such as &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>;, from &lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;flood forecasting&lt;/a>; to similar societal needs, such as &lt;a href=&quot;https://blog.google/products/search/mapping-wildfires-with-satellite-data/&quot;>;wildfire forecasting&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;heat alerts&lt;/a>;. Our continued engagements with organizations led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/early-warning-system-wmo-google/&quot;>;support of additional efforts&lt;/a>;, such as the World Meteorological Organization&#39;s (WMO) &lt;a href=&quot;https://public.wmo.int/en/earlywarningsforall&quot;>;Early Warnings For All Initiative&lt;/a>;. The continued engagement with communities has allowed us to learn about our users&#39; needs on a societal level over time, expand our efforts, and compound the societal reach and impact of our efforts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Further supporting Society-Centered AI research&lt;/h2>; &lt;p>; &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;We recently funded&lt;/a>; 18 university research proposals exemplifying a Society-Centered AI approach, a new track within the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;Google Award for Inclusion Research Program&lt;/a>;. These researchers are taking the Society-Centered AI methodology and helping create beneficial applications across the world. Examples of some of the projects funded include: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;AI-Driven Monitoring of Attitude Polarization in Conflict-Affected Countries for Inclusive Peace Process and Women&#39;s Empowerment:&lt;/strong>; This project&#39;s goal is to create LLM-powered tools that can be used to monitor peace in online conversations in developing nations. The initial target communities are where peace is in flux and the effort will put a particular emphasis on mitigating polarization that impacts women and promoting harmony.&lt;/li>; &lt;li>;&lt;strong>;AI-Assisted Distributed Collaborative Indoor Pollution Meters: A Case Study , Requirement Analysis, and Low-Cost Healthy Home Solution for Indian Communities: &lt;/strong>;This project is looking at the usage of low-cost pollution monitors combined with AI-assisted methodology for identifying recommendations for communities to improve air quality and at home健康。 The initial target communities are highly impacted by pollution, and the joint work with them includes the goal of developing how to measure improvement in outcomes in the local community. &lt;/li>; &lt;li>;&lt;strong>;Collaborative Development of AI Solutions for Scaling Up Adolescent Access to Sexual and Reproductive Health Education and Services in Uganda: &lt;/strong>;This project&#39;s goal is to create LLM-powered tools to provide personalized coaching and learning for users&#39; needs on topics of sexual and reproductive health education in low-income settings in Sub-Saharan Africa. The local societal need is significant, with an estimated &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9652643/&quot;>;25% rate of teenage pregnancy&lt;/a>;, and the project aims to address the needs with a collective development process for the AI solution.&lt;strong>; &lt;/strong>; &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; Focusing on society&#39;s needs, working via multidisciplinary collective research, and measuring the impact on society helps lead to AI solutions that are relevant, long-lasting, empowering, and beneficial. See the &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/&quot;>;AI for the Global Goals&lt;/a>; to learn more about potential Society-Centered AI research problems. &lt;a href=&quot;https://blog.google/outreach-initiatives/google-org/httpsbloggoogleoutreach-initiativesgoogle-orgunited-nations-global-goals-google-ai-/&quot;>;Our efforts&lt;/a>; with non-profits in these areas is complementary to the research that we are doing and encouraging. We believe that further initiatives using Society-Centered AI will help the collective research community solve problems and positively impact society at large. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many thanks to the many individuals who have worked on these projects at Google including Shruti Sheth, Reena Jana, Amy Chung-Yu Chou, Elizabeth Adkison, Sophie Allweis, Dan Altman, Eve Andersson, Ayelet Benjamini&lt;/em>;, &lt;em>;Julie Cattiau, Yuval Carny, Richard Cave, Katherine Chou, Greg Corrado, Carlos De Segovia, Remi Denton, Dotan Emanuel, Ashley Gardner, Oren Gilon, Taylor Goddu, Brigitte Hoyer Gosselink, Jordan Green, Alon Harris&lt;/em>;, &lt;em>;Avinatan Hassidim, Rus Heywood, Sunny Jansen, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Ronit Levavi Morad, Bob MacDonald, Alicia Martin, Shakir Mohamed, Philip Nelson, Moriah Royz, Katie Seaver, Joel Shor, Milind Tambe, Aparna Taneja, Divy Thakkar, Jimmy Tobin, Katrin Tomanek, Blake Walsh, Gal Weiss, Kasumi Widner, Lihong Xi, and teams.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6563276834599281497/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot; rel=&quot;alternate&quot; title=&quot;Emerging practices for Society-Centered AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6928081948246813203&lt;/id>;&lt;published>;2023-11-16T13:11:00.000-08:00&lt;/published>;&lt;updated>;2023-12-07T08:16:11.190-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Adversarial testing for generative AI safety&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kathy Meier-Hellstern, Building Responsible AI &amp;amp; Data Systems, Director, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The&lt;em>; &lt;/em>;&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; (RAI-HCT) team within Google Research is committed to advancing the theory and practice of responsible human-centered AI through a lens of culturally-aware research, to meet the needs of billions of users today, and blaze the path forward for a better AI future. The BRAIDS (Building Responsible AI Data and Solutions) team within RAI-HCT aims to simplify the adoption of RAI practices through the utilization of scalable tools, high-quality data, streamlined processes, and novel research with a current emphasis on addressing the unique challenges posed by &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>; (GenAI). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; GenAI models have enabled unprecedented capabilities leading to a rapid surge of innovative applications. Google actively leverages &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;GenAI&lt;/a>; to &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview&quot;>;enhance&lt;/a>; its &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai-in-workspace-now-available&quot;>;products&#39; utility&lt;/a>; and to improve lives. While enormously beneficial, GenAI also presents risks for disinformation, bias, and security. In 2018, Google pioneered the &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>;, emphasizing beneficial use and prevention of harm. Since then, Google has focused on effectively implementing our principles in &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; through 1) a comprehensive risk assessment framework, 2) internal governance structures, 3) education, empowering Googlers to integrate AI Principles into their work, and 4) the development of processes and tools that identify, measure, and analyze ethical risks throughout the lifecycle of AI-powered products. The BRAIDS team focuses on the last area, creating tools and techniques for identification of ethical and safety risks in GenAI products that enable teams within Google to apply appropriate mitigations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;What makes GenAI challenging to build responsibly?&lt;/h2>; &lt;p>; The unprecedented capabilities of GenAI models have been accompanied by a new spectrum of potential failures, underscoring the urgency for a comprehensive and systematic RAI approach to understanding and mitigating potential safety concerns before the model is made broadly available. One key technique used to understand potential risks is &lt;em>;adversarial testing&lt;/em>;, which is testing performed to systematically evaluate the models to learn how they behave when provided with malicious or inadvertently harmful inputs across a range of scenarios. To that end, our research has focused on three directions: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Scaled adversarial data generation&lt;/em>;&lt;br />; Given the diverse user communities, use cases, and behaviors, it is difficult to comprehensively identify critical safety issues prior to launching a product or service. Scaled adversarial data generation with humans-in-the-loop addresses this need by creating test sets that contain a wide range of diverse and potentially unsafe model inputs that stress the model capabilities under adverse circumstances. Our unique focus in BRAIDS lies in identifying societal harms to the diverse user communities impacted by our models. &lt;/li>; &lt;li>;&lt;em>;Automated test set evaluation and community engagement&lt;/em>;&lt;br />; Scaling the testing process so that many thousands of model responses can be quickly evaluated to learn how the model responds across a wide range of potentially harmful scenarios is aided with automated test set evaluation. Beyond testing with adversarial test sets, community engagement is a key component of our approach to identify “unknown unknowns” and to seed the data generation process.&lt;/li>; &lt;li>;&lt;em>;Rater diversity&lt;/em>;&lt;br />; Safety evaluations rely on human judgment, which is shaped by community and culture and is not easily automated. To address this, we prioritize research on rater diversity.&lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scaled adversarial data generation&lt;/h2>; &lt;p>; High-quality, comprehensive data underpins many key programs across Google. Initially reliant on manual data generation, we&#39;ve made significant strides to automate the adversarial data generation process. A centralized data repository with use-case and policy-aligned prompts is available to jump-start the generation of new adversarial tests. We have also developed multiple synthetic data generation tools based on large language models (LLMs) that prioritize the generation of data sets that reflect diverse societal contexts and that integrate data quality metrics for improved dataset quality and diversity. &lt;/p>; &lt;p>; Our data quality metrics include: &lt;/p>; &lt;ul>; &lt;li>;Analysis of language styles, including query length, query similarity, and diversity of language styles.&lt;/li>; &lt;li>;Measurement across a wide range of societal and multicultural dimensions, leveraging datasets such as &lt;a href=&quot;https://github.com/google-research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>;, &lt;a href=&quot;https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;, the &lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;Societal Context Repository&lt;/a>;.&lt;/li>; &lt;li>;Measurement of alignment with Google&#39;s &lt;a href=&quot;https://policies.google.com/terms/generative-ai/use-policy&quot;>;generative AI policies&lt;/a>; and intended use cases.&lt;/li>; &lt;li>;Analysis of adversariality to ensure that we examine both explicit (the input is clearly designed to produce an unsafe output) and implicit (where the input is innocuous but the output is harmful) queries. &lt;/li>; &lt;/ul>; &lt;p>; One of our approaches to scaled data generation is exemplified in our paper on &lt;a href=&quot;https://arxiv.org/abs/2311.08592&quot;>;AI-Assisted Red Teaming&lt;/a>; (AART). AART generates evaluation datasets with high diversity (eg, sensitive and harmful concepts specific to a wide range of cultural and geographic regions), steered by AI-assisted recipes to define, scope and prioritize diversity within an application context. Compared to some state-of-the-art tools, AART shows promising results in terms of concept coverage and data quality. Separately, we are also working with MLCommons to contribute to &lt;a href=&quot;https://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot;>;public benchmarks for AI Safety&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Adversarial testing and community insights &lt;/h2>; &lt;p>; Evaluating model output with adversarial test sets allows us to identify critical safety issues prior to deployment. Our initial evaluations relied exclusively on human ratings, which resulted in slow turnaround times and inconsistencies due to a lack of standardized safety definitions and policies. We have improved the quality of evaluations by introducing policy-aligned rater guidelines to improve human rater accuracy, and are researching additional improvements to better reflect the perspectives of diverse communities. Additionally, automated test set evaluation using LLM-based auto-raters enables efficiency and scaling, while allowing us to direct complex or ambiguous cases to humans for expert rating. &lt;/p>; &lt;p>; Beyond testing with adversarial test sets, gathering community insights is vital for continuously discovering “unknown unknowns”. To provide high quality human input that is required to seed the scaled processes, we partner with groups such as the &lt;a href=&quot;https://sites.google.com/corp/google.com/earr-external-research-group/home&quot;>;Equitable AI Research Round Table&lt;/a>; (EARR), and with our internal ethics and analysis teams to ensure that we are representing the diverse communities who use our models. The &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;Adversarial Nibbler Challenge&lt;/a>; engages external users to understand potential harms of &lt;a href=&quot;https://arxiv.org/abs/2305.14384&quot;>;unsafe, biased or violent outputs&lt;/a>; to end users at scale. Our continuous commitment to community engagement includes gathering feedback from diverse communities and collaborating with the research community, for example during &lt;a href=&quot;https://sites.google.com/view/art-of-safety&quot;>;The ART of Safety workshop&lt;/a>; at the &lt;a href=&quot;http://www.ijcnlp-aacl2023.org/&quot;>;Asia-Pacific Chapter of the Association for Computational Linguistics Conference&lt;/a>; (IJCNLP-AACL 2023) to address adversarial testing challenges for GenAI. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Rater diversity in safety evaluation&lt;/h2>; &lt;p>; Understanding and mitigating GenAI safety risks is both a technical and social challenge. Safety perceptions are intrinsically subjective and influenced by a wide range of intersecting factors. Our in-depth study on demographic influences on safety perceptions explored the &lt;a href=&quot;https://arxiv.org/abs/2306.11530&quot;>;intersectional effects of rater demographics&lt;/a>; (eg, race/ethnicity, gender, age) and content characteristics (eg, degree of harm) on safety assessments of GenAI outputs. Traditional approaches largely ignore inherent subjectivity and the systematic disagreements among raters, which can mask important cultural differences. Our &lt;a href=&quot;https://arxiv.org/abs/2311.05074&quot;>;disagreement analysis framework&lt;/a>; surfaced a variety of disagreement patterns between raters from diverse backgrounds including also with “ground truth” expert ratings. This paves the way to new approaches for assessing quality of human annotation and model evaluations beyond the simplistic use of gold labels. Our &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;NeurIPS 2023 publication&lt;/a>; introduces the &lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset&quot;>;DICES&lt;/a>; (Diversity In Conversational AI Evaluation for Safety) dataset that facilitates nuanced safety evaluation of LLMs and accounts for variance, ambiguity, and diversity in various cultural contexts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Summary&lt;/h2>; &lt;p>; GenAI has resulted in a technology transformation, opening possibilities for rapid development and customization even without coding. However, it also comes with a risk of generating harmful outputs. Our proactive adversarial testing program identifies and mitigates GenAI risks to ensure inclusive model behavior. Adversarial testing and red teaming are essential components of a Safety strategy, and conducting them in a comprehensive manner is essential. The rapid pace of innovation demands that we constantly challenge ourselves to find “unknown unknowns” in cooperation with our internal partners, diverse user communities, and other industry experts. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6928081948246813203/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Adversarial testing for generative AI safety&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1338549955376716163&lt;/id>;&lt;published>;2023-11-14T12:28:00.000-08:00&lt;/published>;&lt;updated>;2023-11-14T14:09:51.250-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Scaling multimodal understanding to long videos&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Isaac Noble, Software Engineer, Google Research, and Anelia Angelova, Research Scientist, Google DeepMind &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s1600/mirasol.png&quot; style=&quot;display: none;&quot; />; &lt;p>; When building machine learning models for real-life applications, we need to consider inputs from multiple modalities in order to capture various aspects of the world around us. For example, audio, video, and text all provide varied and complementary information about a visual input. However, building multimodal models is challenging due to the heterogeneity of the modalities. Some of the modalities might be well synchronized in time (eg, audio, video) but not aligned with text. Furthermore, the large volume of data in video and audio signals is much larger than that in text, so when combining them in multimodal models, video and audio often cannot be fully consumed and need to be disproportionately compressed. This problem is exacerbated for longer video inputs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2311.05698&quot;>;Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities&lt;/a>;”, we introduce a multimodal &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive&lt;/a>; model (Mirasol3B) for learning across audio, video, and text方式。 The main idea is to decouple the multimodal modeling into separate focused autoregressive models, processing the inputs according to the characteristics of the modalities. Our model consists of an autoregressive component for the time-synchronized modalities (audio and video) and a separate autoregressive component for modalities that are not necessarily time-aligned but are still sequential, eg, text inputs, such as a title or description. Additionally, the time-aligned modalities are partitioned in time where local features can be jointly learned. In this way, audio-video inputs are modeled in time and are allocated comparatively more parameters than prior works. With this approach, we can effortlessly handle much longer videos (eg, 128-512 frames) compared to other multimodal models. At 3B parameters, Mirasol3B is compact compared to prior &lt;a href=&quot;https://arxiv.org/abs/2204.14198&quot;>;Flamingo&lt;/a>; (80B) and &lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>;PaLI-X&lt;/a>; (55B) models. Finally, Mirasol3B outperforms the state-of-the-art approaches on &lt;a href=&quot;https://blog.research.google/2022/08/efficient-video-text-learning-with.html&quot;>;video question answering&lt;/a>; (video QA), long video QA, and audio-video-text benchmarks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s1240/image5.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1240&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Mirasol3B architecture consists of an autoregressive model for the time-aligned modalities (audio and video), which are partitioned in chunks, and a separate autoregressive model for the unaligned context modalities (eg, text). Joint feature learning is conducted by the Combiner, which learns compact but sufficiently informative features, allowing the processing of long video/audio inputs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Coordinating time-aligned and contextual modalities&lt;/h2>; &lt;p>; Video, audio and text are diverse modalities with distinct characteristics. For example, video is a spatio-temporal visual signal with 30–100 frames per second, but due to the large volume of data, typically only 32–64 frames &lt;em>;per video&lt;/em>; are consumed by current models. Audio is a one-dimensional temporal signal obtained at much higher frequency than video (eg, at 16 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hertz&quot;>;Hz&lt;/a>;), whereas text inputs that apply to the whole video, are typically 200–300 word-sequence and serve as a context to the audio-video inputs. To that end, we propose a model consisting of an autoregressive component that fuses and jointly learns the time-aligned signals, which occur at high frequencies and are roughly synchronized, and another autoregressive component for processing non-aligned signals. Learning between the components for the time-aligned and contextual modalities is coordinated via cross-attention mechanisms that allow the two to exchange information while learning in a sequence without having to synchronize them in time. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Time-aligned autoregressive modeling of video and audio&lt;/h2>; &lt;p>; Long videos can convey rich information and activities happening in a sequence. However, present models approach video modeling by extracting all the information at once, without sufficient temporal information. To address this, we apply an autoregressive modeling strategy where we condition jointly learned video and audio representations for one time interval on feature representations from previous time intervals. This preserves temporal information. &lt;/p>; &lt;p>; The video is first partitioned into smaller video chunks. Each chunk itself can be 4–64 frames. The features corresponding to each chunk are then processed by a learning module, called the Combiner (described below), which generates a joint audio and video feature representation at the current step — this step extracts and compacts the most important information per chunk. Next, we process this joint feature representation with an autoregressive Transformer, which applies attention to the previous feature representation and generates the joint feature representation for the next step. Consequently, the model learns how to represent not only each individual chunk, but also how the chunks relate temporally. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s1259/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;744&quot; data-original-width=&quot;1259&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use an autoregressive modeling of the audio and video inputs, partitioning them in time and learning joint feature representations, which are then autoregressively learned in sequence.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Modeling long videos with a modality combiner&lt;/h2>; &lt;p>; To combine the signals from the video and audio information in each video chunk, we propose a learning module called the Combiner. Video and audio signals are aligned by taking the audio inputs that correspond to a specific video timeframe. We then process video and audio inputs spatio-temporally, extracting information particularly relevant to &lt;em>;changes in the inputs&lt;/em>; (for videos we use &lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;sparse video tubes&lt;/a>;, and for audio we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;>;spectrogram&lt;/a>; representation, both of which are processed by a &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer)&lt;/a>;. We concatenate and input these features to the Combiner, which is designed to learn a new feature representation capturing both these inputs. To address the challenge of the large volume of data in video and audio signals, another goal of the Combiner is to reduce the dimensionality of the joint video/audio inputs, which is done by selecting a smaller number of output features to be produced. The Combiner can be implemented simply as a causal Transformer, which processes the inputs in the direction of time, ie, using only inputs of the prior steps or the current one. Alternatively, the Combiner can have a learnable memory, described below. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Combiner styles&lt;/h2>; &lt;p>; A simple version of the Combiner adapts a Transformer architecture. More specifically, all audio and video features from the current chunk (and optionally prior chunks) are input to a Transformer and projected to a lower dimensionality, ie, a smaller number of features are selected as the output “combined” features. While Transformers are not typically used in this context, we find it effective for reducing the dimensionality of the input features, by selecting the last &lt;em>;m&lt;/em>; outputs of the Transformer, if &lt;em>;m&lt;/em>; is the desired output dimension (shown below). Alternatively, the Combiner can have a memory component. For example, we use the &lt;a href=&quot;https://arxiv.org/abs/2211.09119&quot;>;Token Turing Machine&lt;/a>; (TTM), which supports a differentiable memory unit, accumulating and compressing features from all previous timesteps 。 Using a fixed memory allows the model to work with a more compact set of features at every step, rather than process all the features from previous steps, which reduces computation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s1798/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;619&quot; data-original-width=&quot;1798&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a simple Transformer-based Combiner (&lt;b>;left&lt;/b>;) and a Memory Combiner (&lt;b>;right&lt;/b>;), based on the Token Turing Machine (TTM), which uses memory to compress previous history of features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our approach on several benchmarks, &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1906.02467&quot;>;ActivityNet-QA&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>;, for the video QA task, where a text-based question about a video is issued and the model needs to answer. This evaluates the ability of the model to understand both the text-based question and video content, and to form an answer, focusing on only relevant information. Of these benchmarks, the latter two target long video inputs and feature more complex questions. &lt;/p>; &lt;p>; We also evaluate our approach in the more challenging open-ended text generation setting, wherein the model generates the answers in an unconstrained fashion as free form text, requiring an exact match to the ground truth answer. While this stricter evaluation counts synonyms as incorrect, it may better reflect a model&#39;s ability to generalize. &lt;/p>; &lt;p>; Our results indicate improved performance over state-of-the-art approaches for most benchmarks, including all with open-ended generation evaluation — notable considering our model is only 3B parameters, considerably smaller than prior approaches, eg, Flamingo 80B. We used only video and text inputs to be comparable to other work. Importantly, our model can process 512 frames without needing to increase the model parameters, which is crucial for handling longer videos. Finally with the TTM Combiner, we see both better or comparable performance while reducing compute by 18%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s1200/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>; (video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJlAND23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s1200/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;860&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJlAND23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>; benchmark, which features long videos for the video QA task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results on audio-video benchmarks&lt;/h2>; &lt;p>; Results on the popular audio-video datasets &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG-Sound&lt;/a>; and &lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; are shown below. Since these benchmarks are classification-only, we treat them as an open-ended text generative setting where our model produces the text of the desired class; eg, for the class ID corresponding to the “playing drums” activity, we expect the model to generate the text “playing drums”. In some cases our approach outperforms the prior state of the art by large margins, even though our model outputs the results in the generative open-ended setting. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s1200/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG-Sound&lt;/a>; (audio-video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s1200/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; (audio-video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Benefits of autoregressive modeling&lt;/h2>; &lt;p>; We conduct an ablation study comparing our approach to a set of baselines that use the same input information but with standard methods (ie, without autoregression and the Combiner). We also compare the effects of pre-training. Because standard methods are ill-suited for processing longer video, this experiment is conducted for 32 frames and four chunks only, across all settings for fair comparison. We see that Mirasol3B&#39;s improvements are still valid for relatively short videos. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s1200/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Ablation experiments comparing the main components of our model. Using the Combiner, the autoregressive modeling, and pre-training all improve performance.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion &lt;/h2>; &lt;p>; We present a multimodal autoregressive model that addresses the challenges associated with the heterogeneity of multimodal data by coordinating the learning between time-aligned and time-unaligned modalities. Time-aligned modalities are further processed autoregressively in time with a Combiner, controlling the sequence length and producing powerful representations. We demonstrate that a relatively small model can successfully represent long video and effectively combine with other modalities. We outperform the state-of-the-art approaches (including some much bigger models) on video- and audio-video question answering. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research is co-authored by AJ Piergiovanni, Isaac Noble, Dahun Kim, Michael Ryoo, Victor Gomes, and Anelia Angelova. We thank Claire Cui, Tania Bedrax-Weiss, Abhijit Ogale, Yunhsuan Sung, Ching-Chung Chang, Marvin Ritter, Kristina Toutanova, Ming-Wei Chang, Ashish Thapliyal, Xiyang Luo, Weicheng Kuo, Aren Jansen, Bryan Seybold, Ibrahim Alabdulmohsin, Jialin Wu, Luke Friedman, Trevor Walker, Keerthana Gopalakrishnan, Jason Baldridge, Radu Soricut, Mojtaba Seyedhosseini, Alexander D&#39;Amour, Oliver Wang, Paul Natsev, Tom Duerig, Younghui Wu, Slav Petrov, Zoubin Ghahramani for their help and support. We also thank Tom Small for preparing the animation. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1338549955376716163/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html&quot; rel=&quot;alternate&quot; title=&quot;Scaling multimodal understanding to long videos&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s72-c/mirasol.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5546775652504697591&lt;/id>;&lt;published>;2023-11-10T10:05:00.000-08:00&lt;/published>;&lt;updated>;2023-11-10T10:05:13.301-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling large-scale health studies for the research community&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Chintan Ghate, Software Engineer, and Diana Mincu, Research Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s1100/MSSignals-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; As consumer technologies like &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/google-cloud-fitbit-haga-collaborate-on-pilot-heart-study&quot;>;fitness trackers&lt;/a>; and &lt;a href=&quot;https://blog.google/products/pixel/health-ai-better-sleep/&quot;>;mobile phones&lt;/a>; become more widely used for health-related data collection, so does the opportunity to leverage these data pathways to study and advance our understanding of medical conditions. We have &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research.html&quot;>;previously&lt;/a>; touched upon how our work explores the use of this technology within the context of chronic diseases, in particular multiple sclerosis (MS). This effort leverages the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies platform&lt;/a>;, an open-source platform used to create clinical study apps, that makes it easier for anyone to run their own studies and collect good quality healthcare data, in a trusted and safe way. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we describe the setup that we developed by expanding the FDA MyStudies platform and demonstrate how it can be used to set up a digital health study. We also present our exploratory research study created through this platform, called MS Signals, which consists of a symptom tracking app for MS patients. The goal for this app is twofold: 1) to ensure that the enhancements to the FDA MyStudies platform made for a more streamlined study creation experience; and 2) to understand how new data collection mechanisms can be used to revolutionize patients&#39; chronic disease management and tracking. We have &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open sourced&lt;/a>; our extension to the FDA MyStudies platform under the &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;>;Apache 2.0 license&lt;/a>; to provide a resource for the community to build their own studies. &lt;/p>; &lt;br />; &lt;h2>;Extending the FDA MyStudies platform&lt;/h2>; &lt;p>; The original FDA MyStudies platform allowed people to configure their own study apps, manage participants, and create separate iOS and Android apps. To simplify the study creation process and ensure increased study engagement, we made a number of accessibility changes. Some of the main improvements include: cross-platform (iOS and Android) app generation through the use of &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>;, an open source framework by Google for building multi-platform applications from a single codebase; a simplified setup, so that users can prototype their study quickly (under a day in most cases); and, most importantly, an emphasis on accessibility so that diverse patient&#39;s voices are heard. The accessibility enhancements include changes to the underlying features of the platform and to the particular study design of the MS Signals study app. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Multi-platform support with rapid prototyping&lt;/h3>; &lt;p>; We decided on the use of Flutter as it would be a single point that would generate both iOS and Android apps in one go, reducing the work required to support multiple platforms. Flutter also provides &lt;a href=&quot;https://docs.flutter.dev/tools/hot-reload&quot;>;hot-reloading&lt;/a>;, which allows developers to build &amp;amp; preview features quickly. The design-system in the app takes advantage of this feature to provide a central point from which the branding &amp;amp; theme of the app can be changed to match the tone of a new study and previewed instantly. The demo environment in the app also utilizes this feature to allow developers to mock and preview questionnaires locally on their machines. In our experience this has been a huge time-saver in A/B testing the UX and the format and wording of questions live with clinicians. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;System accessibility enhancements&lt;/h3>; &lt;p>; To improve the accessibility of the platform for more users, we made several usability enhancements: &lt;/p>; &lt;ol>; &lt;li>;Light &amp;amp; dark theme support&lt;/li>; &lt;li>;Bold text &amp;amp; variable font-sizes&lt;/li>; &lt;li>;High-contrast mode&lt;/li>; &lt;li>;Improving user awareness of accessibility settings&lt;/li>; &lt;/ol>; &lt;p>; Extended exposure to bright light themes can strain the eyes, so supporting dark theme features was necessary to make it easier to use the study app frequently. Some small or light text-elements are illegible to users with vision impairments, so we added 1) bold-text and support for larger font-sizes and 2) high-contrast color-schemes. To ensure that accessibility settings are easy to find, we placed an introductory one-time screen that was presented during the app&#39;s first launch, which would directly take users to their system accessibility settings. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study accessibility enhancements&lt;/h3>; &lt;p>; To make the study itself easier to interact with and reduce cognitive overload, we made the following changes: &lt;/p>; &lt;ol>; &lt;li>;Clarified the onboarding process&lt;/li>; &lt;li>;Improved design for questionnaires&lt;/li>; &lt;/ol>; &lt;p>; First, we clarified the on-boarding process by presenting users with a list of required steps when they first open the app in order to reduce confusion and participant drop-off. &lt;/p>; &lt;p>; The original questionnaire design in the app presented each question in a card format, which utilizes part of the screen for shadows and depth effects of the card. In many situations, this is a pleasant aesthetic, but in apps where accessibility is priority, these visual elements restrict the space available on the screen. Thus, when more accessible, larger font-sizes are used there are more frequent word breaks, which reduces readability. We fixed this simply by removing the card design elements and instead using the entire screen, allowing for better visuals with larger font-sizes. &lt;/p>; &lt;br />; &lt;h2>;The MS Signals prototype study&lt;/h2>; &lt;p>; To test the usability of these changes, we used our redesigned platform to create a prototype study app called MS Signals, which uses surveys to gather information about a participant&#39;s MS-related symptoms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/s1760/MSSignals.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1728&quot; data-original-width=&quot;1760&quot; height=&quot;628&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/w640-h628/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1971&quot; data-original-width=&quot;1999&quot; height=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/w640-h632/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MS Studies app design&lt;/h3>; &lt;p>; As a first step, before entering any study information, participants are asked to complete an eligibility and study comprehension questionnaire to ensure that they have read through the potentially lengthy terms of study participation. This might include, for example, questions like &quot;In what country is the study available?&quot; or “Can you withdraw from the study?&quot; A section like this is common in most health studies, and it tends to be the first drop-off point for participants. &lt;/p>; &lt;p>; To minimize study drop-off at this early stage, we kept the eligibility test brief and reflected correct answers for the comprehension test back to the participants. This helps minimize the number of times a user may need to go through the initial eligibility questionnaire and ensures that the important aspects of the study protocol are made clear to them. &lt;/p>; &lt;p>; After successful enrollment, participants are taken to the main app view, which consists of three pages: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Activities:&lt;/strong>; &lt;br />; This page lists the questionnaires available to the participant and is where the majority of their time is spent. The questionnaires vary in frequency — some are one-time surveys created to gather medical history, while others are repeated daily, weekly or monthly, depending on the symptom or area they are exploring. For the one-time survey we provide a counter above each question to signal to users how far they have come and how many questions are left, similar to the questionnaire during the eligibility and comprehension step. &lt;/li>; &lt;li>;&lt;strong>;Dashboard:&lt;/strong>; &lt;br />; To ensure that participants get something back in return for the information they enter during a study, the Dashboard area presents a summary of their responses in graph or pie chart form. Participants could potentially show this data to their care provider as a summary of their condition over the last 6 months, an improvement over the traditional pen and paper methods that many employ today. &lt;/li>; &lt;li>;&lt;strong>;Resources:&lt;/strong>; &lt;br />; A set of useful links, help articles and common questions related to MS. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Questionnaire design&lt;/h3>; &lt;p>; Since needing to frequently input data can lead to cognitive overload, participant drop off, and bad data quality, we reduced the burden in two ways: &lt;/p>; &lt;ol>; &lt;li>;We break down large questionnaires into smaller ones, resulting in 6 daily surveys, containing 3–5 questions each, where each question is multiple choice and related to a single symptom. This way we cover a total of 20 major symptoms, and present them in a similar way to how a clinician would ask these questions in an in-clinic setting.&lt;/li>; &lt;li>;We ensure previously entered information is readily available in the app, along with the time of the entry.&lt;/li>; &lt;/ol>; &lt;p>; In designing the survey content, we collaborated closely with experienced clinicians and researchers to finalize the wording and layout. While studies in this field typically use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale&quot;>;Likert scale&lt;/a>; to gather symptom information, we defined a more intuitive verbose scale to provide better experience for participants tracking their disease and the clinicians or researchers viewing the disease history. For example, in the case of vision issues, rather than asking participants to rate their symptoms on a scale from 1 to 10, we instead present a multiple choice question where we detail common vision problems that they may be experiencing. &lt;/p>; &lt;p>; This verbose scale helps patients track their symptoms more accurately by including context that helps them more clearly define their symptoms. This approach also allows researchers to answer questions that go beyond symptom correlation. For example, for vision issues, data collected using the verbose scale would reveal to researchers whether &lt;a href=&quot;https://www.webmd.com/eye-health/nystagmus&quot;>;nystagmus&lt;/a>; is more prominent in patients with MS compared to double vision. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/s1780/MSSignals-2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1754&quot; data-original-width=&quot;1780&quot; height=&quot;630&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/w640-h630/MSSignals-2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Side-by-side comparison with a Likert scale on the left, and a Verbose scale on the right.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1901&quot; data-original-width=&quot;1999&quot; height=&quot;608&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/w640-h608/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Side by side comparison with a Likert scale on the left, and a Verbose scale on the right.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;h2>;Focusing on accessibility&lt;/h2>; &lt;p>; Mobile-based studies can often present additional challenges for participants with chronic conditions: the text can be hard to read, the color contrast could make it difficult to see certain bits of information, or it may be challenging to scroll through pages. This may result in participant drop off, which, in turn, could yield a biased dataset if the people who are experiencing more advanced forms of a disease are unable to provide data. &lt;/p>; &lt;p>; In order to prevent such issues, we include the following accessibility features: &lt;/p>; &lt;ul>; &lt;li>;Throughout, we employ color blind accessible color schemes. This includes improving the contrast between crucial text and important additional information, which might otherwise be presented in a smaller font and a faded text color.&lt;/li>; &lt;li>;We reduced the amount of movement required to access crucial controls by placing all buttons close to the bottom of the page and ensuring that pop-ups are controllable from the bottom part of the screen.&lt;/li>; &lt;/ul>; &lt;p>; To test the accessibility of MS Signals, we collaborated with the &lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;National MS Society&lt;/a>; to recruit participants for a user experience study. For this, a call for participation was sent out by the Society to their members, and 9 respondents were asked to test out the various app flows. The majority indicated that they would like a better way than their current method to track their symptom data, that they considered MS Signals to be a unique and valuable tool that would enhance the accuracy of their symptom tracking, and that they would want to share the dashboard view with their healthcare providers. &lt;/p>; &lt;br />; &lt;h2>;Next steps&lt;/h2>; &lt;p>; We want to encourage everyone to make use of the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open source&lt;/a>; platform to start setting up and running their own studies. We are working on creating a set of standard study templates, which would incorporate what we learned from above, and we hope to release those soon. For any issues, comments or questions please check out our &lt;a href=&quot;https://goo.gle/ms-signals&quot;>;resource page&lt;/a>;. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5546775652504697591/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling large-scale health studies for the research community&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s72-c/MSSignals-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7987071997778787277&lt;/id>;&lt;published>;2023-11-09T14:23:00.000-08:00&lt;/published>;&lt;updated>;2023-11-09T14:23:38.431-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Africa&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Context in AI Research (CAIR)&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Katherine Heller, Research Scientist, Google Research, on behalf of the CAIR Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s1100/CAIR-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Artificial intelligence (AI) and related machine learning (ML) technologies are increasingly influential in the world around us, making it imperative that we consider the potential impacts on society and individuals in all aspects of the technology that we create. To these ends, the Context in AI Research (CAIR) team develops novel AI methods in the context of the entire AI pipeline:&lt;strong>; &lt;/strong>;from data to end-user feedback. The pipeline for building an AI system typically starts with &lt;em>;data&lt;/em>; collection, followed by designing a &lt;em>;model&lt;/em>; to run on that data, &lt;em>;deployment&lt;/em>; of the model in the real world, and lastly, compiling and incorporation of &lt;em>;human feedback&lt;/em>;. Originating in the health space, and now expanded to additional areas, the work of the CAIR team impacts every aspect of this pipeline. While specializing in model building, we have a particular focus on building systems with responsibility in mind, including fairness, robustness, transparency, and inclusion. &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ Margin-Left：auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC -Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/s1050/CAIR.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;383&quot; data-original-width=&quot;1050 &quot; height=&quot;234&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC-Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/w640-h234/CAIR.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Data &lt;/h2>; &lt;p>; The CAIR team focuses on understanding the data on which ML systems are built. Improving the standards for the transparency of ML datasets is instrumental in our work. First, we employ documentation frameworks to elucidate dataset and model characteristics as guidance in the development of data and model documentation techniques — &lt;a href=&quot;https://arxiv.org/abs/1803.09010&quot;>;Datasheets for Datasets&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1810.03993&quot;>;Model Cards for Model Reporting&lt;/a>;. &lt;/p>; &lt;p>; For example, health datasets are highly sensitive and yet can have high impact. For this reason, we developed &lt;a href=&quot;https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533239&quot;>;Healthsheets&lt;/a>;, a health-contextualized adaptation of a Datasheet. Our motivation for developing a health-specific sheet lies in the limitations of existing regulatory frameworks for AI and health. &lt;a href=&quot;https://www.nejm.org/doi/10.1056/NEJMp1816373&quot;>;Recent research&lt;/a>; suggests that data privacy regulation and standards (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act&quot;>;HIPAA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/General_Data_Protection_Regulation&quot;>;GDPR&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act&quot;>;California Consumer Privacy Act&lt;/a>;) do not ensure ethical collection, documentation, and use of data. Healthsheets aim to fill this gap in ethical dataset analysis. The development of Healthsheets was done in collaboration with many stakeholders in relevant job roles, including clinical, legal and regulatory, bioethics, privacy, and product. &lt;/p>; &lt;p>; Further, we studied how Datasheets and Healthsheets could serve as diagnostic tools that surface the limitations and strengths of datasets. Our aim was to start a conversation in the community and tailor Healthsheets to dynamic healthcare scenarios over time. &lt;/p>; &lt;p>; To facilitate this effort, we joined the &lt;a href=&quot;http://www.datadiversity.org/&quot;>;STANDING Together&lt;/a>; initiative, a consortium that aims to develop &lt;a href=&quot;https://www.nature.com/articles/s41591-022-01987-w&quot;>;international, consensus-based standards for documentation of diversity and representation&lt;/a>; within health datasets and to provide guidance on how to mitigate risk of bias translating to harm and health inequalities. Being part of this international, interdisciplinary partnership that spans academic, clinical, regulatory, policy, industry, patient, and charitable organizations worldwide enables us to engage in the conversation about responsibility in AI for healthcare internationally. Over 250 stakeholders from across 32 countries have contributed to refining the standards&lt;strong>;.&lt;/strong>; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s1050/Healthsheets.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;731&quot; data-original-width=&quot;1050&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s16000/Healthsheets.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Healthsheets and STANDING Together: towards health data documentation and standards.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Model&lt;/h2>; &lt;p>; When ML systems are deployed in the real world, they may fail to behave in expected ways, making poor predictions in new contexts. Such failures can occur for a myriad of reasons and can carry negative consequences, especially within the context of healthcare. Our work aims to identify situations where unexpected model behavior may be discovered, before it becomes a substantial problem, and to mitigate the unexpected and undesired consequences. &lt;/p>; &lt;p>; Much of the CAIR team&#39;s modeling work focuses on identifying and mitigating when models are &lt;a href=&quot;https://blog.research.google/2021/10/how-underspecification-presents.html&quot;>;underspecified&lt;/a>;. We show that models that perform well on held-out data drawn from a training domain are not equally robust or fair under distribution shift because the models vary in the extent to which they rely on spurious correlations. This poses a risk to users and practitioners because it can be difficult to anticipate model instability using standard model evaluation practices. &lt;a href=&quot;https://www.jmlr.org/papers/v23/20-1335.html&quot;>;We have demonstrated&lt;/a>; that this concern arises in several domains, including computer vision, natural language processing, medical imaging, and prediction from electronic health records. &lt;/p>; &lt;p>; We have also shown how to use knowledge of causal mechanisms to diagnose and mitigate fairness and robustness issues in new contexts. Knowledge of causal structure allows practitioners to anticipate &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html&quot;>;the generalizability of fairness properties under distribution shift in real-world medical settings&lt;/a>;. Further, investigating the capability for specific causal pathways, or “shortcuts”, to introduce bias in ML systems, we demonstrate how to identify cases where &lt;a href=&quot;https://www.nature.com/articles/s41467-023-39902-7&quot;>;shortcut learning&lt;/a>; leads to predictions in ML systems that are unintentionally dependent on sensitive attributes (eg, age, sex, race). We have shown how to use causal &lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_acyclic_graph&quot;>;directed acyclic graphs&lt;/a>; to &lt;a href=&quot;https://proceedings.mlr.press/v206/alabdulmohsin23a&quot;>;adapt ML systems to changing environments&lt;/a>; under complex forms of distribution shift. Our team is currently investigating how a causal interpretation of different forms of bias, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Selection_bias&quot;>;selection bias&lt;/a>;, label bias, and measurement error, &lt;a href=&quot;https://nips.cc/virtual/2022/58452&quot;>;motivates the design of techniques to mitigate bias during model development and evaluation&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s727/image6.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;442&quot; data-original-width=&quot;727&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s1078/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;382&quot; data-original-width=&quot;1078&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Shortcut Learning: For some models, age may act as a shortcut in classification when using medical images.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The CAIR team focuses on developing methodology to build more inclusive models broadly. For example, we also have &lt;a href=&quot;https://arxiv.org/abs/2302.03874&quot;>;work on the design of participatory systems&lt;/a>;, which allows individuals to choose whether to disclose sensitive attributes, such as race, when an ML system makes predictions. We hope that our methodological research positively impacts the societal understanding of inclusivity in AI method development. &lt;/p>; &lt;br />; &lt;h2>;Deployment &lt;/h2>; &lt;p>; The CAIR team aims to build technology that improves the lives of all people through the use of mobile device technology. We aim to reduce suffering from health conditions, address systemic inequality, and enable transparent device-based data collection. As consumer technology, such as fitness trackers and mobile phones, become central in data collection for health, we explored the use of these technologies within the context of chronic disease, in particular, for &lt;a href=&quot;https://en.wikipedia.org/wiki/Multiple_sclerosis&quot;>;multiple sclerosis&lt;/a>; (MS). We developed new data collection mechanisms and predictions that we hope will eventually revolutionize patient&#39;s chronic disease management, clinical trials, medical reversals and drug development. &lt;/p>; &lt;p>; First, we extended the open-source &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies platform&lt;/a>;, which is used to create clinical study apps, to make it easier for anyone to run their own studies and collect good quality data, in a trusted and safe way. Our improvements include zero-config setups, so that researchers can prototype their study in a day, cross-platform app generation through the use of &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>; and, most importantly, an emphasis on accessibility so that all patient&#39;s voices are heard. We are excited to announce this work has now been &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open sourced&lt;/a>; as an extension to the original FDA-Mystudies platform. You can start setting up your own studies today! &lt;/p>; &lt;p>; To test this platform, we built a prototype app, which we call MS Signals, that uses surveys to interface with patients in a novel consumer setting. We collaborated with the &lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;National MS Society&lt;/a>; to recruit participants for a user experience study for the app, with the goal of reducing dropout rates and improving the platform further. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/s915/MSSignals.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;915&quot; height=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/w640-h632/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&amp;nbsp;&lt;strong>;Left:&lt;/strong>;&amp;nbsp;Study welcome screen.&amp;nbsp;&lt;strong>;Right:&lt;/strong>;&amp;nbsp;Questionnaire.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Once data is collected, researchers could potentially use it to drive the frontier of ML research in MS. In a separate study, we established a research collaboration with the &lt;a href=&quot;https://neurology.duke.edu/&quot;>;Duke Department of Neurology&lt;/a>; and demonstrated that ML models can &lt;a href=&quot;https://www.researchsquare.com/article/rs-2547289/v1&quot;>;accurately predict the incidence of high-severity symptoms&lt;/a>; within three months using continuously collected data from mobile apps. Results suggest that the trained models can be used by clinicians to evaluate the symptom trajectory of MS participants, which may inform decision making for administering interventions. &lt;/p>; &lt;p>; The CAIR team has been involved in the deployment of many other systems, for both internal and external use. For example, we have also partnered with &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; to &lt;a href=&quot;https://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html&quot;>;build a book recommendation system&lt;/a>; for children with learning disabilities, such as dyslexia. We hope that our work positively impacts future product development. &lt;/p>; &lt;br />; &lt;h2>;Human feedback &lt;/h2>; &lt;p>; As ML models become ubiquitous throughout the developed world, it can be far too easy to leave voices in less developed countries behind. A priority of the CAIR team is to bridge this gap, develop deep relationships with communities, and work together to address ML-related concerns through community-driven approaches. &lt;/p>; &lt;p>; One of the ways we are doing this is through working with grassroots organizations for ML, such as &lt;a href=&quot;https://www.sisonkebiotik.africa/home&quot;>;Sisonkebiotik&lt;/a>;, an open and inclusive community of researchers, practitioners and enthusiasts at the intersection of ML and healthcare working together to build capacity and drive forward research initiatives in Africa. We worked in collaboration with the Sisonkebiotik community to detail limitations of historical top-down approaches for global health, and suggested complementary health-based methods, specifically those of grassroots participatory communities (GPCs). We jointly created a &lt;a href=&quot;https://openreview.net/forum?id=jHY_G91R880&quot;>;framework for ML and global health&lt;/a>;, laying out a practical roadmap towards setting up, growing and maintaining GPCs, based on common values across various GPCs such as &lt;a href=&quot;https://www.masakhane.io/&quot;>;Masakhane&lt;/a>;, Sisonkebiotik and &lt;a href=&quot;https://ro-ya-cv4africa.github.io/homepage/&quot;>;Ro&#39;ya&lt;/a>;. &lt;/p>; &lt;p>; We are engaging with open initiatives to better understand the role, perceptions and use cases of AI for health in non-western countries through human feedback, with an initial focus in Africa. Together with &lt;a href=&quot;https://ghananlp.org/&quot;>;Ghana NLP&lt;/a>;, we have worked to detail the need to &lt;a href=&quot;https://arxiv.org/abs/2304.02190&quot;>;better understand algorithmic fairness and bias in health in non-western contexts&lt;/a>;. We recently launched a study to expand on this work using human feedback. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy/s927/MLPipelineBiases.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;927&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy/s16000/MLPipelineBiases.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Biases along the ML pipeline and their associations with African-contextualized axes of disparities.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The CAIR team is committed to creating opportunities to hear more perspectives in AI development. We partnered with Sisonkebiotik to co-organize the &lt;a href=&quot;https://www.sisonkebiotik.africa/events/workshops/dl-indaba-2023&quot;>;Data Science for Health Workshop&lt;/a>; at &lt;a href=&quot;https://deeplearningindaba.com/2023/&quot;>;Deep Learning Indaba 2023&lt;/a>; in Ghana. Everyone&#39;s voice is crucial to developing a better future using AI technology. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Negar Rostamzadeh, Stephen Pfohl, Subhrajit Roy, Diana Mincu, Chintan Ghate, Mercy Asiedu, Emily Salkey, Alexander D&#39;Amour, Jessica Schrouff, Chirag Nagpal, Eltayeb Ahmed, Lev Proleev, Natalie Harris, Mohammad Havaei, Ben Hutchinson, Andrew Smart, Awa Dieng, Mahima Pushkarna, Sanmi Koyejo, Kerrie Kauer, Do Hee Park, Lee Hartsell, Jennifer Graves, Berk Ustun, Hailey Joren, Timnit Gebru and Margaret Mitchell for their contributions and influence, as well as our many friends and collaborators at Learning Ally, National MS Society, Duke University Hospital, STANDING Together, Sisonkebiotik, and Masakhane.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7987071997778787277/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Context in AI Research (CAIR)&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s72-c/CAIR-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1303378857635363504&lt;/id>;&lt;published>;2023-11-09T11:20:00.002-08:00&lt;/published>;&lt;updated>;2023-11-09T11:59:02.393-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Overcoming leakage on error-corrected quantum processors&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kevin Miao and Matt McEwen, Research Scientists, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidYny_9v0IyOBy7QbYVgwS5FAHzgHhVUt5FQvjXhi6WWJHyIIal3awBfaXNu9l3g47HElKNkFNf6XfpU7a_9ExGLlKFgOWvUOaT0PLhEoCfofJrazqT2IL1fBMtZMwfGJSnrpBrQZwQkYJOV6iMOmhWBWIH5OCPcsEPkgw-LuiPDBoLYToGGIN3Hjfpmwr/s320/Quantum%20leakage.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Qubit&quot;>;qubits&lt;/a>; that make up &lt;a href=&quot;https://quantumai.google/hardware&quot;>;Google quantum devices&lt;/a>; are delicate and noisy, so it&#39;s necessary to incorporate error correction procedures that identify and account for qubit errors on the way to building a useful quantum computer. Two of the most prevalent error mechanisms are &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_error_correction#Bit_flip_code&quot;>;bit-flip errors&lt;/a>; (where the energy state of the qubit changes) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_error_correction#Sign_flip_code&quot;>;phase-flip errors&lt;/a>; (where the phase of the encoded quantum information changes). &lt;a href=&quot;https://blog.research.google/2021/08/demonstrating-fundamentals-of-quantum.html&quot;>;Quantum error correction&lt;/a>; (QEC) promises to address and mitigate these two prominent errors. However, there is an assortment of other error mechanisms that challenges the effectiveness of QEC. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While we want qubits to behave as ideal &lt;a href=&quot;https://en.wikipedia.org/wiki/Two-state_quantum_system&quot;>;two-level systems&lt;/a>; with no loss mechanisms, this is not the case in reality. We use the lowest two energy levels of our qubit (which form the &lt;em>;computational basis&lt;/em>;) to carry out computations. These two levels correspond to the absence (computational ground state) or presence (computational excited state) of an excitation in the qubit, and are labeled |0⟩ (“&lt;a href=&quot;https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation&quot;>;ket&lt;/a>; zero”) and |1⟩ (“ket one”), respectively. However, our qubits also host many higher levels called &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/1509.05470&quot;>;leakage states&lt;/a>;&lt;/em>;, which can become occupied. Following the convention of labeling the level by indicating how many excitations are in the qubit, we specify them as |2⟩, |3⟩, |4⟩, and so on. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;Overcoming leakage in quantum error correction&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/nphys/&quot;>;Nature Physics&lt;/a>;&lt;/em>;, we identify when and how our qubits leak energy to higher states, and show that the leaked states can corrupt nearby qubits through our two-qubit gates. We then identify and implement a strategy that can remove leakage and convert it to an error that QEC can efficiently fix. Finally, we show that these operations lead to notably improved performance and stability of the QEC process. This last result is particularly critical, since additional operations take time, usually leading to more errors. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Working with imperfect qubits&lt;/h2>; &lt;p>; Our quantum processors are built from superconducting qubits called &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Transmon&quot;>;transmons&lt;/a>;&lt;/em>;. Unlike an ideal qubit, which only has two computational levels — a computational ground state and a computational excited state — transmon qubits have many additional states with higher energy than the computational excited state. These higher leakage states are useful for particular operations that generate &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;>;entanglement&lt;/a>;, a necessary resource in quantum algorithms, and also keep transmons from becoming too non-linear and difficult to operate. However, the transmon can also be inadvertently excited into these leakage states through a variety of processes, including imperfections in the control pulses we apply to perform operations or from the small amount of stray heat leftover in our cryogenic refrigerator. These processes are collectively referred to as &lt;em>;leakage&lt;/em>;, which describes the transition of the qubit from computational states to leakage states. &lt;/p>; &lt;p>; Consider a particular two-qubit operation that is used extensively in our QEC experiments: the &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates:~:text=%5B1%5D-,Controlled%2DZ,-%2C%0Acontrolled%20sign&quot;>;CZ gate&lt;/a>;. This gate operates on two qubits, and when both qubits are in their |1⟩ level, an interaction causes the two individual excitations to briefly “bunch” together in one of the qubits to form |2⟩, while the other qubit becomes |0⟩, before returning to the original configuration where each qubit is in |1⟩. This bunching underlies the entangling power of the CZ gate. However, with a small probability, the gate can encounter an error and the excitations do not return to their original configuration, causing the operation to leave a qubit in |2⟩, a leakage state. When we execute hundreds or more of these CZ gates, this small leakage error probability accumulates. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwtMGSiPIWruSj-S6Nleklj7YF9Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/s559/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;559&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwtMGSiPIWruSj-S6Nleklj7YF9Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/w400-h400/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transmon qubits support many leakage states (|2⟩, |3⟩, |4⟩, …) beyond the computational basis (|0⟩ and |1⟩). While we typically only use the computational basis to represent quantum information, sometimes the qubit enters these leakage states, and disrupts the normal operation of our qubits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; A single leakage event is especially damaging to normal qubit operation because it induces many individual errors. When one qubit starts in a leaked state, the CZ gate no longer correctly entangles the qubits, preventing the algorithm from executing correctly. Not only that, but CZ gates applied to one qubit in leaked states can cause the other qubit to leak as well, spreading leakage through the device. Our work includes extensive characterization of how leakage is caused and how it interacts with the various operations we use in our quantum processor. &lt;/p>; &lt;p>; Once the qubit enters a leakage state, it can remain in that state for many operations before relaxing back to the computational states. This means that a single leakage event interferes with many operations on that qubit, creating operational errors that are bunched together in time (&lt;em>;time-correlated &lt;/em>;errors). The ability for leakage to spread between the different qubits in our device through the CZ gates means we also concurrently see bunches of errors on neighboring qubits (&lt;em>;space-correlated&lt;/em>; errors). The fact that leakage induces patterns of space- and time-correlated errors makes it especially hard to diagnose and correct from the perspective of QEC algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The effect of leakage in QEC&lt;/h2>; &lt;p>; We aim to mitigate qubit errors by implementing &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;surface code QEC&lt;/a>;, a set of operations applied to a collection of imperfect physical qubits to form a &lt;em>;logical qubit&lt;/em>;, which has properties much closer to an ideal qubit. In a nutshell, we use a set of qubits called &lt;em>;data qubits&lt;/em>; to hold the quantum information, while another set of &lt;em>;measure qubits&lt;/em>; check up on the data qubits, reporting on whether they have suffered any errors, without destroying the delicate quantum state of the data qubits. One of the key underlying assumptions of QEC is that errors occur independently for each operation, but leakage can persist over many operations and cause a correlated pattern of multiple errors. The performance of our QEC strategies is significantly limited when leakage causes this assumption to be violated. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2-15jJ6uX2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g-WzBDS3jM-tuGSnxOHFb8kM4shNqKGPqGtKzs8FFRCPs-hH8mjsvkKLrNza/s1588/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1588&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2-15jJ6uX2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g-WzBDS3jM-tuGSnxOHFb8kM4shNqKGPqGtKzs8FFRCPs-hH8mjsvkKLrNza/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Once leakage manifests in our surface code transmon grid, it persists for a long time relative to a single surface code QEC cycle. To make matters worse, leakage on one qubit can cause its neighbors to leak as well.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our &lt;a href=&quot;https://blog.research.google/2021/08/demonstrating-fundamentals-of-quantum.html#:~:text=the%20Sycamore%20device.-,Leaky%20Qubits,-The%20goal%20of&quot;>;previous work&lt;/a>; has shown that we can remove leakage from measure qubits using an operation called &lt;em>;multi-level reset&lt;/em>; (MLR). This is possible because once we perform a measurement on measure qubits, they no longer hold any important quantum information. At this point, we can interact the qubit with a very lossy frequency band, causing whichever state the qubit was in (including leakage states) to decay to the computational ground state |0⟩. If we picture a &lt;em>;Jenga&lt;/em>; tower representing the excitations in the qubit, we tumble the entire stack over. Removing just one brick, however, is much more challenging. Likewise, MLR doesn&#39;t work with data qubits because they &lt;em>;always&lt;/em>; hold important quantum information, so we need a new leakage removal approach that minimally disturbs the computational basis states. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Gently removing leakage&lt;/h2>; &lt;p>; We introduce a new quantum operation called&lt;em>; data qubit leakage removal &lt;/em>;(DQLR), which targets leakage states in a data qubit and converts them into computational states in the data qubit and a neighboring measure qubit. DQLR consists of a two-qubit gate (dubbed &lt;em>;Leakage iSWAP&lt;/em>; — an &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates:~:text=1%5D%5B6%5D-,Imaginary%20swap,-2&quot;>;iSWAP&lt;/a>; operation with leakage states) inspired by and similar to our CZ gate, followed by a rapid reset of the measure qubit to further remove errors. The Leakage iSWAP gate is very efficient and greatly benefits from our extensive characterization and calibration of CZ gates within the surface code experiment. &lt;/p>; &lt;p>; Recall that a CZ gate takes two single excitations on two different qubits and briefly brings them to one qubit, before returning them to their respective qubits. A Leakage iSWAP gate operates similarly, but almost in reverse, so that it takes a single qubit with two excitations (otherwise known as |2⟩) and splits them into |1⟩ on two qubits. The Leakage iSWAP gate (and for that matter, the CZ gate) is particularly effective because it does not operate on the qubits if there are fewer than two excitations present. We are precisely removing the |2⟩ &lt;em>;Jenga&lt;/em>; brick without toppling the entire tower. &lt;/p>; &lt;p>; By carefully measuring the population of leakage states on our transmon grid, we find that DQLR can reduce average leakage state populations over all qubits to about 0.1%, compared to nearly 1% without it. Importantly, we no longer observe a gradual rise in the amount of leakage on the data qubits, which was always present to some extent prior to using DQLR. &lt;/p>; &lt;p>; This outcome, however, is only half of the puzzle. As mentioned earlier, an operation such as MLR could be used to effectively remove leakage on the data qubits, but it would also completely erase the stored quantum state. We also need to demonstrate that DQLR is compatible with the preservation of a logical quantum state. &lt;/p>; &lt;p>; The second half of the puzzle comes from executing the QEC experiment with this operation interleaved at the end of each QEC cycle, and observing the logical performance. Here, we use a metric called &lt;em>;detection probability&lt;/em>; to gauge how well we are executing QEC. In the presence of leakage, time- and space-correlated errors will cause a gradual rise in detection probabilities as more and more qubits enter and stay in leakage states. This is most evident when we perform no reset at all, which rapidly leads to a transmon grid plagued by leakage, and it becomes inoperable for the purposes of QEC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s1532/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1532&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The prior state-of-the-art in our QEC experiments was to use MLR on the measure qubits to remove leakage. While this kept leakage population on the measure qubits (green circles) sufficiently low, data qubit leakage population (green squares) would grow and saturate to a few percent. With DQLR, leakage population on both the measure (blue circles) and data qubits (blue squares) remain acceptably low and stable.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; With MLR, the large reduction in leakage population on the measure qubits drastically decreases detection probabilities and mitigates a considerable degree of the gradual rise. This reduction in detection probability happens even though we spend more time dedicated to the MLR gate, when other errors can potentially occur. Put another way, the correlated errors that leakage causes on the grid can be much more damaging than the uncorrelated errors from the qubits waiting idle, and it is well worth it for us to trade the former for the latter. &lt;/p>; &lt;p>; When only using MLR, we observed a small but persistent residual rise in detection probabilities. We ascribed this residual increase in detection probability to leakage accumulating on the data qubits, and found that it disappeared when we implemented DQLR. And again, the observation that the detection probabilities end up lower compared to only using MLR indicates that our added operation has removed a damaging error mechanism while minimally introducing uncorrelated errors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s1205/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;1205&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Leakage manifests during surface code operation as increased errors (shown as error detection probabilities) over the number of cycles. With DQLR, we no longer see a notable rise in detection probability over more surface code cycles.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prospects for QEC scale-up&lt;/h2>; &lt;p>; Given these promising results, we are eager to implement DQLR in future QEC experiments, where we expect error mechanisms outside of leakage to be greatly improved, and sensitivity to leakage to be enhanced as we work with larger and larger transmon grids. In particular, our simulations indicate that scale-up of our surface code will almost certainly require a large reduction in leakage generation rates, or an active leakage removal technique over all qubits, such as DQLR. &lt;/p>; &lt;p>; Having laid the groundwork by understanding where leakage is generated, capturing the dynamics of leakage after it presents itself in a transmon grid, and showing that we have an effective mitigation strategy in DQLR, we believe that leakage and its associated errors no longer pose an existential threat to the prospects of executing a surface code QEC protocol on a large grid of transmon qubits. With one fewer challenge standing in the way of demonstrating working QEC, the pathway to a useful quantum computer has never been more promising. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work would not have been possible without the contributions of the entire Google Quantum AI Team.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1303378857635363504/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/overcoming-leakage-on-error-corrected.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1303378857635363504&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1303378857635363504&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/overcoming-leakage-on-error-corrected.html&quot; rel=&quot;alternate&quot; title=&quot;Overcoming leakage on error-corrected quantum processors&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidYny_9v0IyOBy7QbYVgwS5FAHzgHhVUt5FQvjXhi6WWJHyIIal3awBfaXNu9l3g47HElKNkFNf6XfpU7a_9ExGLlKFgOWvUOaT0PLhEoCfofJrazqT2IL1fBMtZMwfGJSnrpBrQZwQkYJOV6iMOmhWBWIH5OCPcsEPkgw-LuiPDBoLYToGGIN3Hjfpmwr/s72-c/Quantum%20leakage.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4661241136828203614&lt;/id>;&lt;published>;2023-11-07T12:34:00.003-08:00&lt;/published>;&lt;updated>;2023-11-07T12:34:21.947-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Alternating updates for efficient transformers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Xin Wang, Software Engineer, and Nishanth Dikkala, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk/s1600/altup.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Contemporary deep learning models have been remarkably successful in many domains, ranging from natural language to computer vision. &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer neural networks&lt;/a>; (transformers) are a popular deep learning architecture that today comprise the foundation for most tasks in natural language processing and also are starting to extend to applications in other domains, such as &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;computer vision&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.html&quot;>;robotics&lt;/a>;, and &lt;a href=&quot;https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/&quot;>;autonomous driving&lt;/a>;. Moreover, they form the backbone of all the current state-of-the-art &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;>;language models&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Increasing scale in Transformer networks has led to improved performance and the &lt;a href=&quot;https://arxiv.org/abs/2206.07682&quot;>;emergence of behavior&lt;/a>; not present in smaller networks. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. A natural question is whether we can reap the benefits of larger models without incurring the computational burden. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2301.13310&quot;>;Alternating Updates for Efficient Transformers&lt;/a>;”, accepted as a Spotlight at &lt;a href=&quot;https://neurips.cc/virtual/2023/poster/72994&quot;>;NeurIPS 2023&lt;/a>;, we introduce AltUp, a method to take advantage of increased token representation without increasing the computation cost. AltUp is easy to implement, widely applicable to any transformer architecture, and requires minimal &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter tuning&lt;/a>;. For instance, using a variant of AltUp on a 770M parameter T5-Large model, the addition of ~100 parameters yields a model with a significantly better quality. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; To understand how we can achieve this, we dig into how transformers工作。 First, they partition the input into a sequence of tokens. Each token is then mapped to an embedding vector (via the means of an embedding table) called the token embedding. We call the dimension of this vector the token representation dimension. The Transformer then operates on this sequence of token embeddings by applying a series of computation modules (called layers&lt;em>;)&lt;/em>; using its network parameters. The number of parameters in each transformer layer is a function of the layer&#39;s &lt;em>;width&lt;/em>;, which is determined by the token representation dimension. &lt;/p>; &lt;p>; To achieve benefits of scale without incurring the compute burden, prior works such as sparse mixture-of-experts (Sparse MoE) models (eg, &lt;a href=&quot;https://arxiv.org/abs/2101.03961&quot;>;Switch Transformer&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2022/11/mixture-of-experts-with-expert-choice.html?m=1&quot;>;Expert Choice&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2022/01/scaling-vision-with-sparse-mixture-of.html?m=1&quot;>;V-MoE&lt;/a>;) have predominantly focused on efficiently scaling up the network parameters (in the self-attention and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;>;feedforward layers&lt;/a>;) by conditionally activating a subset based on the input. This allows us to scale up network size without significantly increasing compute per input. However, there is a research gap on scaling up the token representation dimension itself by conditionally activating parts of the token representation vector. &lt;/p>; &lt;p>; Recent works (for example, &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;>;scaling laws&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2011.14522&quot;>;infinite-width networks)&lt;/a>; have empirically and theoretically established that a wider token representation helps in learning more complicated functions. This phenomenon is also evident in modern architectures of increasing capability. For instance, the representation dimension grows from 512 (small) to 768 (base) and 1024 (corresponding to models with 770M, 3B, and 11B parameters respectively) in &lt;a href=&quot;https://arxiv.org/abs/1910.10683 &quot;>;T5 models&lt;/a>;, and from 4096 (8B) to 8192 (64B) and 18432 (540B) in &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM models&lt;/a>; 。 A widened representation dimension also significantly improves performance for dual encoder retrieval models. However, naïvely widening the representation vector requires one to increase the model dimension accordingly, which quadratically&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;1&lt;/a>;&lt;/sup>; increases the amount of computation in the feedforward computation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Method&lt;/h2>; &lt;p>; AltUp works by partitioning a widened representation vector into equal sized blocks, processing only a single block at each layer, and using an efficient prediction-correction mechanism to infer the outputs of the other blocks (shown below on the right). This allows AltUp to simultaneously keep the model dimension, hence the computation cost, roughly constant and take advantage of using an increased token dimension. The increased token dimension allows the model to pack more information into each token&#39;s embedding. By keeping the width of each transformer layer constant, AltUp avoids incurring the quadratic increase in computation cost that would otherwise be present with a naïve expansion of the representation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s1367/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;823&quot; data-original-width=&quot;1367&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of widening the token representation without (&lt;b>;left&lt;/b>;) and with AltUp (&lt;b>;right&lt;/b>;). This widening causes a near-quadratic increase in computation in a vanilla transformer due to the increased layer width. In contrast, Alternating Updates keeps the layer width constant and efficiently computes the output by operating on a sub-block of the representation at each layer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More specifically, the input to each layer is two or more blocks, one of which is passed into the 1x width transformer layer (see figure below). We refer to this block as the “activated” block. This computation results in the exact output for the activated block. In parallel, we invoke a lightweight predictor that computes a weighted combination of all the input blocks. The predicted values, along with the computed value of the activated block, are passed on to a lightweight corrector that updates the predictions based on the observed values. This correction mechanism enables the inactivated blocks to be updated as a function of the activated one. Both the prediction and correction steps only involve a limited number of vector additions and multiplications and hence are much faster than a regular transformer layer. We note that this procedure can be generalized to an arbitrary number of blocks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s957/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;504&quot; data-original-width=&quot;957&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The predictor and corrector computations: The predictor mixes sub-blocks with trainable scalar coefficients; the corrector returns a weighted average of the predictor output and the transformer output. The predictor and corrector perform scalar-vector multiplications and incur negligible computation cost compared to the transformer. The predictor outputs a linear mixing of blocks with scalar mixing coefficients p&lt;sub>;i, j&lt;/sub>; , and the corrector combines predictor output and transformer output with weights g&lt;sub>;i&lt;/sub>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; At a higher level, AltUp is similar to sparse MoE in that it is a method to add capacity to a model in the form of conditionally accessed (external) parameters. In sparse MoE, the additional parameters take the form of feed forward network (FFN) experts and the conditionality is with respect to the input. In AltUp, the external parameters come from the widened embedding table and the conditionality takes the form of alternating block-wise activation of the representation vector, as in the figure above. Hence, AltUp has the same underpinning as sparse MoE models. &lt;/p>; &lt;p>; An advantage of AltUp over sparse MoE is that it does not necessitate sharding since the number of additional parameters introduced is a factor&lt;sup id=&quot;fnref2&quot;>;&lt;a href=&quot;#fn2&quot; rel=&quot;footnote&quot;>;2&lt;/a>;&lt;/sup>; of the embedding table size, which typically makes up a small fraction of the overall model size. Moreover, since AltUp focuses on conditionally activating parts of a wider token representation, it can be applied synergistically with orthogonal techniques like MoE to obtain complementary performance gains. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; AltUp was evaluated on T5 models on various benchmark language tasks. Models augmented with AltUp are uniformly faster than the extrapolated dense models at the same accuracy. For example, we observe that a T5 Large model augmented with AltUp leads to a 27%, 39%, 87%, and 29% speedup on &lt;a href=&quot;https://gluebenchmark.com/&quot;>;GLUE&lt;/a>;, &lt;a href=&quot;https://super.gluebenchmark.com/&quot;>;SuperGLUE&lt;/a>;, &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;>;SQuAD&lt;/a>;, and &lt;a href=&quot;https://nlp.cs.washington.edu/triviaqa/&quot;>;Trivia-QA&lt;/a>; benchmarks, respectively. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s1386/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;982&quot; data-original-width=&quot;1386&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Evaluations of AltUp on T5 models of various sizes and popular benchmarks. AltUp consistently leads to sizable speedups relative to baselines at the same accuracy. Latency is measured on &lt;a href=&quot;https://cloud.google.com/tpu/docs/system-architecture-tpu-vm&quot;>;TPUv3&lt;/a>; with 8 cores. Speedup is defined as the change in latency divided by the AltUp latency (B = T5 Base, L = T5 Large, XL = T5 XL models).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; AltUp&#39;s relative performance improves as we apply it to larger models — compare the relative speedup of T5 Base + AltUp to that of T5 Large + AltUp. This demonstrates the scalability of AltUp and its improved performance on even larger models. Overall, AltUp consistently leads to models with better predictive performance than the corresponding baseline models with the same speed on all evaluated model sizes and benchmarks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Extensions: Recycled AltUp&lt;/h2>; &lt;p>; The AltUp formulation adds an insignificant amount of per-layer computation, however, it does require using a wider embedding table. In certain scenarios where the vocabulary size (ie, the number of distinct tokens the tokenizer can produce) is very large, this may lead to a non-trivial amount of added computation for the initial embedding lookup and the final &lt;a href=&quot;https://www.google.com/url?q=https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1699300629405641&amp;amp;usg=AOvVaw3tnZ18-LvY0tPP_NClQ_P-&quot;>;linear + softmax operation&lt;/a>;. A very large vocabulary may also lead to an undesirable amount of added embedding parameters. To address this, Recycled-AltUp is an extension of AltUp that avoids these computational and parameter costs by keeping the embedding table&#39;s width the same. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s989/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;515&quot; data-original-width=&quot;989&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the Architecture for Recycled-AltUp with &lt;em>;K&lt;/em>; = 2.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In Recycled-AltUp, instead of widening the initial token embeddings, we replicate the embeddings &lt;em>;K&lt;/em>; times to form a wider token representation. Hence, Recycled-AltUp adds virtually no additional parameters relative to the baseline transformer, while benefiting from a wider token representation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRoANGe48-HncQDuNpxAVSUGf2GPp1qp6KpnvqMbQ5Mejlq4gWpq96vu9FxkDub8aO3RqYyDmTki2Xqkj9drOprwqrHDyoQvHI4oziYFguhDnkkYZO4GXdhAKgIlfffHJ5Po7mEVGCaAkOH7oobnt-8qKjWlerZp4EeoAjZg6IG_CEVO3hhsQvlPaV2GHU/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;643&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRoANGe48-HncQDuNpxAVSUGf2GPp1qp6KpnvqMbQ5Mejlq4gWpq96vu9FxkDub8aO3RqYyDmTki2Xqkj9drOprwqrHDyoQvHI4oziYFguhDnkkYZO4GXdhAKgIlfffHJ5Po7mEVGCaAkOH7oobnt-8qKjWlerZp4EeoAjZg6IG_CEVO3hhsQvlPaV2GHU/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Recycled-AltUp on T5-B/L/XL compared to baselines. Recycled-AltUp leads to strict improvements in pre-training performance without incurring any perceptible slowdown.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also evaluate the lightweight extension of AltUp, Recycled-AltUp, with &lt;em>;K&lt;/em>; = 2 on T5 base, large, and XL models and compare its pre-trained accuracy and speed to those of baselines. Since Recycled-AltUp does not require an expansion in the embedding table dimension, the models augmented with it have virtually the same number of trainable parameters as the baseline models. We again observe consistent improvements compared to the dense baselines. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Why does AltUp work?&lt;/h2>; &lt;p>; AltUp increases a model&#39;s capacity by adding and &lt;em>;efficiently&lt;/em>; leveraging auxiliary parameters to the embedding table, and maintaining the higher dimensional representation across the layers. We believe that a key ingredient in this computation lies in AltUp&#39;s prediction mechanism that performs an ensemble of the different blocks. This weighted combination enables continuous message passing to the entire vector despite activating only sub-blocks of it in each layer. Recycled-AltUp, on the other hand, does not add any additional parameters to the token embeddings. However, it still confers the benefit of simulating computation in a higher dimensional representation space since a higher dimensional representation vector is maintained when moving from one transformer layer to another. We conjecture that this aids the training by augmenting the flow of information through the network. An interesting research direction is to explore whether the benefits of Recycled-AltUp can be explained entirely by more favorable training dynamics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;We thank our collaborators Cenk Baykal, Dylan Cutler, and Rina Panigrahy at Google Research, and Nikhil Ghosh at University of California, Berkeley (work done during research internship at Google).&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;This is because the feedforward layers of a Transformer are typically scaled quadratically with the model dimension.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;br />; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn2&quot;>;&lt;b>;2&lt;/b>;&lt;/a>;&lt;/sup>;This factor depends on the user-specified expansion factor, but is typically 1, ie, we double the embedding table dimension.&amp;nbsp;&lt;a href=&quot;#fnref2&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4661241136828203614/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/alternating-updates-for-efficient.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4661241136828203614&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4661241136828203614&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/alternating-updates-for-efficient.html&quot; rel=&quot;alternate&quot; title=&quot;Alternating updates for efficient transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk/s72-c/altup.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8959024707515398632&lt;/id>;&lt;published>;2023-11-03T11:23:00.001-07:00&lt;/published>;&lt;updated>;2023-11-03T11:27:07.156-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Best of both worlds: Achieving scalability and quality in text clustering&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sara Ahmadian and Mehran Kazemi, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR2mD5afdT_Qg9Ex4Lj94FaxB9KHiq20iLoDlOY8S8Rk5XsV6n_4dU9CFbCvSeBSONGQYqy-yMGY6-KU_y_RGICOpz76GNdzv7ecxor_rVnF31lZOd3STQF4MIE4F_EadrSI1DXY67MXnsCswY3w3X8vX8KgU_rRPs6eTZndYAbVcEezgwaUsdZEAHD59Z/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;Clustering&lt;/a>; is a fundamental, ubiquitous problem in data mining and &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;>;unsupervised&lt;/a>; machine learning, where the goal is to group together similar items. The standard forms of clustering are metric clustering and graph clustering. In metric clustering, a given metric space defines distances between data points, which are grouped together based on their &lt;em>;separation&lt;/em>;. In graph clustering, a given graph connects similar data points through edges, and the clustering process groups data points together based on the &lt;em>;connections&lt;/em>; between them. Both clustering forms are particularly useful for large corpora where class labels can&#39;t be defined. Examples of such corpora are the ever-growing digital text collections of various internet platforms, with applications including organizing and searching documents, identifying patterns in text, and recommending relevant documents to users (see more examples in the following posts: &lt;a href=&quot;https://blog.research.google/2010/10/clustering-related-queries-based-on.html?m=1&quot;>;clustering related queries based on user intent&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2021/10/practical-differentially-private.html&quot;>;practical differentially private clustering&lt;/a>;). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The choice of text clustering method often presents a dilemma. One approach is to use embedding models, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/BERT_(language_model)&quot;>;BERT&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>;, to define a metric clustering problem. Another is to utilize &lt;a href=&quot;https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-cross-attention-3b396266d82e&quot;>;cross-attention&lt;/a>; (CA) models, such as &lt;a href=&quot;https://blog.research.google/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_pre-trained_transformer&quot;>;GPT&lt;/a>;, to define a graph clustering problem. CA models can provide highly accurate similarity scores, but constructing the input graph may require a prohibitive quadratic number of inference calls to the model. On the other hand, a metric space can efficiently be defined by distances of embeddings produced by embedding models. However, these similarity distances are typically of substantial lower-quality compared to the similarity signals of CA models, and hence the produced clustering can be of much lower-quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRHbZ9egdnFlc6ZCmLeODTdPZoOXcjjEGCLHP8Ve3aihMExIjyLiX4n0Zy9l4nmkx-3k6rxYN5696irWyubaYAqecpxFTmP1wzjBo0MKnZMnnTjQrdSxKIWGzsySs8XB-OVHyyxKJBWI0dlrYvb_S204S6aSbkPTx8_FQTJXk8f_WDIoGgqNJTACuXNewu/s835/figure%20top.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;641&quot; data-original-width=&quot;835&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRHbZ9egdnFlc6ZCmLeODTdPZoOXcjjEGCLHP8Ve3aihMExIjyLiX4n0Zy9l4nmkx-3k6rxYN5696irWyubaYAqecpxFTmP1wzjBo0MKnZMnnTjQrdSxKIWGzsySs8XB-OVHyyxKJBWI0dlrYvb_S204S6aSbkPTx8_FQTJXk8f_WDIoGgqNJTACuXNewu/s16000/figure%20top.png&quot; />;&lt;/a>;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoY4juklU42gca4fdtnUN4t5j8Z-4rEEhyphenhyphenDi4kRfkV1wfqBs_wyldX9sU2pwtMHdVZPZf2vWnDweEPzwiLseYZenC3afIuaDRf_7arpoF-xnMAwgGXs48a_3wNG2trhN0CTkHgMVa6ZQ7ToP7IE6fpIFmbTF7YYGPFgYU3cMtyyBYIfSWVLVvRXY6Dueiy/s835/figure%20bottom.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;641&quot; data-original-width=&quot;835&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoY4juklU42gca4fdtnUN4t5j8Z-4rEEhyphenhyphenDi4kRfkV1wfqBs_wyldX9sU2pwtMHdVZPZf2vWnDweEPzwiLseYZenC3afIuaDRf_7arpoF-xnMAwgGXs48a_3wNG2trhN0CTkHgMVa6ZQ7ToP7IE6fpIFmbTF7YYGPFgYU3cMtyyBYIfSWVLVvRXY6Dueiy/s16000/figure%20bottom.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the embedding-based and cross-attention–based similarity scoring functions and their scalability vs. quality dilemma.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>; Motivated by this, in “&lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals”, presented at ICLR 2023&lt;/a>;, we describe a novel clustering algorithm that effectively combines the scalability benefits from embedding models and the quality from CA models. This graph clustering algorithm has query access to both the CA model and the embedding model, however, we apply a budget on the number of queries made to the CA model. This algorithm uses the CA model to answer edge queries, and benefits from unlimited access to similarity scores from the embedding model. We describe how this proposed setting bridges algorithm design and practical considerations, and can be applied to other clustering problems with similar available scoring functions, such as clustering problems on images and media. We demonstrate how this algorithm yields high-quality clusters with almost a linear number of query calls to the CA model. We have also &lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks.zip&quot;>;open-sourced&lt;/a>; the data used in our experiments. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The clustering algorithm&lt;/h2>; &lt;p>; The KwikBucks algorithm is an extension of the well-known &lt;a href=&quot;https://cesa-bianchi.di.unimi.it/Algo2/Note/kwik.pdf&quot;>;KwikCluster algorithm (Pivot algorithm)&lt;/a>;. The high-level idea is to first select a set of documents (ie, centers) with no similarity edge between them, and then form clusters around these centers. To obtain the quality from CA models and the runtime efficiency from embedding models, we introduce the novel &lt;em>;combo similarity oracle&lt;/em>; mechanism. In this approach, we utilize the embedding model to guide the selection of queries to be sent to the CA model. When given a set of center documents and a target document, the combo similarity oracle mechanism outputs a center from the set that is similar to the target document, if present. The combo similarity oracle enables us to save on budget by limiting the number of query calls to the CA model when selecting centers and forming clusters. It does this by first ranking centers based on their embedding similarity to the target document, and then querying the CA model for the pair (ie, target document and ranked center), as shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s1601/process.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s16000/process.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A combo similarity oracle that for a set of documents and a target document, returns a similar document from the set, if present.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We then perform a post processing step to merge clusters if there is a strong connection between two of them, ie, when the number of connecting edges is higher than the number of missing edges between two clusters. Additionally, we apply the following steps for further computational savings on queries made to the CA model, and to improve performance at runtime: &lt;/p>; &lt;ol>; &lt;li>;We leverage &lt;a href=&quot;https://arxiv.org /pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; to form a set of centers from a set of randomly selected documents instead of selecting these centers from all the documents (in the illustration below, the center nodes are red ）。 &lt;/li>;&lt;li>;We apply the combo similarity oracle mechanism to perform the cluster assignment step in parallel for all non-center documents and leave documents with no similar center as singletons. In the illustration below, the assignments are depicted by blue arrows and initially two (non-center) nodes are left as singletons due to no assignment. &lt;/li>;&lt;li>;In the post-processing step, to ensure scalability, we use the embedding similarity scores to filter down the potential mergers (in the illustration below, the green dashed boundaries show these merged clusters). &lt;/li>; &lt;/ol>;&lt;div>;&lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1bJ65zsG-PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s1322/Kwikbux%20gif.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;704&quot; data-original-width=&quot;1322&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1bJ65zsG-PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s16000/Kwikbux%20gif.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of progress of the clustering algorithm on a given graph instance. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate the novel clustering algorithm on various datasets with different properties using different embedding-based and cross-attention–based models. We compare the clustering algorithm&#39;s performance with the two best performing baselines (see the &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;paper&lt;/a>; for more details): &lt;/p>; &lt;ul>; &lt;li>;The &lt;a href=&quot;https://arxiv.org/pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; algorithm for budgeted clustering with access to CA only. &lt;/li>;&lt;li>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_clustering&quot;>;Spectral clustering&lt;/a>; on the &lt;em>;&lt;a href=&quot;https://en.wikipedia .org/wiki/Nearest_neighbor_graph&quot;>;k-nearest neighbor graph&lt;/a>;&lt;/em>; (kNN) formed by querying the CA model for the &lt;em>;k&lt;/em>;-nearest neighbors of each vertex from embedding-based相似。 &lt;/li>; &lt;/ul>; &lt;p>; To evaluate the quality of clustering, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision and recall&lt;/a>;. Precision is used to calculate the percentage of similar pairs out of all co-clustered pairs and recall is the percentage of co-clustered similar pairs out of all similar pairs. To measure the quality of the obtained solutions from our experiments, we use the &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1-score&lt;/a>;, which is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Harmonic_mean&quot;>;harmonic mean&lt;/a>; of the precision and recall, where 1.0 is the highest possible value that indicates perfect precision and recall, and 0 is the lowest possible value that indicates if either precision or recall are zero. The table below reports the F1-score for Kwikbucks and various baselines in the case that we allow only a linear number of queries to the CA model. We show that Kwikbucks offers a substantial boost in performance with a 45% relative improvement compared to the best baseline when averaging across all datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s1574/results.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;582&quot; data-original-width=&quot;1574&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s16000/results.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparing the clustering algorithm to two baseline algorithms using various public datasets: (1) The &lt;a href=&quot;https://arxiv.org/pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; algorithm for budgeted clustering with access to CA only, and (2) &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_clustering&quot;>;spectral clustering&lt;/a>; on the &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Nearest_neighbor_graph&quot;>;k-nearest neighbor (kNN) graph&lt;/a>;&lt;/em>; formed by querying the CA model for the &lt;em>;k&lt;/em>;-nearest neighbors of each vertex from embedding-based similarity. Pre-processed datasets can be downloaded &lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks.zip&quot;>;here&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The figure below compares the clustering algorithm&#39;s performance with baselines using different query budgets. We observe that KwikBucks consistently outperforms other baselines at various budgets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3CboOkZ4lbp9dmH-5EkPGe1zo6JPlOQbXT1n67ke4qN0kXeZXMQKb4SUM-E6TXuyWF9UF7vvKTtl2lecg-Z5tV0mA6Hobh9NKbmy__dqGRIYcTEIzdfGjCCyih2eZW4xF33n7Y1pJGpwyDvQ75rwQsp0kZICGSPoyR8ahiBQwcZvT7ZLLeAA1BQkJtLwU/s1236/stackoverflow.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1236&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3CboOkZ4lbp9dmH-5EkPGe1zo6JPlOQbXT1n67ke4qN0kXeZXMQKb4SUM-E6TXuyWF9UF7vvKTtl2lecg-Z5tV0mA6Hobh9NKbmy__dqGRIYcTEIzdfGjCCyih2eZW4xF33n7Y1pJGpwyDvQ75rwQsp0kZICGSPoyR8ahiBQwcZvT7ZLLeAA1BQkJtLwU/s16000/stackoverflow.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A comparison of KwikBucks with top-2 baselines when allowed different budgets for querying the cross-attention model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Text clustering often presents a dilemma in the choice of similarity function: embedding models are scalable but lack quality, while cross-attention models offer quality but substantially hurt scalability. We present a clustering algorithm that offers the best of both worlds: the scalability of embedding models and the quality of cross-attention models. KwikBucks can also be applied to other clustering problems with multiple similarity oracles of varying accuracy levels. This is validated with an exhaustive set of experiments on various datasets with diverse properties. See the &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This project was initiated during Sandeep Silwal&#39;s summer internship at Google in 2022. We would like to express our gratitude to our co-authors, Andrew McCallum, Andrew Nystrom, Deepak Ramachandran, and Sandeep Silwal, for their valuable contributions to this work. We also thank Ravi Kumar and John Guilyard for assistance with this blog post.&lt;/em>; &lt;/p>;&lt;/div>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8959024707515398632/ comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/best-of-both -worlds-achieving.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/8959024707515398632&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8959024707515398632&quot; rel=&quot; self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/best-of-both-worlds-achieving.html&quot; rel=&quot;alternate&quot; title =&quot;Best of both worlds: Achieving scalability and quality in text clustering&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile /12098626514775266161&lt;/uri>;&lt; ：//img1.blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail height =“ 72” url =“ https：//blogdger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR2mD5afdT_Qg9Ex4Lj94FaxB9KHiq20iLoDlOY8S8Rk5XsV6n_4dU9CFbCvSeBSONGQYqy-yMGY6-KU_y_RGICOpz76GNdzv7ecxor_rVnF31lZOd3STQF4MIE4F_EadrSI1DXY67MXnsCswY3w3X8vX8KgU_rRPs6eTZndYAbVcEezgwaUsdZEAHD59Z/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt; /media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;