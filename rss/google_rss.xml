<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-07-25T10:48:18.191-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="Education"></category><category term="conferences"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="MOOC"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="accessibility"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="optimization"></category><category term="ACL"></category><category term="Android"></category><category term="Awards"></category><category term="ICML"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="Environment"></category><category term="Google Maps"></category><category term="Google Translate"></category><category term="Responsible AI"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="DeepMind"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Acoustic Modeling"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Interspeech"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="RAI-HCT Highlights"></category><category term="Recommender Systems"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Social Networks"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://ai.googleblog.com/feeds/posts/default&quot; rel=&quot;http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt=atom&amp;start-index =26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2 005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com&quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1259&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:items每页>;25&lt;/opensearch:itemsperpage>;&lt;entry>;&lt;id>;标签:blogger.com,1999:blog-8474926331452026626.post-2519516457542613363&lt;/id>;&lt;发布>;2023-07-23T14:13:00.000-07:00&lt;/发布>;&lt;更新>;2 023-07-23T14:13:29.179-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category ory>;&lt;title type=&quot;text&quot;>;Google 在 ICML 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 项目经理 Cat Armato&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBU FTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4nvXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3 LUkU/s320/Google-ICML-hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google 各个团队积极开展机器学习 (ML) 领域的理论和应用研究。我们构建机器学习系统来解决语言、音乐、视觉处理、算法开发等领域的深层科学和工程挑战。我们的目标是通过开源工具和数据集、发布我们的工作并积极参加会议，与更广泛的机器学习研究社区建立一个更具协作性的生态系统。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;Google 很荣幸成为第 40 届&lt;a href=&quot;https://icml.cc/virtual/2023/index.html&quot;>;机器学习国际会议&lt;/a>; (ICML 2023) 的&lt;a href=&quot;https://icml.cc/virtual/2023/sponsor_list&quot;>;钻石赞助商&lt;/a>;，这是一项重要的年度会议，将于本周在霍诺举行露露，夏威夷。作为 ML 研究领域的领导者，Google 在今年的会议上表现强劲，收到了 120 多篇论文，并积极参与了许多研讨会和教程。 Google 还很荣幸成为&lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>;和&lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;机器学习中的女性&lt;/a>;研讨会的白金赞助商。我们期待分享我们的一些广泛的机器学习研究，并扩大我们与更广泛的机器学习研究社区的合作伙伴关系。 &lt;/p>; &lt;p>; 已注册 ICML 2023？我们希望您能够参观 Google 展位，详细了解解决该领域最有趣挑战的部分令人兴奋的工作、创造力和乐趣。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter 帐户，了解 Google 展位活动（例如演示和问答环节）。请参阅 &lt;a href=&quot;http://www.deepmind.com/blog/google-deepmind-research-at-icml-2023&quot;>;Google DeepMind 的博客&lt;/a>;，了解他们在 ICML 2023 上的技术参与情况。&lt;/p>; &lt;p>;查看下文，了解有关在 ICML 2023 上展示的 Google 研究的更多信息（Google 隶属关系以&lt;strong>;粗体&lt;/strong>;显示）。&lt;/p>; &lt;br>; &lt;div style=&quot;line-height : 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;董事会和组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 董事会成员包括：&lt;strong>;&lt;em>;Corinna Cortes&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>; &lt;br>; 辅导主席包括：&lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展台活动&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 演讲者：&lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>; &lt;br>; 标题：无监督图 Em bedding @ Google（&lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;论文&lt;/a>;，&lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;EXPO 研讨会&lt;/a>;）&lt;br>; 7 月 25 日星期二上午 10:30 HST &lt;/p>; &lt;p>; 演讲者：&lt;strong>;&lt;em>;郑旭&lt;/em>;&lt;/ strong>; &lt;br>; 标题：具有差异隐私的 Gboard 语言模型的联合学习（&lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;论文 1&lt;/a>;、&lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;论文 2&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2023/05/making -ml-models- Differentially-private.html&quot;>;博客文章&lt;/a>;) &lt;br>; 7 月 25 日星期二下午 3:30 HST &lt;/p>; &lt;p>; 演讲者：&lt;strong>;&lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;br>; 标题：自监督场景理解（&lt;a href=&quot;https://arxiv.org/abs/2302.04973&quot;>;论文 1&lt;/a>;，&lt;a href=&quot;https://arxiv. org/abs/2203.11194&quot;>;论文 2&lt;/a>;) &lt;br>; 7 月 26 日星期三上午 10:30 HST &lt;/p>; &lt;p>; 演讲者：&lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Max Vladymyrov&lt;/em>;&lt;/strong>; &lt;br>; 标题：变形金刚通过梯度下降在上下文中学习 (&lt;a href=&quot;https://openreview.net/pdf? id=tHvXrFQma5&quot;>;论文&lt;/a>;) &lt;br>; 7 月 26 日星期三下午 3:30 HST &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;已接受论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Lhyy8H75 KA&quot;>;将 Vision Transformer 扩展到 220 亿个参数&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;博文&lt;/a>;）&lt;br>; &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Josip Djolonga&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Basil Mustafa&lt;/em>;&lt;/strong>; >;、&lt;strong>;&lt;em>;Piotr Padlewski&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·希克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;贾斯汀·吉尔默&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;安德烈亚斯·施泰纳&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;玛蒂尔德·卡隆&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伯特·盖霍斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;易卜拉欣·阿拉杜lmohsin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;鲁道夫·杰纳顿&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;卢卡斯·拜尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Michael Tschann&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿努拉格·阿尔纳布&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;小王&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;卡洛斯·里克尔梅&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Matthias Minderer&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Joan Puigcerver、&lt;/em>; &lt;em>;Utku Evci&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Manoj Kumar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Gamaleldin F. Elsayed&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Aravindh Mahend跑&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Fisher Yu&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿维塔尔·奥利弗&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;芳汀·胡特&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马克·帕特里克·科利尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿列克谢·格里森科&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Vighnesh Birodkar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Cristina Vasconcelos&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thomas Mensink&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Alexander Kolesnikov&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Filip Pavetić&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dustin Tran &lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;托马斯·基普夫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马里奥·卢西奇&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;翟晓华&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;丹尼尔·凯瑟斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;耶利米·哈姆森&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尼尔·霍尔斯比&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=C9NEblP8vS&quot;>;通过推测解码从 Transformers 进行快速推理&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yaniv Leviathan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Matan Kalman&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yossi Matias&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openre view.net/pdf?id=bUFUaawOTk&quot;>;两全其美的策略优化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;、&lt;em>;Chen-Yu Wei&lt;/em>;、&lt;strong>;&lt;em>;Julian Zimmert&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=9PJ2V6qvQL&quot;>;流入、流出和再循环机器学习中的互易性&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Walid Krichene&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;变形金刚通过梯度下降在上下文中学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;艾温德·尼克拉松&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;埃托雷·兰达佐&lt;/em>;&lt;/strong>;、&lt;em>;若昂·萨克拉门托&lt;/em>;、&lt;strong>;&lt;em>;亚历山大·莫德温采夫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;安德烈·日莫吉诺夫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马克斯·弗拉迪米罗夫&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=EfhmBBrXY2&quot;>;算术采样：大型语言模型的并行多样化解码&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Luke Vilnis&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;、&lt;em>;Patrick Murray*&lt;/em>;、&lt;em>;Alexandre Passos*&lt;/em>;、 &lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ayBKRjGDEI&quot;>;具有可证明近似保证的差分私有层次聚类&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/05/ Differentially-private-clustering-for.html&quot;>;博文&lt;/a>;）&lt;br>; &lt;em>;雅各布·伊莫​​拉&lt;/em>;*、&lt;strong>;&lt;em>;亚历山德罗·埃帕斯托&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;穆罕默德·马赫迪安&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;文森特·科恩-阿达德&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ZVxT2 ToHR5&quot;>;私有机器学习的多时代矩阵分解机制&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;H. Brendan McMahan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Keith Rush&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=1UaGAhLAsL&quot;>;无论模型选择如何，随机分类噪声都无法击败所有凸潜力助推器&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Richard Nock&lt;/em>;&lt;/strong>;、&lt;em>;Robert Williamson&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=qw8zAw6mzJ&quot;>;单纯形随机特征&lt;/a>; &lt;br>; &lt;em>;Isaac Reid&lt;/em>;、&lt;strong>; &lt;em>;Krzysztof Choroman Ski&lt;/em>;&lt;/strong>;、&lt;em>;Valerii Likhosherstov&lt;/em>;、&lt;em>;Adrian Weller&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=bF1LVbP493&quot;>;Pix2Struct：屏幕截图解析作为视觉语言理解预训练&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mandar Joshi&lt;/em>;&lt;/strong>;、&lt;em>;Iulia Turc&lt;/em>;、&lt;strong>; &lt;em>;胡鹤翔&lt;/em>;&lt;/strong>;、&lt;em>;刘芳宇&lt;/em>;、&lt;strong>; &lt;em>;Julian Eisenschlos&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Peter Shaw&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张明伟&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;Kristina Toutanova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=eIQIcUKs0T&quot;>;Mu&lt;sup>;2&lt;/sup>;SLAM：多任务、多语言语音和语言模型&lt;/a>; &lt;br>;&lt;strong>;&lt;em>;Yong Cheng&lt;/​​em>;&lt;/strong>;、&lt;strong>;&lt;em>;张宇&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Melvin Johnson&lt;/ em>;&lt;/strong>;、&lt;strong>; &lt;em>;Wolfgang Macherey&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ankur Bapna&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=5h42xM0pwn&quot;>;单个样本的稳健预算节奏&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Santiago Balseiro&lt;/em>;&lt;/strong>;、&lt;em>;Rachitesh Kumar&lt;/em>;*、&lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Balasubramanian Sivan&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Di Wang&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=0bR5JuxaoN&amp;amp;name=pdf&quot;>;基于检索的模型的统计视角s&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Soumya Basu&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;、&lt;em>;Manzil Zaheer&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=XjTcC4EA4P&quot;>;张量分解的近似最佳核心形状&lt;/a>; &lt;br>; &lt;em>;Me hrdad Ghadiri&lt;/em>;、&lt;strong>; &lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Gang Fu&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=rWGp9FbS0Q&amp;amp;name=pdf&quot;>;使用批次的高效列表可解码回归&lt;/em>; a>; &lt;br>; &lt;strong>;&lt;em>;Abhimanyu Das&lt;/em>;&lt;/strong>;、&lt;em>;Ayush Jain&lt;/em>;*、&lt;strong>; &lt;em>;孔伟豪&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rajat Sen&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=SpFIO5Mdso&amp;amp;name=pdf&quot;>;语言模型的高效训练使用少样本学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Sashank J. Reddi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sobhan Miryoosefi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Stefani Karp&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shankar Krishnan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Seungyeon Kim&lt;/em>;&lt;/ strong>;,&lt;strong>;&lt;em>;Sanjiv Kumar&lt;/em>;&lt;br />;&lt;/strong>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=Bj76bauv1Q&amp;amp;name=pdf&quot;>;拟阵上的完全动态子模最大化&lt;/a>;&lt;br>;&lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;、&lt;em>;Federico Fusco&lt;/em>;、&lt;strong>;&lt;em>;Silvio Lattanzi &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Morteza Zadimoghaddam&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=VlEAJkmlMs&amp;amp;name=pdf&quot;>;用于学习组合潜变量模型的 GFlowNet-EM&lt;/a>; &lt;br>; &lt;em>;Edward J Hu&lt;/em>;、&lt;em>;Nikolay Malkin&lt;/em>;、&lt;em>;Moksh Jain&lt;/em>;、&lt;strong>;&lt;em>;Katie Everett&lt;/em>;&lt;/strong>;、&lt;em>;Alexandros Graikos&lt;/em>;、&lt;em>;Yoshua Bengio&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rB0VaD44FZ&quot;>;改进的点击率预测在线学习算法广告拍卖&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;冯哲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>;、&lt;em>;周子鑫&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sfdKdeczaw&amp;amp;name=pdf&quot;>;大型语言模型努力学习长尾知识&lt;/a>; &lt;br>; &lt;em>;Nikhir Kand pal&lt;/em>;、&lt;em>;邓海康&lt;/em>;、&lt;strong>; &lt;em>;Adam Roberts&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Eric Wallace&lt;/em>;&lt;/strong>;、&lt;em>;Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=UdiUd99I81&quot;>;预算和投资回报率限制的多渠道自动出价&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Y uan Deng&lt;/​​em>;&lt;/strong>;、&lt;em>;Negin Golrezaei&lt;/em>;、&lt;em>;Patrick Jaillet&lt;/em>;、&lt;em>;Jason Cheuk Nam Liang&lt;/em>;、&lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZMvv6laV5b&amp;amp;name=pdf&quot;>;多层神经网络作为可训练的阶梯希尔伯特空间&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;陈正道&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfkSyUJyqg&quot;>;关于用户级私有凸优化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Badih Ghazi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pritish Kamath&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;R avi Kumar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Raghu Meka&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pasin Manurangsi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张驰远&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zAgouWgI7b&amp;amp;name=pdf&quot;>;通过不变表示进行 PAC 泛化&lt;/a>; &lt;br>; &lt;em>;Ad vait U Parulekar&lt;/em>;，&lt;strong>; Karthikeyan Shanmugam&lt;/em>;&lt;/strong>;，&lt;em>;Sanjay Shakkottai&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0rZvMIfECW&amp;amp;name=pdf&quot;>;正则化和方差加权回归在线性 MDP 中实现极小极大最优：理论与实践&lt;/a>; &lt; br>; &lt;em>;Toshinori Kitamura&lt;/em>;、&lt;em>;Tadashi Kozuno&lt;/em>;、&lt;em>;唐云浩&lt;/em>;、&lt;strong>;&lt;em>;Nino Vieillard&lt;/em>;&lt;/strong>;、&lt;em>;Michal Valko&lt;/em>;、&lt;em>;杨文浩&lt;/em>;、&lt;strong>; &lt;em>;梅金城&lt;/em>;&lt;/strong>;、&lt;em>;Pierre Menard&lt;/em>;、&lt;em>;Mohammad Gheshlaghi Azar &lt;/em>;、&lt;em>;Remi Munos&lt;/em>;、&lt;strong>;&lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;、&lt;em>;Csaba Szepesvari&lt;/em>;、&lt;em>;Wataru Kumagai&lt;/em>;、&lt;em>;松尾裕&lt;/em>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mrykt 39VUw&amp;amp;name=pdf&quot;>;通过最小违规排列加速 Bellman Ford&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;、&lt;em>;Ola Svensson&lt;/em>;、&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LxodbQa62n&amp;amp; name=pdf&quot;>;学习算法的统计不可区分性&lt;/a>; &lt;br>; &lt;em>;Alkis Kalavasis&lt;/em>;、&lt;strong>; &lt;em>;Amin Karbasi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shay Moran&lt;/em>;&lt;/strong>;、&lt;em>;Grigoris Velegkas&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=G5vKSJVhJL&quot;>;测试-以时隙为中心的模型的时间适应&lt;/a>; &lt;br>; &lt;em>;Mihir Prabhudesai&lt;/em>;、&lt;em>;Anirudh Goyal&lt;/em>;、&lt;strong>; &lt;em>;Sujoy Paul&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Gaurav Aggarwal&lt;/em>;&lt;/strong>; ,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;, &lt;em>;Deepak Pathak&lt;/em>;, &lt;em>;Katerina Fragkiadaki>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=2WEMW6rGgG&quot;>;用户级隐私下直方图估计的边界贡献算法&lt;/a>; &lt;br>; &lt;em>;Yuhan Liu&lt;/em>;*, &lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Wennan Zhu&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Marco Gruteser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=SgeIqUvo4w&amp;amp;name=pdf&quot;>;带有提示和查询的 Bandit 在线线性优化&lt;/em>;&lt;/strong>; a>; &lt;br>; &lt;em>;Aditya Bhaskara&lt;/em>;、&lt;em>;Ashok Cutkosky&lt;/em>;、&lt;strong>; &lt;em>;Ravi Kumar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Manish Purohit&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=wagsJnR5GO&amp;amp;name=pdf&quot;>;CLUTR：通过无监督任务表示进行课程学习学习&lt;/a>; &lt;br>; &lt;em>;Abdus Salam Azad&lt;/em>;、&lt;strong>; &lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;、&lt;em>;Jasper Emhoff&lt;/em>;、&lt;em>;Nathaniel Alexis&lt;/em>;、&lt;strong>; &lt;em>;Aleksandra Faust&lt;/em>;&lt;/strong>;、&lt;em>;Pieter Abbeel&lt;/em>;、&lt;em>;Ion Stoica&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/attachment?id=R3WrLjtzG8&amp;amp;name=pdf&quot;>;CSP：地理空间视觉表示的自监督对比空间预训练&lt;/a>; &lt;br>; &lt;em>;Gengchen Mai&lt;/em>;，&lt;strong>;&lt;em>;Ni Lao&lt;/em>;&lt;/strong>;，&lt;em>;Yutong He&lt;/em>;，&lt;em>;Jiaming Song&lt;/em>;，&lt;em>;Stefano Ermon&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=vd5JYAml0A&amp;amp;name=pdf&quot;>;基于 Ewald 的分子图远程消息传递&lt;/a>; &lt;br>; &lt;em>;Arthur Kosmala&lt;/em>;、&lt;strong>; &lt;em>;Johannes Gasteiger&lt;/em>;&lt;/strong>;、&lt;em>;Nicholas Gau&lt;/em>;、&lt;em>;Stephan Günnemann&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Iey50XHA3g&amp;amp;name=pdf&quot;>;二元矩阵分解的快速 (1+ε) 近似算法&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;、&lt;em>;Maximilian Vötsch&lt;/em>;、&lt;strong>;&lt;em>;David Woodruff&lt;/em>;&lt;/strong>;、&lt;em>; Samson Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=b9opfVNw6O&amp;amp;name=pdf&quot;>;具有用户级差分隐私的联合线性上下文强盗&lt;/a>; &lt;br>; &lt;em>;Ruiquan Huang&lt;/em>;、&lt;em>;Huanyu Zhang&lt;/em>;、&lt;em>;Luca Melis&lt;/em>;、&lt;em>;Milan Shen&lt;/em>;、&lt;strong>; &lt;em>;Meisam He jazinia&lt;/em>;&lt;/strong>;、&lt;em>;Jing Yang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=m21SgZnBWZ&amp;amp;name=pdf&quot;>;研究基于模型的学习在探索和迁移中的作用&lt;/a>; &lt;br>; &lt;em>;Jacob C Walker&lt;/em>;、&lt;em>;Eszter Vértes&lt;/em>;、&lt;em>;Yazhe Li&lt;/em>;、&lt;强>; &lt;em>;Gabriel Dulac-Arnold&lt;/em>;&lt;/strong>;、&lt;em>;Ankesh Anand&lt;/em>;、&lt;em>;Theophane Weber&lt;/em>;、&lt;em>;Jessica B Hamrick&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=K1sJiHvy02&quot;>;标签差异化隐私和私人训练数据发布&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Robert Busa-Fekete&lt;/em>;&lt;/ strong>;、&lt;strong>; &lt;em>;Andres Munoz&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Umar Syed&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sergei Vassivitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Q4QFG5Fe4O&amp;amp;name=pdf&quot;>;与分发专业专家一起进行终身语言预训练&lt;/a>; &lt;br>; &lt;em>;陈武阳&lt;/em>;*、&lt;strong>;&lt;em>;周艳琪&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杜楠&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;黄艳萍&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;詹姆斯·劳登&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陈志峰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;崔怡&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openre view.net/attachment?id=06djx2x2Rf&amp;amp;name=pdf&quot;>;低排名奖励的多用户强化学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Dheeraj Mysore Nagaraj&lt;/em>;&lt;/strong>;、&lt;em>;Suhas S Kowshik&lt;/em>;、&lt;strong>;&lt;em>;Naman Agarwal&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Praneeth Netrapalli&lt;/em>;&lt;/strong>; >;,&lt;strong>; &lt;em>;Prateek Jain&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DwOUndjwiV&amp;amp;name=pdf&quot;>;用于视觉机器人操作的多视图蒙版世界模型&lt;/a>; &lt;br>; &lt;em>;Younggyo Seo&lt;/em>;、&lt;em>;Junsu Kim&lt;/em>;、&lt;em>;Stephen James&lt;/em>;、&lt;strong>; &lt;em>;Kimin Lee &lt;/em>;&lt;/strong>;、&lt;em>;Jinwoo Shin&lt;/em>;、&lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VTpHpqM3Cf&amp;amp;name=pdf&quot;>;PaLM-E：体现的多模态语言模型&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language .html&quot;>;博客文章&lt;/a>;) &lt;br>;&lt;strong>;&lt;em>;Danny Driess&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;夏飞&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Corey Lynch&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Brian Ichter&lt;/em>;&lt;/strong>; >;、&lt;strong>;&lt;em>;Ayzaan Wahid&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jonathan Tompson&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Quan Vuong&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;余天河&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;黄文龙&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yevgen Chebotar&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pierre Sermanet&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;丹尼尔·达克沃斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;谢尔盖·莱文&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;文森特·范霍克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;卡罗尔·豪斯曼&lt;/em>;&lt;/strong>;、&lt;em>;马克·图桑&lt;/em>;、&lt;strong>;&lt;em>;克劳斯·格雷夫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;曾安迪&lt;/em>;&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;Igor Mordatch&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Pete Florence&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=y8qAZhWbNs&quot;>;使用自动调整压缩的私人联合学习&lt;/a>; &lt;br>; &lt;em>;Enayat Ullah&lt;/em>;*、&lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=7WdMBofQFx&amp;amp;name=pdf&quot;>;使用线性函数逼近的对抗性 MDP 的精致遗憾&lt;/a>; &lt;br>; &lt;em>;Yan Dai&lt;/em>;, &lt;em>;罗海鹏&lt;/em>; , &lt;em>;Chen-Yu Wei&lt;/em>;, &lt;strong>;&lt;em>;Julian Zimmert&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ccwSdYv1GI&amp;amp;name=pdf&quot;>;利用恒定内存将数据集蒸馏扩展到 ImageNet-1K&lt;/a>; &lt;br>; &lt;em>;Justin Cui&lt;/em>;,&lt;em>; Ruoche Wan&lt;/em>;, &lt;strong>;&lt;em>; Si Si&lt;/em>;&lt;/strong>;、&lt;em>;Cho-Jui Hsieh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=X7jMTrwuCz&amp;amp;name=pdf&quot;>;采用 AdaGrad 步长的 SGD：对未知参数、无界梯度和仿射方差的高概率完全自适应&lt;/a>; &lt;br>; &lt;em>;Amit Attia&lt;/em>;、&lt;strong>;&lt;em >;Tomer Koren&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6EVUnWGBMU&quot;>;分位数时间差异学习在价值估计中的统计优势&lt;/a>; &lt;br>; &lt;em>;Mark Rowland&lt;/em>;、&lt;em>;Yunhao Tang&lt;/em>;、&lt;em>;Clare Lyle&lt;/em>;、&lt;em>;Rémi Munos&lt;/em>;、&lt;strong>;&lt;em>;Marc G. Bellemare&lt;/em>;&lt;/strong>;、&lt;em>;Will Dabney&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=iMHNLJRSVz&amp;amp;name=pdf&quot;>;通过图像特征的迷雾揭开位置信息模式的面具&lt;/a>; &lt;br>; &lt;em>;Chieh Hubert Lin&lt;/em>;、&lt;em>;Hung-Yu Tseng&lt;/​​em>;、&lt;em>;Hsin-Ying Lee &lt;/em>;、&lt;em>;Maneesh Kumar Singh&lt;/em>;、&lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4UStsbnfVT&quot;>;具有最优速率的用户级私有随机凸优化&lt;/a>; &lt;br>; &lt;em>;Raef Bassily&lt;/em>;、&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6MU5xdrO7t&amp;amp;name=pdf&quot;>;一种改进文本图像模型中提示集成的简单零样本提示加权技术&lt;/a>; &lt;br>; &lt;em>;James Urquhart Allingham&lt;/em>;*、&lt;strong>;&lt;em>;Jie Ren&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Michael W Dusenberry&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;顾秀野&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;崔银&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Dustin Tran&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘哲利&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mXv2 aVqUGG&amp;amp;name=pdf&quot;>;大型语言模型可以推理程序不变量吗？&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;裴可欣&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;David Bieber&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Kensen Shi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Charles Sutton&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;殷鹏程&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/attachment?id=pWeQdceMHL&amp;amp;name=pdf&quot;>;持续观察下的并发随机差异隐私&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jay Tenenbaum&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Haim Kaplan&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Uri Stemmer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Xqedp0Iu1S&amp;amp;name=pdf&quot;>;常量：差分隐私持续观察中的细粒度误差&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Hendrik Fichtenberger&lt;/em>;&lt;/strong>;、&lt;em>;Monika Henzinger&lt;/em>;、&lt;em>;Jalaj Upadhyay&lt;/em>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://openreview.net/attachment?id=NfCA622s8O&amp;amp;name=pdf&quot;>;交叉熵损失函数：理论分析与应用&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;、&lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;、&lt;em>;钟宇涛&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment? id=5UZYtGEPTt&amp;amp;name=pdf&quot;>;使用在线函数逼近的对抗性上下文 MDP 的高效速率最优遗憾&lt;/a>; &lt;br>; &lt;em>;Orin Levy&lt;/em>;、&lt;strong>;&lt;em>;Alon Cohen&lt;/em>;&lt;/strong>;、&lt;em>; Asaf Cassel&lt;/em>;、&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openre view.net/attachment?id=KrsaROSs8b&amp;amp;name=pdf&quot;>;拟阵约束下流子模最大化的公平性&lt;/a>; &lt;br>; &lt;em>;Marwa El Halabi&lt;/em>;、&lt;em>;Federico Fusco&lt;/em>;、&lt;strong>;&lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jakab Tardos&lt;/em>;&lt;/strong>;、&lt;em>;Jakub Tarnawski &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZX4uS605XV&amp;amp;name=pdf&quot;>;Flan Collection：设计用于有效指令调优的数据和方法&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html&quot;>;博客文章&lt;/a>;）&lt;br>;&lt;em>;Shayne Longpre&lt;/em>;，&lt; strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tu Vu&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Albert Webson&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Hyung Won Chung&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Quoc V Le&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; Barret Zoph&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jason Wei&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Adam Roberts&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=rzN05i4GOE&amp;amp;name=pdf&quot;>;通过双层优化实现网络控制的图强化学习&lt;/a>; &lt;br>; &lt;em>;Daniele Gammelli&lt;/em>;、&lt;strong>; &lt;em>; James Harrison&lt;/em>;&lt;/strong>;、&lt;em>;Kaidi Yang&lt;/em>;、&lt;em>;Marco Pavone&lt;/em>;、&lt;em>;Filipe Rodrigues&lt;/em>;、&lt;em>;Francisco C. Pereira&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Fgn23Fsmtv&amp;amp;name=pdf&quot;>;用于多分位数发布的学习增强私有算法&lt;/a >; &lt;br>; &lt;em>;米哈伊尔·霍达克&lt;/em>;*、&lt;strong>;&lt;em>;卡里姆·阿明&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;特拉维斯·迪克&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=uSHBQdWmuC&amp;amp;name=pdf&quot;>;传奇特隆：雷霆队正确的多类损失学习&lt;/a>; &lt;br>; &lt;em>;Kevin H Lam&lt;/em>;、&lt;strong>;&lt;em>;Christian Walder&lt;/em>;&lt;/strong>;、&lt;em>;Spiridon Penev&lt;/em>;、&lt;strong>; &lt;em>;Richard Nock&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VnGIZsmxDG&amp;amp;name=pdf&quot;>;衡量编程语言分布的影响&lt;/ a>; &lt;br>;&lt;em>;加布里埃尔·奥尔兰斯基&lt;/em>;*、&lt;strong>;&lt;em>;肖克凡&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;泽维尔·加西亚&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;许志坚&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;约书亚·豪兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·马尔莫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;雅各布·奥斯汀&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;Rishabh Singh&lt;/em>;&lt;/strong>;、&lt;em>;Michele Catasta&lt;/em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=f69OtekDi4&quot;>;分布倾斜下的多任务差分隐私&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Walid Krichene&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;宋爽&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张丽&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=hi9UssZdHR&quot;>;Muse：通过蒙版生成转换器生成文本到图像&lt;/a>; &lt;br>; &lt;strong>;&lt; em>;Huiwen Chang&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张涵&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;贾里德·巴伯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;AJ Maschinot&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;José Lezama&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;蒋璐&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;凯文·墨菲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;威廉·T·弗里曼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迈克尔·鲁宾斯坦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;李元珍&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迪利普·克里希南&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf &quot;>;论联邦平均与循环客户端参与的融合&lt;/a>; &lt;br>; &lt;em>;Yae Jee Cho&lt;/em>;、&lt;em>;Pranay Sharma&lt;/em>;、&lt;em>;Gauri Joshi&lt;/em>;、&lt;strong>;&lt;em>;郑旭&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Tong Zhu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/attachment?id=GimajxXNc0&amp;amp;name=pdf&quot;>;通过在线到非凸转换实现最优随机非平滑非凸优化&lt;/a>; &lt;br>; &lt;em>;Ashok Cutkosky&lt;/em>;、&lt;strong>;&lt;em>;Harsh Mehta&lt;/em>;&lt;/strong>;、&lt;em>;Francesco Orabona&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ Attachment?id=4SHQv4cp3I&amp;amp;name=pdf&quot;>;通过有针对性的增强实现域外鲁棒性&lt;/a>; &lt;br>; &lt;em>;Irena Gau&lt;/em>;、&lt;em>;Shiori Sakawa&lt;/em>;、&lt;strong>; &lt;em>;Pang Wei Koh&lt;/em>;&lt;/strong>;、&lt;em>;Tatsunori Hashimoto&lt;/em>;、&lt;em>;Percy Liang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ pdf?id=b6Hxt4Jw10&quot;>;无界高斯混合模型的多项式时间和私有学习&lt;/a>; &lt;br>; &lt;em>;Jamil Arbas&lt;/em>;、&lt;em>;Hassan Ashtiani&lt;/em>;、&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=nlUAvrMbUZ&amp;amp;name=pdf&quot;>;预计算内存还是即时编码？检索增强的混合方法可充分利用您的计算&lt;/a>; &lt;br>; &lt;em>;Michiel de Jong&lt;/em>;、&lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Nicholas FitzGerald&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Fei Sha&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;William W. Cohen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1FldU7JzGh&amp;amp;name=pdf&quot;>;用于迭代生成的可扩展自适应计算&lt;/a>; &lt;br>; &lt;em>;Allan Jabri&lt;/em>;*、&lt;strong>;&lt;em>;David J. Fleet&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ting Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HiKPaeowPB&quot;>;缩放球形 CNN&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Carlos Esteves&lt;/em>;&lt;/strong>;、&lt;em>;Jean-Jacques Slotine&lt;/em>;、&lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/attachment?id=0O7b2Y198V&amp;amp;name=pdf&quot;>;步骤：在有前提条件的情况下从头开始学习 N:M 结构化稀疏掩模&lt;/a>; &lt;br>; &lt;em>;Yu Cheng Lu&lt;/em>;、&lt;strong>;&lt;em>;Shivani Agrawal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Suvinay Subramanian&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Oleg Rybakov&lt;/em>;&lt;/strong>; >;、&lt;em>;Christopher De Sa&lt;/em>;、&lt;em>;Amir Yazdanbakhsh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LZt1HIEoAf&amp;amp;name=pdf&quot;>;拒绝的分层对抗鲁棒性&lt;/a>; &lt;br>;&lt;em>;Jiefeng Chen&lt;/em>;、&lt;em>;Jayaram Raghuram&lt;/em>;、&lt;em>;Jihye Choi &lt;/em>;,&lt;strong>; &lt;em>;Xi Wu&lt;/em>;&lt;/strong>;, &lt;em>;Yingyu Liang&lt;/em>;, &lt;em>;Somesh Jha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=CnHxxjqkMi&amp;amp;name=pdf&quot;>;特权信息何时解释消除标签噪声？&lt;/a>; &lt;br>; &lt;em>;Guillermo Ortiz-Jimenez&lt;/em>;*,&lt;strong>; &lt;em>;马克·科利尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;阿南特·纳瓦尔加里亚&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;亚历山大·达穆尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杰西·贝伦特&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;鲁道夫·杰纳顿&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Effrosyni Kokiopoulou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/attachment?id=2bGTacOn8v&amp;amp;name=pdf&quot;>;具有弹性输入序列的自适应计算&lt;/a>; &lt;br>; &lt;em>;薛福兆&lt;/em>;*, &lt;strong>;&lt;em>;Valerii Likhosherstov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>; Mostafa Dehghani&lt;/em>;&lt;/strong>;、&lt;em>;杨游&lt;/em>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Pbaiy3fRCt&amp;amp;name=pdf&quot;>;神经网络记忆可以本地化吗？&lt;/a>; &lt;br>; &lt;em>;Pratyush Maini&lt;/em>;、&lt;strong>; &lt;em>;Michael C. Mozer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Hanie Sedghi&lt;/ em>;&lt;/strong>;、&lt;em>;Zachary C. Lipton&lt;/em>;、&lt;em>;J. Zico Kolter，&lt;strong>;张驰元&lt;/em>;&lt;/strong>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Ct2N6RWZpQ&amp;amp;name=pdf&quot;>;可控性感知无监督技能发现&lt;/a>; &lt;br>;&lt;em>;Seohong Park&lt;/em>;，&lt;strong>;&lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;，&lt;em>;Youngwoon Lee&lt;/em>; , &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2Mbo7IEtZW&amp;amp;name=pdf&quot;>;基于 Bi-Stride 多尺度图神经网络的基于网格的物理模拟的高效学习&lt;/a>; &lt;br>; &lt;em>;Yadi Cao&lt;/em>;,&lt;strong>; &lt;em>;柴梦雷&lt;/em>;&lt;/strong>;, &lt;em>;李敏辰&lt;/em>;, &lt;em>;蒋陈凡富&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zN4oRCrlnM&amp;amp;name=pdf&quot;>;线性草图下的联邦重量级恢复&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Adria Gascon&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Peter Kairouz&lt;/strong>;&lt;/em>;,&lt;strong>; &lt;em>;孙子腾&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;用于对图神经网络进行基准测试的图生成模型&lt;/a>; &lt;br>; &lt;em>;Minji Yoon&lt;/em>;、&lt;em>;Yue Wu&lt;/em>;、&lt;strong>;&lt;em>;John Palowitch&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Bryan Perozzi&lt;/em>; &lt;/strong>;, &lt;em>;Russ Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=IFhGrPAn8f&amp;amp;name=pdf&quot;>;成对误排序损失代理的 H 一致性界限&lt;/a>; &lt;br>; &lt;em>;毛安琪&lt;/em>;，&lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;，&lt;em>;钟玉涛&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DF6ypWrepg&amp;amp;name=pdf&quot;>;通过线性函数逼近改进高效在线强化学习的遗憾&lt;/a>; &lt;br>; &lt;em>;Uri Sherman&lt;/em>;、&lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZXeTCRZJp9&amp;amp;name=pdf&quot;>;不变槽位注意力：使用以槽位为中心的参考系进行对象发现&lt;/a>; &lt;br>; &lt;em>;Ondrej Biza&lt;/em>;*、&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ga maleldin Fathy Elsayed&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=a35tteW8if&quot;>;从强盗反馈中进行多任务非策略学习&lt;/a>; &lt;br>; &lt;em>;Joey Hong&lt;/em>;、&lt;em>;Branislav Kveton &lt;/em>;、&lt;em>;Manzil Zaheer&lt;/em>;、&lt;em>;Sumeet Katariya&lt;/em>;、&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=yv8GUQREda&amp;amp;name=pdf&quot;>;单边 Lipschitz 函数的最佳无悔学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting &lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Guru Guruganesh&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jon Schneider&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Joshua Ruizhi Wang&lt;/em>;&lt;/strong>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=AwxfYvdPZV&amp;amp;name=pdf&quot;>;平均场博弈中高效独立学习的策略镜像上升&lt;/em>;&lt;/strong>;&lt;/em>;&lt;/strong>; a>; &lt;br>; &lt;em>;Batuhan Yardim&lt;/em>;、&lt;em>;Semih Cayci&lt;/em>;、&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;、&lt;em>;鸟河&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ILMHlUn4k6&amp;amp;name=pdf&quot;>;一般和马尔可夫博弈中的遗憾最小化和均衡收敛&lt;/a >; &lt;br>; &lt;em>;Liad Erez&lt;/em>;、&lt;em>;Tal Lancewicki&lt;/em>;、&lt;em>;Uri Sherman&lt;/em>;、&lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=skDVsmXjPR&amp;amp;name=pdf&quot;>;强化学习可以更高效高效且具有多种奖励&lt;/a>; &lt;br>; &lt;strong>;Christoph Dann&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rdOuTlTUMX&quot;>;基于历史相关动态上下文的强化学习&lt;/a>; &lt;br>; &lt;strong>; &lt;em>;Guy Tennenholtz&lt;/em>;&lt;/strong>;、&lt;em>;Nadav Merlis&lt;/em>;、&lt;strong>;&lt;em>;Lior Shani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Martin Mladenov&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Craig Boutlier&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sdHCjMzhHN&amp;amp;name=pdf&quot;>;用户-物理动力系统扩散模型中的定义事件采样和不确定性量化&lt;/a>; &lt;br>; &lt;em>;Marc Anton Finzi&lt;/em>;*、&lt;strong>; &lt;em>;Anudhyan Boral&lt;/em>;&lt;/strong>;、&lt;em>;Andrew Gordon Wilson&lt;/em>;、&lt;strong>; &lt;em>;Fei Sha&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Leonardo Zepeda-Nunez&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LDBIVZCnLl&amp;amp;name=pdf&quot;>;离散键值瓶颈&lt;/a>; &lt;br>; &lt;em>;Frederik Träuble&lt;/em>;、&lt;em>;Anirudh Goyal&lt;/em>;、&lt;em>;Nasim Rahaman&lt;/em>;、&lt;strong>; &lt;em>;Michael Curtis Mozer&lt;/em>;&lt;/strong>;、&lt;em>;Kenji Kawaguchi&lt;/em>;、&lt;em>;Y oshua Bengio&lt;/em>;、&lt;em>;Bernhard Schölkopf&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nVO6YTca8O&quot;>;DSGD-CECA：采用通信最优精确共识算法的去中心化 SGD&lt;/a>; &lt;br>; &lt;em>;Lisang Ding&lt;/em>;、&lt;em>;Kexin Jin&lt;/em>;、&lt;strong>; &lt;em>;Bi Cheng Ying&lt;/em>;&lt;/strong>; , &lt;em>;Kun Yuan&lt;/em>;, &lt;em>;Wotao Yin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3Ge74dgjjU&amp;amp;name=pdf&quot;>;Exphormer：图的稀疏变换器&lt;/a>; &lt;br>; &lt;em>;Hamed Shirzad&lt;/em>;，&lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>; Balaji Venkatachalam&lt;/em>;&lt;/strong>;、&lt;em>;Danica J. Sutherland&lt;/em>;、&lt;strong>; &lt;em>;Ali Kemal Sinop&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=dolp65Z6re&amp;amp;name=pdf&quot;>;快速、可微和稀疏 Top-k：凸分析视角&lt;/a>; &lt;br>; &lt;em>;Michael Eli Sander&lt;/em>; *, &lt;strong>;&lt;em>;Joan Puigcerver&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Josip Djolonga&lt;/em>;&lt;/strong>;, &lt;em>;Gabriel Peyré&lt;/em>;,&lt;strong>; &lt;em>;Mathieu Blondel&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=priTMs7n6e&quot;>;改进算法资源分配随机试验的政策评估&lt;/em>; a>; &lt;br>; &lt;strong>;&lt;em>;Aditya Mate&lt;/em>;&lt;/strong>;、&lt;em>;Bryan Wilder&lt;/em>;、&lt;strong>; &lt;em>;Aparna Taneja&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Milind Tambe&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Yh9sFZQk7Y&amp;amp;name=pdf&quot;>;寻找源自由域的通用方法改编&lt;/a>; &lt;br>; &lt;em>;Malik Boudiaf&lt;/em>;*、&lt;strong>;&lt;em>;Tom Denton&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Bart van Merrienboer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vincent Dumoulin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Eleni Triantafillou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=m SofpvUxCL&amp;amp;name=pdf&quot;>;存在分布偏移的学习率表&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Adel Javanmard&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Pratik Worah&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=slM2r4bRD1&amp;amp;name=pdf&quot;>;并非所有语义都是平等的：具有自动温度个体化的对比自我监督学习&lt;/a>; &lt;br>; &lt;em>;Zi-Hao Qiu&lt;/em>;、&lt;em>;Quanqi Hu&lt;/em>;、&lt;em>;Zhuoning Yuan&lt;/em>;、&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;、&lt;em>;Lijun Zhu&lt;/em>;、&lt;em>;杨天宝&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=EUQsBO975P&amp;amp;name=pdf&quot;>;论解释与预测之间的关系：因果观&lt;/a>; &lt;br>; &lt;em>;Amir-Hossein Karimi&lt;/em>;*、&lt;em>;Krikamol Muandet&lt;/em>;、&lt;strong>; &lt;em>;Simon Kornblith&lt;/em>;&lt;/strong>;、&lt;em>;B ernhard Schölkopf&lt;/em>;，&lt;strong>; &lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=qorOnDor89&amp;amp;name=pdf&quot;>;论注意力在提示调整中的作用&lt;/a>; &lt;br>; &lt;em>;Samet Oymak&lt;/em>;，&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;，&lt;em>;Mahdi Soltanolkotabi&lt;/em>;、&lt;em>;Christos Thrampoulidis&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2jvwyTm6Pk&amp;amp;name=pdf&quot;>;PLay：使用潜在扩散生成参数化条件布局&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Chin-Yi Cheng&lt;/​​em>;&lt;/strong>;、&lt;strong>;&lt;em>;Forrest Huang&lt;/em>;&lt;/strong>; ,&lt;strong>; &lt;em>;李刚&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;杨丽&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=631FTQB0UB&amp;amp;name=pdf&quot;>;学习局部线性模型在非线性策略优化中的力量&lt;/a>; &lt;br>; &lt;em>;Daniel Pfrommer&lt;/em>;, &lt;em>;Max Simchowitz&lt;/em>;, &lt;em>;Tyl er Westenbroek&lt;/em>;、&lt;em>;Nikolai Matni&lt;/em>;、&lt;strong>; &lt;em>;Stephen Tu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=BDYIci7bVs&amp;amp;name=pdf&quot;>;解释图神经网络的相关步行搜索&lt;/a>; &lt;br>; &lt;em>;Ping Xiong&lt;/em>;、&lt;em>;Thomas Schnake&lt;/em>;、&lt;em>; Michael Gastegger&lt;/em>;、&lt;em>;Grégoire Montavon&lt;/em>;、&lt;strong>; &lt;em>;Klaus Robert Muller&lt;/em>;&lt;/strong>;、&lt;em>;Shinichi Nakajima&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RX70NHEPE0&amp;amp;name=pdf&quot;>;代码大型语言模型的存储库级提示生成&lt;/a>; &lt;br>; &lt;em>;Disha Shri vastava&lt;/em>;,&lt;strong>;&lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;Daniel Tarlow&lt;/em>;&lt;/strong>;&lt;/p>;&lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=r3M5cBtpYq&amp;amp;name=pdf&quot;>;稳健且私密的随机线性强盗&lt;/a>;&lt;br>;&lt;em>;Vasileios Charisopoulos&lt;/em>;*,&lt;strong>;&lt; em>;Hossein Esfandiari&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6l9YG3wHA9&amp;amp;name=pdf&quot;>;简单扩散：高分辨率图像的端到端扩散&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Emiel Hoogeboom&lt;/em>;&lt;/strong>;， &lt;strong>; &lt;em>;Jonathan Heek&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tim Salimans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=cw6Zb0sEiT&amp;amp;name=pdf&quot;>;捆绑增强：控制表示相似性改进数据增强&lt;/a>; &lt;br>; &lt;em>;Emirhan Kurtulus&lt;/em>;、&lt;strong>;&lt;em>;Zichao Li&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yann Dauphin&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ekin D. Cubuk&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1d3O0b1rbL&amp;amp;name=pdf&quot;>;为什么私模培训需要公开预训？&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Arun Ganesh&lt;/em>; &lt;/strong>;、&lt;em>;Mahdi Haghifam&lt;/em>;*、&lt;strong>;&lt;em>;Milad Nasr&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thomas Steinke&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Om Thakkar&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Abhradeep Guha Thakurta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Lun Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XXC601YWgq&amp;amp;name=pdf&quot;>;强化学习中一步强化学习与批评正则化之间的联系&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Benjamin Eysenbach&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Sergey Levine&lt;/em>; >;&lt;/strong>;, &lt;em>;Ruslan Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;差分隐私优化中超越均匀 Lipschitz 条件&lt;/a>; &lt;br>; &lt;em>;Rudrajit Das&lt;/em>;*, &lt;strong>;&lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;郑旭&lt;/em >;&lt;/strong>;,&lt;strong>;张潼&lt;/em>;&lt;/strong>;，Sujay Sanghavi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Y5jGkbZ0W3&amp;amp;name=pdf&quot;>;高效图场积分器满足点云&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Krzysztof Choromanski&lt;/em>;&lt;/strong>;，&lt;em>;Ar ijit Sehanobish&lt;/em>;、&lt;em>;韩林&lt;/em>;、&lt;em>;赵云帆&lt;/em>;、&lt;em>;Eli Berger、&lt;/em>; &lt;em>;Tetiana Parshakova&lt;/em>;、&lt;em>;Alvin Pan&lt;/em>;、&lt;em>;David Watkins&lt;/em>;、&lt;em>;张天一&lt;/em>;、&lt;em>;Valerii Likhosherstov&lt;/em>;、&lt;em>;Somnath Basu Roy Chowdhury&lt;/em>; >;、&lt;strong>;&lt;em>;Avinava Dubey&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Deepali Jain&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tamas Sarlos&lt;/em>;&lt;/strong>;、&lt;em>;Snigdha Chaturvedi&lt;/em>;、&lt;em>;Adrian Weller&lt;/em>;&lt;/p>;&lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=RAeN6s9RZV&amp;amp;name=pdf&quot;>; Fast as CHITA：组合优化的神经网络剪枝&lt;/a>; &lt;br>; &lt;em>;Riade Benbaki&lt;/em>;、&lt;em>;陈文宇&lt;/em>;、&lt;em>;孟翔&lt;/em>;、&lt;strong>;&lt;em>;Hussein Hazimeh&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;赵哲&lt;/em>;&lt;/strong>;、&lt;em>;Rahul Maz umder&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2M7lwN0DTp&amp;amp;name=pdf&quot;>;快速启动强化学习&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html&quot;>;博客文章&lt;/a>;）&lt;br>; &lt;em>;Ikechukwu Uchendu&lt;/em>;*, &lt;strong>; &lt;em>;Ted Shaw&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;姚路&lt;/em>;&lt;/em>;、&lt;em>;朱邦华&lt;/em>;、&lt;em>;严孟源&lt;/em>;、&lt;em>;Joséphine Simon&lt;/em>;、&lt;em>;Matthew Bennice&lt;/em>;、&lt;em>;付楚源&lt;/em>;、&lt;em>;马聪&lt;/em>;、&lt;em>;焦建涛&lt;/em>;、&lt;strong>;&lt;em>;Sergey Levine&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Karol Hausman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=WPjMrOi1KE&amp;amp;name=pdf&quot;>;POMDP 中的学习具有样本效率和事后可观察性&lt;/a>; &lt;br>; &lt;em>;Jonathan Lee&lt;/em>;、&lt;strong>; &lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; Christoph Dann&lt;/em>;&lt;/strong>;、&lt;em>;Tong Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=K0InBsKODr&amp;amp;name=pdf&quot;>;使用 ES-Single 展开计算图中的低方差梯度估计&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Vicol&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/attachment?id=Qh0Gbq3lkh&amp;amp;name=pdf&quot;>;用于预测、表示和控制的掩蔽轨迹模型&lt;/a>; &lt;br>; &lt;em>;Philipp Wu&lt;/em>;、&lt;em>;Arjun Majumdar&lt;/em>;、&lt;em>;Kevin Stone&lt;/em>;、&lt;em>;Yixin Lin&lt;/em>;、&lt;strong>; &lt;em>;Igor Mordatch&lt;/em>;&lt;/strong>;、&lt;em>;Pieter Abbeel&lt;/ em>;、&lt;em>;Aravind Rajeswaran&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DnTVBs6zbz&amp;amp;name=pdf&quot;>;使用特征筛克服深度网络中的简单性偏差&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Rishabh Tiwari&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Pradeep Shenoy&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://openreview.net/attachment?id=KKaTURYcKG&amp;amp;name=pdf&quot;>;广告拍卖中福利最大化点击率预测的成对排名损失&lt;/a>; &lt;br>; &lt;em>;吕博相&lt;/em>;、&lt;strong>; &lt;em>;冯哲&lt;/em>;&lt;/strong>;、&lt;em>;Zachary Robertson&lt;/em>;、&lt;strong>; &lt;em>;Sanmi Koyejo&lt;/em>;&lt;/strong >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=UTtYSDO1MK&amp;amp;name=pdf&quot;>;Faster Ford-Fulkerson 的预测流程&lt;/a>; &lt;br>; &lt;em>;Sami Davies&lt;/em>;、&lt;em>;Benjamin Moseley&lt;/em>;、&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yuyan Wang&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=SVCYSBgFIr&amp;amp;name=pdf&quot;>;多语言神经机器翻译的缩放法则&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Patrick Fernandes&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Behrooz Ghorbani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Markus Freitag&lt;/ em>;&lt;/strong>;、&lt;strong>; Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=msZrQQAlBA&amp;amp;name=pdf&quot;>;用于时间序列结构发现的顺序蒙特卡罗学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Feras Saad&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Brian Patton&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Matthew Douglas H offman&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rif A. Saurous&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vikash Mansinghka&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XqyXhjVRxR&amp;amp;name=pdf&quot;>;随机梯度成功实现强盗&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;梅金城&lt;/em>; &lt;/strong>;, &lt;em>;Zixin Zhu&lt;/em>;,&lt;strong>; &lt;em>;Bo Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;, &lt;em>;Csaba Szepesvari&lt;/em>;,&lt;strong>; &lt;em>;Dale Schuurmans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nm4NwFfp7a&quot;>;基于子集的实例私人估计中的最优性&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Travis Dick&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alex Kulesza&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;孙子腾&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zvCSNsoyKW&amp;amp;name=pdf &quot;>;机器翻译的小样本学习的不合理有效性&lt;/a>; &lt;br>; &lt;em>;Xavier Garcia&lt;/em>;、&lt;em>;Yamini Bansal&lt;/em>;、&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔治·福斯特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Maxim Krikun&lt;/em>;&lt;/strong>;、&lt;em>;Melvin Johnson&lt;/em>;、&lt;em>;Orhan Firat&lt;/em>;&lt; br />; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21552&quot;>;视觉自我监督学习：从研究进展到最佳实践&lt;/a>; &lt;br>; &lt;em>;X inlei Chen&lt;/em>;、&lt;em>;Ishan Misra&lt;/em>;、&lt;em>;Randall Balestriero&lt;/em>;、&lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;、&lt;em>;Christoph Feichtenhofer&lt;/em>;、&lt;em>;Mark Ibrahim&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21560&quot;>;如何 DP-fy ML ：差异隐私机器学习实用教程&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/05/making-ml-models- Differentially-private.html&quot;>;博文&lt;/a>;）&lt;br>;&lt;strong>;&lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;郑旭&lt;/em>;&lt;/strong>;&lt;/p >; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21558&quot;>;神经网络泛化理论的最新进展&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;马腾宇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alex Damian&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h 2>;世博日研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;Tensorflow 中的图神经网络：实用指南&lt;/a>; &lt;br>; 研讨会组织者包括：&lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anton Tsitsulin &lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jonathan Halcrow&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 赞助的亲和力研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.latinxinai.org/ic ml-2023&quot;>;AI 中的拉丁语&lt;/a>; (LAXAI) &lt;br>;白金赞助商&lt;br>;主题演讲者：&lt;em>; &lt;strong>;Monica Ribero&lt;/strong>;&lt;/em>; &lt;br>; 小组成员：&lt;strong>;&lt;em>;姚勤&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?au因此er=0&quot;>;机器学习中的女性&lt;/a>; (WiML) &lt;br>;白金赞助商&lt;br>;小组成员：&lt;strong>;&lt;em>;姚勤&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://fl-icml2023.github .io/&quot;>;联邦学习与分析实践：算法、系统、应用程序和机遇&lt;/a>; &lt;br>; 组织者：&lt;strong>;&lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;徐峥&lt;/em>;&lt;/strong>; &lt;br>;演讲者：&lt;strong>;&lt;em>;Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/imlh2023/home? authuser=1&quot;>;医疗保健中的可解释机器学习&lt;/a>; (IMLH) &lt;br>; 组织者：&lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://klr-icml2023.github.io/&quot;>;数据驱动学习时代的知识和逻辑推理&lt;/a>; &lt;br>; 组织者：&lt;strong>;&lt;em>;Beliz Günel&lt;/em>;&lt;/strong>;&lt; br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mfpl-icml-2023&quot;>;基于偏好的学习的方方面面&lt;/a>; (MFPL) &lt;br>; 组织者：&lt;strong>;&lt;em>;Robert Busa-Fekete&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// syns-ml.github.io/2023/&quot;>;科学和机器学习建模的协同作用&lt;/a>; (SynS &amp;amp; ML) &lt;br>; 演讲者：&lt;strong>;&lt;em>;Sercan Arik&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://tomworkshop.github.io/&quot;>;沟通主体的心智理论&lt;/a>; &lt;br>; 组织者：&lt;strong>;&lt;em>;周培&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/aihci/home&quot;>;Arti官方情报与人机交互&lt;/a>; &lt;br>; 主办方：&lt;em>; &lt;strong>;杨力&lt;/strong>;&lt;/em>;、&lt;strong>; &lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://dmlr.ai/&quot;>;以数据为中心的机器学习研究&lt;/a>; (DMLR) &lt;br>; 主办方：&lt;strong>;&lt;em>;Alicia Parrish&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; >; &lt;br>; 演讲者：&lt;strong>;&lt;em>;Peter Mattson&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://neuralcompression.github.io/workshop23&quot;>;神经压缩：从信息理论到应用&lt;/a>; &lt;br>; 演讲者：&lt;strong>;&lt;em>;Johannes Ballé&lt;/em>;&lt;/strong>; &lt;br>; 小组成员：&lt;strong>;&lt;em>;George Toderici&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/teach-icml-23/home&quot;>;神经对话 AI 研讨会 - TEACH（值得信赖、增强型、适应性强、有能力且以人为本）聊天机器人还剩下什么？&lt;/a>; &lt;br>; 组织者：&lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view /scis-workshop-23&quot;>;虚假相关性、不变性和稳定性&lt;/a>; (SCIS) &lt;br>; 组织者：&lt;strong>;&lt;em>;Amir Feder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2519516457542613363/comments/default&quot; rel=&quot;replies&quot; title=&quot;帖子评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-icml-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/2519516457542613363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-icml-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICML 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:缩略图高度=“72”网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4nvXkj-FzG ykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s72-c/Google-ICML-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail >;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4421751509192561388&lt;/id>;&lt;已发布>;2023-07-20T09:22:00.002-07:00&lt;/已发布>;&lt;updated>;2023-07-20T15:31:26.737-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT要点&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;利用社会背景知识促进人工智能的负责任应用&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部技术项目经理、社会情境理解工具和解决方案 (SCOUTS) 主管 Donald Martin, Jr.&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng 5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUla5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh8 26JMQ2KtD9nu1vu/s1100/scouts.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人工智能相关产品和技术是在&lt;em>;社会背景&lt;/em>;中构建和部署的：即社会、文化、历史、政治和经济环境的动态且复杂的集合。由于社会环境本质上是动态的、复杂的、非线性的、有争议的、主观的和高度定性的，因此将它们转化为主导标准机器学习 (ML) 方法和负责任的人工智能产品开发实践的定量表示、方法和实践具有挑战性。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;人工智能产品开发的第一阶段是&lt;em>;问题理解&lt;/em>;，这个阶段对于如何为机器学习系统制定问题（例如，提高癌症筛查的可用性和准确性）以解决以及许多其他下游决策（例如数据集和机器学习架构选择）具有巨大影响。当产品运行的社会环境没有得到充分阐明，无法形成强有力的问题理解时，最终的机器学习解决方案可能会很脆弱，甚至会传播不公平的偏见。 &lt;/p>; &lt;p>; 当人工智能产品开发人员缺乏在开发过程中有效理解和考虑社会背景所需的知识和工具时，他们往往会将其抽象化。这种抽象使他们对他们寻求解决的问题有一个浅层的、定量的理解，而产品用户和社会利益相关者——他们最接近这些问题并嵌入相关的社会背景——往往对这些相同的问题有深刻的定性理解。这种理解复杂问题的定性-定量差异将产品用户和社会与开发人员区分开来，这就是我们所说的“问题理解鸿沟”。 &lt;/p>; &lt;p>; 这种鸿沟在现实世界中产生了影响：例如，它是&lt;a href=&quot;https://www.science.org/doi/10.1126/science.aax2342&quot;>;广泛使用的医疗算法发现的种族偏见&lt;/a>;的根本原因，该算法旨在解决为特殊项目选择具有最复杂医疗需求的患者的问题。对算法运行的社会背景的不完全理解导致系统设计者对关键问题因素形成了错误且过于简单化的因果理论。关键的社会结构因素，包括缺乏医疗保健、对医疗保健系统缺乏信任以及由于人为偏见而导致的诊断不足，都被排除在外，而医疗保健支出则被强调为复杂健康需求的预测因素。为了负责任地弥合问题理解鸿沟，人工智能产品开发人员需要工具，将经过社区验证的、有关复杂社会问题的社会背景的结构化知识放在触手可及的地方——从问题理解开始，贯穿整个产品开发生命周期。为此，&lt;a href=&quot;https://sites.research.google/scouts/&quot;>;社会情境理解工具和解决方案&lt;/a>; (SCOUTS) 是 Google 研究院&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;负责任的人工智能和以人为本的技术&lt;/a>; (RAI-HCT) 团队的一部分，是一个专门的研究团队，致力于“为人们提供可扩展、值得信赖的社会情境知识，帮助他们实现负责任、强大的人工智能并解决世界上最复杂的社会问题。” SCOUTS 的动力来自于阐明社会背景的重大挑战，它进行创新的基础和应用研究，以产生结构化的社会背景知识，并将其整合到人工智能相关产品开发生命周期的所有阶段。去年，我们&lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;宣布&lt;/a>;，&lt;a href=&quot;https://jigsaw.google.com/&quot;>;Jigsaw&lt;/a>;是 Google 的孵化器，用于构建探索开放社会威胁解决方案的技术，在模型开发的数据准备和评估阶段，利用我们的结构化社会背景知识方法，缓解其广泛使用的 &lt;a href=&quot;https://perspectiveapi.com/&quot;>;Perspective API&lt;/a>; 毒性分类器的规模偏差。展望未来，SCOUTS 的研究议程重点关注人工智能相关产品开发的问题理解阶段，目标是弥合问题理解鸿沟。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;弥合人工智能问题理解鸿沟&lt;/h2>; &lt;p>;弥合人工智能问题理解鸿沟需要两个关键要素：1）用于组织结构化社会背景知识的参考框架；2）参与式、非提取性方法，以引出有关复杂问题的社区专业知识并将其表示为结构化知识。 SCOUTS 在这两个领域都发表了创新研究。&lt;/p>; &lt;br>; &lt;video autoplay=&quot;&quot;loop=&quot;&quot; muted=&quot;&quot;playsinline=&quot;&quot; style=&quot;margin-left: 0%; margin-right: 0%;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://sites.research.google/scouts/videos/scouts_pull_intro.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;div style=&quot;line-height: 80%;&quot;>; &lt;br />; &lt;/div>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right : auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;理解问题的鸿沟说明。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;社会背景参考框架&lt;/h3>; &lt;p>; 产生结构化知识的一个基本要素是创建组织结构的分类法。 SCOUTS 与其他 RAI-HCT 团队（&lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;TasC&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2023/03/responsible-ai-at-google-research.html&quot;>;Impact Lab&lt;/a>;）、&lt;a href=&quot;https://www.deepmind.com/&quot;>;Google Deep 合作Mind&lt;/a>; 和外部系统动力学专家&lt;a href=&quot;https://arxiv.org/abs/2006.09663&quot;>;为社会背景开发分类参考框架&lt;/a>;。为了应对社会背景的复杂性、动态性和适应性，我们利用复杂适应性系统（CAS）理论提出了一种用于组织社会背景知识的高级分类模型。该模型精确指出了社会环境的三个关键要素以及将它们结合在一起的动态反馈循环：代理、戒律和工件。 &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;代理人&lt;/em>;：可以是个人或机构。 &lt;/li>;&lt;li>;&lt;em>;戒律&lt;/em>;：约束和驱动主体行为的先入之见，包括信仰、价值观、刻板印象和偏见。基本规则的一个例子是“所有篮球运动员都超过 6 英尺高”。这种限制性假设可能会导致无法识别身材较小的篮球运动员。 &lt;/li>;&lt;li>;&lt;em>;人工制品&lt;/em>;：主体行为产生多种人工制品，包括语言、数据、技术、社会问题和产品。 &lt;/li>; &lt;/ul>; &lt;p>; 这些实体之间的关系是动态且复杂的。我们的工作假设戒律是社会环境中最关键的元素，并且我们强调&lt;em>;人们感知到的问题&lt;/em>;以及&lt;em>;他们所持有的关于这些问题为何存在的因果理论&lt;/em>;作为特别有影响力的戒律，这些戒律是理解社会环境的核心。例如，在前面描述的医疗算法中存在种族偏见的情况下，设计者所持有的因果理论规则是，复杂的健康问题将导致所有人群的医疗支出增加。 That incorrect precept directly led to the choice of healthcare spending as the proxy variable for the model to predict complex healthcare need, which in turn led to the model being biased against Black patients who, due to societal factors such as lack of access to healthcare and underdiagnosis due to bias on average, do not always spend more on healthcare when they have complex healthcare needs. A key open question is how can we ethically and equitably elicit causal theories from the people and communities who are most proximate to problems of inequity and transform them into useful structured knowledge? &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s1600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrative version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s1600/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Taxonomic version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Working with communities to foster the responsible application of AI to healthcare&lt;/h3>; &lt;p>; Since its inception, SCOUTS has worked&lt;a href=&quot;https://accelerate.withgoogle.com/stories/exploring-systems-dynamics-inclusive-ml-and-societal-impact-meet-googlers-donald-martin-and-jamaal-sebastian-barnes&quot;>; to build capacity&lt;/a>; in historically marginalized communities to articulate the broader societal context of the complex problems that matter to them using a practice called community based system dynamics (CBSD). &lt;a href=&quot;https://en.wikipedia.org/wiki/System_dynamics&quot;>;System dynamics&lt;/a>; (SD) is a methodology for articulating causal theories about complex problems, both &lt;em>;qualitatively&lt;strong>; &lt;/strong>;&lt;/em>;as causal loop and &lt;a href=&quot;https://online.visual-paradigm.com/knowledge/business-design/what-is-stock-and-flow-diagram/&quot;>;stock and flow diagrams&lt;/a>; (CLDs and SFDs, respectively) and &lt;em>;quantitatively&lt;/em>; as simulation models. The inherent support of visual qualitative tools, quantitative methods, and collaborative model building makes it an ideal ingredient for bridging the problem understanding chasm. CBSD is a &lt;a href=&quot;https://medium.com/people-ai-research/qa-donald-martin-on-community-based-system-dynamics-and-machine-learning-3fa21e42c680&quot;>;community-based, participatory variant of SD&lt;/a>; specifically focused on building capacity within communities to collaboratively describe and model the problems they face as causal theories, directly without intermediaries. With CBSD we&#39;ve witnessed community groups learn the basics and begin drawing CLDs within 2 hours. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s1147/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;863&quot; data-original-width=&quot;1147&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community members learning system dynamics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There is a huge potential for AI to &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9955430/#:~:text=AI%20algorithms%20can%20analyze%20medical,diseases%20more%20accurately%20and%20quickly.&quot;>;improve medical diagnosis&lt;/a>;. But the safety, equity, and reliability of AI-related health diagnostic algorithms depends on diverse and balanced training datasets. An open challenge in the health diagnostic space is the dearth of training sample data from historically marginalized groups. SCOUTS collaborated with the &lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community and CBSD experts to produce &lt;a href=&quot;https://arxiv.org/abs/2305.13485&quot;>;qualitative and quantitative causal theories&lt;/a>; for the data gap problem. The theories include critical factors that make up the broader societal context surrounding health diagnostics, including cultural memory of death and trust in medical care. &lt;/p>; &lt;p>; The figure below depicts the causal theory generated during the collaboration described above as a CLD. It hypothesizes that trust in medical care influences all parts of this complex system and is the key lever for increasing screening, which in turn generates data to overcome the data diversity gap. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s1024/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;835&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s1072/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;561&quot; data-original-width=&quot;1072&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Causal loop diagram of the health diagnostics data gap&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; These community-sourced causal theories are a first step to bridge the problem understanding chasm with trustworthy societal context knowledge. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; As discussed in this blog, the problem understanding chasm is a critical open challenge in responsible AI. SCOUTS conducts exploratory and applied research in collaboration with other teams within Google Research, external community, and academic partners across multiple disciplines to make meaningful progress solving it. Going forward our work will focus on three key elements, guided by our &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;: &lt;/p>; &lt;ol>; &lt;li>;Increase awareness and understanding of the problem understanding chasm and its implications through talks, publications, and training. &lt;/li>;&lt;li>;Conduct foundational and applied research for representing and integrating societal context knowledge into AI product development tools and workflows, from conception to monitoring, evaluation and adaptation. &lt;/li>;&lt;li>;Apply community-based causal modeling methods to the AI health equity domain to realize impact and build society&#39;s and Google&#39;s capability to produce and leverage global-scale societal context knowledge to realize responsible AI. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s960/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SCOUTS flywheel for bridging the problem understanding chasm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;Thank you to John Guilyard for graphics development, everyone in SCOUTS, and all of our collaborators and sponsors.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4421751509192561388/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html&quot; rel=&quot;alternate&quot; title=&quot;Using societal context knowledge to foster the responsible application of AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s72-c/scouts.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8123471024413959829&lt;/id>;&lt;published>;2023-07-18T13:15:00.000-07:00&lt;/published>;&lt;updated>;2023-07-18T13:15:15.633-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Self-Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SimPer: Simple self-supervised learning of periodic targets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Daniel McDuff, Staff Research Scientist, and Yuzhe Yang, Student Researcher, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s320/SimPer%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; Learning from periodic data (signals that repeat, such as a heart beat or the daily temperature changes on Earth&#39;s surface) is crucial for many real-world applications, from &lt;a href=&quot;https://cloud.google.com/blog/topics/sustainability/weather-prediction-with-ai&quot;>;monitoring weather systems&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/take-pulse-health-and-wellness-your-phone/&quot;>;detecting vital signs&lt;/a>;. For example, in the environmental remote sensing domain, periodic learning is often needed to enable nowcasting of environmental changes, such as &lt;a href=&quot;https://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html&quot;>;precipitation patterns or land surface temperature&lt;/a>;. In the health domain, learning from video measurement has shown to extract (quasi-)periodic vital signs such as &lt;a href=&quot;https://www.ahajournals.org/doi/full/10.1161/JAHA.118.008585&quot;>;atrial fibrillation&lt;/a>; and &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9298820&quot;>;sleep apnea episodes&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Approaches like &lt;a href=&quot;https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html&quot;>;RepNet&lt;/a>; highlight the importance of these types of tasks, and present a solution that recognizes repetitive activities within a single video. However, these are supervised approaches that require a significant amount of data to capture repetitive activities, all labeled to indicate the number of times an action was repeated. Labeling such data is often challenging and resource-intensive, requiring researchers to manually capture gold-standard temporal measurements that are synchronized with the modality of interest (eg, video or satellite imagery). &lt;/p>; &lt;p>; Alternatively, self-supervised learning (SSL) methods (eg, &lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&quot;>;SimCLR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;), which leverage a large amount of unlabeled data to learn representations that capture periodic or quasi-periodic temporal dynamics, have demonstrated success in &lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j.html&quot;>;solving classification tasks&lt;/a>;. However, they overlook the intrinsic periodicity (ie, the ability to identify if a frame is part of a periodic process) in data and fail to learn robust representations that capture periodic or frequency attributes. This is because periodic learning exhibits characteristics that are distinct from prevailing learning tasks. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s1338/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1338&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Feature similarity is different in the context of periodic representations as compared to static features (eg, images). For example, videos that are offset by short time delays or are reversed should be similar to the original sample, whereas videos that have been upsampled or downsampled by a factor &lt;em>;x&lt;/em>; should be different from the original sample by a factor of &lt;em>;x&lt;/em>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To address these challenges, in “&lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;SimPer: Simple Self-Supervised Learning of Periodic Targets&lt;/a>;”, published at the eleventh &lt;a href=&quot;https://iclr.cc/&quot;>;International Conference on Learning Representations&lt;/a>; (ICLR 2023), we introduced a self-supervised contrastive framework for learning periodic information in data. Specifically, SimPer leverages the temporal properties of periodic targets using &lt;em>;temporal self-contrastive learning&lt;/em>;, where positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the &lt;em>;same&lt;/em>; input instance. We propose &lt;em>;periodic feature similarity&lt;/em>; that explicitly defines how to measure similarity in the context of periodic learning. Moreover, we design a generalized contrastive loss&lt;em>; &lt;/em>;that extends the classic &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE loss&lt;/a>; to a soft regression variant that enables contrasting over continuous labels (frequency). Next, we demonstrate that SimPer effectively learns period feature representations compared to state-of-the-art SSL methods, highlighting its intriguing properties including better data efficiency, robustness to spurious correlations, and generalization to distribution shifts. Finally, we are excited to release the &lt;a href=&quot;https://github.com/YyzHarry/SimPer&quot;>;SimPer code repo&lt;/a>; with the research community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The SimPer framework&lt;/h2>; &lt;p>; SimPer introduces a temporal self-contrastive learning framework. Positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the same input instance. For temporal video examples, periodicity-invariant changes are cropping, rotation or flipping, whereas periodicity-variant changes involve increasing or decreasing the speed of a video. &lt;/p>; &lt;p>; To explicitly define how to measure similarity in the context of periodic learning, SimPer proposes periodic feature similarity. This construction allows us to formulate training as a contrastive learning task. A model can be trained with data without any labels and then fine-tuned if necessary to map the learned features to specific frequency values. &lt;/p>; &lt;p>; Given an input sequence &lt;em>;x&lt;/em>;, we know there&#39;s an underlying associated periodic signal. We then transform &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo- speed or frequency labels for the unlabeled input x. &lt;/p>; &lt;p>; Conventional &lt;a href=&quot;https://en.wikipedia.org/wiki/Similarity_measure&quot;>;similarity measures&lt;/a>; such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; emphasize strict proximity between two feature vectors, and are sensitive to index shifted features (which represent different time stamps), reversed features, and features with changed frequencies. In contrast, periodic feature similarity should be high for samples with small temporal shifts and or reversed indexes, while capturing a continuous similarity change when the feature frequency varies. This can be achieved via a similarity metric in the frequency domain, such as the distance between two &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;>;Fourier transforms&lt;/a>;. &lt;/p>; &lt;p>; To harness the intrinsic continuity of augmented samples in the frequency domain, SimPer designs a generalized contrastive loss that extends the classic &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE&lt;/a>; loss to a soft regression variant that enables contrasting over continuous labels (frequency). This makes it suitable for regression tasks, where the goal is to recover a continuous signal, such as a heart beat. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;361&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SimPer constructs negative views of data through transformations in the frequency domain. The input sequence &lt;em>;x&lt;/em>; has an underlying associated periodic signal. SimPer transforms &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo speed or frequency labels for unlabeled input &lt;em>;x&lt;/em>; (periodicity-variant augmentations &lt;em>;τ&lt;/em>;). SimPer takes transformations that do not change the identity of the input and defines these as periodicity-invariant augmentations &lt;em>;σ&lt;/em>;, thus creating different positive views of the sample. Then, it sends these augmented views to the encoder &lt;em>;f&lt;/em>;,&lt;em>; &lt;/em>;which extracts corresponding features. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate SimPer&#39;s performance, we benchmarked it against state-of-the-art SSL schemes (eg, &lt;a href=&quot;https://github.com/google-research/simclr&quot;>;SimCLR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf&quot;>;BYOL&lt;/a>;, &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Spatiotemporal_Contrastive_Video_Representation_Learning_CVPR_2021_paper.html&quot;>;CVRL&lt;/a>;) on a set of six diverse periodic learning datasets for common real-world tasks in human behavior analysis, environmental remote sensing, and healthcare. Specifically, below we present results on heart rate measurement and exercise repetition counting from video. The results show that SimPer outperforms the state-of-the-art SSL schemes across all six datasets, highlighting its superior performance in terms of data efficiency, robustness to spurious correlations, and generalization to unseen targets. &lt;/p>; &lt;p>; Here we show quantitative results on two representative datasets using SimPer pre-trained using various SSL methods and fine-tuned on the labeled data. First, we pre-train SimPer using the &lt;a href=&quot;https://sites.google.com/corp/view/ybenezeth/ubfcrppg&quot;>;Univ. Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy&lt;/a>; (UBFC) dataset, a human &lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram#Remote_photoplethysmography&quot;>;photoplethysmography&lt;/a>; and heart rate prediction dataset, and compare its performance to state-of-the-art SSL methods. We observe that SimPer outperforms SimCLR, MoCo v2, BYOL, and CVRL methods. The results on the human action counting dataset, &lt;a href=&quot;https://sites.google.com/corp/view/repnet&quot;>;Countix&lt;/a>;, further confirm the benefits of SimPer over others methods as it notably outperforms the supervised baseline. For the feature evaluation results and performance on other datasets, please refer to the &lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results of SimCLR, MoCo v2, BYOL, CVRL and SimPer on the Univ. Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy (UBFC) and Countix datasets. Heart rate and repetition count performance is reported as &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;mean absolute error&lt;/a>; (MAE).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and applications&lt;/h2>; &lt;p>; We present SimPer, a self-supervised contrastive framework for learning periodic information in data. We demonstrate that by combining a temporal self-contrastive learning framework, periodicity-invariant and periodicity-variant augmentations, and continuous periodic feature similarity, SimPer provides an intuitive and flexible approach for learning strong feature representations for periodic signals. Moreover, SimPer can be applied to various fields, ranging from environmental remote sensing to healthcare. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Yuzhe Yang, Xin Liu, Ming-Zher Poh, Jiang Wu, Silviu Borac, and Dina Katabi for their contributions to this work. &lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8123471024413959829/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html&quot; rel=&quot;alternate&quot; title=&quot;SimPer: Simple self-supervised learning of periodic targets&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s72-c/SimPer%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2013323302512835157&lt;/id>;&lt;published>;2023-07-13T14:01:00.001-07:00&lt;/published>;&lt;updated>;2023-07-13T14:01:18.428-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Symbol tuning improves in-context learning in language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jerry Wei, Student Researcher, and Denny Zhou, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s1200/SymbolTuning.png&quot; style=&quot;display: none;&quot; />; &lt;p>; A key feature of human intelligence is that humans can learn to perform new tasks by reasoning using only a few examples. Scaling up language models has unlocked a range of new applications and paradigms in machine learning, including the ability to perform challenging reasoning tasks via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>;. Language models, however, are still sensitive to the way that prompts are given, indicating that they are not reasoning in a robust manner. For instance, language models often require heavy prompt engineering or phrasing tasks as instructions, and they exhibit unexpected behaviors such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;performance on tasks being unaffected even when shown incorrect labels&lt;/a>;. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.08298&quot;>;Symbol tuning improves in-context learning in language models&lt;/a>;”, we propose a simple fine-tuning procedure that we call &lt;em>;symbol tuning&lt;/em>;, which can improve in-context learning by emphasizing input–label mappings. We experiment with symbol tuning across &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Flan-PaLM&lt;/a>; models and observe benefits across various settings. &lt;/p>; &lt;ul>; &lt;li>;Symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. &lt;/li>;&lt;li>;Symbol-tuned models are much stronger at algorithmic reasoning tasks. &lt;/li>;&lt;li>;Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior knowledge. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s1035/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;663&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of symbol tuning, where models are fine-tuned on tasks where natural language labels are replaced with arbitrary symbols. Symbol tuning relies on the intuition that when instruction and relevant labels are not available, models must use in-context examples to learn the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Motivation&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a common fine-tuning method that has been shown to improve performance and allow models to better follow in-context examples. One shortcoming, however, is that models are not forced to learn to use the examples because the task is redundantly defined in the evaluation example via instructions and natural language labels. For example, on the left in the figure above, although the examples can help the model understand the task (sentiment analysis), they are not strictly necessary since the model could ignore the examples and just read the instruction that indicates what the task is. &lt;/p>; &lt;p>; In symbol tuning, the model is fine-tuned on examples where the instructions are removed and natural language labels are replaced with semantically-unrelated labels (eg, “Foo,” “Bar,” etc.). In this setup, the task is unclear without looking at the in-context examples. For example, on the right in the figure above, multiple in-context examples would be needed to figure out the task. Because symbol tuning teaches the model to reason over the in-context examples, symbol-tuned models should have better performance on tasks that require reasoning between in-context examples and their labels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s1035/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;237&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Datasets and task types used for symbol tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Symbol-tuning procedure&lt;/h2>; &lt;p>; We selected 22 publicly-available &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) datasets that we use for our symbol-tuning procedure. These tasks have been widely used in the past, and we only chose classification-type tasks since our method requires discrete labels. We then remap labels to a random label from a set of ~30K arbitrary labels selected from one of three categories: integers, character combinations, and words. &lt;/p>; &lt;p>; For our experiments, we symbol tune &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, the instruction-tuned variants of &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;. We use three different sizes of Flan-PaLM models: Flan-PaLM-8B, Flan-PaLM-62B, and Flan-PaLM-540B. We also tested &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;Flan-cont-PaLM-62B&lt;/a>; (Flan-PaLM-62B at 1.3T tokens instead of 780B tokens), which we abbreviate as 62B-c. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s861/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;168&quot; data-original-width=&quot;861&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a set of ∼300K arbitrary symbols from three categories (integers, character combinations, and words). ∼30K symbols are used during tuning and the rest are held out for evaluation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experimental setup&lt;/h2>; &lt;p>; We want to evaluate a model&#39;s ability to perform unseen tasks, so we cannot evaluate on tasks used in symbol tuning (22 datasets) or used during instruction tuning (1.8K tasks). Hence, we choose 11 NLP datasets that were not used during fine-tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;In-context learning&lt;/h2>; &lt;p>; In the symbol-tuning procedure, models must learn to reason with in-context examples in order to successfully perform tasks because prompts are modified to ensure that tasks cannot simply be learned from relevant labels or instructions. Symbol-tuned models should perform better in settings where tasks are unclear and require reasoning between in-context examples and their labels. To explore these settings, we define four in-context learning settings that vary the amount of reasoning required between inputs and labels in order to learn the task (based on the availability of instructions/relevant labels) &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s936/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;936&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depending on the availability of instructions and relevant natural language labels, models may need to do varying amounts of reasoning with in-context examples. When these features are not available, models must reason with the given in-context examples to successfully perform the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Symbol tuning improves performance across all settings for models 62B and larger, with small improvements in settings with relevant natural language labels (+0.8% to +4.2%) and substantial improvements in settings without relevant natural language labels (+5.5% to +15.5%). Strikingly, when relevant labels are unavailable, symbol-tuned Flan-PaLM-8B outperforms FlanPaLM-62B, and symbol-tuned Flan-PaLM-62B outperforms Flan-PaLM-540B. This performance difference suggests that symbol tuning can allow much smaller models to perform as well as large models on these tasks (effectively saving ∼10X inference compute). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s900/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;495&quot; data-original-width=&quot;900&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Large-enough symbol-tuned models are better at in-context learning than baselines, especially in settings where relevant labels are not available. Performance is shown as average model accuracy (%) across eleven tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Algorithmic reasoning&lt;/h2>; &lt;p>; We also experiment on algorithmic reasoning tasks from &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;. There are two main groups of tasks: 1) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/list_functions&quot;>;List functions&lt;/a>; — identify a transformation function (eg, remove the last element in a list) between input and output lists containing non-negative integers; and 2) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/simp_turing_concept&quot;>;simple turing concepts&lt;/a>; — reason with binary strings to learn the concept that maps an input to an output (eg, swapping 0s and 1s in a string). &lt;/p>; &lt;p>; On the list function and simple turing concept tasks, symbol tuning results in an average performance improvement of 18.2% and 15.3%, respectively. Additionally, Flan-cont-PaLM-62B with symbol tuning outperforms Flan-PaLM-540B on the list function tasks on average, which is equivalent to a ∼10x reduction in inference compute. These improvements suggest that symbol tuning strengthens the model&#39;s ability to learn in-context for unseen task types, as symbol tuning did not include any algorithmic data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s908/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;496&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models achieve higher performance on list function tasks and simple turing concept tasks. (A–E): categories of list functions tasks. (F): simple turing concepts task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In the flipped-label experiment, labels of in-context and evaluation examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as “negative sentiment”), thereby allowing us to study whether models can override prior knowledge. &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;Previous work&lt;/a>; has shown that while pre-trained models (without instruction tuning) can, to some extent, follow flipped labels presented in-context, instruction tuning degraded this ability. &lt;/p>; &lt;p>; We see that there is a similar trend across all model sizes — symbol-tuned models are much more capable of following flipped labels than instruction-tuned models. We found that after symbol tuning, Flan-PaLM-8B sees an average improvement across all datasets of 26.5%, Flan-PaLM-62B sees an improvement of 33.7%, and Flan-PaLM-540B sees an improvement of 34.0%. Additionally, symbol-tuned models achieve similar or better than average performance as pre-training–only models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s914/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;914&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models are much better at following flipped labels presented in-context than instruction-tuned models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented symbol tuning, a new method of tuning models on tasks where natural language labels are remapped to arbitrary symbols. Symbol tuning is based off of the intuition that when models cannot use instructions or relevant labels to determine a presented task, it must do so by instead learning from in-context examples. We tuned four language models using our symbol-tuning procedure, utilizing a tuning mixture of 22 datasets and approximately 30K arbitrary symbols as labels. &lt;/p>; &lt;p>; We first showed that symbol tuning improves performance on unseen in-context learning tasks, especially when prompts do not contain instructions or relevant labels. We also found that symbol-tuned models were much better at algorithmic reasoning tasks, despite the lack of numerical or algorithmic data in the symbol-tuning procedure. Finally, in an in-context learning setting where inputs have flipped labels, symbol tuning (for some datasets) restores the ability to follow flipped labels that was lost during instruction tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; Through symbol tuning, we aim to increase the degree to which models can examine and learn from input–label mappings during in-context learning. We hope that our results encourage further work towards improving language models&#39; ability to reason over symbols presented in-context. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. This work was conducted by Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. We would like to thank our colleagues at Google Research and Google DeepMind for their advice and helpful discussions.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2013323302512835157/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/symbol-tuning-improves-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/symbol-tuning-improves-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Symbol tuning improves in-context learning in language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s72-c/SymbolTuning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8984134419460793359&lt;/id>;&lt;published>;2023-07-11T10:00:00.010-07:00&lt;/published>;&lt;updated>;2023-07-11T10:44:15.111-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;An open-source gymnasium for machine learning assisted computer architecture design&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Amir Yazdanbakhsh, Research Scientist, and Vijay Janapa Reddi, Visiting Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s1200/ArchGym-animation2.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_architecture&quot;>;Computer Architecture&lt;/a>; research has a long history of developing simulators and tools to evaluate and shape the design of computer systems. For example, the &lt;a href=&quot;https://ieeexplore.ieee.org/iel5/2/21180/00982917.pdf?casa_token=M_ZAQmgCbKkAAAAA:Y9wFTB9OQBwzXXpw7kNbq4asdlCCHYRaq7qsqcRBpYyh6734aHmr57ll4Vb1_zcG5ukNvNONNQ&quot;>;SimpleScalar&lt;/a>; simulator was introduced in the late 1990s and allowed researchers to explore various &lt;a href=&quot;https://en.wikipedia.org/wiki/Microarchitecture&quot;>;microarchitectural&lt;/a>; ideas. Computer architecture simulators and tools, such as &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;gem5&lt;/a>;, &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>;, and many more have played a significant role in advancing computer architecture research. Since then, these shared resources and infrastructure have benefited industry and academia and have enabled researchers to systematically build on each other&#39;s work, leading to significant advances in the field. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Nonetheless, computer architecture research is evolving, with industry and academia turning towards machine learning (ML) optimization to meet stringent domain-specific requirements, such as &lt;a href=&quot;https://ai.googleblog.com/2021/02/machine-learning-for-computer.html&quot;>;ML for computer architecture&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2201.01863.pdf&quot;>;ML for TinyML acceleration&lt;/a>;,&amp;nbsp;&lt;a href=&quot;https://ai.googleblog.com/2022/03/offline-optimization-for-architecting.html&quot;>;DNN&lt;/a>; &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9804604&quot;>;accelerator&lt;/a>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3503222.3507767&quot;>;datapath optimization&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1394608.1382172?casa_token=3g46kAHeN0QAAAAA:5pKdYXamA_vstsO5LqA0_4rRy5o2Z46Ks-OJLDUe7ZSLtDfkaSS5i0zLfMe3Y7gAuY2bFMZ1yGOEMA&quot;>;memory controllers&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40&quot;>;power consumption&lt;/a>;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/7430287&quot;>;security&lt;/a>;, and &lt;a href=&quot;https://www.sigarch.org/tag/privacy-preserving-computing/&quot;>;privacy&lt;/a>;. Although prior work has demonstrated the benefits of ML in design optimization, the lack of strong, reproducible baselines hinders fair and objective comparison across different methods and poses several challenges to their deployment. To ensure steady progress, it is imperative to understand and tackle these challenges collectively. &lt;/p>; &lt;p>; To alleviate these challenges, in “&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3579371.3589049&quot;>;ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design&lt;/a>;”, accepted at &lt;a href=&quot;https://www.iscaconf.org/isca2023/program/&quot;>;ISCA 2023&lt;/a>;, we introduced ArchGym, which includes a variety of computer architecture simulators and ML algorithms. Enabled by ArchGym, our results indicate that with a sufficiently large number of samples, any of a diverse collection of ML algorithms are capable of finding the optimal set of architecture design parameters for each target problem; &lt;i>;no one solution is necessarily better than another&lt;/i>;. These results further indicate that selecting the optimal hyperparameters for a given ML algorithm is essential for finding the optimal architecture design, but choosing them is non-trivial. We &lt;a href=&quot;https://bit.ly/ArchGym&quot;>;release&lt;/a>; the code and dataset across multiple computer architecture simulations and ML algorithms.&lt;/p>; &lt;br />; &lt;h2>;Challenges in ML-assisted architecture research &lt;/h2>; &lt;p>; ML-assisted architecture research poses several challenges, including: &lt;/p>; &lt;ol>; &lt;li>;For a specific ML-assisted computer architecture problem (eg, finding an optimal solution for a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_random-access_memory&quot;>;DRAM&lt;/a>; controller) there is no systematic way to identify optimal ML algorithms or hyperparameters (eg, learning rate, warm-up steps, etc.). There is a wider range of ML and heuristic methods, from &lt;a href=&quot;https://arxiv.org/abs/2008.03639&quot;>;random walk&lt;/a>; to &lt;a href=&quot;http://incompleteideas.net/book/RLbook2020.pdf&quot;>;reinforcement learning&lt;/a>; (RL), that can be employed for &lt;a href=&quot;https://en.wikipedia.org/wiki/Design_space_exploration&quot;>;design space exploration&lt;/a>; (DSE). While these methods have shown noticeable performance improvement over their choice of baselines, it is not evident whether the improvements are because of the choice of optimization algorithms or hyperparameters.&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;Thus, to ensure reproducibility and facilitate widespread adoption of ML-aided architecture DSE, it is necessary to outline a systematic benchmarking methodology.&lt;/em>; &lt;br />;&lt;br />; &lt;/li>; &lt;li>;While computer architecture simulators have been the backbone of architectural innovations, there is an emerging need to address the trade-offs between accuracy, speed, and cost in architecture exploration. The accuracy and speed of performance estimation widely varies from one simulator to another, depending on the underlying modeling details (eg, &lt;a href=&quot;https://biblio.ugent.be/publication/2968322/file/6776727.pdf&quot;>;cycle&lt;/a>;-&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;accurate&lt;/a>; vs. &lt;a href=&quot;https://arxiv.org/abs/2210.03894&quot;>;ML&lt;/a>;-&lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;based&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1808.07412&quot;>;proxy&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1803.02329&quot;>;models&lt;/a>;). While analytical or ML-based proxy models are nimble by virtue of discarding low-level details, they generally suffer from high prediction error. Also, due to commercial licensing, there can be strict &lt;a href=&quot;https://ieeexplore.ieee.org/document/7945172&quot;>;limits on the number of runs collected from a simulator&lt;/a>;. Overall, these constraints exhibit distinct performance vs. sample efficiency trade-offs, affecting the choice of optimization algorithm for architecture exploration. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;It is challenging to delineate how to systematically compare the effectiveness of various ML algorithms under these constraints.&lt;/em>; &lt;br />; &lt;br />; &lt;/li>; &lt;li>;Finally, the landscape of ML algorithms is rapidly evolving and some ML algorithms need data to be useful. Additionally, rendering the outcome of DSE into meaningful artifacts such as datasets is critical for drawing insights about the design space. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;In this rapidly evolving ecosystem, it is consequential to ensure how to amortize the overhead of search algorithms for architecture exploration. It is not apparent, nor systematically studied how to leverage exploration data while being agnostic to the underlying search algorithm.&lt;/em>; &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;ArchGym design&lt;/h2>; &lt;p>; ArchGym addresses these challenges by providing a unified framework for evaluating different ML-based search algorithms fairly. It comprises two main components: 1) the ArchGym environment and 2) the ArchGym agent. The environment is an encapsulation of the architecture cost model — which includes latency, throughput, area, energy, etc., to determine the computational cost of running the workload, given a set of architectural parameters — paired with the target workload(s). The agent is an encapsulation of the ML algorithm used for the search and consists of hyperparameters and a guiding policy. The hyperparameters are intrinsic to the algorithm for which the model is to be optimized and can significantly influence performance. The policy, on the other hand, determines how the agent selects a parameter iteratively to optimize the target objective. &lt;/p>; &lt;p>; Notably, ArchGym also includes a standardized interface that connects these two components, while also saving the exploration data as the ArchGym Dataset. At its core, the interface entails three main signals: &lt;i>;hardware state&lt;/i>;, &lt;i>;hardware parameters&lt;/i>;, and &lt;i>;metrics&lt;/i>;. These signals are the bare minimum to establish a meaningful communication channel between the environment and the agent.&amp;nbsp;Using these signals, the agent observes the state of the hardware and suggests a set of hardware parameters to iteratively optimize a (user-defined) reward. The reward is a function of hardware performance metrics, such as performance, energy consumption, etc.&amp;nbsp;&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s1226/ArchGym-animation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;722&quot; data-original-width=&quot;1226&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s16000/ArchGym-animation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ArchGym comprises two main components: the ArchGym environment and the ArchGym agent. The ArchGym environment encapsulates the cost model and the agent is an abstraction of a policy and hyperparameters. With a standardized interface that connects these two components, ArchGym provides a unified framework for evaluating different ML-based search algorithms fairly while also saving the exploration data as the ArchGym Dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;ML algorithms could be equally favorable to meet user-defined target specifications&lt;/h2>; &lt;p>; Using ArchGym, we empirically demonstrate that across different optimization objectives and DSE problems, &lt;em>;at least one set of hyperparameters exists that results in the same hardware performance as other ML algorithms&lt;/em>;. A poorly selected (random selection) hyperparameter for the ML algorithm or its baseline can lead to a misleading conclusion that a particular family of ML algorithms is better than another. We show that with sufficient hyperparameter tuning, different search algorithms, even &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;random walk&lt;/a>; (RW), are able to identify the best possible reward. However, note that finding the right set of hyperparameters may require exhaustive search or even luck to make it competitive. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1158&quot; data-original-width=&quot;1999&quot; height=&quot;371&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/w640-h371/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With a sufficient number of samples, there exists at least one set of hyperparameters that results in the same performance across a range of search algorithms. Here the dashed line represents the maximum normalized reward. &lt;i>;Cloud-1&lt;/i>;, &lt;i>;cloud-2&lt;/i>;, &lt;i>;stream&lt;/i>;, and &lt;i>;random&lt;/i>; indicate four different memory traces for &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>; (DRAM subsystem design space exploration framework).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset construction and high-fidelity proxy model training&lt;/h2>; &lt;p>; Creating a unified interface using ArchGym also enables the creation of datasets that can be used to design better data-driven ML-based proxy architecture cost models to improve the speed of architecture simulation. To evaluate the benefits of datasets in building an ML model to approximate architecture cost, we leverage ArchGym&#39;s ability to log the data from each run from DRAMSys to create four dataset variants, each with a different number of data points. For each variant, we create two categories: (a) Diverse Dataset, which represents the data collected from different agents (&lt;a href=&quot;https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms&quot;>;ACO&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_algorithm&quot;>;GA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;RW&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions.&quot;>;BO&lt;/a>;), and (b) ACO only, which shows the data collected exclusively from the ACO agent, both of which are released along with ArchGym. We train a proxy model on each dataset using &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;>;random forest regression&lt;/a>; with the objective to predict the latency of designs for a &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAM simulator&lt;/a>;. Our results show that: &lt;/p>; &lt;ol>; &lt;li>;As we increase the dataset size, the average normalized &lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;root mean squared error&lt;/a>; (RMSE) slightly decreases. &lt;/li>;&lt;li>;However, as we introduce diversity in the dataset (eg, collecting data from different agents), we observe 9× to 42× lower RMSE across different dataset sizes. &lt;/li>; &lt;/ol>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/s1826/ArchGym2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1143&quot; data-original-width=&quot;1826&quot; height=&quot;401&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/w640-h401/ArchGym2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diverse dataset collection across different agents using ArchGym interface.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/s1803/ArchGym1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1803&quot; height=&quot;407&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/w640-h407/ArchGym1.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The impact of a diverse dataset and dataset size on the normalized RMSE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;The need for a community-driven ecosystem for ML-assisted architecture research&lt;/h2>; &lt;p>;While, ArchGym is an initial effort towards creating an open-source ecosystem that (1) connects a broad range of search algorithms to computer architecture simulators in an unified and easy-to-extend manner, (2) facilitates research in ML-assisted computer architecture, and (3) forms the scaffold to develop reproducible baselines, there are a lot of open challenges that need community-wide support. Below we outline some of the open challenges in ML-assisted architecture design. Addressing these challenges requires a well coordinated effort and a community driven ecosystem.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;969&quot; data-original-width=&quot;1999&quot; height=&quot;310&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/w640-h310/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Key challenges in ML-assisted architecture design.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We call this ecosystem &lt;a href=&quot;https://www.sigarch.org/architecture-2-0-why-computer-architects-need-a-data-centric-ai-gymnasium/&quot;>;Architecture 2.0&lt;/a>;.&amp;nbsp;We outline the key challenges and a vision for building an inclusive ecosystem of interdisciplinary researchers to tackle the long-standing open problems in applying ML for computer architecture research.&amp;nbsp;If you are interested in helping shape this ecosystem, please fill out the &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSfIYeSBoEi-DIHizPx4-FTEcZUSY_uUcKe0rdHC0tkCWp3Gag/viewform&quot;>;interest survey&lt;/a>;.&lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>;&lt;a href=&quot;https://bit.ly/ArchGym&quot;>;ArchGym&lt;/a>; is an open source gymnasium for ML architecture DSE and enables an standardized interface that can be readily extended to suit different use cases. Additionally, ArchGym enables fair and reproducible comparison between different ML algorithms and helps to establish stronger baselines for computer architecture research problems. &lt;/p>; &lt;p>; We invite the computer architecture community as well as the ML community to actively participate in the development of ArchGym. We believe that the creation of a gymnasium-type environment for computer architecture research would be a significant step forward in the field and provide a platform for researchers to use ML to accelerate research and lead to new and innovative designs. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;This blogpost is based on joint work with several co-authors at Google and Harvard University.&amp;nbsp;&lt;/i>;&lt;i>;We would like to acknowledge and highlight Srivatsan Krishnan (Harvard) who contributed several ideas to this project in collaboration with Shvetank Prakash (Harvard), Jason Jabbour (Harvard), Ikechukwu Uchendu (Harvard), Susobhan Ghosh (Harvard), Behzad Boroujerdian (Harvard), Daniel Richins (Harvard), Devashree Tripathy (Harvard), and Thierry Thambe (Harvard).&amp;nbsp; In addition, we would also like to thank James Laudon, Douglas Eck, Cliff Young, and Aleksandra Faust for their support, feedback, and motivation for this work. We would also like to thank John Guilyard for the animated figure used in this post. Amir Yazdanbakhsh is now a Research Scientist at Google DeepMind and Vijay Janapa Reddi is an Associate Professor at Harvard.&lt;/i>;&lt;/p>;&lt;div>;&lt;br />;&lt;/div>;&lt;br />;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8984134419460793359/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/an-open-source-gymnasium-for-computer.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/an-open-source-gymnasium-for-computer.html&quot; rel=&quot;alternate&quot; title=&quot;An open-source gymnasium for machine learning assisted computer architecture design&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s72-c/ArchGym-animation2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7093565002706757508&lt;/id>;&lt;published>;2023-07-10T06:02:00.005-07:00&lt;/published>;&lt;updated>;2023-07-21T13:24:14.452-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ACL 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s1040/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the &lt;a href=&quot;https://2023.aclweb.org/&quot;>;61st annual meeting&lt;/a>; of the &lt;a href=&quot;https://www.aclweb.org/&quot;>;Association for Computational Linguistics&lt;/a>; (ACL), a premier conference covering a broad spectrum of research areas that are concerned with computational approaches to natural language, is taking place in Vancouver, BC. As a leader in natural language processing and understanding, and a&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/sponsors/&quot;>;Diamond Level sponsor&lt;/a>;&amp;nbsp;of&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, Google will showcase the latest research in the field with over 50 publications, and active involvement in a variety of workshops and tutorials.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;If you&#39;re registered for ACL 2023, we hope that you&#39;ll visit the Google booth to learn more about the projects at Google that go into solving interesting problems for billions of people. You can also learn more about Google&#39;s participation below (Google affiliations in &lt;b>;bold&lt;/b>;).&lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Area chairs include: &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>; &lt;br />; Workshop chairs include: &lt;strong>;&lt;em>;Annie Louis&lt;/em>;&lt;/strong>; &lt;br />; Publication chairs include: &lt;strong>;&lt;em>;Lei Shu&lt;/em>;&lt;/strong>; &lt;br />; Program Committee includes: &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Spotlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09648.pdf&quot;>;NusaCrowd: Open Source Initiative for Indonesian NLP Resources&lt;/a>; &lt;br />; &lt;em>;Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Winata, Bryan Wilie, Fajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Muhammad Satrio Wicaksono, Ivan Parmonangan, Ika Alfina, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh Dhole, Arie Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Adilazuarda, Ryan Hadiwijaya, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Haryo Wibowo, Cuk Tho, Ichwanul Karo Karo, Tirana Fatyanosa, Ziwei Ji, Graham Neubig, Timothy Baldwin, &lt;strong>;Sebastian Ruder&lt;/strong>;, Pascale Fung, Herry Sujaini, Sakriani Sakti, Ayu Purwarianti&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2205.12680.pdf&quot;>;Optimizing Test-Time Query Representations for Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Mujeen Sung, Jungsoo Park, Jaewoo Kang, Danqi Chen, &lt;strong>;Jinhyuk Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10750.pdf&quot;>;PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition&lt;/a>; &lt;br />; &lt;em>;Sihao Chen*, &lt;strong>;Senaka Buthpitiya&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, Dan Roth, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02301.pdf&quot;>;Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes&lt;/a>; &lt;br />; &lt;em>;Cheng-Yu Hsieh*, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chih-Kuan Yeh&lt;/strong>;, &lt;strong>;Hootan Nakhost&lt;/strong>;, &lt;strong>;Yasuhisa Fujii&lt;/strong>;, Alex Ratner, Ranjay Krishna, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05110.pdf&quot;>;Large Language Models with Controllable Working Memory&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Daliang Li&lt;/b>;, &lt;b>;Ankit Singh Rawat&lt;/b>;, &lt;b>;Manzil Zaheer&lt;/b>;, &lt;b>;Xin Wang&lt;/b>;, &lt;b>;Michal Lukasik&lt;/b>;, &lt;b>;Andreas Veit&lt;/b>;, &lt;b>;Felix Yu&lt;/b>;,&lt;b>; Sanjiv Kumar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10791.pdf&quot;>;OpineSum: Entailment-Based Self-Training for Abstractive Opinion Summarization&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Annie Louis&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08775.pdf&quot;>;RISE: Leveraging Retrieval Techniques for Summarization Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Uthus&lt;/b>;, &lt;b>;Jianmo Ni&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b5b9bb4ec1e1d7416f88f8b3a1649d1235d747b8.pdf&quot;>;Follow the Leader(board) with Confidence: Estimating p-Values from a Single Test Set with Item and Response Variance&lt;/a>; &lt;br />; &lt;i>; Shira Wein*, Christopher Homan, &lt;strong>;Lora Aroyo&lt;/strong>;, &lt;strong>;Chris Welty&lt;/strong>;&lt;/i>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.02516.pdf&quot;>;SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives&lt;/a>; &lt;br />; &lt;i>;&lt;strong>;Fedor Moiseev&lt;/strong>;, &lt;strong>;Gustavo Hernandez Abrego&lt;/strong>;, &lt;strong>;Peter Dornbach&lt;/strong>;, &lt;strong>;Imed Zitouni&lt;/strong>;, &lt;strong>;Enrique Alfonseca&lt;/strong>;, &lt;strong>;Zhe Dong&lt;/strong>;&lt;/i>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10266.pdf&quot;>;Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM&#39;s Translation Capability&lt;/a>; &lt;br />; &lt;em>;Eleftheria Briakou, &lt;strong>;Colin Cherry&lt;/strong>;, &lt;strong>;George Foster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09102.pdf&quot;>;Prompting PaLM for Translation: Assessing Strategies and Performance&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Vilar&lt;/b>;, &lt;b>;Markus Freitag&lt;/b>;, &lt;b>;Colin Cherry&lt;/b>;, &lt;b>;Jiaming Luo&lt;/b>;, &lt;b>;Viresh Ratnakar&lt;/b>;, &lt;b>;George Foster&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.17525.pdf&quot;>;Query Refinement Prompts for Closed-Book Long-Form QA&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kellie Webster&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10381.pdf&quot;>;To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering&lt;/a>; &lt;br />; &lt;em>;Dheeru Dua*, &lt;strong>;Emma Strubell&lt;/strong>;, Sameer Singh, &lt;strong>;Pat Verga&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00193.pdf&quot;>;FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/02/frmt-benchmark-for-few-shot-region.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;b>;Parker Riley&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Jan A. Botha&lt;/b>;, &lt;b>;Xavier Garcia&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Jason Riesa&lt;/b>;, &lt;b>;Orhan Firat&lt;/b>;,&lt;b>; Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.00397.pdf&quot;>;Conditional Generation with a Question-Answering Blueprint&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kuzman Ganchev&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;, &lt;b>;Fantine Huot&lt;/b>;, &lt;b>;Anders Sandholm&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Mirella Lapata&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12142.pdf&quot;>;Coreference Resolution Through a Seq2Seq Transition-Based System&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Bernd Bohnet&lt;/b>;, &lt;b>;Chris Alberti&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00106.pdf&quot;>;Cross-Lingual Transfer with Language-Specific Subnetworks for Low-Resource Dependency Parsing&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni, &lt;strong>;Dan Garrette&lt;/strong>;, Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08054.pdf&quot;>;DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue&lt;/a>; &lt;br />; &lt;em>;William Held*, &lt;strong>;Christopher Hidey&lt;/strong>;, &lt;strong>;Fei Liu&lt;/strong>;, &lt;strong>;Eric Zhu&lt;/strong>;, &lt;strong>;Rahul Goel&lt;/strong>;, Diyi Yang, &lt;strong>;Rushin Shah&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.08726.pdf&quot;>;RARR: Researching and Revising What Language Models Say, Using Language Models&lt;/a>; &lt;br />; Luyu Gao*, &lt;strong>;Zhuyun Dai&lt;/strong>;, &lt;strong>;Panupong Pasupat&lt;/strong>;, Anthony Chen*, &lt;strong>;Arun Tejasvi Chaganty&lt;/strong>;, &lt;strong>;Yicheng Fan&lt;/strong>;, &lt;strong>;Vincent Y. Zhao&lt;/strong>;, &lt;strong>;Ni Lao&lt;/strong>;, &lt;strong>;Hongrae Lee&lt;/strong>;, &lt;strong>;Da-Cheng Juan&lt;/strong>;, &lt;strong>;Kelvin Guu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.16793.pdf&quot;>;Benchmarking Large Language Model Capabilities for Conditional Generation&lt;/a>; &lt;br />; &lt;em>;Joshua Maynez, Priyanka Agrawal, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.01786.pdf&quot;>;Crosslingual Generalization Through Multitask Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Niklas Muennighoff, Thomas Wang, Lintang Sutawika, &lt;strong>;Adam Roberts&lt;/strong>;, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05655.pdf&quot;>;DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering&lt;/a>; &lt;br />; &lt;em>;Ella Neeman, &lt;strong>;Roee Aharoni&lt;/strong>;, Or Honovich, Leshem Choshen, &lt;strong>;Idan Szpektor&lt;/strong>;, Omri Abend&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10933.pdf&quot;>;Resolving Indirect Referring Expressions for Entity Selection&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mohammad Javad Hosseini&lt;/b>;, &lt;b>;Filip Radlinski&lt;/b>;, &lt;b>;Silvia Pareti&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11840.pdf&quot;>;SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models&lt;/a>; &lt;br />; &lt;em>;Akshita Jha*, &lt;strong>;Aida Mostafazadeh Davani&lt;/strong>;, Chandan K Reddy, &lt;strong>;Shachi Dave&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;, &lt;strong>;Sunipa Dev&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.10040.pdf&quot;>;The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks&lt;/a>; &lt;br />; &lt;em>;Nikil Selvam, &lt;strong>;Sunipa Dev&lt;/strong>;, Daniel Khashabi, Tushar Khot, Kai-Wei Chang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10562.pdf&quot;>;Character-Aware Models Improve Visual Text Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Rosanne Liu&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Chitwan Saharia&lt;/b>;, &lt;b>;William Chan&lt;/b>;, &lt;b>;Adam Roberts&lt;/b>;, &lt;b>;Sharan Narang&lt;/b>;, &lt;b>;Irina Blok&lt;/b>;, &lt;b>;RJ Mical&lt;/b>;, &lt;b>;Mohammad Norouzi&lt;/b>;, &lt;b>;Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06995.pdf&quot;>;Cold-Start Data Selection for Better Few-Shot Language Model Fine-Tuning: A Prompt-Based Uncertainty Propagation Approach&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment &lt;br />; &lt;em>;&lt;b>;Roni Rabin&lt;/b>;, &lt;b>;Alexandre Djerbetian&lt;/b>;, &lt;b>;Roee Engelberg&lt;/b>;, &lt;b>;Lidan Hackmon&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;, &lt;b>;Amir Globerson&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02549.pdf&quot;>;FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Chen-Yu Lee&lt;/b>;, &lt;b>;Chun-Liang Li&lt;/b>;, &lt;b>;Hao Zhang&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Vincent Perot&lt;/b>;, &lt;b>;Guolong Su&lt;/b>;, &lt;b>;Xiang Zhang&lt;/b>;,&lt;b>; Kihyuk Sohn&lt;/b>;, &lt;b>;Nikolay Glushinev&lt;/b>;, &lt;b>;Renshen Wang&lt;/b>;, &lt;b>;Joshua Ainslie&lt;/b>;, &lt;b>;Shangbang Long&lt;/b>;, &lt;b>;Siyang Qin&lt;/b>;,&lt;b>; Yasuhisa Fujii&lt;/b>;, &lt;b>;Nan Hua&lt;/b>;, &lt;b>;Tomas Pfister&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00922.pdf&quot;>;Dialect-Robust Evaluation of Generated Text&lt;/a>; &lt;br />; &lt;em>;Jiao Sun*, &lt;strong>;Thibault Sellam&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Tu Vu*, &lt;strong>;Timothy Dozat&lt;/strong>;, &lt;strong>;Dan Garrette&lt;/strong>;, &lt;strong>;Aditya Siddhant&lt;/strong>;, &lt;strong>;Jacob Eisenstein&lt;/strong>;, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.03950.pdf&quot;>;MISGENDERED: Limits of Large Language Models in Understanding Pronouns&lt;/a>; &lt;br />; &lt;em>;Tamanna Hossain, &lt;strong>;Sunipa Dev&lt;/strong>;, Sameer Singh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13894.pdf&quot;>;LAMBADA: Backward Chaining for Automated Reasoning in Natural Language&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mehran Kazemi&lt;/b>;, &lt;b>;Najoung Kim&lt;/b>;, &lt;b>;Deepti Bhatia&lt;/b>;, &lt;b>;Xin Xu&lt;/b>;, &lt;b>;Deepak Ramachandran&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.19585.pdf&quot;>;LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction&lt;/a>; &lt;br />; &lt;em>;Jeremiah Milbauer*, &lt;strong>;Annie Louis&lt;/strong>;, &lt;strong>;Mohammad Javad Hosseini&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, &lt;strong>;Donald Metzler&lt;/strong>;, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05392.pdf&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Andy Zeng&lt;/strong>;, Trevor Darrell, Dan Klein&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10001.pdf&quot;>;Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters&lt;/a>; &lt;br />; &lt;em>;Boshi Wang, Sewon Min, Xiang Deng, &lt;strong>;Jiaming Shen&lt;/strong>;, &lt;strong>;You Wu&lt;/strong>;, Luke Zettlemoyer and Huan Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14106.pdf&quot;>;Better Zero-Shot Reasoning with Self-Adaptive Prompting&lt;/a>; &lt;br />; &lt;em>;Xingchen Wan*, &lt;strong>;Ruoxi Sun&lt;/strong>;, Hanjun Dai, &lt;strong>;Sercan Ö. Arik&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.00186.pdf&quot;>;Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Paul Roit&lt;/b>;, &lt;b>;Johan Ferret&lt;/b>;, &lt;b>;Lior Shani&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Geoffrey Cideron&lt;/b>;, &lt;b>;Robert Dadashi&lt;/b>;, &lt;b>;Matthieu Geist&lt;/b>;,&lt;b>; Sertan Girgin&lt;/b>;, &lt;b>;Léonard Hussenot&lt;/b>;, &lt;b>;Orgad Keller&lt;/b>;, &lt;b>;Nikola Momchev&lt;/b>;, &lt;b>;Sabela Ramos&lt;/b>;, &lt;b>;Piotr Stanczyk&lt;/b>;, &lt;b>;Nino Vieillard&lt;/b>;, &lt;b>;Olivier Bachem&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Avinatan Hassidim&lt;/b>;, &lt;b>;Olivier Pietquin&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09248.pdf&quot;>;Natural Language to Code Generation in Interactive Data Science Notebooks&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Pengcheng Yin&lt;/b>;, &lt;b>;Wen-Ding Li&lt;/b>;, &lt;b>;Kefan Xiao&lt;/b>;, &lt;b>;Abhishek Rao&lt;/b>;, &lt;b>;Yeming Wen&lt;/b>;, &lt;b>;Kensen Shi&lt;/b>;, &lt;b>;Joshua Howland&lt;/b>;,&lt;b>; Paige Bailey&lt;/b>;, &lt;b>;Michele Catasta&lt;/b>;, &lt;b>;Henryk Michalewski&lt;/b>;, &lt;b>;Oleksandr Polozov&lt;/b>;, &lt;b>;Charles Sutton&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08410.pdf&quot;>;Teaching Small Language Models to Reason&lt;/a>; &lt;br />; &lt;em>;Lucie Charlotte Magister*, &lt;strong>;Jonathan Mallinson&lt;/strong>;, &lt;strong>;Jakub Adamek&lt;/strong>;, &lt;strong>;Eric Malmi&lt;/strong>;, &lt;strong>;Aliaksei Severyn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/assets/pdf/papers/NeuPSL.pdf&quot;>;Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic&lt;/a>; &lt;br />; &lt;em>;Connor Pryor*, &lt;strong>;Quan Yuan&lt;/strong>;, &lt;strong>;Jeremiah Liu&lt;/strong>;, &lt;strong>;Mehran Kazemi&lt;/strong>;, &lt;strong>;Deepak Ramachandran&lt;/strong>;, &lt;strong>;Tania Bedrax-Weiss&lt;/strong>;, Lise Getoor&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10397.pdf&quot;>;A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization&lt;/a>; &lt;br />; &lt;em>;Lining Zhang, Simon Mille, Yufang Hou, &lt;strong>;Daniel Deutsch&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Yixin Liu, Saad Mahamood, &lt;strong>;Sebastian Gehrmann&lt;/strong>;, Miruna Clinciu, Khyathi Raghavi Chandu and João Sedoc&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Industry Track papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18465.pdf&quot;>;Federated Learning of Gboard Language Models with Differential Privacy&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Zheng Xu&lt;/b>;, &lt;b>;Yanxiang Zhang&lt;/b>;, &lt;b>;Galen Andrew&lt;/b>;, &lt;b>;Christopher Choquette&lt;/b>;, &lt;b>;Peter Kairouz&lt;/b>;, &lt;b>;Brendan McMahan&lt;/b>;, &lt;b>;Jesse Rosenstock&lt;/b>;, &lt;b>;Yuanbo Zhang&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18373.pdf&quot;>;KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models&lt;/a>; &lt;br />; &lt;em>;Zhiwei Jia*, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Arjun Akula&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, Hao Su, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;ACL Findings papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10622.pdf&quot;>;Multilingual Summarization with Factual Consistency Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Jonathan Herzig&lt;/b>;, &lt;b>;Elizabeth Clark&lt;/b>;, &lt;b>;Mirella Lapata &lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06767.pdf&quot;>;Parameter-Efficient Fine-Tuning for Robust Continual Multilingual Learning&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Kartikeya Badola&lt;/b>;, &lt;b>;Shachi Dave&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08153.pdf&quot;>;FiDO: Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference&lt;/a>; &lt;br />; &lt;em>;Michiel de Jong*, &lt;strong>;Yury Zemlyanskiy&lt;/strong>;, &lt;strong>;Joshua Ainslie&lt;/strong>;, &lt;strong>;Nicholas FitzGerald&lt;/strong>;, &lt;strong>;Sumit Sanghai&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;William Cohen&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00609.pdf&quot;>;A Simple, Yet Effective Approach to Finding Biases in Code Generation&lt;/a>; &lt;br />; &lt;em>;Spyridon Mouselinos, Mateusz Malinowski, &lt;strong>;Henryk Michalewski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09261.pdf&quot;>;Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mirac Suzgun&lt;/b>;, &lt;b>;Nathan Scales&lt;/b>;, &lt;b>;Nathanael Scharli&lt;/b>;, &lt;b>;Sebastian Gehrmann&lt;/b>;, &lt;b>;Yi Tay&lt;/b>;, &lt;b>;Hyung Won Chung&lt;/b>;,&lt;b>; Aakanksha Chowdhery&lt;/b>;, &lt;b>;Quoc Le&lt;/b>;, &lt;b>;Ed Chi&lt;/b>;, &lt;b>;Denny Zhou&lt;/b>;, &lt;b>;Jason Wei&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.07730.pdf&quot;>;QueryForm: A Simple Zero-Shot Form Entity Query Framework&lt;/a>; &lt;br />; &lt;em>;Zifeng Wang*, &lt;strong>;Zizhao Zhang&lt;/strong>;, &lt;strong>;Jacob Devlin&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Guolong Su&lt;/strong>;, &lt;strong>;Hao Zhang&lt;/strong>;, Jennifer Dy, &lt;strong>;Vincent Perot&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10703.pdf&quot;>;ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09682.pdf&quot;>;Multilingual Sequence-to-Sequence Models for Hebrew NLP&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Matan Eyal&lt;/b>;, &lt;b>;Hila Noga&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.04009.pdf&quot;>;Triggering Multi-Hop Reasoning for Question Answering in Language Models Using Soft Prompts and Random Walks&lt;/a>; &lt;br />; &lt;em>;Kanishka Misra*, &lt;strong>;Cicero Nogueira dos Santos&lt;/strong>;, Siamak Shakeri&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Complex Reasoning in Natural Language&lt;/a>;&lt;br />; &lt;em>;Wenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, &lt;strong>;Aman Madaan&lt;/strong>;, Tao Yu&lt;/em>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Generating Text from Language Models&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Afra Amini&lt;/b>;, &lt;b>;Ryan Cotterell&lt;/b>;, &lt;b>;John Hewitt&lt;/b>;, &lt;b>;Clara Meister&lt;/b>;, &lt;b>;Tiago Pimentel&lt;/b>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/sustainlp2023&quot;>;Simple and Efficient Natural Language Processing (SustaiNLP)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Tal Schuster&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.workshopononlineabuse.com/&quot;>;Workshop on Online Abuse and Harms (WOAH)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Aida Mostafazadeh Davani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://doc2dial.github.io/workshop2023/&quot;>;Document-Grounded Dialogue and Conversational Question Answering (DialDoc)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/5thnlp4convai/home?authuser=0&quot;>;NLP for Conversational AI&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cawl.wellformedness.com/&quot;>;Computation and Written Language (CAWL)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;b>;Kyle Gorman&lt;/b>;, &lt;b>;Brian Roark&lt;/b>;, &lt;b>;Richard Sproat&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sigmorphon.github.io/workshops/2023/&quot;>;Computational Morphology and Phonology (SIGMORPHON)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Kyle Gorman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/umass.edu/wnu2023&quot;>;Workshop on Narrative Understanding (WNU)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7093565002706757508/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-acl-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-acl-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ACL 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s72-c/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6926843365781180400&lt;/id>;&lt;published>;2023-07-07T11:01:00.001-07:00&lt;/published>;&lt;updated>;2023-07-07T11:09:20.724-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;video&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Modular visual question answering via code generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sanjay Subramanian, PhD student, UC Berkeley, and Arsha Nagrani, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://visualqa.org/&quot;>;Visual question answering&lt;/a>; (VQA) is a machine learning task that requires a model to answer a question about an image or &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;a set of images&lt;/a>;. Conventional VQA approaches need a large amount of labeled training data consisting of thousands of human-annotated question-answer pairs associated with images. In recent years, advances in large-scale pre-training have led to the development of VQA methods that perform well with &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;fewer than fifty training examples&lt;/a>; (few-shot) and &lt;a href=&quot;https://ai.googleblog.com/2022/07/rewriting-image-captions-for-visual.html&quot;>;without any human-annotated VQA training data&lt;/a>; (zero-shot). However, there is still a significant performance gap between these methods and state-of-the-art fully supervised VQA methods, such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html&quot;>;MaMMUT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;. In particular, few-shot methods struggle with spatial reasoning, counting, and multi-hop reasoning. Furthermore, few-shot methods have generally been limited to answering questions about single images. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To improve accuracy on VQA examples that involve complex reasoning, in “&lt;a href=&quot;https://arxiv.org/abs/2306.05392&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>;,” to appear at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, we introduce CodeVQA, a framework that answers visual questions using program synthesis. Specifically, when given a question about an image or set of images, CodeVQA generates a Python program (code) with simple visual functions that allow it to process images, and executes this program to determine the answer. We demonstrate that in the few-shot setting, CodeVQA outperforms prior work by roughly 3% on the &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; dataset and 2% on the &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; dataset. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;CodeVQA&lt;/h2>; &lt;p>; The CodeVQA approach uses a code-writing large language model (LLM), such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PALM&lt;/a>;, to generate &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;Python&lt;/a>; programs (code). We guide the LLM to correctly use visual functions by crafting a prompt consisting of a description of these functions and fewer than fifteen “in-context” examples of visual questions paired with the associated Python code for them. To select these examples, we compute embeddings for the input question and of all of the questions for which we have annotated programs (a randomly chosen set of fifty). Then, we select questions that have the highest similarity to the input and use them as in-context examples. Given the prompt and question that we want to answer, the LLM generates a Python program representing that question. &lt;/p>; &lt;p>; We instantiate the CodeVQA framework using three visual functions: (1) &lt;code>;query&lt;/code>;, (2) &lt;code>;get_pos&lt;/code>;, and (3) &lt;code>;find_matching_image&lt;/code>;. &lt;/p>; &lt;ul>; &lt;li>;&lt;code>;Query&lt;/code>;, which answers a question about a single image, is implemented using the few-shot &lt;a href=&quot;https://arxiv.org/abs/2210.08773&quot;>;Plug-and-Play VQA&lt;/a>; (PnP-VQA) method. PnP-VQA generates captions using &lt;a href=&quot;https://arxiv.org/abs/2201.12086&quot;>;BLIP&lt;/a>; — an image-captioning &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformer&lt;/a>; pre-trained on millions of image-caption pairs — and feeds these into a LLM that outputs the answers to the question. &lt;/li>;&lt;li>;&lt;code>;Get_pos&lt;/code>;, which is an object localizer that takes a description of an object as input and returns its position in the image, is implemented using &lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GradCAM&lt;/a>;. Specifically, the description and the image are passed through the BLIP joint text-image encoder, which predicts an image-text matching score. GradCAM takes the gradient of this score with respect to the image features to find the region most relevant to the text. &lt;/li>;&lt;li>;&lt;code>;Find_matching_image&lt;/code>;, which is used in multi-image questions to find the image that best matches a given input phrase, is implemented by using BLIP text and image encoders to compute a text embedding for the phrase and an image embedding for each image. Then the dot products of the text embedding with each image embedding represent the relevance of each image to the phrase, and we pick the image that maximizes this relevance. &lt;/li>; &lt;/ul>; &lt;p>; The three functions can be implemented using models that require very little annotation (eg, text and image-text pairs collected from the web and a small number of VQA examples). Furthermore, the CodeVQA framework can be easily generalized beyond these functions to others that a user might implement (eg, object detection, image segmentation, or knowledge base retrieval). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s1024/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;501&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the CodeVQA method. First, a large language model generates a Python program (code), which invokes visual functions that represent the question. In this example, a simple VQA method (&lt;code>;query&lt;/code>;) is used to answer one part of the question, and an object localizer (&lt;code>;get_pos&lt;/code>;) is used to find the positions of the objects mentioned. Then the program produces an answer to the original question by combining the outputs of these functions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; The CodeVQA framework correctly generates and executes Python programs not only for single-image questions, but also for multi-image questions. For example, if given two images, each showing two pandas, a question one might ask is, “Is it true that there are four pandas?” In this case, the LLM converts the counting question about the pair of images into a program in which an object count is obtained for each image (using the &lt;em>;query&lt;/em>; function). Then the counts for both images are added to compute a total count, which is then compared to the number in the original question to yield a yes or no answer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s1080/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We evaluate CodeVQA on three visual reasoning datasets: &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; (single-image), &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; (multi-image), and &lt;a href=&quot;https://lil.nlp.cornell.edu/nlvr/&quot;>;NLVR2&lt;/a>; (multi-image). For GQA, we provide 12 in-context examples to each method, and for COVR and NLVR2, we provide six in-context examples to each method. The table below shows that CodeVQA improves consistently over the baseline few-shot VQA method on all three datasets. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;GQA&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;COVR&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;NLVR2&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.56 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.06 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;63.37 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.03 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;54.11 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64.04 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the GQA, COVR, and NLVR2 datasets, showing that CodeVQA consistently improves over few-shot PnP-VQA. The metric is exact-match accuracy, ie, the percentage of examples in which the predicted answer exactly matches the ground-truth answer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that in GQA, CodeVQA&#39;s accuracy is roughly 30% higher than the baseline on spatial reasoning questions, 4% higher on “and” questions, and 3% higher on “or” questions. The third category includes multi-hop questions such as “Are there salt shakers or skateboards in the picture?”, for which the generated program is shown below. &lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;img = open_image(&quot;Image13.jpg&quot;) salt_shakers_exist = query(img, &quot;Are there any salt shakers?&quot;) skateboards_exist = query(img, &quot;Are there any skateboards?&quot;) if salt_shakers_exist == &quot;yes&quot; or skateboards_exist == &quot;yes&quot;: answer = &quot;yes&quot; else: answer = &quot;no&quot; &lt;/font>;&lt;/pre>; &lt;br />; &lt;p>; In COVR, we find that CodeVQA&#39;s gain over the baseline is higher when the number of input images is larger, as shown in the table below. This trend indicates that breaking the problem down into single-image questions is beneficial. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Number of images&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;3&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>;&amp;nbsp; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;91.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;51.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;47.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;75.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.2 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present CodeVQA, a framework for few-shot visual question answering that relies on code generation to perform multi-step visual reasoning. Exciting directions for future work include expanding the set of modules used and creating a similar framework for visual tasks beyond VQA. We note that care should be taken when considering whether to deploy a system such as CodeVQA, since vision-language models like the ones used in our visual functions &lt;a href=&quot;https://arxiv.org/abs/2108.02818&quot;>;have been shown to exhibit social biases&lt;/a>;. At the same time, compared to monolithic models, CodeVQA offers additional interpretability (through the Python program) and controllability (by modifying the prompts or visual functions), which are useful in production systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was a collaboration between &lt;a href=&quot;https://bair.berkeley.edu/baircommons.html&quot;>;UC Berkeley&#39;s Artificial Intelligence Research lab&lt;/a>; (BAIR) and Google Research, and was conducted by Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6926843365781180400/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot; rel=&quot;alternate&quot; title=&quot;Modular visual question answering via code generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-561916049750681872&lt;/id>;&lt;published>;2023-07-06T12:50:00.000-07:00&lt;/published>;&lt;updated>;2023-07-06T12:50:04.339-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Pic2Word: Mapping pictures to words for zero-shot composed image retrieval&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kuniaki Saito, Student Researcher, Google Research, Cloud AI Team, and Kihyuk Sohn, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s1100/Pic2Word.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Image retrieval plays a crucial role in search engines. Typically, their users rely on either image or text as a query to retrieve a desired target image. However, text-based retrieval has its limitations, as describing the target image accurately using words can be challenging. For instance, when searching for a fashion item, users may want an item whose specific attribute, eg, the color of a logo or the logo itself, is different from what they find in a website. Yet searching for the item in an existing search engine is not trivial since precisely describing the fashion item by text can be challenging. To address this fact, &lt;a href=&quot;https://arxiv.org/pdf/1812.07119.pdf&quot;>;composed image retrieval&lt;/a>; (CIR) retrieves images based on a query that combines both an image and a text sample that provides instructions on how to modify the image to fit the intended retrieval target. Thus, CIR allows precise retrieval of the target image by combining image and text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, CIR methods require large amounts of labeled data, ie, triplets of a 1) query image, 2) description, and 3) target image. Collecting such labeled data is costly, and models trained on this data are often tailored to a specific use case, limiting their ability to generalize to different datasets. &lt;/p>; &lt;p>; To address these challenges, in “&lt;a href=&quot;https://arxiv.org/abs/2302.03084&quot;>;Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval&lt;/a>;”, we propose a task called zero-shot CIR (ZS-CIR). In ZS-CIR, we aim to build a single CIR model that performs a variety of CIR tasks, such as &lt;a href=&quot;https://www.zheyuanliu.me/CIRR/&quot;>;object composition,&lt;/a>; &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;attribute editing&lt;/a>;, or domain conversion, without requiring labeled triplet data. Instead, we propose to train a retrieval model using large-scale image-caption pairs and unlabeled images, which are considerably easier to collect than supervised CIR datasets at scale. To encourage reproducibility and further advance this space, we also &lt;a href=&quot;https://github.com/google-research/composed_image_retrieval&quot;>;release the code&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s1062/image9.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;342&quot; data-original-width=&quot;1062&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Description of existing composed image retrieval model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCWi9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s949/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;476&quot; data-original-width=&quot;949&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCWi9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We train a composed image retrieval model using image-caption data only. Our model retrieves images aligned with the composition of the query image and text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Method overview&lt;/h2>; &lt;p>; We propose to leverage the language capabilities of the language encoder in the &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;contrastive language-image pre-trained model&lt;/a>; (CLIP), which excels at generating semantically meaningful language embeddings for a wide range of textual concepts and attributes. To that end, we use a lightweight mapping sub-module in CLIP that is designed to map an input picture (eg, a photo of a cat) from the image embedding space to a word token (eg, “cat”) in the textual input space. The whole network is optimized with the vision-language contrastive loss to again ensure the visual and text embedding spaces are as close as possible given a pair of an image and its textual description. Then, the query image can be treated as if it is a word. This enables the flexible and seamless composition of query image features and text descriptions by the language encoder. We call our method Pic2Word and provide an overview of its training process in the figure below. We want the mapped token &lt;em>;s&lt;/em>; to represent the input image in the form of word token. Then, we train the mapping network to reconstruct the image embedding in the language embedding, &lt;em>;p&lt;/em>;. Specifically, we optimize the contrastive loss proposed in CLIP computed between the visual embedding &lt;em>;v&lt;/em>; and the textual embedding &lt;em>;p&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYHXfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s611/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;611&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYHXfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Training of the mapping network (&lt;em>;f&lt;sub>;M&lt;/sub>;&lt;/em>;) using unlabeled images only. We optimize only the mapping network with a frozen visual and text encoder.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Given the trained mapping network, we can regard an image as a word token and pair it with the text description to flexibly compose the joint image-text query as shown in the figure below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s827/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-width=&quot;827&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With the trained mapping network, we regard the image as a word token and pair it with the text description to flexibly compose the joint image-text query.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We conduct a variety of experiments to evaluate Pic2Word&#39;s performance on a variety of CIR tasks. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Domain conversion&lt;/h3>; &lt;p>; We first evaluate the capability of compositionality of the proposed method on domain conversion — given an image and the desired new image domain (eg, sculpture, origami, cartoon, toy), the output of the system should be an image with the same content but in the new desired image domain or style. As illustrated below, we evaluate the ability to compose the category information and domain description given as an image and text, respectively. We evaluate the conversion from real images to four domains using &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; and &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>;. &lt;/p>; &lt;p>; To compare with approaches that do not require supervised training data, we pick three approaches: (i) &lt;em>;image only&lt;/em>; performs retrieval only with visual embedding, (ii) &lt;em>;text only &lt;/em>;employs only text embedding, and (iii) &lt;em>;image + text &lt;/em>;averages the visual and text embedding to compose the query. The comparison with (iii) shows the importance of composing image and text using a language encoder. We also compare with &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;Combiner&lt;/a>;, which trains the CIR model on &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; or &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRR&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s965/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;513&quot; data-original-width=&quot;965&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We aim to convert the domain of the input query image into the one described with text, eg, origami.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As shown in figure below, our proposed approach outperforms baselines by a large margin. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s754/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;754&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results (&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;recall&lt;/a>;@10, ie, the percentage of relevant instances in the first 10 images retrieved.) on composed image retrieval for domain conversion.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Fashion attribute composition&lt;/h3>; &lt;p>; Next, we evaluate the composition of fashion attributes, such as the color of cloth, logo, and length of sleeve, using the &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; dataset. The figure below illustrates the desired output given the query. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s818/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;309&quot; data-original-width=&quot;818&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of CIR for fashion attributes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In the figure below, we present a comparison with baselines, including supervised baselines that utilized triplets for training the CIR model: (i) &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;CB&lt;/a>; uses the same architecture as our approach, (ii) &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRPLANT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2203.08101.pdf&quot;>;ALTEMIS&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2007.00145.pdf&quot;>;MAAF&lt;/a>; use a smaller backbone, such as ResNet50. Comparison to these approaches will give us the understanding on how well our zero-shot approach performs on this task. &lt;/p>; &lt;p>; Although CB outperforms our approach, our method performs better than supervised baselines with smaller backbones. This result suggests that by utilizing a robust CLIP model, we can train a highly effective CIR model without requiring annotated triplets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s807/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;498&quot; data-original-width=&quot;807&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results (&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;recall&lt;/a>;@10, ie, the percentage of relevant instances in the first 10 images retrieved.) on composed image retrieval for Fashion-IQ dataset (higher is better). Light blue bars train the model using triplets. Note that our approach performs on par with these supervised baselines with shallow (smaller) backbones.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Qualitative results&lt;/h3>; &lt;p>; We show several examples in the figure below. Compared to a baseline method that does not require supervised training data (text + image feature averaging), our approach does a better job of correctly retrieving the target image. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s1999/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Qualitative results on diverse query images and text description.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; In this article, we introduce Pic2Word, a method for mapping pictures to words for ZS-CIR. We propose to convert the image into a word token to achieve a CIR model using only an image-caption dataset. Through a variety of experiments, we verify the effectiveness of the trained model on diverse CIR tasks, indicating that training on an image-caption dataset can build a powerful CIR model. One potential future research direction is utilizing caption data to train the mapping network, although we use only image data in the present work. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Also thanks to Zizhao Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/561916049750681872/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/pic2word-mapping-pictures-to-words-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/pic2word-mapping-pictures-to-words-for.html&quot; rel=&quot;alternate&quot; title=&quot;Pic2Word: Mapping pictures to words for zero-shot composed image retrieval&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s72-c/Pic2Word.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-667019077470952746&lt;/id>;&lt;published>;2023-06-29T14:08:00.000-07:00&lt;/published>;&lt;updated>;2023-06-29T14:08:24.386-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Announcing the first Machine Unlearning Challenge&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Fabian Pedregosa and Eleni Triantafillou, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s900/Unlearning.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Deep learning has recently driven tremendous progress in a wide array of applications, ranging from &lt;a href=&quot;https://imagen.research.google/&quot;>;realistic image generation&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;impressive retrieval systems&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/ai/bard-google-ai-search-updates/&quot;>;language models that can hold human-like conversations&lt;/a>;. While this progress is very exciting, the widespread use of deep neural network models requires caution: as guided by Google&#39;s AI &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Principles&lt;/a>;, we seek to develop AI technologies responsibly by understanding and mitigating potential risks, such as the propagation and amplification of unfair biases and protecting user privacy. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Fully erasing the influence of the data requested to be deleted is challenging since, aside from simply deleting it from databases where it&#39;s stored, it also requires erasing the influence of that data on other artifacts such as trained machine learning models. Moreover, recent research [&lt;a href=&quot;https://arxiv.org/abs/1610.05820&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;2&lt;/a>;] has shown that in some cases it may be possible to infer with high accuracy whether an example was used to train a machine learning model using &lt;a href=&quot;https://en.wikipedia.org/wiki/Adversarial_machine_learning#Model_extraction&quot;>;membership inference attacks&lt;/a>; (MIAs). This can raise privacy concerns, as it implies that even if an individual&#39;s data is deleted from a database, it may still be possible to infer whether that individual&#39;s data was used to train a model. &lt;/p>; &lt;p>; Given the above, &lt;em>;machine unlearning&lt;/em>; is an emergent subfield of machine learning that aims to remove the influence of a specific subset of training examples — the &quot;forget set&quot; — from a trained model. Furthermore, an ideal unlearning algorithm would remove the influence of certain examples &lt;em>;while maintaining&lt;/em>; other beneficial properties, such as the accuracy on the rest of the train set and generalization to held-out examples. A straightforward way to produce this unlearned model is to retrain the model on an adjusted training set that excludes the samples from the forget set. However, this is not always a viable option, as retraining deep models can be computationally expensive. An ideal unlearning algorithm would instead use the already-trained model as a starting point and efficiently make adjustments to remove the influence of the requested data. &lt;/p>; &lt;p>; Today we&#39;re thrilled to announce that we&#39;ve teamed up with a broad group of academic and industrial researchers to organize the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>;. The competition considers a realistic scenario in which after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. The competition will be hosted on &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>;, and submissions will be automatically scored in terms of both forgetting quality and model utility. We hope that this competition will help advance the state of the art in machine unlearning and encourage the development of efficient, effective and ethical unlearning algorithms. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Machine unlearning applications&lt;/h2>; &lt;p>; Machine unlearning has applications beyond protecting user privacy. For instance, one can use unlearning to erase inaccurate or outdated information from trained models (eg, due to errors in labeling or changes in the environment) or remove harmful, manipulated, or outlier data. &lt;/p>; &lt;p>; The field of machine unlearning is related to other areas of machine learning such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1802.07569&quot;>;life-long learning&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Fairness_(machine_learning)&quot;>;fairness&lt;/a>;. Differential privacy aims to guarantee that no particular training example has too large an influence on the trained model; a stronger goal compared to that of unlearning, which only requires erasing the influence of the designated forget set. Life-long learning research aims to design models that can learn continuously while maintaining previously-acquired skills. As work on unlearning progresses, it may also open additional ways to boost fairness in models, by correcting unfair biases or disparate treatment of members belonging to different groups (eg, demographics, age groups, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s720/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Anatomy of unlearning.&lt;/b>; An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the &quot;forget set&quot;). From the model, forget set, and retain set, the unlearning algorithm produces an updated model. An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Challenges of machine unlearning&lt;/h2>; &lt;p>; The problem of unlearning is complex and multifaceted as it involves several conflicting objectives: forgetting the requested data, maintaining the model&#39;s utility (eg, accuracy on retained and held-out data), and efficiency. Because of this, existing unlearning algorithms make different trade-offs. For example, full retraining achieves successful forgetting without damaging model utility, but with poor efficiency, while &lt;a href=&quot;https://arxiv.org/abs/2007.02923&quot;>;adding noise&lt;/a>; to the weights achieves forgetting at the expense of utility. &lt;/p>; &lt;p>; Furthermore, the evaluation of forgetting algorithms in the literature has so far been highly inconsistent. While some &lt;a href=&quot;https://arxiv.org/abs/1911.04933&quot;>;works&lt;/a>; report the classification accuracy on the samples to unlearn, &lt;a href=&quot;https://proceedings.mlr.press/v119/wu20b.html&quot;>;others&lt;/a>; report distance to the fully retrained model, and yet others use the error rate of membership inference attacks as a metric for forgetting quality [&lt;a href=&quot;https://arxiv.org/abs/2302.09880&quot;>;4&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2010.10981&quot;>;5&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.02205&quot;>;6&lt;/a>;]. &lt;/p>; &lt;p>; We believe that the inconsistency of evaluation metrics and the lack of a standardized protocol is a serious impediment to progress in the field — we are unable to make direct comparisons between different unlearning methods in the literature. This leaves us with a myopic view of the relative merits and drawbacks of different approaches, as well as open challenges and opportunities for developing improved algorithms. To address the issue of inconsistent evaluation and to advance the state of the art in the field of machine unlearning, we&#39;ve teamed up with a broad group of academic and industrial researchers to organize the first unlearning challenge. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Announcing the first Machine Unlearning Challenge&lt;/h2>; &lt;p>; We are pleased to announce the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>;, which will be held as part of the &lt;a href=&quot;https://neurips.cc/Conferences/2023/CompetitionTrack&quot;>;NeurIPS 2023 Competition Track.&lt;/a>; The goal of the competition is twofold. First, by unifying and standardizing the evaluation metrics for unlearning, we hope to identify the strengths and weaknesses of different algorithms through apples-to-apples comparisons. Second, by opening this competition to everyone, we hope to foster novel solutions and shed light on open challenges and opportunities. &lt;/p>; &lt;p>; The competition will be hosted on &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; and run between mid-July 2023 and mid-September 2023. As part of the competition, today we&#39;re announcing the availability of the &lt;a href=&quot;https://github.com/unlearning-challenge/starting-kit&quot;>;starting kit&lt;/a>;. This starting kit provides a foundation for participants to build and test their unlearning models on a toy dataset. &lt;/p>; &lt;p>; The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. For this, we will make available as part of the starting kit a dataset of synthetic faces (samples shown below) and we&#39;ll also use several real-face datasets for evaluation of submissions. The participants are asked to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We will also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s2000/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;2000&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Excerpt images from the &lt;a href=&quot;https://github.com/microsoft/FaceSynthetics&quot;>;Face Synthetics&lt;/a>; dataset together with age annotations. The competition considers the scenario in which an age predictor has been trained on face images like the above, and, after training, a certain subset of the training images must be forgotten.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For evaluating forgetting, we will use tools inspired by MIAs, such as &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;LiRA&lt;/a>;. MIAs were first developed in the privacy and security literature and their goal is to infer which examples were part of the training set. Intuitively, if unlearning is successful, the unlearned model contains no traces of the forgotten examples, causing MIAs to fail: the attacker would be &lt;em>;unable&lt;/em>; to infer that the forget set was, in fact, part of the original training set. In addition, we will also use statistical tests to quantify how different the distribution of unlearned models (produced by a particular submitted unlearning algorithm) is compared to the distribution of models retrained from scratch. For an ideal unlearning algorithm, these two will be indistinguishable. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Machine unlearning is a powerful tool that has the potential to address several open problems in machine learning. As research in this area continues, we hope to see new methods that are more efficient, effective, and responsible. We are thrilled to have the opportunity via this competition to spark interest in this field, and we are looking forward to sharing our insights and findings with the community. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. We are writing this blog post on behalf of the organization team of the Unlearning Competition: Eleni Triantafillou*, Fabian Pedregosa* (*equal contribution), Meghdad Kurmanji, Kairan Zhao, Gintare Karolina Dziugaite, Peter Triantafillou, Ioannis Mitliagkas, Vincent Dumoulin, Lisheng Sun Hosoya, Peter Kairouz, Julio CS Jacques Junior, Jun Wan, Sergio Escalera and Isabelle Guyon.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/667019077470952746/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html&quot; rel=&quot;alternate&quot; title=&quot;Announcing the first Machine Unlearning Challenge&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s72-c/Unlearning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9161751123508054082&lt;/id>;&lt;published>;2023-06-29T12:10:00.004-07:00&lt;/published>;&lt;updated>;2023-07-18T14:55:41.583-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;On-device diffusion plugins for conditioned text-to-image generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao and Tingbo Hou, Software Engineers, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s800/MediaPipeDiffusion-hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown great success in text-to-image generation, achieving high image quality, improved inference performance, and expanding our creative inspiration. Nevertheless, it is still challenging to efficiently control the generation, especially with conditions that are difficult to describe with text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we announce &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; diffusion plugins, which enable controllable text-to-image generation to be run on-device. Expanding upon our &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;prior work&lt;/a>; on GPU inference for on-device large generative models, we introduce new low-cost solutions for controllable text-to-image generation that can be plugged into existing diffusion models and their Low-Rank Adaptation (&lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;>;LoRA&lt;/a>;) variants. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s1080/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;780&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Text-to-image generation with control plugins running on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Background&lt;/h2>; &lt;p>; With diffusion models, image generation is modeled as an iterative denoising process. Starting from a noise image, at each step, the diffusion model gradually denoises the image to reveal an image of the target concept. Research shows that leveraging language understanding via text prompts can greatly improve image generation. For text-to-image generation, the text embedding is connected to the model via cross-attention layers. Yet, some information is difficult to describe by text prompts, eg, the position and pose of an object. To address this problem, researchers add additional models into the diffusion to inject control information from a condition image. &lt;/p>; &lt;p>; Common approaches for controlled text-to-image generation include &lt;a href=&quot;https://arxiv.org/abs/2211.12572&quot;>;Plug-and-Play&lt;/a>;, &lt;a href=&quot;https://github.com/lllyasviel/ControlNet&quot;>;ControlNet&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2302.08453&quot;>;T2I Adapter&lt;/a>;. Plug-and-Play applies a widely used denoising diffusion implicit model (&lt;a href=&quot;https://arxiv.org/abs/2010.02502&quot;>;DDIM&lt;/a>;) inversion approach that reverses the generation process starting from an input image to derive an initial noise input, and then employs a copy of the diffusion model (860M parameters for Stable Diffusion 1.5) to encode the condition from an input image. Plug-and-Play extracts spatial features with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;self-attention&lt;/a>; from the copied diffusion, and injects them into the text-to-image diffusion. ControlNet creates a trainable copy of the encoder of a diffusion model, which connects via a convolution layer with zero-initialized parameters to encode conditioning information that is conveyed to the decoder layers. However, as a result, the size is large, half that of the diffusion model (430M parameters for Stable Diffusion 1.5). T2I Adapter is a smaller network (77M parameters) and achieves similar effects in controllable generation. T2I Adapter only takes the condition image as input, and its output is shared across all diffusion iterations. Yet, the adapter model is not designed for portable devices. &lt;/p>; &lt;br />; &lt;h2>;The MediaPipe diffusion plugins&lt;/h2>; &lt;p>; To make conditioned generation efficient, customizable, and scalable, we design the MediaPipe diffusion plugin as a separate network that is: &lt;/p>; &lt;ul>; &lt;li>;&lt;i>;Plugable&lt;/i>;: It can be easily connected to a pre-trained base model. &lt;/li>;&lt;li>;&lt;i>;Trained from scratch&lt;/i>;: It does not use pre-trained weights from the base model. &lt;/li>;&lt;li>;&lt;i>;Portable&lt;/i>;: It runs outside the base model on mobile devices, with negligible cost compared to the base model inference. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Parameter Size&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Plugable&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;From Scratch&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Portable&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Plug-and-Play &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;860M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;430M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;T2I Adapter &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;77M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Plug-and-Play, ControlNet, T2I Adapter, and the MediaPipe diffusion plugin.&lt;br />;* The number varies depending on the particulars of the diffusion model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The MediaPipe diffusion plugin is a portable on-device model for text-to-image generation. It extracts multiscale features from a conditioning image, which are added to the encoder of a diffusion model at corresponding levels. When connecting to a text-to-image diffusion model, the plugin model can provide an extra conditioning signal to the image generation. We design the plugin network to be a lightweight model with only 6M parameters. It uses depth-wise convolutions and inverted bottlenecks from &lt;a href=&quot;https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html&quot;>;MobileNetv2&lt;/a>; for fast inference on mobile devices. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s1724/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;628&quot; data-original-width=&quot;1724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the MediaPipe diffusion model plugin. The plugin is a separate network, whose output can be plugged into a pre-trained text-to-image generation model. Features extracted by the plugin are applied to the associated downsampling layer of the diffusion model (blue).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Unlike ControlNet, we inject the same control features in all diffusion iterations. That is, we only run the plugin once for one image generation, which saves computation. We illustrate some intermediate results of a diffusion process below. The control is effective at every diffusion step and enables controlled generation even at early steps. More iterations improve the alignment of the image with the text prompt and generate more detail. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s1280/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the generation process using the MediaPipe diffusion plugin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Examples&lt;/h2>; &lt;p>; In this work, we developed plugins for a diffusion-based text-to-image generation model with MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmark&lt;/a>;, MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmark&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_map&quot;>;depth maps&lt;/a>;, and &lt;a href=&quot;https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html&quot;>;Canny edge&lt;/a>;. For each task, we select about 100K images from a &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;web-scale image-text dataset&lt;/a>;, and compute control signals using corresponding MediaPipe solutions. We use refined captions from &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; for training the plugins. &lt;/p>; &lt;br />; &lt;h3>;Face Landmark&lt;/h3>; &lt;p>; The MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmarker&lt;/a>; task computes 478 landmarks (with attention) of a human face. We use the &lt;a href=&quot;https://github.com/google/mediapipe/blob/26a7ca5c64cd885978677931a7218d33cd7d1dec/mediapipe/python/solutions/drawing_utils.py&quot;>;drawing utils&lt;/a>; in MediaPipe to render a face, including face contour, mouth, eyes, eyebrows, and irises, with different colors. The following table shows randomly generated samples by conditioning on face mesh and prompts. As a comparison, both ControlNet and Plugin can control text-to-image generation with given conditions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s1346/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1018&quot; data-original-width=&quot;1346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Face-landmark plugin for text-to-image generation, compared with ControlNet.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Holistic Landmark&lt;/h3>; &lt;p>; MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmarker&lt;/a>; task includes landmarks of body pose, hands, and face mesh. Below, we generate various stylized images by conditioning on the holistic features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s1351/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;970&quot; data-original-width=&quot;1351&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Holistic-landmark plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Depth&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s1344/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depth-plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Canny Edge&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s1345/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;955&quot; data-original-width=&quot;1345&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Canny-edge plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We conduct a quantitative study of the &lt;em>;face landmark&lt;/em>; plugin to demonstrate the model&#39;s performance. The evaluation dataset contains 5K human images. We compare the generation quality as measured by the widely used metrics, &lt;a href=&quot;https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance&quot;>;Fréchet Inception Distance&lt;/a>; (FID) and &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>; scores. The base model is a pre-trained text-to-image diffusion model. We use &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>; v1.5 here. &lt;/p>; &lt;p>; As shown in the following table, both ControlNet and the MediaPipe diffusion plugin produce much better sample quality than the base model, in terms of FID and CLIP scores. Unlike ControlNet, which needs to run at every diffusion step, the MediaPipe plugin only runs once for each image generated. We measured the performance of the three models on a server machine (with Nvidia V100 GPU) and a mobile phone (Galaxy S23). On the server, we run all three models with 50 diffusion steps, and on mobile, we run 20 diffusion steps using the &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/image_generator&quot;>;MediaPipe image generation app&lt;/a>;. Compared with ControlNet, the MediaPipe plugin shows a clear advantage in inference efficiency while preserving the sample quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Model&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;FID↓&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;CLIP↑&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Inference Time (s)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Nvidia V100&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;10.32 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.26 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.5 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.51 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.31 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.4 (+48%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;18.2 (+58.3%) &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.30 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 (+0.2%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.8 (+2.6%) &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quantitative comparison on FID, CLIP, and inference time.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We test the performance of the plugin on a wide range of mobile devices from mid-tier to high-end. We list the results on some representative devices in the following table, covering both Android and iOS. &lt;/p>; &lt;p>; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Device&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;7&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Android&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;iOS&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 4&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 6&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 7&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 12 Pro&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 13 Pro&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;strong>;Time&lt;/strong>; (ms) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;128 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;68 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;48 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;73 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;63 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Inference time (ms) of the plugin on different mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we present MediaPipe, a portable plugin for conditioned text-to-image generation. It injects features extracted from a condition image to a diffusion model, and consequently controls the image generation. Portable plugins can be connected to pre-trained diffusion models running on servers or devices. By running text-to-image generation and plugins fully on-device, we enable more flexible applications of generative AI. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank all team members who contributed to this work: Raman Sarokin and Juhyun Lee for the GPU inference solution; Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, and Matthias Grundmann for leadership. Special thanks to Jiuqiang Tang&lt;/em>;&lt;i>;, Joe Zou and Lu wang,&lt;/i>;&lt;em>;&amp;nbsp;who made this technology and all the demos running on-device.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/9161751123508054082/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/on-device-diffusion-plugins-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/on-device-diffusion-plugins-for.html&quot; rel=&quot;alternate&quot; title=&quot;On-device diffusion plugins for conditioned text-to-image generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s72-c/MediaPipeDiffusion-hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6036162561497757163&lt;/id>;&lt;published>;2023-06-27T14:19:00.000-07:00&lt;/published>;&lt;updated>;2023-06-27T14:19:04.029-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Unifying image-caption and image-classification datasets with prefix conditioning&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kuniaki Saito, Student Researcher, Cloud AI Team, and Kihyuk Sohn, Research Scientist, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s320/Prefix%20conditioning%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Pre-training visual language (VL) models on web-scale image-caption datasets has recently emerged as a powerful alternative to traditional pre-training on image classification data. Image-caption datasets are considered to be more “open-domain” because they contain broader scene types and vocabulary words, which result in models with &lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;strong performance in few- and zero-shot recognition tasks&lt;/a>;. However, images with fine-grained class descriptions can be rare, and the class distribution can be imbalanced since image-caption datasets do not go through manual curation. By contrast, large-scale classification datasets, such as &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>;, are often curated and can thus provide fine-grained categories with a balanced label distribution. While it may sound promising, directly combining caption and classification datasets for pre-training is often unsuccessful as it can result in biased representations that do not generalize well to various downstream tasks. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2206.01125&quot;>;Prefix Conditioning Unifies Language and Label Supervision&lt;/a>;”, presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we demonstrate a pre-training strategy that uses both classification and caption datasets to provide complementary benefits. First, we show that naïvely unifying the datasets results in sub-optimal performance on downstream zero-shot recognition tasks as the model is affected by dataset bias: the coverage of image domains and vocabulary words is different in each dataset. We address this problem during training through &lt;em>;prefix conditioning&lt;/em>;, a novel simple and effective method that uses prefix tokens to disentangle dataset biases from visual concepts. This approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. Prefix conditioning is a generic method that can be easily integrated into existing VL pre-training objectives, such as &lt;a href=&quot;https://openai.com/research/clip&quot;>;Contrastive Language-Image Pre-training&lt;/a>; (CLIP) or &lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;Unified Contrastive Learning&lt;/a>; (UniCL). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;High-level idea&lt;/h2>; &lt;p>; We note that classification datasets tend to be biased in at least two ways: (1) the images mostly contain single objects from restricted domains, and (2) the vocabulary is limited and lacks the linguistic flexibility required for zero-shot learning. For example, the class embedding of “a photo of a dog” optimized for ImageNet usually results in a photo of one dog in the center of the image pulled from the ImageNet dataset, which does not generalize well to other datasets containing images of multiple dogs in different spatial locations or a dog with other subjects. &lt;/p>; &lt;p>; By contrast, caption datasets contain a wider variety of scene types and vocabularies. As shown below, if a model simply learns from two datasets, the language embedding can entangle the bias from the image classification and caption dataset, which can decrease the generalization in zero-shot classification. If we can disentangle the bias from two datasets, we can use language embeddings that are tailored for the caption dataset to improve generalization. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/s1150/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1150&quot; data-original-width=&quot;856&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/w476-h640/image1.png&quot; width=&quot;476&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Top&lt;/strong>;: Language embedding entangling the bias from image classification and caption dataset. &lt;strong>;Bottom&lt;/strong>;: Language embeddings disentangles the bias from two datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prefix conditioning&lt;/h2>; &lt;p>; Prefix conditioning is partially inspired by &lt;a href=&quot;https://ai.googleblog.com/2022/02/guiding-frozen-language-models-with.html&quot;>;prompt tuning&lt;/a>;, which prepends learnable tokens to the input token sequences to instruct a pre-trained model backbone to learn task-specific knowledge that can be used to solve downstream tasks. The prefix conditioning approach differs from prompt tuning in two ways: (1) it is designed to unify image-caption and classification datasets by disentangling the dataset bias, and (2) it is applied to VL pre-training while the standard prompt tuning is used to fine-tune models. Prefix conditioning is an explicit way to specifically steer the behavior of model backbones based on the type of datasets provided by users. This is especially helpful in production when the number of different types of datasets is known ahead of time. &lt;/p>; &lt;p>; During training, prefix conditioning learns a text token (prefix token) for each dataset type, which absorbs the bias of the dataset and allows the remaining text tokens to focus on learning visual concepts. Specifically, it prepends prefix tokens for each dataset type to the input tokens that inform the language and visual encoder of the input data type (eg, classification vs. caption). Prefix tokens are trained to learn the dataset-type-specific bias, which enables us to disentangle that bias in language representations and utilize the embedding learned on the image-caption dataset during test time, even without an input caption. &lt;/p>; &lt;p>; We utilize prefix conditioning for CLIP using a language and visual encoder. During test time, we employ the prefix used for the image-caption dataset since the dataset is supposed to cover broader scene types and vocabulary words, leading to better performance in zero-shot recognition. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s1038/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;1038&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the Prefix Conditioning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; We apply prefix conditioning to two types of contrastive loss, &lt;a href=&quot;http://proceedings.mlr.press/v139/radford21a&quot;>;CLIP&lt;/a>; and &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf&quot;>;UniCL,&lt;/a>; and evaluate their performance on zero-shot recognition tasks compared to models trained with &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet21K&lt;/a>; (IN21K) and &lt;a href=&quot;https://github.com/google-research-datasets/conceptual-12m&quot;>;Conceptual 12M&lt;/a>; (CC12M). CLIP and UniCL models trained with two datasets using prefix conditioning show large improvements in zero-shot classification accuracy. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s440/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;440&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Zero-shot classification accuracy of models trained with only IN21K or CC12M compared to CLIP and UniCL models trained with both two datasets using prefix conditioning (“Ours”). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on test-time prefix&lt;/h3>; &lt;p>; The table below describes the performance change by the prefix used during test time. We demonstrate that by using the same prefix used for the classification dataset (“Prompt”), the performance on the classification dataset (&lt;a href=&quot;https://www.image-net.org/&quot;>;IN-1K&lt;/a>;) improves. When using the same prefix used for the image-caption dataset (“Caption”), the performance on other datasets (Zero-shot AVG) improves. This analysis illustrates that if the prefix is tailored for the image-caption dataset, it achieves better generalization of scene types and vocabulary words. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/w640-h396/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis of the prefix used for test-time. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on robustness to image distribution shift &lt;/h3>; &lt;p>; We study the shift in image distribution using ImageNet variants. We see that the “Caption” prefix performs better than “Prompt” in &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>; (IN-R) and &lt;a href=&quot;https://github.com/HaohanWang/ImageNet-Sketch&quot;>;ImageNet-Sketch&lt;/a>; (IN-S), but underperforms in &lt;a href=&quot;https://github.com/modestyachts/ImageNetV2&quot;>;ImageNet-V2&lt;/a>; (IN-V2). This indicates that the “Caption” prefix achieves generalization on domains far from the classification dataset. Therefore, the optimal prefix probably differs by how far the test domain is from the classification dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis on the robustness to image-level distribution shift. IN: ImageNet, IN-V2: ImageNet-V2, IN-R: Art, Cartoon style ImageNet, IN-S: ImageNet Sketch. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We introduce prefix conditioning, a technique for unifying image caption and classification datasets for better zero-shot classification. We show that this approach leads to better zero-shot classification accuracy and that the prefix can control the bias in the language embedding. One limitation is that the prefix learned on the caption dataset is not necessarily optimal for the zero-shot classification. Identifying the optimal prefix for each test dataset is an interesting direction for future work. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Thanks to Zizhao Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6036162561497757163/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html&quot; rel=&quot;alternate&quot; title=&quot;Unifying image-caption and image-classification datasets with prefix conditioning&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s72-c/Prefix%20conditioning%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8471212041038116532&lt;/id>;&lt;published>;2023-06-23T12:24:00.001-07:00&lt;/published>;&lt;updated>;2023-06-23T12:34:21.337-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Preference learning with automated feedback for cache eviction&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ramki Gummadi, Software Engineer, Google and Kevin Chen, Software Engineer, YouTube&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeCdR0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s320/Halp%20hero%20gif.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Caching is a ubiquitous idea in computer science that significantly improves the performance of storage and retrieval systems by storing a subset of popular items closer to the client based on request patterns. An important algorithmic piece of cache management is the decision policy used for dynamically updating the set of items being stored, which has been extensively optimized over several decades, resulting in several &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;>;efficient and robust heuristics&lt;/a>;. While applying machine learning to cache policies has shown promising results in recent years (eg, &lt;a href=&quot;https://www.usenix.org/conference/nsdi20/presentation/song&quot;>;LRB&lt;/a>;, &lt;a href=&quot;https://www.usenix.org/system/files/conference/nsdi18/nsdi18-beckmann.pdf&quot;>;LHD&lt;/a>;, &lt;a href=&quot;https://www.pdl.cmu.edu/PDL-FTP/Storage/MLSys2021-zhou.pdf&quot;>;storage applications&lt;/a>;), it remains a challenge to outperform robust heuristics in a way that can generalize reliably beyond benchmarks to production settings, while maintaining competitive compute and memory overheads. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c9ebaf640152028a6cdedb684577a7c9e52b6f10.pdf&quot;>;HALP: Heuristic Aided Learned Preference Eviction Policy for YouTube Content Delivery Network&lt;/a>;”, presented at &lt;a href=&quot;https://www.usenix.org/conference/nsdi23&quot;>;NSDI 2023&lt;/a>;, we introduce a scalable state-of-the-art cache eviction framework that is based on learned rewards and uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;preference learning&lt;/a>; with automated feedback. The Heuristic Aided Learned Preference (HALP) framework is a meta-algorithm that uses randomization to merge a lightweight heuristic baseline eviction rule with a learned reward model. The reward model is a lightweight neural network that is continuously trained with ongoing automated feedback on preference comparisons designed to mimic the &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0020019006003449&quot;>;offline oracle&lt;/a>;. We discuss how HALP has improved infrastructure efficiency and user video playback latency for YouTube&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Content_delivery_network&quot;>;content delivery network&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learned preferences for cache eviction decisions&lt;/h2>; &lt;p>; The HALP framework computes cache eviction decisions based on two components: (1) a neural reward model trained with automated feedback via preference learning, and (2) a meta-algorithm that combines a learned reward model with a fast heuristic. As the cache observes incoming requests, HALP continuously trains a small neural network that predicts a scalar reward for each item by formulating this as a preference learning method via &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_to_rank#Pairwise_approach&quot;>;pairwise&lt;/a>; preference feedback. This aspect of HALP is similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback&quot;>;reinforcement learning from human feedback&lt;/a>; (RLHF) systems, but with two important distinctions: &lt;/p>; &lt;ul>; &lt;li>;Feedback is &lt;em>;automated&lt;/em>; and leverages well-known results about the structure of offline &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0020019006003449&quot;>;optimal&lt;/a>; cache eviction policies. &lt;/li>;&lt;li>;The model is &lt;em>;learned continuously&lt;/em>; using a transient buffer of training examples constructed from the automated feedback process. &lt;/li>; &lt;/ul>; &lt;p>; The eviction decisions rely on a filtering mechanism with two steps. First, a small subset of candidates is selected using a heuristic that is efficient, but suboptimal in terms of performance. Then, a re-ranking step optimizes from within the baseline candidates via the sparing use of a neural network scoring function to “boost” the quality of the final decision. &lt;/p>; &lt;p>; As a production ready cache policy implementation, HALP not only makes eviction decisions, but also subsumes the end-to-end process of sampling pairwise preference queries used to efficiently construct relevant feedback and update the model to power eviction decisions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A neural reward model&lt;/h2>; &lt;p>; HALP uses a light-weight two-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) as its reward model to selectively score individual items in the cache. The features are constructed and managed as a metadata-only “ghost cache” (similar to classical policies like &lt;a href=&quot;https://en.wikipedia.org/wiki/Adaptive_replacement_cache&quot;>;ARC&lt;/a>;). After any given lookup request, in addition to regular cache operations, HALP conducts the book-keeping (eg, tracking and updating feature metadata in a capacity-constrained key-value store) needed to update the dynamic internal representation. This includes: (1) externally tagged features provided by the user as input, along with a cache lookup request, and (2) internally constructed dynamic features (eg, time since last access, average time between accesses) constructed from lookup times observed on each item. &lt;/p>; &lt;p>; HALP learns its reward model fully online starting from a random weight initialization. This might seem like a bad idea, especially if the decisions are made exclusively for optimizing the reward model. However, the eviction decisions rely on both the learned reward model and a suboptimal but simple and robust heuristic like &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU&quot;>;LRU&lt;/a>;. This allows for optimal performance when the reward model has fully generalized, while remaining robust to a temporarily uninformative reward model that is yet to generalize, or in the process of catching up to a changing environment. &lt;/p>; &lt;p>; Another advantage of online training is specialization. Each cache server runs in a potentially different environment (eg, geographic location), which influences local network conditions and what content is locally popular, among other things. Online training automatically captures this information while reducing the burden of generalization, as opposed to a single offline training solution.&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scoring samples from a randomized priority queue &lt;/h2>; &lt;p>; It can be impractical to optimize for the quality of eviction decisions with an exclusively learned objective for two reasons. &lt;/p>; &lt;ol>; &lt;li>;Compute efficiency constraints: Inference with a learned network can be significantly more expensive than the computations performed in practical cache policies operating at scale. This limits not only the expressivity of the network and features, but also how often these are invoked during each eviction decision. &lt;/li>;&lt;li>;Robustness for generalizing out-of-distribution: HALP is deployed in a setup that involves continual learning, where a quickly changing workload might generate request patterns that might be temporarily out-of-distribution with respect to previously seen data. &lt;/li>; &lt;/ol>; &lt;p>; To address these issues, HALP first applies an inexpensive heuristic scoring rule that corresponds to an eviction priority to identify a small candidate sample. This process is based on efficient random sampling that &lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;approximates&lt;/a>; exact &lt;a href=&quot;https://en.wikipedia.org/wiki/Priority_queue&quot;>;priority queues&lt;/a>;. The priority function for generating candidate samples is intended to be quick to compute using existing manually-tuned algorithms, eg, LRU. However, this is configurable to approximate other cache replacement heuristics by editing a simple cost function. Unlike &lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;prior work&lt;/a>;, where the randomization was used to tradeoff approximation for efficiency, HALP also &lt;em>;relies&lt;/em>; on the inherent randomization in the sampled candidates across time steps for providing the necessary &lt;a href=&quot;https://lilianweng.github.io/posts/2020-06-07-exploration-drl/&quot;>;exploratory&lt;/a>; diversity in the sampled candidates for both training and inference. &lt;/p>; &lt;p>; The final evicted item is chosen from among the supplied candidates, equivalent to the &lt;a href=&quot;https://openai.com/research/measuring-goodharts-law&quot;>;best-of-n&lt;/a>; reranked sample, corresponding to maximizing the predicted&lt;em>; &lt;/em>;preference score according to the neural reward model. The same pool of candidates used for eviction decisions is also used to construct the pairwise preference queries for automated feedback, which helps minimize the training and inference skew between samples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEEvfAA7aieN2cZjyMIm7_MVPockLfhOAvIp7HEiF60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s1470/Halp.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;523&quot; data-original-width=&quot;1470&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEEvfAA7aieN2cZjyMIm7_MVPockLfhOAvIp7HEiF60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s16000/Halp.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the two-stage process invoked for each eviction decision.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Online preference learning with automated feedback&lt;/h2>; &lt;p>; The reward model is learned using online feedback, which is based on automatically assigned preference labels that indicate, wherever feasible, the ranked preference ordering for the time taken to receive future re-accesses, starting from a given snapshot in time among each queried sample of items. This is similar to the oracle optimal policy, which, at any given time, evicts an item with the farthest future access from all the items in the cache. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3JlTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbVHH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s1200/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3JlTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbVHH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Generation of the automated feedback for learning the reward model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To make this feedback process informative, HALP constructs pairwise preference queries that are most likely to be relevant for eviction decisions. In sync with the usual cache operations, HALP issues a small number of pairwise preference queries while making each eviction decision, and appends them to a set of &lt;em>;pending comparisons. &lt;/em>;The labels for these pending comparisons can only be resolved at a random future time. To operate online, HALP also performs some additional book-keeping after each lookup request to process any pending comparisons that can be labeled incrementally after the current request. HALP indexes the pending comparison buffer with each element involved in the comparison, and recycles the memory consumed by stale comparisons (neither of which may ever get a re-access) to ensure that the memory overhead associated with feedback generation remains bounded over time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS89UV1sxOsECA8H4PvInlUTLeB7X5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/s1468/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1032&quot; data-original-width=&quot;1468&quot; height=&quot;450&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS89UV1sxOsECA8H4PvInlUTLeB7X5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/w640-h450/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of all main components in HALP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results: Impact on the YouTube CDN&lt;/h2>; &lt;p>; Through empirical analysis, we show that HALP compares favorably to state-of-the-art cache policies on public benchmark traces in terms of cache miss rates. However, while public benchmarks are a useful tool, they are rarely sufficient to capture all the usage patterns across the world over time, not to mention the diverse hardware configurations that we have already deployed. &lt;/p>; &lt;p>; Until recently, YouTube servers used an optimized LRU-variant for memory cache eviction. HALP increases YouTube&#39;s memory egress/ingress — the ratio of the total bandwidth egress served by the CDN to that consumed for retrieval (ingress) due to cache misses — by roughly 12% and memory hit rate by 6%. This reduces latency for users, since memory reads are faster than disk reads, and also improves egressing capacity for disk-bounded machines by shielding the disks from traffic. &lt;/p>; &lt;p>; The figure below shows a visually compelling reduction in the &lt;a href=&quot;https://www.usenix.org/legacy/publications/library/proceedings/usits97/full_papers/cao/cao_html/node12.html&quot;>;byte miss ratio&lt;/a>; in the days following HALP&#39;s final rollout on the YouTube CDN, which is now serving significantly more content from within the cache with lower latency to the end user, and without having to resort to more expensive retrieval that increases the operating costs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1Tl2wPMFShOcCQm8rfTRwa0CrNYhvDhrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s505/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;257&quot; data-original-width=&quot;505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1Tl2wPMFShOcCQm8rfTRwa0CrNYhvDhrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Aggregate worldwide YouTube byte miss ratio before and after rollout (vertical dashed line).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; An aggregated performance improvement could still hide important regressions. In addition to measuring overall impact, we also conduct an analysis in the paper to understand its impact on different racks using a machine level analysis, and find it to be overwhelmingly positive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduced a scalable state-of-the-art cache eviction framework that is based on learned rewards and uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;preference learning&lt;/a>; with automated feedback. Because of its design choices, HALP can be deployed in a manner similar to any other cache policy without the operational overhead of having to separately manage the labeled examples, training procedure and the model versions as additional offline pipelines common to most machine learning systems. Therefore, it incurs only a small extra overhead compared to other classical algorithms, but has the added benefit of being able to take advantage of additional features to make its eviction decisions and continuously adapt to changing access patterns. &lt;/p>; &lt;p>; This is the first large-scale deployment of a learned cache policy to a widely used and heavily trafficked CDN, and has significantly improved the CDN infrastructure efficiency while also delivering a better quality of experience to users. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Ramki Gummadi is now part of Google DeepMind. We would like to thank John Guilyard for help with the illustrations and Richard Schooler for feedback on this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8471212041038116532/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning-with-automated.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8471212041038116532&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8471212041038116532&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning-with-automated.html&quot; rel=&quot;alternate&quot; title=&quot;Preference learning with automated feedback for cache eviction&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeCdR0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s72-c/Halp%20hero%20gif.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3798161588273442525&lt;/id>;&lt;published>;2023-06-22T11:33:00.000-07:00&lt;/published>;&lt;updated>;2023-06-22T11:33:53.578-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Audio&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SoundStorm: Efficient parallel audio generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zalán Borsos, Research Software Engineer, and Marco Tagliasacchi, Senior Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZaYunzxOsmQf8vZZYzCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s2400/SoundStorm.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The recent progress in generative AI unlocked the possibility of creating new content in several different domains, including text, vision and audio. These models often rely on the fact that raw data is first converted to a compressed format as a sequence of tokens. In the case of audio, neural audio codecs (eg, &lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;>;SoundStream&lt;/a>; or &lt;a href=&quot;https://github.com/facebookresearch/encodec&quot;>;EnCodec&lt;/a>;) can efficiently compress waveforms to a compact representation, which can be inverted to reconstruct an approximation of the original audio signal. Such a representation consists of a sequence of discrete audio tokens, capturing the local properties of sounds (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Phoneme&quot;>;phonemes&lt;/a>;) and their temporal structure (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Prosody_(linguistics)&quot;>;prosody&lt;/a>;). By representing audio as a sequence of discrete tokens, audio generation can be performed with &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>;-based sequence-to-sequence models — this has unlocked rapid progress in speech continuation (eg, with &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;), text-to-speech (eg, with &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>;), and general audio and music generation (eg, &lt;a href=&quot;https://felixkreuk.github.io/audiogen/&quot;>;AudioGen&lt;/a>; and &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;). Many generative audio models, including AudioLM, rely on auto-regressive decoding, which produces tokens one by one. While this method achieves high acoustic quality, inference (ie, calculating an output) can be slow, especially when decoding long sequences. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To address this issue, in “&lt;a href=&quot;https://google-research.github.io/seanet/soundstorm/examples/&quot;>;SoundStorm: Efficient Parallel Audio Generation&lt;/a>;”, we propose a new method for efficient and high-quality audio generation. SoundStorm addresses the problem of generating long audio token sequences by relying on two novel elements: 1) an architecture adapted to the specific nature of audio tokens as produced by the SoundStream neural codec, and 2) a decoding scheme inspired by &lt;a href=&quot;https://arxiv.org/abs/2202.04200&quot;>;MaskGIT&lt;/a>;, a recently proposed method for image generation, which is tailored to operate on audio tokens. Compared to the autoregressive decoding approach of AudioLM, SoundStorm is able to generate tokens in parallel, thus decreasing the inference time by 100x for long sequences, and produces audio of the same quality and with higher consistency in voice and acoustic conditions. Moreover, we show that SoundStorm, coupled with the text-to-semantic modeling stage of &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>;, can synthesize high-quality, natural dialogues, allowing one to control the spoken content (via transcripts), speaker voices (via short voice prompts) and speaker turns (via transcript annotations), as demonstrated by the examples below: &lt;/p>; &lt;br>; &lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/SoundStorm_promo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;br>; &lt;br>; &lt;table>; &lt;tr>; &lt;td>;&lt;b>;Input: Text&lt;em>; (transcript used to drive the audio generation in bold)&lt;/em>;&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical-align: top&quot;>;&lt;span style=&quot;font-size: small;&quot;>;&lt;em>;Something really funny happened to me this morning. | Oh wow, what? | &lt;b>;Well, uh I woke up as usual. | Uhhuh | Went downstairs to have uh breakfast. | Yeah | Started eating. Then uh 10 minutes later I realized it was the middle of the night. | Oh no way, that&#39;s so funny!&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical-align: top&quot;>;&lt;span style=&quot;font-size: small;&quot;>;&lt;em>;I didn&#39;t sleep well last night. |不好了。发生了什么？ | &lt;b>;I don&#39;t know. II just couldn&#39;t seem to uh to fall asleep somehow, I kept tossing and turning all night. |这太糟糕了。 Maybe you should uh try going to bed earlier tonight or uh maybe you could try reading a book. | Yeah, thanks for the suggestions, I hope you&#39;re right. |没问题。 II hope you get a good night&#39;s sleep&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;Input: Audio prompt&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/mp_joke_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/jm_sleep_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;Output: Audio prompt + generated audio&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/mp_joke_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/jm_sleep_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;SoundStorm design&lt;/h2>; &lt;p>; In our previous work on AudioLM, we showed that audio generation can be decomposed into two steps: 1) semantic modeling, which generates semantic tokens from either previous semantic tokens or a conditioning signal (eg, a transcript as in SPEAR-TTS, or a text prompt as in MusicLM), and 2) acoustic modeling, which generates acoustic tokens from semantic tokens. With SoundStorm we specifically address this second, acoustic modeling step, replacing slower autoregressive decoding with faster parallel decoding. &lt;/p>; &lt;p>; SoundStorm relies on a bidirectional attention-based &lt;a href=&quot;https://arxiv.org/abs/2005.08100&quot;>;Conformer&lt;/a>;, a model architecture that combines a Transformer with convolutions to capture both local and global structure of a sequence of tokens. Specifically, the model is trained to predict audio tokens produced by SoundStream given a sequence of semantic tokens generated by AudioLM as input. When doing this, it is important to take into account the fact that, at each time step &lt;em>;t&lt;/em>;, SoundStream uses up to &lt;em>;Q&lt;/em>; tokens to represent the audio using a method known as &lt;em>;residual vector quantization&lt;/em>; (RVQ), as illustrated below on the right. The key intuition is that the quality of the reconstructed audio progressively increases as the number of generated tokens at each step goes from 1 to &lt;em>;Q&lt;/em>;. &lt;/p>; &lt;p>; At inference time, given the semantic tokens as input conditioning signal, SoundStorm starts with all audio tokens masked out, and fills in the masked tokens over multiple iterations, starting from the coarse tokens at RVQ level &lt;em>;q = 1&lt;/em>; and proceeding level-by-level with finer tokens until reaching level &lt;em>;q = Q&lt;/em>;. &lt;/p>; &lt;p>; There are two crucial aspects of SoundStorm that enable fast generation: 1) tokens are predicted in parallel during a single iteration within a RVQ level and, 2) the model architecture is designed in such a way that the complexity is only mildly affected by the number of levels &lt;em>;Q&lt;/em>;. To support this inference scheme, during training a carefully designed masking scheme is used to mimic the iterative process used at inference. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOtw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7zSt2d985P6OC9On0xDwrLfj9OzJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;951&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOtw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7zSt2d985P6OC9On0xDwrLfj9OzJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SoundStorm model architecture. &lt;em>;T&lt;/em>; denotes the number of time steps and &lt;em>;Q&lt;/em>; the number of RVQ levels used by SoundStream. The semantic tokens used as conditioning are time-aligned with the SoundStream frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Measuring SoundStorm performance&lt;/h2>; &lt;p>; We demonstrate that SoundStorm matches the quality of AudioLM&#39;s acoustic generator, replacing both AudioLM&#39;s stage two (coarse acoustic model) and stage three (fine acoustic model). Furthermore, SoundStorm produces audio 100x faster than AudioLM&#39;s hierarchical autoregressive acoustic generator (top half below) with matching quality and improved consistency in terms of speaker identity and acoustic conditions (bottom half below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZgYL6-0bZWo8aLWAuYQrOJPf_vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s1999/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1122&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZgYL6-0bZWo8aLWAuYQrOJPf_vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Runtimes of SoundStream decoding, SoundStorm and different stages of AudioLM on a TPU-v4.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C-XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cpH9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCefUib8dFmGUm/s1312/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;702&quot; data-original-width=&quot;1312&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C-XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cpH9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCefUib8dFmGUm/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Acoustic consistency between the prompt and the generated audio. The shaded area represents the inter-quartile range.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Safety and risk mitigation&lt;/h2>; &lt;p>; We acknowledge that the audio samples produced by the model may be influenced by the unfair biases present in the training data, for instance in terms of represented accents and voice characteristics. In our generated samples, we demonstrate that we can reliably and responsibly control speaker characteristics via prompting, with the goal of avoiding unfair biases. A thorough analysis of any training data and its limitations is an area of future work in line with our responsible &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. &lt;/p>; &lt;p>; In turn, the ability to mimic a voice can have numerous malicious applications, including bypassing biometric identification and using the model for the purpose of impersonation. Thus, it is crucial to put in place safeguards against potential misuse: to this end, we have verified that the audio generated by SoundStorm remains detectable by a dedicated classifier using the same classifier as described in our original AudioLM paper. Hence, as a component of a larger system, we believe that SoundStorm would be unlikely to introduce additional risks to those discussed in our earlier papers on AudioLM and SPEAR-TTS. At the same time, relaxing the memory and computational requirements of AudioLM would make research in the domain of audio generation more accessible to a wider community. In the future, we plan to explore other approaches for detecting synthesized speech, eg, with the help of audio watermarking, so that any potential product usage of this technology strictly follows our responsible AI Principles. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have introduced SoundStorm, a model that can efficiently synthesize high-quality audio from discrete conditioning tokens. When compared to the acoustic generator of &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;, SoundStorm is two orders of magnitude faster and achieves higher temporal consistency when generating long audio samples. By combining a text-to-semantic token model similar to &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>; with SoundStorm, we can scale text-to-speech synthesis to longer contexts and generate natural dialogues with multiple speaker turns, controlling both the voices of the speakers and the generated content. SoundStorm is not limited to generating speech. For example, MusicLM uses SoundStorm to synthesize longer outputs efficiently (&lt;a href=&quot;https://ai.googleblog.com/2023/05/google-research-at-io-2023.html&quot;>;as seen at I/O&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The work described here was authored by Zalán Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour and Marco Tagliasacchi. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3798161588273442525/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient-parallel-audio.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3798161588273442525&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3798161588273442525&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient-parallel-audio.html&quot; rel=&quot;alternate&quot; title=&quot;SoundStorm: Efficient parallel audio generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZaYunzxOsmQf8vZZYzCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s72-c/SoundStorm.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3542229559799425219&lt;/id>;&lt;published>;2023-06-21T13:57:00.004-07:00&lt;/published>;&lt;updated>;2023-06-22T11:02:35.392-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Automatic Speech Recognition&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: AI for Social Good&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jimmy Tobin and Katrin Tomanek, Software Engineers, Google Research, AI for Social Good&lt;/span>; &lt;p>; Google&#39;s &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI for Social Good&lt;/a>; team consists of researchers, engineers, volunteers, and others with a shared focus on positive social impact. Our mission is to demonstrate AI&#39;s societal benefit by enabling real-world value, with projects spanning work in &lt;a href=&quot;https://blog.google/technology/health/making-data-useful-public-health/&quot;>;public health&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/making-android-more-accessible/&quot;>;accessibility&lt;/a>;, &lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;crisis response&lt;/a>;, &lt;a href=&quot;https://sustainability.google/&quot;>;climate and energy,&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;nature and society&lt;/a>;. We believe that the best way to drive positive change in underserved communities is by partnering with change-makers and the organizations they serve. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this blog post we discuss work done by &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;>;Project Euphonia&lt;/a>;, a team within &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI for Social Good&lt;/a>;, that aims to &lt;a href=&quot;https://ai.googleblog.com/2019/08/project-euphonias-personalized-speech.html&quot;>;improve automatic speech recognition&lt;/a>; (ASR) for people with disordered speech. For people with typical speech, an ASR model&#39;s word error rate (WER) can be less than 10%. But for people with disordered speech patterns, such as stuttering, &lt;a href=&quot;https://www.mayoclinic.org/diseases-conditions/dysarthria/symptoms-causes/syc-20371994&quot;>;dysarthria&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Apraxia&quot;>;apraxia&lt;/a>;, the WER could reach 50% or even 90% depending on the etiology and severity. To help address this problem, we worked with more than 1,000 participants to &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/project-euphonia-1000-hours-speech-recordings/&quot;>;collect over 1,000 hours of disordered speech samples&lt;/a>; and used the data to show that ASR &lt;a href=&quot;https://ai.googleblog.com/2021/09/personalized-asr-models-from-large-and.html&quot;>;personalization&lt;/a>; is a viable avenue for bridging the performance gap for users with disordered speech. We&#39;ve shown that personalization can be successful with as little as &lt;a href=&quot;https://arxiv.org/abs/2110.04612&quot;>;3-4 minutes of training speech&lt;/a>;&amp;nbsp;using &lt;a href=&quot;https://arxiv.org/abs/1907.13511&quot;>;layer freezing techniques&lt;/a>;. &lt;/p>; &lt;p>; This work led to the development of &lt;a href=&quot;https://sites.research.google/relate/&quot;>;Project Relate&lt;/a>;&amp;nbsp;for anyone with atypical speech who could benefit from a personalized speech model. Built in partnership with &lt;a href=&quot;https://research.google/research-areas/speech-processing/&quot;>;Google&#39;s Speech team&lt;/a>;, Project Relate enables people who find it hard to be understood by other people and technology to train their own models. People can use these personalized models to communicate more effectively and gain more independence. To make ASR more accessible and usable, we describe how we fine-tuned Google&#39;s &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; (USM) to better understand disordered speech out of the box, without personalization, for use with digital assistant technologies, dictation apps, and in conversations. &lt;/p>; &lt;br />; &lt;h2>;Addressing the challenges&lt;/h2>; &lt;p>; Working closely with Project Relate users, it became clear that personalized models can be very useful, but for many users, recording dozens or hundreds of examples can be challenging. In addition, the personalized models did not always perform well in freeform conversation. &lt;/p>; &lt;p>; To address these challenges, Euphonia&#39;s research efforts have been focusing on &lt;em>;speaker independent&lt;/em>; ASR (SI-ASR) to make models work better out of the box for people with disordered speech so that no additional training is necessary. &lt;/p>; &lt;br />; &lt;h2>;Prompted Speech dataset for SI-ASR&lt;/h2>; &lt;p>; The first step in building a robust SI-ASR model was to create representative dataset splits. We created the Prompted Speech dataset by splitting the &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2021/macdonald21_interspeech.pdf&quot;>;Euphonia corpus&lt;/a>; into train, validation and test portions, while ensuring that each split spanned a range of speech impairment severity and underlying etiology and that no speakers or phrases appeared in multiple splits. The training portion consists of over 950k speech utterances from over 1,000 speakers with disordered speech. The test set contains around 5,700 utterances from over 350 speakers. Speech-language pathologists manually reviewed all of the utterances in the test set for transcription accuracy and audio quality. &lt;/p>; &lt;br />; &lt;h2>;Real Conversation test set&lt;/h2>; &lt;p>; Unprompted or conversational speech differs from prompted speech in several ways. In conversation, people speak faster and enunciate less. They repeat words, repair misspoken words, and use a more expansive vocabulary that is specific and personal to themselves and their community. To improve a model for this use case, we created the Real Conversation test set to benchmark performance. &lt;/p>; &lt;p>; The Real Conversation test set was created with the help of trusted testers who recorded themselves speaking during conversations. The audio was reviewed, any personally identifiable information (PII) was removed, and then that data was transcribed by speech-language pathologists. The Real Conversation test set contains over 1,500 utterances from 29 speakers. &lt;/p>; &lt;br />; &lt;h2>;Adapting USM to disordered speech&lt;/h2>; &lt;p>; We then tuned USM on the training split of the Euphonia Prompted Speech set to improve its performance on disordered speech. Instead of fine-tuning the full model, our tuning was based on &lt;a href=&quot;http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf&quot;>;residual adapters&lt;/a>;, a parameter-efficient tuning approach that adds tunable bottleneck layers as residuals between the transformer layers. Only these layers are tuned, while the rest of the model weights are untouched. We have &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.541/&quot;>;previously shown&lt;/a>; that this approach works very well to adapt ASR models to disordered speech. Residual adapters were only added to the encoder layers, and the bottleneck dimension was set to 64. &lt;/p>; &lt;br />; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate the adapted USM, we compared it to older ASR models using the two test sets described above. For each test, we compare adapted USM to the pre-USM model best suited to that task: (1) For short prompted speech, we compare to Google&#39;s production ASR model optimized for short form ASR; (2) for longer Real Conversation speech, we compare to a model &lt;a href=&quot;https://arxiv.org/abs/2005.03271&quot;>;trained for long form ASR&lt;/a>;. USM improvements over pre-USM models can be explained by USM&#39;s relative size increase, 120M to 2B parameters, and other improvements discussed in the &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;USM blog post&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model word error rates (WER) for each test set (lower is better).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We see that the USM adapted with disordered speech significantly outperforms the other models. The adapted USM&#39;s WER on Real Conversation is 37% better than the pre-USM model, and on the Prompted Speech test set, the adapted USM performs 53% better. &lt;/p>; &lt;p>; These findings suggest that the adapted USM is significantly more usable for an end user with disordered speech. We can demonstrate this improvement by looking at transcripts of Real Conversation test set recordings from a trusted tester of Euphonia and Project Relate (see below). &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; -->; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example audio and transcriptions of a trusted tester&#39;s speech from the Real Conversation test set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A comparison of the Pre-USM and adapted USM transcripts revealed some key advantages: &lt;/p>; &lt;ul>;&lt;li>; The first example shows that Adapted USM is better at recognizing disordered speech patterns. The baseline misses key words like “XBox” and “controller” that are important for a listener to understand what they are trying to say. &lt;/li>; &lt;li>; The second example is a good example of how deletions are a primary issue with ASR models that are not trained with disordered speech. Though the baseline model did transcribe a portion correctly, a large part of the utterance was not transcribed, losing the speaker&#39;s intended message. &lt;/li>;&lt;/ul>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We believe that this work is an important step towards making speech recognition more accessible to people with disordered speech. We are continuing to work on improving the performance of our models. With the rapid advancements in ASR, we aim to ensure people with disordered speech benefit as well. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Key contributors to this project include Fadi Biadsy, Michael Brenner, Julie Cattiau, Richard Cave, Amy Chung-Yu Chou, Dotan Emanuel, Jordan Green, Rus Heywood, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Bob MacDonald, Philip Nelson, Katie Seaver, Joel Shor, Jimmy Tobin, Katrin Tomanek, and Subhashini Venugopalan. We gratefully acknowledge the support Project Euphonia received from members of the USM research team including Yu Zhang, Wei Han, Nanxin Chen, and many others. Most importantly, we wanted to say a huge thank you to the 2,200+ participants who recorded speech samples and the many &lt;a href=&quot;https://sites.research.google/euphonia/about/#thank-partners&quot;>;advocacy groups&lt;/a>; who helped us connect with these participants.&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;Audio volume has been adjusted for ease of listening, but the original files would be more consistent with those used in training and would have pauses, silences, variable volume, etc.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3542229559799425219/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: AI for Social Good&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s72-w640-h396-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4041992163804186827&lt;/id>;&lt;published>;2023-06-21T10:29:00.000-07:00&lt;/published>;&lt;updated>;2023-06-21T10:29:53.804-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;The world&#39;s first braiding of non-Abelian anyons&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Trond Andersen and Yuri Lensky, Research Scientists, Google Quantum AI Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s320/Non-Abelian%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Imagine you&#39;re shown two identical objects and then asked to close your eyes. When you open your eyes, you see the same two objects in the same position. How can you determine if they have been swapped back and forth? Intuition and the laws of &lt;a href=&quot;https://en.wikipedia.org/wiki/Identical_particles#Quantum_mechanical_description_of_identical_particles&quot;>;quantum mechanics agree&lt;/a>;: If the objects are truly identical, there is no way to tell. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While this sounds like common sense, it only applies to our familiar three-dimensional world. Researchers have predicted that for a special type of particle, called an &lt;a href=&quot;https://en.wikipedia.org/wiki/Anyon&quot;>;anyon&lt;/a>;, that is restricted to move only in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Plane_(mathematics)&quot;>;two-dimensional&lt;/a>; (2D) plane, quantum mechanics allows for something quite different. Anyons are indistinguishable from one another and some, non-Abelian anyons, have a special property that causes observable differences in the shared quantum state under exchange, making it possible to tell when they have been exchanged, despite being fully indistinguishable from one another. While researchers have managed to detect their relatives, Abelian anyons, whose change under exchange is more subtle and impossible to directly detect, realizing “non-Abelian exchange behavior” has proven more difficult due to challenges with both control and detection. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-05954-4&quot;>;Non-Abelian braiding of graph vertices in a superconducting processor&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;Nature&lt;/a>;&lt;/em>;, we report the observation of this non-Abelian exchange behavior for the first time. Non-Abelian anyons could open a new avenue for quantum computation, in which quantum operations are achieved by swapping particles around one another like strings are swapped around one another to create braids. Realizing this new exchange behavior on our &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;superconducting quantum processor&lt;/a>; could be an alternate route to so-called &lt;a href=&quot;https://arxiv.org/abs/quant-ph/9707021&quot;>;topological quantum computation&lt;/a>;, which benefits from being robust against environmental noise. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Exchange statistics and non-Abelian anyons&lt;/h2>; &lt;p>; In order to understand how this strange non-Abelian behavior can occur, it&#39;s helpful to consider an analogy with the braiding of two strings. Take two identical strings and lay them parallel next to one another. Swap their ends to form a double-helix shape. The strings are identical, but because they wrap around one another when the ends are exchanged, it is very clear when the two ends are swapped. &lt;/p>; &lt;p>; The exchange of non-Abelian anyons can be visualized in a similar way, where the strings are made from extending the particles&#39; positions into the time dimension to form “world-lines.” Imagine plotting two particles&#39; locations vs. time. If the particles stay put, the plot would simply be two parallel lines, representing their constant locations. But if we exchange the locations of the particles, the world lines wrap around one another. Exchange them a second time, and you&#39;ve made a &lt;a href=&quot;https://en.wikipedia.org/wiki/Knot_theory&quot;>;knot&lt;/a>;. &lt;/p>; &lt;p>; While a bit difficult to visualize, knots in four dimensions (three spatial plus one time dimension) can always easily be undone. They are trivial — like a shoelace, simply pull one end and it unravels. But when the particles are restricted to two spatial dimensions, the knots are in three total dimensions and — as we know from our everyday 3D lives — cannot always be easily untied. The braiding of the non-Abelian anyons&#39; world lines can be used as quantum computing operations to transform the state of the particles. &lt;/p>; &lt;p>; A key aspect of non-Abelian anyons is “degeneracy”: the full state of several separated anyons is not completely specified by local information, allowing the same anyon configuration to represent superpositions of several quantum states. Winding non-Abelian anyons about each other can change the encoded state. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;How to make a non-Abelian anyon&lt;/h2>; &lt;p>; So how do we realize non-Abelian braiding with one of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sycamore_processor&quot;>;Google&#39;s quantum processors&lt;/a>;? We start with the familiar surface code, which we recently used to achieve a &lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;milestone in quantum error correction&lt;/a>;, where qubits are arranged on the vertices of a checkerboard pattern. Each color square of the checkerboard represents one of two possible joint measurements that can be made of the qubits on the four corners of the square. These so-called “stabilizer measurements” can return a value of either + or – 1. The latter is referred to as a plaquette violation, and can be created and moved diagonally — just like bishops in chess — by applying single-qubit &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_logic_gate&quot;>;X- and Z-gates&lt;/a>;. Recently, we showed that these bishop-like &lt;a href=&quot;https://arxiv.org/abs/2104.01180&quot;>;plaquette violations are Abelian anyons&lt;/a>;. In contrast to non-Abelian anyons, the state of Abelian anyons changes only subtly when they are swapped — so subtly that it is impossible to directly detect. While Abelian anyons are interesting, they do not hold the same promise for topological quantum computing that non-Abelian anyons do. &lt;/p>; &lt;p>; To produce non-Abelian anyons, we need to control the degeneracy (ie, the number of &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function&quot;>;wavefunctions&lt;/a>; that causes all &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;stabilizer&lt;/a>; measurements to be +1). Since a stabilizer measurement returns two possible values, each stabilizer cuts the degeneracy of the system in half, and with sufficiently many stabilizers, only one wave function satisfies the criterion. Hence, a simple way to increase the degeneracy is to merge two stabilizers together. In the process of doing so, we remove one edge in the stabilizer grid, giving rise to two points where only three edges intersect. These points, referred to as “&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;degree-3 vertices&lt;/a>;” (D3Vs), are predicted to be non-Abelian anyons. &lt;/p>; &lt;p>; In order to braid the D3Vs, we have to move them, meaning that we have to stretch and squash the stabilizers into new shapes. We accomplish this by implementing two-qubit gates between the anyons and their neighbors (middle and right panels shown below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/s661/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;661&quot; height=&quot;529&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/w640-h529/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Non-Abelian anyons in stabilizer codes.&lt;strong>; a:&lt;/strong>; Example of a knot made by braiding two anyons&#39; world lines. &lt;strong>;b: &lt;/strong>;Single-qubit gates can be used to create and move stabilizers with a value of –1 (red squares). Like bishops in chess, these can only move diagonally and are therefore constrained to one sublattice in the regular surface code. This constraint is broken when D3Vs (yellow triangles) are introduced.&lt;strong>; c: &lt;/strong>;Process to form and move D3Vs (predicted to be non-Abelian anyons). We start with the surface code, where each square corresponds to a joint measurement of the four qubits on its corners (&lt;strong>;left&lt;/strong>; panel). We remove an edge separating two neighboring squares, such that there is now a single joint measurement of all six qubits (&lt;strong>;middle&lt;/strong>; panel). This creates two D3Vs, which are non-Abelian anyons. We move the D3Vs by applying two-qubit gates between neighboring sites (right panel). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Now that we have a way to create and move the non-Abelian anyons, we need to verify their anyonic behavior. For this we examine three characteristics that would be expected of non-Abelian anyons: &lt;/p>; &lt;ol>; &lt;li>;The “&lt;a href=&quot;https://en.wikipedia.org/wiki/Fusion_of_anyons&quot;>;fusion rules&lt;/a>;” — What happens when non-Abelian anyons collide with each other? &lt;/li>;&lt;li>;Exchange statistics — What happens when they are braided around one another? &lt;/li>;&lt;li>;Topological quantum computing primitives — Can we encode qubits in the non-Abelian anyons and use braiding to perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;>;two-qubit entangling operations&lt;/a>;? &lt;/li>; &lt;/ol>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The fusion rules of non-Abelian anyons&lt;/h2>; &lt;p>; We investigate fusion rules by studying how a pair of D3Vs interact with the bishop-like plaquette violations introduced above. In particular, we create a pair of these and bring one of them around a D3V by applying single-qubit gates. &lt;/p>; &lt;p>; While the rules of bishops in chess dictate that the plaquette violations can never meet, the dislocation in the checkerboard lattice allows them to break this rule, meet its partner and annihilate with it. The plaquette violations have now disappeared! But bring the non-Abelian anyons back in contact with one another, and the anyons suddenly morph into the missing plaquette violations. As weird as this behavior seems, it is a manifestation of exactly the fusion rules that we expect these entities to obey. This establishes confidence that the D3Vs are, indeed, non-Abelian anyons. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/s982/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;826&quot; data-original-width=&quot;982&quot; height=&quot;538&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/w640-h538/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Demonstration of anyonic fusion rules (starting with &lt;strong>;panel I&lt;/strong>;, in the &lt;strong>;lower left&lt;/strong>;). We form and separate two D3Vs (yellow triangles), then form two adjacent plaquette violations (red squares) and pass one between the D3Vs. The D3Vs deformation of the “chessboard” changes the bishop rules of the plaquette violations. While they used to lie on adjacent squares, they are now able to move along the same diagonals and collide (as shown by the red lines). When they do collide, they annihilate one another. The D3Vs are brought back together and surprisingly morph into the missing adjacent red plaquette violations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Observation of non-Abelian exchange statistics&lt;/h2>; &lt;p>; After establishing the fusion rules, we want to see the real smoking gun of non-Abelian anyons: non-Abelian exchange statistics. We create two pairs of non-Abelian anyons, then braid them by wrapping one from each pair around each other (shown below). When we fuse the two pairs back together, two pairs of plaquette violations appear. The simple act of braiding the anyons around one another changed the observables of our system. In other words, if you closed your eyes while the non-Abelian anyons were being exchanged, you would still be able to tell that they had been exchanged once you opened your eyes. This is the hallmark of non-Abelian statistics. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/s1648/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;1648&quot; height=&quot;437&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/w640-h437/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Braiding non-Abelian anyons. We make two pairs of D3Vs (&lt;strong>;panel II&lt;/strong>;), then bring one from each pair around each other (&lt;strong>;III-XI&lt;/strong>;). When fusing the two pairs together again in panel XII, two pairs of plaquette violations appear! Braiding the non-Abelian anyons changed the observables of the system from panel I to panel XII; a direct manifestation of non-Abelian exchange statistics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Topological quantum computing&lt;/h2>; &lt;p>; Finally, after establishing their fusion rules and exchange statistics, we demonstrate how we can use these particles in quantum computations. The non-Abelian anyons can be used to encode information, represented by &lt;a href=&quot;https://ai.googleblog.com/2021/08/demonstrating-fundamentals-of-quantum.html&quot;>;logical qubits&lt;/a>;, which should be distinguished from the actual &lt;em>;physical&lt;/em>; qubits used in the experiment. The number of logical qubits encoded in &lt;em>;N&lt;/em>; D3Vs can be shown to be &lt;em>;N&lt;/em>;/2–1, so we use &lt;em>;N&lt;/em>;=8 D3Vs to encode three logical qubits, and perform braiding to entangle them. By studying the resulting state, we find that the braiding has indeed led to the formation of the desired, well-known quantum entangled state called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenberger%E2%80%93Horne%E2%80%93Zeilinger_state&quot;>;Greenberger-Horne-Zeilinger&lt;/a>; (GHZ) state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/s750/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;544&quot; data-original-width=&quot;750&quot; height=&quot;464&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/w640-h464/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using non-Abelian anyons as logical qubits.&lt;strong>; a, &lt;/strong>;We braid the non-Abelian anyons to entangle three qubits encoded in eight D3Vs. &lt;strong>;b, &lt;/strong>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_tomography&quot;>;Quantum state tomography&lt;/a>; allows for reconstructing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Density_matrix&quot;>;density matrix&lt;/a>;, which can be represented in a 3D bar plot and is found to be consistent with the desired highly entangled GHZ-state.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our experiments show the first observation of non-Abelian exchange statistics, and that braiding of the D3Vs can be used to perform quantum computations. With future additions, including error correction during the braiding procedure, this could be a major step towards topological quantum computation, a long-sought method to endow qubits with intrinsic resilience against fluctuations and noise that would otherwise cause errors in computations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Katie McCormick, our Quantum Science Communicator, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4041992163804186827/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html&quot; rel=&quot;alternate&quot; title=&quot;The world&#39;s first braiding of non-Abelian anyons&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s72-c/Non-Abelian%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9139300663122353070&lt;/id>;&lt;published>;2023-06-18T11:00:00.018-07:00&lt;/published>;&lt;updated>;2023-06-19T10:54:26.149-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at CVPR 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shaina Mehta, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s1200/CVPR%20Design-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week marks the beginning of the premier annual &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;Computer Vision and Pattern Recognition&lt;/a>; conference (CVPR 2023), held in-person in Vancouver, BC (with additional virtual content). As a leader in computer vision research and a &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/Sponsors&quot;>;Platinum Sponsor&lt;/a>;, &lt;a href=&quot;https://research.google/&quot;>;Google Research&lt;/a>; will have a strong presence across CVPR 2023 with ~90 papers being presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers&quot;>;main conference&lt;/a>; and active involvement in over 40 conference &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/workshop-list&quot;>;workshops&lt;/a>; and &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/tutorial-list&quot;>;tutorials&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you are attending CVPR this year, please stop by our booth to chat with our researchers who are actively exploring the latest techniques for application to various areas of &lt;a href=&quot;https://research.google/pubs/?area=machine-perception&quot;>;machine perception&lt;/a>;. Our researchers will also be available to talk about and demo several recent efforts, including on-device ML applications with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>;, strategies for differential privacy, neural radiance field technologies and much more. &lt;/p>; &lt;p>; You can also learn more about our research being presented at CVPR 2023 in the list below (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>; Board and organizing committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Senior area chairs include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Area chairs include: &lt;em>;&lt;strong>;Andre Araujo&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Rodrigo Benenson&lt;/strong>;, &lt;strong>;Ayan Chakrabarti&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;, &lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Da-Cheng Jua&lt;/strong>;, &lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Stephen Lombardi&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;Saurabh Singh&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Pratul P. Srinivasan&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;, &lt;strong>;Jasper Uijlings&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Publicity Chair: &lt;strong>;&lt;em>;Boqing Gong&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Demonstration Chair: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Program Advisory Board includes: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Panels&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>;&lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;History and Future of Artificial Intelligence and Computer Vision &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>;&lt;/div>; &lt;p>; &lt;/p>; &lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;Scientific Discovery and the Environment &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;/div>; &lt;/div>; &lt;br />; &lt;h2>;Best Paper Award candidates&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.00277.pdf&quot;>;MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhiqin Chen&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11082.pdf&quot;>;DynIBaR: Neural Dynamic Image-Based Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Qianqian Wang&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Richard Tucker&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.12242.pdf&quot;>;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/a>; &lt;br />; &lt;em>;Nataniel Ruiz*, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Kfir Aberman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03142.pdf&quot;>;On Distillation of Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Chenlin Meng, Robin Rombach,&lt;strong>; Ruiqi Gao&lt;/strong>;,&lt;strong>; Diederik Kingma&lt;/strong>;, Stefano Ermon,&lt;strong>; Jonathan Ho&lt;/strong>;,&lt;strong>; Tim Salimans&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Highlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.11217.pdf&quot;>;Connecting Vision and Language with Video Localized Narratives&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Paul Voigtlaender&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.05496.pdf&quot;>;MaskSketch: Unpaired Structure-Guided Masked Image Generation&lt;/a>; &lt;br />; &lt;em>;Dina Bashkirova*,&lt;strong>; Jose Lezama&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11738.pdf&quot;>;SPARF: Neural Radiance Fields from Sparse and Noisy Poses&lt;/a>; &lt;br />; &lt;em>;Prune Truong*,&lt;strong>; Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Fabian Manhardt&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05199.pdf&quot;>;MAGVIT: Masked Generative Video Transformer&lt;/a>; &lt;br />; &lt;em>;Lijun Yu*,&lt;strong>; Yong Cheng&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alexander Hauptmann&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf&quot;>;Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.02291.pdf&quot;>;I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification&lt;/a>; &lt;br />; &lt;em>;Muhammad Ferjad Naeem, Gul Zain Khan,&lt;strong>; Yongqin Xian&lt;/strong>;, Muhammad Zeshan Afzal, Didier Stricker, Luc Van Gool,&lt;strong>; Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12624.pdf&quot;>;Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization&lt;/a>; &lt;br />; &lt;em>;Zifan Wang*,&lt;strong>; Nan Ding&lt;/strong>;, &lt;strong>;Tomer Levinboim&lt;/strong>;, &lt;strong>;Xi Chen&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/a>; (see &lt;a href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Su Wang&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Ceslee Montgomery&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Shai Noy&lt;/strong>;, &lt;strong>;Stefano Pellegrini&lt;/strong>;, &lt;strong>;Yasumasa Onoe&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;William Cha&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14306.pdf&quot;>;RUST: Latent Neural Scene Representations from Unposed Imagery&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mehdi SM Sajjadi&lt;/strong>;, &lt;strong>;Aravindh Mahendran&lt;/strong>;, &lt;strong>;Thomas Kipf&lt;/strong>;, &lt;strong>;Etienne Pot&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Mario Lučić&lt;/strong>;, &lt;strong>;Klaus Greff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05221.pdf&quot;>;REVEAL: Retrieval-Augmented Visual-Language Pre-training with Multi-Source Multimodal Knowledge Memory&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Ziniu Hu*,&lt;strong>; Ahmet Iscen&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Zirui Wang&lt;/strong>;, Kai-Wei Chang,&lt;strong>; Yizhou Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;David Ross&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.00833.pdf&quot;>;RobustNeRF: Ignoring Distractors with Robust Losses&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Sara Sabour&lt;/strong>;, &lt;strong>;Suhani Vora&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Ivan Krasin&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09682.pdf&quot;>;AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training&lt;/a>; &lt;br />; &lt;em>;Yifan Jiang*, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, Dejia Xu, &lt;strong>;Jonathan T. Barron&lt;/strong>;, Zhangyang Wang, Tianfan Xue*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kania_BlendFields_Few-Shot_Example-Driven_Facial_Modeling_CVPR_2023_paper.pdf&quot;>;BlendFields: Few-Shot Example-Driven Facial Modeling&lt;/a>; &lt;br />; &lt;em>;Kacper Kania, Stephan Garbin, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00583.pdf&quot;>;Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints&lt;/a>; &lt;br />; &lt;em>;Guilherme Potje, Felipe Cadar, &lt;strong>;Andre Araujo&lt;/strong>;, Renato Martins, Erickson Nascimento&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf&quot;>;How Can Objects Help Action Recognition?&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.12652.pdf&quot;>;Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur&lt;/a>; &lt;br />; &lt;em>;Peng Dai, &lt;strong>;Yinda Zhang&lt;/strong>;, Xin Yu, Xiaoyang Lyu, Xiaojuan Qi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14396.pdf&quot;>;IFSeg: Image-Free Semantic Segmentation via Vision-Language Model&lt;/a>; &lt;br />; &lt;em>;Sukmin Yun, Seong Park, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-Aware Saliency Modeling&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Shi Chen*, &lt;strong>;Nachiappan Valliappan&lt;/strong>;, &lt;strong>;Shaolei Shen&lt;/strong>;, &lt;strong>;Xinyu Ye&lt;/strong>;, &lt;strong>;Kai Kohlhoff&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09117.pdf&quot;>;MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis&lt;/a>; &lt;br />; &lt;em>;Tianhong Li*,&lt;strong>; Huiwen Chang&lt;/strong>;, Shlok Kumar Mishra,&lt;strong>; Han Zhang&lt;/strong>;, Dina Katabi,&lt;strong>; Dilip Krishnan&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17603.pdf&quot;>;NeRF-Supervised Deep Stereo&lt;/a>; &lt;br />; &lt;em>;Fabio Tosi, &lt;strong>;Alessio Tonioni&lt;/strong>;, Daniele Gregorio, Matteo Poggi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Suhail_Omnimatte3D_Associating_Objects_and_Their_Effects_in_Unconstrained_Monocular_Video_CVPR_2023_paper.pdf&quot;>;Omnimatte3D: Associating Objects and their Effects in Unconstrained Monocular Video&lt;/a>; &lt;br />; &lt;em>;Mohammed Suhail, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, Leon Sigal, &lt;strong>;Forrester Cole&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15654.pdf&quot;>;OpenScene: 3D Scene Understanding with Open Vocabularies&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Songyou Peng&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;, &lt;strong>;Chiyu Jiang&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Marc Pollefeys&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.08504.pdf&quot;>;PersonNeRF: Personalized Reconstruction from Photo Collections&lt;/a>; &lt;br />; &lt;em>;Chung-Yi Weng,&lt;strong>; Pratul Srinivasan&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Prefix_Conditioning_Unifies_Language_and_Label_Supervision_CVPR_2023_paper.pdf&quot;>;Prefix Conditioning Unifies Language and Label Supervision&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, Kate Saenko, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.pdf&quot;>;Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;AJ Piergiovanni&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01194.pdf&quot;>;Burstormer: Burst Image Restoration and Enhancement Transformer&lt;/a>; &lt;br />; &lt;em>;Akshay Dudhane, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan,&lt;strong>; Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15774.pdf&quot;>;Decentralized Learning with Multi-Headed Distillation&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Andrey Zhmoginov&lt;/strong>;, &lt;strong>;Mark Sandler&lt;/strong>;, &lt;strong>;Nolan Miller&lt;/strong>;, &lt;strong>;Gus Kristiansen&lt;/strong>;, &lt;strong>;Max Vladymyrov&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.02163.pdf&quot;>;GINA-3D: Learning to Generate Implicit Neural Assets in the Wild&lt;/a>; &lt;br />; &lt;em>;Bokui Shen, Xinchen Yan, Charles R. Qi, Mahyar Najibi, Boyang Deng,&lt;strong>; Leonidas Guibas&lt;/strong>;, Yin Zhou, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.11846.pdf&quot;>;Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent with Learned Distance Functions&lt;/a>; &lt;br />; &lt;em>;Yun He, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;, Xiangyang Xue, Yanwei Fu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.11042.pdf&quot;>;Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery from Sparse Image Ensemble&lt;/a>; &lt;br />; &lt;em>;Chun-Han Yao*, Wei-Chih Hung, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.00653.pdf&quot;>;Hyperbolic Contrastive Learning for Visual Representations beyond Objects&lt;/a>; &lt;br />; &lt;em>;Songwei Ge, Shlok Mishra,&lt;strong>; Simon Kornblith&lt;/strong>;, &lt;strong>;Chun-Liang Li, &lt;/strong>;David Jacobs&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09276.pdf&quot;>;Imagic: Text-Based Real Image Editing with Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Bahjat Kawar*,&lt;strong>; Shiran Zada&lt;/strong>;, &lt;strong>;Oran Lang&lt;/strong>;, &lt;strong>;Omer Tov&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Tali Dekel&lt;/strong>;, &lt;strong>;Inbar Mosseri&lt;/strong>;, &lt;strong>;Michal Irani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02743.pdf&quot;>;Incremental 3D Semantic Scene Graph Prediction from RGB Sequences&lt;/a>; &lt;br />; &lt;em>;Shun-Cheng Wu, &lt;strong>;Keisuke Tateno&lt;/strong>;, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.00575.pdf&quot;>;IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction&lt;/a>; &lt;br />; &lt;em>;Dekai Zhu, Guangyao Zhai, Yan Di, &lt;strong>;Fabian Manhardt&lt;/strong>;, Hendrik Berkemeyer, Tuan Tran, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.10844.pdf&quot;>;Learning to Generate Image Embeddings with User-Level Differential Privacy&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zheng Xu, Maxwell Collins, Yuxiao Wang, Liviu Panait, Sewoong Oh, Sean Augenstein, Ting Liu, Florian Schroff, H. Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05866.pdf&quot;>;NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs&lt;/a>; &lt;br />; &lt;em>;Harsh Rangwani, Lavish Bansal, Kartik Sharma,&lt;strong>; Tejan Karmali&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, Venkatesh Babu Radhakrishnan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09794.pdf&quot;>;NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Ron Mokady*, Amir Hertz*, &lt;strong>;Kfir Aberman&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, Daniel Cohen-Or*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14020.pdf&quot;>;SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow&lt;/a>; &lt;br />; &lt;em>;Itai Lang*,&lt;strong>; Dror Aiger&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Shai Avidan&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11674.pdf&quot;>;Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion&lt;/a>; &lt;br />; &lt;em>;Dario Pavllo*,&lt;strong>; David Joseph Tan&lt;/strong>;, &lt;strong>;Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12902.pdf&quot;>;TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Hanzhi Chen, &lt;strong>;Fabian Manhardt&lt;/strong>;, Nassir Navab, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf&quot;>;TryOnDiffusion: A Tale of Two UNets&lt;/a>; &lt;br />; &lt;em>;Luyang Zhu*, &lt;strong>;Dawei Yang&lt;/strong>;, &lt;strong>;Tyler Zhu&lt;/strong>;, &lt;strong>;Fitsum Reda&lt;/strong>;, &lt;strong>;William Chan&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03112.pdf&quot;>;A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning&lt;/a>; &lt;br />; &lt;em>;Aishwarya Kamath*, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;Su Wang&lt;/strong>;, Jing Yu Koh*, &lt;strong>;Alexander Ku&lt;/strong>;, &lt;strong>;Austin Waters&lt;/strong>;, Yinfei Yang*, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Zarana Parekh&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08045.pdf&quot;>;CLIPPO: Image-and-Language Understanding from Pixels Only&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Basil Mustafa&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.04745.pdf&quot;>;Controllable Light Diffusion for Portraits&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;David Futschik&lt;/strong>;, &lt;strong>;Kelvin Ritland&lt;/strong>;, &lt;strong>;James Vecore&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Daniel Sýkora&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Vasconcelos_CUF_Continuous_Upsampling_Filters_CVPR_2023_paper.pdf&quot;>;CUF: Continuous Upsampling Filters&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Cristina Vasconcelos&lt;/strong>;, &lt;strong>;Cengiz Oztireli&lt;/strong>;, &lt;strong>;Mark Matthews&lt;/strong>;, &lt;strong>;Milad Hashemi&lt;/strong>;, &lt;strong>;Kevin Swersky&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01758.pdf&quot;>;Improving Zero-Shot Generalization and Robustness of Multi-modal Models&lt;/a>; &lt;br />; &lt;em>;Yunhao Ge*, &lt;strong>;Jie Ren&lt;/strong>;, &lt;strong>;Andrew Gallagher&lt;/strong>;, &lt;strong>;Yuxiao Wang&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Hartwig Adam&lt;/strong>;, &lt;strong>;Laurent Itti&lt;/strong>;, &lt;strong>;Balaji Lakshminarayanan&lt;/strong>;, &lt;strong>;Jiaping Zhao&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09665.pdf&quot;>;LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding&lt;/a>; &lt;br />; &lt;em>;Gen Li, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, Laura Sevilla-Lara&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03361.pdf&quot;>;Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xiaoshuai Zhang&lt;/strong>;, &lt;strong>;Abhijit Kundu&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Hao Su&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01762.pdf&quot;>;Self-Supervised AutoFlow&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hsin-Ping Huang&lt;/strong>;, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Junhwa Hur&lt;/strong>;, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Kyle Sargent&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf&quot;>;Train-Once-for-All Personalization&lt;/a>; &lt;br />; &lt;em>;Hong-You Chen*,&lt;strong>; Yandong Li&lt;/strong>;, &lt;strong>;Yin Cui&lt;/strong>;, &lt;strong>;Mingda Zhang&lt;/strong>;, Wei-Lun Chao,&lt;strong>; Li Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.14115.pdf&quot;>;Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Antoine Yang*, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Antoine Miech, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, Ivan Laptev, Josef Sivic, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14302.pdf&quot;>;VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Junjie Ke&lt;/strong>;, &lt;strong>;Keren Ye&lt;/strong>;, &lt;strong>;Jiahui Yu&lt;/strong>;, &lt;strong>;Yonghui Wu&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Feng Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11152.pdf&quot;>;You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model&lt;/a>; &lt;br />; &lt;em>;Shengkun Tang, &lt;strong>;Yaqing Wang&lt;/strong>;, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, &lt;strong>;Yi Liang&lt;/strong>;, Dongkuan Xu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.05211.pdf&quot;>;Accidental Light Probes&lt;/a>; &lt;br />; &lt;em>;Hong-Xing Yu, Samir Agarwala, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;, &lt;strong>;Noah Snavely, &lt;/strong>;Jiajun Wu, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.09653.pdf&quot;>;FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning&lt;/a>; &lt;br />; &lt;em>;Yuanhao Xiong, Ruochen Wang, Minhao Cheng,&lt;strong>; Felix Yu&lt;/strong>;, Cho-Jui Hsieh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08013.pdf&quot;>;FlexiViT: One Model for All Patch Sizes&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Lucas Beyer&lt;/strong>;, &lt;strong>;Pavel Izmailov&lt;/strong>;, &lt;strong>;Alexander Kolesnikov&lt;/strong>;, &lt;strong>;Mathilde Caron&lt;/strong>;, &lt;strong>;Simon Kornblith&lt;/strong>;, &lt;strong>;Xiaohua Zhai&lt;/strong>;, &lt;strong>;Matthias Minderer&lt;/strong>;, &lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Ibrahim Alabdulmohsin&lt;/strong>;, &lt;strong>;Filip Pavetic&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03087.pdf&quot;>;Iterative Vision-and-Language Navigation&lt;/a>; &lt;br />; &lt;em>;Jacob Krantz, Shurjo Banerjee, Wang Zhu, Jason Corso,&lt;strong>; Peter Anderson&lt;/strong>;, Stefan Lee, Jesse Thomason&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.08010.pdf&quot;>;MoDi: Unconditional Motion Synthesis from Diverse Data&lt;/a>; &lt;br />; &lt;em>;Sigal Raab, Inbal Leibovitch, Peizhuo Li,&lt;strong>; Kfir Aberman&lt;/strong>;, Olga Sorkine-Hornung, Daniel Cohen-Or&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03369.pdf&quot;>;Multimodal Prompting with Missing Modalities for Visual Recognition&lt;/a>; &lt;br />; &lt;em>;Yi-Lun Lee,&lt;strong>; Yi-Hsuan Tsai&lt;/strong>;, Wei-Chen Chiu,&lt;strong>; Chen-Yu Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf&quot;>;Scene-Aware Egocentric 3D Human Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu,&lt;strong>; Kripasindhu Sarkar&lt;/strong>;, Christian Theobalt&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06247.pdf&quot;>;ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-Based Consistency&lt;/a>; &lt;br />; &lt;em>;Zixuan Huang,&lt;strong>; Varun Jampani&lt;/strong>;, Ngoc Anh Thai,&lt;strong>; Yuanzhen Li&lt;/strong>;, Stefan Stojanov, James M. Rehg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05173.pdf&quot;>;Improving Image Recognition by Retrieving from Web-Scale Image-Text Data&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Ahmet Iscen&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00341.pdf&quot;>;JacobiNeRF: NeRF Shaping with Mutual Information Gradients&lt;/a>; &lt;br />; &lt;em>;Xiaomeng Xu, Yanchao Yang, Kaichun Mo, Boxiao Pan, Li Yi,&lt;strong>; Leonidas Guibas&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01436.pdf&quot;>;Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos&lt;/a>; &lt;br />; &lt;em>;Ziqian Bai*,&lt;strong>; Feitong Tan&lt;/strong>;, &lt;strong>;Zeng Huang&lt;/strong>;, &lt;strong>;Kripasindhu Sarkar&lt;/strong>;, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Di Qiu&lt;/strong>;, &lt;strong>;Abhimitra Meka&lt;/strong>;, &lt;strong>;Ruofei Du&lt;/strong>;, &lt;strong>;Mingsong Dou&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;, Ping Tan,&lt;strong>; Thabo Beeler&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.08556.pdf&quot;>;NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis&lt;/a>; &lt;br />; &lt;em>;Allan Zhou, Mo Jin Kim, Lirui Wang,&lt;strong>; Pete Florence&lt;/strong>;, &lt;strong>;Chelsea Finn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03084.pdf&quot;>;Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*,&lt;strong>; Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13582.pdf&quot;>;SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mikaela Uy&lt;/strong>;, &lt;strong>;Ricardo Martin Brualla&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Ke Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06820.pdf&quot;>;Structured 3D Features for Reconstructing Controllable Avatars&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Enric Corona&lt;/strong>;, &lt;strong>;Mihai Zanfir&lt;/strong>;, &lt;strong>;Thiemo Alldieck&lt;/strong>;, &lt;strong>;Eduard Gabriel Bazavan&lt;/strong>;, &lt;strong>;Andrei Zanfir&lt;/strong>;, &lt;strong>;Cristian Sminchisescu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09119.pdf&quot;>;Token Turing Machines&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael S. Ryoo&lt;/strong>;, &lt;strong>;Keerthana Gopalakrishnan&lt;/strong>;, &lt;strong>;Kumara Kahatapitiya&lt;/strong>;, &lt;strong>;Ted Xiao&lt;/strong>;, &lt;strong>;Kanishka Rao&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Yao Lu&lt;/strong>;, &lt;strong>;Julian Ibarz&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10957.pdf&quot;>;TruFor: Leveraging All-Round Clues for Trustworthy Image Forgery Detection and Localization&lt;/a>; &lt;br />; &lt;em>;Fabrizio Guillaro, Davide Cozzolino,&lt;strong>; Avneesh Sud&lt;/strong>;, &lt;strong>;Nicholas Dufour, &lt;/strong>;Luisa Verdoliva&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.07685.pdf&quot;>;Video Probabilistic Diffusion Models in Projected Latent Space&lt;/a>; &lt;br />; &lt;em>;Sihyun Yu,&lt;strong>; Kihyuk Sohn, &lt;/strong>;Subin Kim, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00990.pdf&quot;>;Visual Prompt Tuning for Generative Transfer Learning&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Luisa Polania&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17811.pdf&quot;>;Zero-Shot Referring Image Segmentation with Global-Local Context Features&lt;/a>; &lt;br />; &lt;em>;Seonghoon Yu,&lt;strong>; Paul Hongsuck Seo&lt;/strong>;, Jeany Son&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.16501.pdf&quot;>;AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.03285.pdf&quot;>;DC2: Dual-Camera Defocus Control by Learning to Refocus&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hadi Alzayer&lt;/strong>;, &lt;strong>;Abdullah Abuolaim&lt;/strong>;, &lt;strong>;Leung Chun Chan&lt;/strong>;, &lt;strong>;Yang Yang&lt;/strong>;, &lt;strong>;Ying Chen Lou&lt;/strong>;, &lt;strong>;Jia-Bin Huang&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_Edges_to_Shapes_to_Concepts_Adversarial_Augmentation_for_Robust_Vision_CVPR_2023_paper.pdf&quot;>;Edges to Shapes to Concepts: Adversarial Augmentation for Robust Vision&lt;/a>; &lt;br />; &lt;em>;Aditay Tripathi*,&lt;strong>; Rishubh Singh&lt;/strong>;, Anirban Chakraborty,&lt;strong>; Pradeep Shenoy&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09898.pdf&quot;>;MetaCLUE: Towards Comprehensive Visual Metaphors Research&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arjun R. Akula&lt;/strong>;, &lt;strong>;Brendan Driscoll&lt;/strong>;, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Zhiwei Jia&lt;/strong>;, &lt;strong>;Suyash Damle&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;William T. Freeman&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13824.pdf&quot;>;Multi-Realism Image Compression with a Conditional Generator&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Eirikur Agustsson&lt;/strong>;, &lt;strong>;David Minnen&lt;/strong>;, &lt;strong>;George Toderici&lt;/strong>;, &lt;strong>;Fabian Mentzer&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.03267.pdf&quot;>;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/a>; &lt;br />; &lt;em>;Congyue Deng, Chiyu Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou,&lt;strong>; Leonidas Guibas&lt;/strong>;, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12053.pdf&quot;>;On Calibrating Semantic Segmentation Models: Analyses and an Algorithm&lt;/a>; &lt;br />; &lt;em>;Dongdong Wang,&lt;strong>; Boqing Gong&lt;/strong>;, Liqiang Wang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13515.pdf&quot;>;Persistent Nature: A Generative Model of Unbounded 3D Worlds&lt;/a>; &lt;br />; &lt;em>;Lucy Chai,&lt;strong>; Richard Tucker&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, Phillip Isola,&lt;strong>; Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13662.pdf&quot;>;Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment&lt;/a>; &lt;br />; &lt;em>;Yiyou Sun*,&lt;strong>; Yaojie Liu&lt;/strong>;, &lt;strong>;Xiaoming Liu&lt;/strong>;, Yixuan Li,&lt;strong>; Wen-Sheng Chu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13277.pdf&quot;>;SINE: Semantic-Driven Image-Based NeRF Editing with Prior-Guided Editing Field&lt;/a>; &lt;br />; &lt;em>;Chong Bao,&lt;strong>; Yinda Zhang&lt;/strong>;, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15533.pdf&quot;>;Sequential Training of GANs Against GAN-Classifiers Reveals Correlated &quot;Knowledge Gaps&quot; Present Among Independently Trained GAN Instances&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arkanath Pathak&lt;/strong>;, &lt;strong>;Nicholas Dufour&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.16991.pdf&quot;>;SparsePose: Sparse-View Camera Pose Regression and Refinement&lt;/a>; &lt;br />; &lt;em>;Samarth Sinha, Jason Zhang,&lt;strong>; Andrea Tagliasacchi&lt;/strong>;, Igor Gilitschenski, David Lindell&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;Teacher-Generated Spatial-Attention Labels Boost Robustness and Accuracy of Contrastive Models&lt;/a>; &lt;br />; &lt;em>;Yushi Yao,&lt;strong>; Chang Ye&lt;/strong>;, &lt;strong>;Gamaleldin F. Elsayed&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://cv4mr.github.io/&quot;>;Computer Vision for Mixed Reality&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ira Kemelmacher-Shlizerman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvpr2023.wad.vision/&quot;>;Workshop on Autonomous Driving (WAD)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://multimodal-content-moderation.github.io/&quot;>;Multimodal Content Moderation (MMCM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Chris Bregler&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mevan Babakar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mcv-workshop.github.io/#updates&quot;>;Medical Computer Vision (MCV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Shekoofeh Azizi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vand-cvpr23/home&quot;>;VAND: Visual Anomaly and Novelty Detection&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Jie Ren&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://struco3d.github.io/cvpr2023/&quot;>;Structural and Compositional Learning on 3D Data&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Fei Xia&lt;/strong>;, &lt;strong>;Amir Hertz&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgvc10&quot;>;Fine-Grained Visual Categorization (FGVC10)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Kimberly Wilber&lt;/strong>;, &lt;strong>;Sara Beery&lt;/strong>;&lt;/em>; &lt;br />; Panelists include: &lt;strong>;&lt;em>;Hartwig Adam&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/xrnerf/&quot;>;XRNeRF: Advances in NeRF for the Metaverse&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnilabel-workshop-cvpr23/overview&quot;>;OmniLabel: Infinite Label Spaces for Semantic Understanding via Natural Language&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Long Zhao&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vittorio Ferrari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://holistic-video-understanding.github.io/workshops/cvpr2023.html&quot;>;Large Scale Holistic Video Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;David Ross&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nice.lgresearch.ai/&quot;>;New Frontiers for Zero-Shot Image Captioning Evaluation (NICE)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ccd2023.github.io/&quot;>;Computational Cameras and Displays (CCD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ulugbek Kamilov&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mauricio Delbracio&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gazeworkshop.github.io/2023/&quot;>;Gaze Estimation and Prediction in the Wild (GAZE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Thabo Beele&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Erroll Wood&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgahi2023/home&quot;>;Face and Gesture Analysis for Health Informatics (FGAHI)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.cv4animals.com/&quot;>;Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;3D Vision and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Pete Florence&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;End-to-End Autonomous Driving: Perception, Prediction, Planning and Simulation (E2EAD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://opendrivelab.com/e2ead/cvpr23&quot;>;End-to-End Autonomous Driving: Emerging Tasks and Challenges&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mula-workshop.github.io/&quot;>;Multi-modal Learning and Applications (MULA)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Aleksander Hołyński&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/sdas2023/&quot;>;Synthetic Data for Autonomous Systems (SDAS)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vdu-cvpr23&quot;>;Vision Datasets Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;José Lezama&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vijay Janapa Reddi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ieeecvf-cvpr2023-precognition/&quot;>;Precognition: Seeing Through the Future&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Utsav Prabhu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvlai.net/ntire/2023/&quot;>;New Trends in Image Restoration and Enhancement (NTIRE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://generative-vision.github.io/workshop-CVPR-23/&quot;>;Generative Models for Computer Vision&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://robustart.github.io/&quot;>;Adversarial Machine Learning on Computer Vision: Art of Robustness&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Xinyun Chen&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Deqing Sun&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/wmf2023/home&quot;>;Media Forensics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Nicholas Carlini&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://taodataset.org/workshop/cvpr23/&quot;>;Tracking and Its Many Guises: Tracking Any Object in Open-World&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Paul Voigtlaender&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://scene-understanding.com/&quot;>;3D Scene Understanding for Vision, Graphics, and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.es.ele.tue.nl/cvpm23/&quot;>;Computer Vision for Physiological Measurement (CVPM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ibug.doc.ic.ac.uk/resources/cvpr-2023-5th-abaw/&quot;>;Affective Behaviour Analysis In-the-Wild&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Stefanos Zafeiriou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2023/home&quot;>;Ethical Considerations in Creative Applications of Computer Vision (EC3V)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Rida Qadri&lt;/strong>;, &lt;strong>;Mohammad Havaei&lt;/strong>;, &lt;strong>;Fernando Diaz&lt;/strong>;, &lt;strong>;Emily Denton&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;Negar Rostamzadeh&lt;/strong>;, &lt;strong>;Pamela Peter-Agbia&lt;/strong>;, &lt;strong>;Eva Kozanecka&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vizwiz.org/workshops/2023-workshop/&quot;>;VizWiz Grand Challenge: Describing Images and Videos Taken by Blind People&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Haoran Qi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ecv23/home&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;blog post&lt;/a>;) &lt;br />; Organizers include: &lt;em>;&lt;strong>;Andrew Howard&lt;/strong>;, &lt;strong>;Chas Leichner&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andrew Howard&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vcdw2023/&quot;>;Visual Copy Detection&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Priya Goyal&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://3dmv2023.github.io/&quot;>;Learning 3D with Multi-View Supervision (3DMV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://image-matching-workshop.github.io/&quot;>;Image Matching: Local Features and Beyond&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Eduard Trulls&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vision4allseason.net/&quot;>;Vision for All Seasons: Adverse Weather and Lightning Conditions (V4AS)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/t4v-cvpr23&quot;>;Transformers for Vision (T4V)&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/academic-cv/&quot;>;Scholars vs Big Models — How Can Academics Adapt?&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Jonathan T. Barron&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://www.scan-net.org/cvpr2023workshop/&quot;>;ScanNet Indoor Scene Understanding Challenge&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Tom Funkhouser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvmi-workshop.github.io/new.html&quot;>;Computer Vision for Microscopy Image Analysis&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Po-Hsuan Cameron Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://embeddedvisionworkshop.wordpress.com/&quot;>;Embedded Vision&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Rahul Sukthankar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sightsound.org/&quot;>;Sight and Sound&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;William Freeman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ai4cc.net/&quot;>;AI for Content Creation&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Tim Salimans&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://computer-vision-in-the-wild.github.io/cvpr-2023/https://computer-vision-in-the-wild.github.io/cvpr-2023/&quot;>;Computer Vision in the Wild&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Xiuye Gu&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vispr-workshop.github.io/&quot;>;Visual Pre-training for Robotics&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnicv2023/home&quot;>;Omnidirectional Computer Vision&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Yi-Hsuan Tsai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://all-things-vits.github.io/atv/&quot;>;All Things ViTs: Understanding and Interpreting Attention in Vision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hila Chefer&lt;/strong>;, &lt;strong>;Sayak Paul&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-tutorial-on-ad/&quot;>;Recent Advances in Anomaly Detection&lt;/a>; &lt;br />; &lt;em>;Guansong Pang, Joey Tianyi Zhou, Radu Tudor Ionescu, Yu Tian,&lt;strong>; Kihyuk Sohn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr-tutorial-2023/home&quot;>;Contactless Healthcare Using Cameras and Wireless Sensors&lt;/a>; &lt;br />; &lt;em>;Wenjin Wang, Xuyu Wang, Jun Luo,&lt;strong>; Daniel McDuff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://osimeoni.github.io/object-localization-for-free/&quot;>;Object Localization for Free: Going Beyond Self-Supervised Learning&lt;/a>; &lt;br />; &lt;em>;Oriane Simeoni,&lt;strong>; &lt;/strong>;Weidi Xie,&lt;strong>; Thomas Kipf&lt;/strong>;, Patrick Pérez&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://prompting-in-vision.github.io/&quot;>;Prompting in Vision&lt;/a>; &lt;br />; &lt;em>;Kaiyang Zhou, Ziwei Liu, Phillip Isola, Hyojin Bahng, Ludwig Schmidt, Sarah Pratt,&lt;strong>; Denny Zhou&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/9139300663122353070/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at CVPR 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s72-c/CVPR%20Design-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2312998356111901143&lt;/id>;&lt;published>;2023-06-15T13:53:00.000-07:00&lt;/published>;&lt;updated>;2023-06-15T13:53:27.418-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Android&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Juhyun Lee and Raman Sarokin, Software Engineers, Core Systems &amp;amp; Experiences&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s700/SpeedHero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The proliferation of large &lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;diffusion models&lt;/a>; for &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;image generation&lt;/a>; has led to a significant increase in model size and inference workloads. On-device ML inference in mobile environments requires meticulous performance optimization and consideration of trade-offs due to resource constraints. Running inference of large diffusion models (LDMs) on-device, driven by the need for cost efficiency and user privacy, presents even greater challenges due to the substantial memory requirements and computational demands of these models. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We address this challenge in our work titled “&lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations&lt;/a>;” (to be presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>; workshop for &lt;a href=&quot;https://sites.google.com/corp/view/ecv23&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>;) focusing on the optimized execution of a foundational LDM model on a mobile GPU. In this blog post, we summarize the core techniques we employed to successfully execute large diffusion models like &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;Stable Diffusion&lt;/a>; at full resolution (512x512 pixels) and 20 iterations on modern smartphones with high-performing inference speed of the original model without distillation of under 12 seconds. As discussed in &lt;a href=&quot;https://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html&quot;>;our previous blog post&lt;/a>;, GPU-accelerated ML inference is often limited by memory performance, and execution of LDMs is no exception. Therefore, the central theme of our optimization is efficient memory input/output (I/O) even if it means choosing memory-efficient algorithms over those that prioritize arithmetic logic unit efficiency. Ultimately, our primary objective is to reduce the overall latency of the ML inference. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s512/image4.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A sample output of an LDM on Mobile GPU with the prompt text: “a photo realistic and high resolution image of a cute puppy with surrounding flowers”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Enhanced attention module for memory efficiency&lt;/h2>; &lt;p>; An ML inference engine typically provides a variety of optimized ML operations. Despite this, achieving optimal performance can still be challenging as there is a certain amount of overhead for executing individual neural net operators on a GPU. To mitigate this overhead, ML inference engines incorporate extensive operator fusion rules that consolidate multiple operators into a single operator, thereby reducing the number of iterations across tensor elements while maximizing compute per iteration. For instance, &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; utilizes operator fusion to combine computationally expensive operations, like convolutions, with subsequent activation functions, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;>;rectified linear units&lt;/a>;, into one. &lt;/p>; &lt;p>; A clear opportunity for optimization is the heavily used attention block adopted in the &lt;a href=&quot;https://arxiv.org/pdf/2304.11267.pdf&quot;>;denoiser model&lt;/a>; in the LDM. The attention blocks allow the model to focus on specific parts of the input by assigning higher weights to important regions. There are multiple ways one can optimize the attention modules, and we selectively employ one of the two optimizations explained below depending on which optimization performs better. &lt;/p>; &lt;p>; The first optimization, which we call &lt;em>;partially fused &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;>;softmax&lt;/a>;&lt;/em>;, removes the need for extensive memory writes and reads between the softmax and the matrix multiplication in the attention module. Let the attention block be just a simple matrix multiplication of the form &lt;b>;&lt;em>;Y &lt;/em>;&lt;/b>;= softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) * &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; where &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; are 2D matrices of shape &lt;em>;a&lt;/em>;×&lt;em>;b&lt;/em>; and &lt;em>;b&lt;/em>;×&lt;em>;c&lt;/em>;, respectively (shown below in the top half). &lt;/p>; &lt;p>; For numerical stability, &lt;b>;&lt;em>;T = &lt;/em>;&lt;/b>;softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) is typically calculated in three passes: &lt;/p>; &lt;ol>; &lt;li>;Determine the maximum value in the list, &lt;em>;ie&lt;/em>;.,&lt;em>; &lt;/em>;for each row in matrix &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; &lt;/li>;&lt;li>;Sum up the differences of the exponential of each list item and the maximum value (from pass 1) &lt;/li>;&lt;li>;Divide the exponential of the items minus the maximum value by the sum from pass 2 &lt;/li>; &lt;/ol>; &lt;p>; Carrying out these passes naïvely would result in a huge memory write for the temporary intermediate tensor &lt;strong>;&lt;em>;T&lt;/em>;&lt;/strong>; holding the output of the entire softmax function. We bypass this large memory write if we only store the results of passes 1 and 2, labeled &lt;b>;&lt;em>;m&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;s&lt;/em>;&lt;/b>;, respectively, which are small vectors, with &lt;em>;a&lt;/em>; elements each, compared to &lt;strong>;&lt;em>;T &lt;/em>;&lt;/strong>;which has &lt;em>;a·b &lt;/em>;elements. With this technique, we are able to reduce tens or even hundreds of megabytes of memory consumption by multiple orders of magnitude (shown below in the bottom half). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s568/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;457&quot; data-original-width=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Attention modules. &lt;b>;Top&lt;/b>;: A naïve attention block, composed of a SOFTMAX (with all three passes) and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;MATMUL&lt;/a>;, requires a large memory write for the big intermediate tensor &lt;em>;T&lt;/em>;. &lt;b>;Bottom&lt;/b>;: Our memory-efficient attention block with partially fused softmax in MATMUL only needs to store two small intermediate tensors for &lt;em>;m&lt;/em>; and s.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The other optimization involves employing &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;>;FlashAttention&lt;/a>;, which is an I/O-aware, exact attention algorithm. This algorithm reduces the number of GPU high-bandwidth memory accesses, making it a good fit for our memory bandwidth–limited use case. However, we found this technique to only work for &lt;a href=&quot;https://en.wikipedia.org/wiki/Static_random-access_memory&quot;>;SRAM&lt;/a>; with certain sizes and to require a large number of registers. Therefore, we only leverage this technique for attention matrices with a certain size on a select set of GPUs. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Winograd fast convolution for 3×3 convolution layers&lt;/h2>; &lt;p>; The backbone of common LDMs heavily relies on 3×3 convolution layers (convolutions with filter size 3×3), comprising over 90% of the layers in the decoder. Despite increased memory consumption and numerical errors, we found that &lt;a href=&quot;https://arxiv.org/abs/1509.09308&quot;>;Winograd fast convolution&lt;/a>; to be effective at speeding up the convolutions. Distinct from the &lt;em>;filter size&lt;/em>; 3x3 used in convolutions, &lt;em>;tile size&lt;/em>; refers to the size of a sub region of the input tensor that is processed at a time. Increasing the tile size enhances the efficiency of the convolution in terms of &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_logic_unit&quot;>;arithmetic logic unit&lt;/a>; (ALU) usage. However, this improvement comes at the expense of increased memory consumption. Our tests indicate that a tile size of 4×4 achieves the optimal trade-off between computational efficiency and memory utilization. &lt;/p>; &lt;br>; &lt;table align=&quot;center&quot; style=&quot;text-align: center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;/td>; &lt;td>;&lt;/td>; &lt;td colspan=&quot;2&quot;>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Memory usage&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Tile size&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; savings&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Intermediate tensors&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Weights&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;2×2 &lt;/td>; &lt;td>;2.25× &lt;/td>; &lt;td>;4.00× &lt;/td>; &lt;td>;1.77× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;4×4&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;2.25×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;6×6 &lt;/td>; &lt;td>;5.06× &lt;/td>; &lt;td>;1.80× &lt;/td>; &lt;td>;7.12× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;8×8 &lt;/td>; &lt;td>;5.76× &lt;/td>; &lt;td>;1.56× &lt;/td>; &lt;td>;11.1× &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Impact of Winograd with varying tile sizes for 3×3 convolutions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Specialized operator fusion for memory efficiency&lt;/h2>; &lt;p>; We discovered that performantly inferring LDMs on a mobile GPU requires significantly larger fusion windows for commonly employed layers and units in LDMs than current off-the-shelf on-device GPU-accelerated ML inference engines provide. Consequently, we developed specialized implementations that could execute a larger range of neural operators than typical fusion rules would permit. Specifically, we focused on two specializations: the &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;>;Gaussian Error Linear Unit&lt;/a>; (GELU) and the &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;>;group normalization&lt;/a>; layer. &lt;/p>; &lt;p>; An approximation of GELU with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_functions&quot;>;hyperbolic tangent&lt;/a>; function requires writing to and reading from seven auxiliary intermediate tensors (shown below as light orange rounded rectangles in the figure below), reading from the input tensor &lt;em>;x&lt;/em>; three times, and writing to the output tensor &lt;em>;y&lt;/em>; once across eight GPU programs implementing the labeled operation each (light blue rectangles). A custom GELU implementation that performs the eight operations in a single shader (shown below in the bottom) can bypass all the memory I/O for the intermediate tensors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s958/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;210&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;GELU implementations. &lt;strong>;Top&lt;/strong>;: A naïve implementation with built-in operations would require 8 memory writes and 10 reads. &lt;strong>;Bottom&lt;/strong>;: Our custom GELU only requires 1 memory read (for &lt;em>;x&lt;/em>;) and 1 write (for &lt;em>;y&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; After applying all of these optimizations, we conducted tests of Stable Diffusion 1.5 (image resolution 512x512, 20 iterations) on high-end mobile devices. Running Stable Diffusion with our GPU-accelerated ML inference model uses 2,093MB for the weights and 84MB for the intermediate tensors. With latest high-end smartphones, Stable Diffusion can be run in under 12 seconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s522/image3.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;270&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stable Diffusion runs on modern smartphones in under 12 seconds. Note that running the decoder after each iteration for displaying the intermediate output in this animated GIF results in a ~2× slowdown.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Performing on-device ML inference of large models has proven to be a substantial challenge, encompassing limitations in model file size, extensive runtime memory requirements, and protracted inference latency. By recognizing memory bandwidth usage as the primary bottleneck, we directed our efforts towards optimizing memory bandwidth utilization and striking a delicate balance between ALU efficiency and memory efficiency. As a result, we achieved state-of-the-art inference latency for large diffusion models. You can learn more about this work in &lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;the paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank Yu-Hui Chen, Jiuqiang Tang, Frank Barchard, Yang Zhao, Joe Zou, Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, Lu Wang, and Matthias Grundmann.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2312998356111901143/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot; rel=&quot;alternate&quot; title=&quot;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s72-c/SpeedHero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3072875289355194363&lt;/id>;&lt;published>;2023-06-14T09:00:00.001-07:00&lt;/published>;&lt;updated>;2023-06-22T14:55:31.687-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computational Photography&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Reconstructing indoor spaces with NeRF&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Marcos Seefelder, Software Engineer, and Daniel Duckworth, Research Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s320/Immersive%20View%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; When choosing a venue, we often find ourselves with questions like the following: Does this restaurant have the right vibe for a date? Is there good outdoor seating? Are there enough screens to watch the game? While photos and videos may partially answer questions like these, they are no substitute for feeling like you&#39;re there, even when visiting in person isn&#39;t an option. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Immersive experiences that are interactive, photorealistic, and multi-dimensional stand to bridge this gap and recreate the feel and vibe of a space, empowering users to naturally and intuitively find the information they need. To help with this, Google Maps launched &lt;a href=&quot;https://blog.google/products/maps/google-maps-updates-immersive-view-trip-planning&quot;>;Immersive View&lt;/a>;, which uses advances in machine learning (ML) and computer vision to fuse billions of &lt;a href=&quot;https://www.google.com/streetview/&quot;>;Street View&lt;/a>; and aerial images to create a rich, digital model of the world. Beyond that, it layers helpful information on top, like the weather, traffic, and how busy a place is. Immersive View provides indoor views of restaurants, cafes, and other venues to give users a virtual up-close look that can help them confidently decide where to go. &lt;/p>; &lt;p>; Today we describe the work put into delivering these indoor views in Immersive View. We build on &lt;a href=&quot;https://arxiv.org/abs/2003.08934&quot;>;neural radiance fields&lt;/a>; (NeRF), a state-of-the-art approach for fusing photos to produce a realistic, multi-dimensional reconstruction within a neural network. We describe our pipeline for creation of NeRFs, which includes custom photo capture of the space using DSLR cameras, image processing and scene reproduction. We take advantage of Alphabet&#39;s &lt;a href=&quot;https://arxiv.org/abs/2008.02268&quot;>;recent&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2111.12077&quot;>;advances&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2202.05263&quot;>;in the field&lt;/a>; to design a method matching or outperforming the prior state-of-the-art in visual fidelity. These models are then embedded as interactive 360° videos following curated flight paths, enabling them to be available on smartphones. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/introduction_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The reconstruction of The Seafood Bar in Amsterdam in Immersive View.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;From photos to NeRFs&lt;/h2>; &lt;p>; At the core of our work is &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>;NeRF&lt;/a>;, a recently-developed method for 3D reconstruction and novel view synthesis. Given a collection of photos describing a scene, NeRF distills these photos into a &lt;a href=&quot;https://arxiv.org/abs/2111.11426&quot;>;neural field&lt;/a>;, which can then be used to render photos from viewpoints not present in the original collection. &lt;/p>; &lt;p>; While NeRF largely solves the challenge of reconstruction, a user-facing product based on real-world data brings a wide variety of challenges to the table. For example, reconstruction quality and user experience should remain consistent across venues, from dimly-lit bars to sidewalk cafes to hotel restaurants. At the same time, privacy should be respected and any potentially personally identifiable information should be removed. Importantly, scenes should be captured consistently and efficiently, reliably resulting in high-quality reconstructions while minimizing the effort needed to capture the necessary photographs. Finally, the same natural experience should be available to all mobile users, regardless of the device on hand. &lt;/p>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/overview_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Immersive View indoor reconstruction pipeline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Capture &amp;amp; preprocessing&lt;/h2>; &lt;p>; The first step to producing a high-quality NeRF is the careful capture of a scene: a dense collection of photos from which 3D geometry and color can be derived. To obtain the best possible reconstruction quality, every surface should be observed from multiple different directions. The more information a model has about an object&#39;s surface, the better it will be in discovering the object&#39;s shape and the way it interacts with lights. &lt;/p>; &lt;p>; In addition, NeRF models place further assumptions on the camera and the scene itself. For example, most of the camera&#39;s properties, such as white balance and aperture, are assumed to be fixed throughout the capture. Likewise, the scene itself is assumed to be frozen in time: lighting changes and movement should be avoided. This must be balanced with practical concerns, including the time needed for the capture, available lighting, equipment weight, and privacy. In partnership with professional photographers, we developed a strategy for quickly and reliably capturing venue photos using DSLR cameras within only an hour timeframe. This approach has been used for all of our NeRF reconstructions to date. &lt;/p>; &lt;p>; Once the capture is uploaded to our system, processing begins. As photos may inadvertently contain sensitive information, we automatically scan and blur personally identifiable content. We then apply a &lt;a href=&quot;https://en.wikipedia.org/wiki/Structure_from_motion&quot;>;structure-from-motion&lt;/a>; pipeline to solve for each photo&#39;s &lt;a href=&quot;https://www.mathworks.com/help/vision/ug/camera-calibration.html&quot;>;camera parameters&lt;/a>;: its position and orientation relative to other photos, along with lens properties like &lt;a href=&quot;https://en.wikipedia.org/wiki/Focal_length&quot;>;focal length&lt;/a>;. These parameters associate each pixel with a point and a direction in 3D space and constitute a key signal in the NeRF reconstruction process. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NeRF reconstruction&lt;/h2>; &lt;p>; Unlike many ML models, a new NeRF model is trained from scratch on each captured location. To obtain the best possible reconstruction quality within a target compute budget, we incorporate features from a variety of published works on NeRF developed at Alphabet. Some of these include: &lt;/p>; &lt;ul>; &lt;li>;We build on &lt;a href=&quot;https://jonbarron.info/mipnerf360/&quot;>;mip-NeRF 360&lt;/a>;, one of the best-performing NeRF models to date. While more computationally intensive than Nvidia&#39;s widely-used &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;Instant NGP&lt;/a>;, we find the mip-NeRF 360 consistently produces fewer artifacts and higher reconstruction quality. &lt;/li>;&lt;li>;We incorporate the low-dimensional generative latent optimization (GLO) vectors introduced in &lt;a href=&quot;https://nerf-w.github.io/&quot;>;NeRF in the Wild&lt;/a>; as an auxiliary input to the model&#39;s radiance network. These are learned real-valued latent vectors that embed appearance information for each image. By assigning each image in its own latent vector, the model can capture phenomena such as lighting changes without resorting to cloudy geometry, a common artifact in casual NeRF captures. &lt;/li>;&lt;li>;We also incorporate exposure conditioning as introduced in &lt;a href=&quot;https://waymo.com/research/block-nerf/&quot;>;Block-NeRF&lt;/a>;. Unlike GLO vectors, which are uninterpretable model parameters, exposure is directly derived from a photo&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Exif&quot;>;metadata&lt;/a>; and fed as an additional input to the model&#39;s radiance network. This offers two major benefits: it opens up the possibility of varying &lt;a href=&quot;https://en.wikipedia.org/wiki/Film_speed#Digital_camera_ISO_speed_and_exposure_index&quot;>;ISO&lt;/a>; and provides a method for controlling an image&#39;s brightness at inference time. We find both properties invaluable for capturing and reconstructing dimly-lit venues. &lt;/li>; &lt;/ul>; &lt;p>; We train each NeRF model on TPU or GPU accelerators, which provide different trade-off points. As with all Google products, we continue to search for new ways to improve, from reducing compute requirements to improving reconstruction quality. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/side_by_side_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A side-by-side comparison of our method and a mip-NeRF 360 baseline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A scalable user experience&lt;/h2>; &lt;p>; Once a NeRF is trained, we have the ability to produce new photos of a scene from any viewpoint and camera lens we choose. Our goal is to deliver a meaningful and helpful user experience: not only the reconstructions themselves, but guided, interactive tours that give users the freedom to naturally explore spaces from the comfort of their smartphones. &lt;/p>; &lt;p>; To this end, we designed a controllable 360° video player that emulates flying through an indoor space along a predefined path, allowing the user to freely look around and travel forward or backwards. As the first Google product exploring this new technology, 360° videos were chosen as the format to deliver the generated content for a few reasons. &lt;/p>; &lt;p>; On the technical side, &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;real-time inference&lt;/a>; and &lt;a href=&quot;https://phog.github.io/snerg/&quot;>;baked representations&lt;/a>; are still resource intensive on a per-client basis (either on device or cloud computed), and relying on them would limit the number of users able to access this experience. By using videos, we are able to scale the storage and delivery of videos to all users by taking advantage of the same video management and serving infrastructure used by YouTube. On the operations side, videos give us clearer editorial control over the exploration experience and are easier to inspect for quality in large volumes. &lt;/p>; &lt;p>; While we had considered capturing the space with a 360° camera directly, using a NeRF to reconstruct and render the space has several advantages. A virtual camera can fly anywhere in space, including over obstacles and through windows, and can use any desired camera lens. The camera path can also be edited post-hoc for smoothness and speed, unlike a live recording. A NeRF capture also does not require the use of specialized camera hardware. &lt;/p>; &lt;p>; Our 360° videos are rendered by &lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_casting&quot;>;ray casting&lt;/a>; through each pixel of a virtual, spherical camera and compositing the visible elements of the scene. Each video follows a smooth path defined by a sequence of keyframe photos taken by the photographer during capture. The position of the camera for each picture is computed during structure-from-motion, and the sequence of pictures is smoothly interpolated into a flight path. &lt;/p>; &lt;p>; To keep speed consistent across different venues, we calibrate the distances for each by capturing pairs of images, each of which is 3 meters apart. By knowing measurements in the space, we scale the generated model, and render all videos at a natural velocity. &lt;/p>; &lt;p>; The final experience is surfaced to the user within &lt;a href=&quot;https://blog.google/products/maps/three-maps-updates-io-2022/&quot;>;Immersive View&lt;/a>;: the user can seamlessly fly into restaurants and other indoor venues and discover the space by flying through the photorealistic 360° videos. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Open research questions&lt;/h2>; &lt;p>; We believe that this feature is the first step of many in a journey towards universally accessible, AI-powered, immersive experiences. From a NeRF research perspective, more questions remain open. Some of these include: &lt;/p>; &lt;ol>; &lt;li>;Enhancing reconstructions with scene segmentation, adding semantic information to the scenes that could make scenes, for example, searchable and easier to navigate. &lt;/li>;&lt;li>;Adapting NeRF to outdoor photo collections, in addition to indoor. In doing so, we&#39;d unlock similar experiences to every corner of the world and change how users could experience the outdoor world. &lt;/li>;&lt;li>;Enabling real-time, interactive 3D exploration through neural-rendering on-device. &lt;/li>; &lt;/ol>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/outdoors_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Reconstruction of an outdoor scene with a NeRF model trained on Street View panoramas.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; As we continue to grow, we look forward to engaging with and contributing to the community to build the next generation of immersive experiences. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Contributors to the project include Jon Barron, Julius Beres, Daniel Duckworth, Roman Dudko, Magdalena Filak, Mike Harm, Peter Hedman, Claudio Martella, Ben Mildenhall, Cardin Moffett, Etienne Pot, Konstantinos Rematas, Yves Sallat, Marcos Seefelder, Lilyana Sirakovat, Sven Tresp and Peter Zhizhin.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Also, we&#39;d like to extend our thanks to Luke Barrington, Daniel Filip, Tom Funkhouser, Charles Goran, Pramod Gupta, Santi López, Mario Lučić, Isalo Montacute and Dan Thomasset for valuable feedback and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3072875289355194363/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot; rel=&quot;alternate&quot; title=&quot;Reconstructing indoor spaces with NeRF&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s72-c/Immersive%20View%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7719238929287079732&lt;/id>;&lt;published>;2023-06-13T10:18:00.004-07:00&lt;/published>;&lt;updated>;2023-06-13T13:54:56.343-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling delightful user experiences via predictive models of human attention&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Junfeng He, Senior Research Scientist, and Kai Kohlhoff, Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4ogEMk7vZJDropM08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s1400/HumanAttention.png&quot; style=&quot;display: none;&quot; />; &lt;p>; People have the remarkable ability to take in a tremendous amount of information (estimated to be ~10&lt;sup>;10&lt;/sup>; bits/s entering the retina) and selectively attend to a few task-relevant and interesting regions for further processing (eg, memory, comprehension, action). Modeling human attention (the result of which is often called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Saliency_map&quot;>;saliency&lt;/a>; model) has therefore been of interest across the fields of neuroscience, psychology, &lt;a href=&quot;https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction&quot;>;human-computer interaction&lt;/a>; (HCI) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_vision&quot;>;computer vision&lt;/a>;. The ability to predict which regions are likely to attract attention has numerous important applications in areas like graphics, photography, image compression and processing, and the measurement of visual quality. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We&#39;ve &lt;a href=&quot;https://ai.googleblog.com/2021/05/accelerating-eye-movement-research-for.html&quot;>;previously discussed&lt;/a>; the possibility of accelerating eye movement research using machine learning and smartphone-based gaze estimation, which earlier required specialized hardware costing up to $30,000 per unit. Related research includes “&lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/look-to-speak/&quot;>;Look to Speak&lt;/a>;”, which helps users with accessibility needs (eg, people with &lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;ALS&lt;/a>;) to communicate with their eyes, and the recently published “&lt;a href=&quot;https://ai.googleblog.com/2023/04/differentially-private-heatmaps.html&quot;>;Differentially private heatmaps&lt;/a>;” technique to compute heatmaps, like those for attention, while protecting users&#39; privacy. &lt;/p>; &lt;p>; In this blog, we present two papers (one from &lt;a href=&quot;https://cvpr2022.thecvf.com/&quot;>;CVPR 2022&lt;/a>;, and one just accepted to &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;) that highlight our recent research in the area of human attention modeling: “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf&quot;>;Deep Saliency Prior for Reducing Visual Distraction&lt;/a>;” and “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;”, together with recent research on saliency driven progressive loading for image compression (&lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html&quot;>;2&lt;/a>;). We showcase how predictive models of human attention can enable delightful user experiences such as image editing to minimize visual clutter, distraction or artifacts, image compression for faster loading of webpages or apps, and guiding ML models towards more intuitive human-like interpretation and model performance. We focus on image editing and image compression, and discuss recent advances in modeling in the context of these applications. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Attention-guided image editing&lt;/h2>; &lt;p>; Human attention models usually take an image as input (eg, a natural image or a screenshot of a webpage), and &lt;a href=&quot;https://dl.acm.org/doi/10.1109/TPAMI.2019.2935715&quot;>;predict a heatmap as output&lt;/a>;. The predicted heatmap on the image is &lt;a href=&quot;https://www.computer.org/csdl/journal/tp/2019/03/08315047/17D45Vw15wU&quot;>;evaluated against ground-truth attention data&lt;/a>;, which are typically collected by an eye tracker or &lt;a href=&quot;https://bubbleview.namwkim.org/&quot;>;approximated via mouse hovering/clicking&lt;/a>;. Previous models leveraged handcrafted features for visual clues, like color/brightness contrast, edges, and shape, while more recent approaches automatically learn discriminative features based on deep neural networks, from &lt;a href=&quot;https://arxiv.org/abs/1805.01047&quot;>;convolutional&lt;/a>; and &lt;a href=&quot;https://arxiv.org/pdf/1610.01708.pdf&quot;>;recurrent neural networks&lt;/a>; to more recent &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0925231222004714&quot;>;vision transformer networks&lt;/a>;. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf&quot;>;Deep Saliency Prior for Reducing Visual Distraction&lt;/a>;” (more information on this &lt;a href=&quot;https://deep-saliency-prior.github.io/&quot;>;project site&lt;/a>;), we leverage deep saliency models for dramatic yet visually realistic edits, which can significantly change an observer&#39;s attention to different image regions. For example, removing distracting objects in the background can reduce clutter in photos, leading to increased user satisfaction. Similarly, in video conferencing, reducing clutter in the background may increase focus on the main speaker (&lt;a href=&quot;https://deep-saliency-prior.github.io/supplementary/index.html&quot;>;example demo here&lt;/a>;). &lt;/p>; &lt;p>; To explore what types of editing effects can be achieved and how these affect viewers&#39; attention, we developed an optimization framework for guiding visual attention in images using a differentiable, predictive saliency model. Our method employs a state-of-the-art deep saliency model. Given an input image and a binary mask representing the distractor regions, pixels within the mask will be edited under the guidance of the predictive saliency model such that the saliency within the masked region is reduced. To make sure the edited image is natural and realistic, we carefully choose four image editing operators: two standard image editing operations, namely recolorization and image warping (shift); and two learned operators (we do not define the editing operation explicitly), namely a multi-layer convolution filter, and a generative model (&lt;a href=&quot;https://arxiv.org/abs/1912.04958&quot;>;GAN&lt;/a>;). &lt;/p>; &lt;p>; With those operators, our framework can produce a variety of powerful effects, with examples in the figure below, including recoloring, inpainting, camouflage, object editing or insertion, and facial attribute editing. Importantly, all these effects are driven solely by the single, pre-trained saliency model, without any additional supervision or training. Note that our goal is not to compete with dedicated methods for producing each effect, but rather to demonstrate how multiple editing operations can be guided by the knowledge embedded within deep saliency models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlABhC7P2SufIfcliXyTopTkNGeNAEmMbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlABhC7P2SufIfcliXyTopTkNGeNAEmMbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of reducing visual distractions, guided by the saliency model with several operators. The distractor region is marked on top of the saliency map (red border) in each example.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Enriching experiences with user-aware saliency modeling&lt;/h2>; &lt;p>; Prior research assumes a single saliency model for the whole population. However, human attention varies between individuals — while the detection of salient clues is fairly consistent, their order, interpretation, and gaze distributions can differ substantially. This offers opportunities to create personalized user experiences for individuals or groups. In “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;”, we introduce a user-aware saliency model, the first that can predict attention for one user, a group of users, and the general population, with a single model. &lt;/p>; &lt;p>; As shown in the figure below, core to the model is the combination of each participant&#39;s visual preferences with a per-user attention map and adaptive user masks. This requires per-user attention annotations to be available in the training data, eg, the &lt;a href=&quot;https://www.nature.com/articles/s41467-020-18360-5&quot;>;OSIE mobile gaze dataset for natural images; FiWI&lt;/a>; and &lt;a href=&quot;http://vision.cs.stonybrook.edu/~soura/websaliency.html&quot;>;WebSaliency&lt;/a>; datasets for web pages. Instead of predicting a single saliency map representing attention of all users, this model predicts per-user attention maps to encode individuals&#39; attention patterns. Further, the model adopts a user mask (a binary vector with the size equal to the number of participants) to indicate the presence of participants in the current sample, which makes it possible to select a group of participants and combine their preferences into a single heatmap. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3hCKho1q3HQKEiYgnwnKQtVnCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s1218/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;724&quot; data-original-width=&quot;1218&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3hCKho1q3HQKEiYgnwnKQtVnCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the user aware saliency model framework. The example image is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>; image set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; During inference, the user mask allows making predictions for any combination of participants. In the following figure, the first two rows are attention predictions for two different groups of participants (with three people in each group) on an image. A &lt;a href=&quot;https://arxiv.org/pdf/1805.01047.pdf&quot;>;conventional attention prediction model&lt;/a>; will predict identical attention heatmaps. Our model can distinguish the two groups (eg, the second group pays less attention to the face and more attention to the food than the first). Similarly, the last two rows are predictions on a webpage for two distinctive participants, with our model showing different preferences (eg, the second participant pays more attention to the left region than the first). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY855PtAIfs1vYISgJb5nSx4A0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s961/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;533&quot; data-original-width=&quot;961&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY855PtAIfs1vYISgJb5nSx4A0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Predicted attention vs. ground truth (GT). EML-Net: predictions from a state-of-the-art model, which will have the same predictions for the two participants/groups. Ours: predictions from our proposed user aware saliency model, which can predict the unique preference of each participant/group correctly. The first image is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>; image set, and the second is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/webpage_saliency.html&quot;>;FiWI&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Progressive image decoding centered on salient features&lt;/h2>; &lt;p>; Besides image editing, human attention models can also improve users&#39; browsing experience. One of the most frustrating and annoying user experiences while browsing is waiting for web pages with images to load, especially in conditions with low network connectivity. One way to improve the user experience in such cases is with progressive decoding of images, which decodes and displays increasingly higher-resolution image sections as data are downloaded, until the full-resolution image is ready. Progressive decoding usually proceeds in a sequential order (eg, left to right, top to bottom). With a predictive attention model (&lt;a href=&quot;https://github.com/google/attention-center&quot;>;1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;2&lt;/a>;), we can instead decode images based on saliency, making it possible to send the data necessary to display details of the most salient regions first. For example, in a portrait, bytes for the face can be prioritized over those for the out-of-focus background. Consequently, users perceive better image quality earlier and experience significantly reduced wait times. More details can be found in our open source blog posts (&lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;post 1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html&quot;>;post 2&lt;/a>;). Thus, predictive attention models can help with image compression and faster loading of web pages with images, improve rendering for large images and streaming/VR applications. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We&#39;ve shown how predictive models of human attention can enable delightful user experiences via applications such as image editing that can reduce clutter, distractions or artifacts in images or photos for users, and progressive image decoding that can greatly reduce the perceived waiting time for users while images are fully rendered. Our user-aware saliency model can further personalize the above applications for individual users or groups, enabling richer and more unique experiences. &lt;/p>; &lt;p>; Another interesting direction for predictive attention models is whether they can help improve robustness of computer vision models in tasks such as object classification or detection. For example, in “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;Teacher-generated spatial-attention labels boost robustness and accuracy of contrastive models&lt;/a>;”, we show that a predictive human attention model can guide &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;>;contrastive learning&lt;/a>; models to achieve better representation and improve the accuracy/robustness of classification tasks (on the &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; and &lt;a href=&quot;https://paperswithcode.com/dataset/imagenet-c&quot;>;ImageNet-C&lt;/a>; datasets). Further research in this direction could enable applications such as using radiologist&#39;s attention on medical images to improve health screening or diagnosis, or using human attention in complex driving scenarios to guide autonomous driving systems. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work involved collaborative efforts from a multidisciplinary team of software engineers, researchers, and cross-functional contributors. We&#39;d like to thank all the co-authors of the papers/research, including Kfir Aberman, Gamaleldin F. Elsayed, Moritz Firsching, Shi Chen, Nachiappan Valliappan, Yushi Yao, Chang Ye, Yossi Gandelsman, Inbar Mosseri, David E. Jacobes, Yael Pritch, Shaolei Shen, and Xinyu Ye. We also want to thank team members Oscar Ramirez, Venky Ramachandran and Tim Fujita for their help. Finally, we thank Vidhya Navalpakkam for her technical leadership in initiating and overseeing this body of work.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7719238929287079732/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7719238929287079732&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7719238929287079732&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling delightful user experiences via predictive models of human attention&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4ogEMk7vZJDropM08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s72-c/HumanAttention.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2015281937814138473&lt;/id>;&lt;published>;2023-06-09T12:09:00.001-07:00&lt;/published>;&lt;updated>;2023-06-12T07:58:34.790-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Image Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Imagen Editor and EditBench: Advancing and evaluating text-guided image inpainting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Su Wang and Ceslee Montgomery, Research Engineers, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCOhgOtXQGmv1getpW3oHgD-4dCvqg9Q_Srs4qnD-FtydmHFb6XEesvUNGkf1eXRemuok7hb58ikl8tSw4xoNSwDd_YZkrdxVgj-6Fb5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s320/Imagen%20Editor%20&amp;amp;%20EditBench%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; In the last few years, &lt;a href=&quot;https://en.wikipedia.org/wiki/Text-to-image_model&quot;>;text-to-image generation&lt;/a>; research has seen an explosion of breakthroughs (notably, &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>;, &lt;a href=&quot;https://cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>;, etc.) that have naturally permeated into related topics. In particular, text-guided image editing (TGIE) is a practical task that involves editing generated and photographed visuals rather than completely redoing them. Quick, automated, and controllable editing is a convenient solution when recreating visuals would be time-consuming or infeasible (eg, tweaking objects in vacation photos or perfecting fine-grained details on a cute pup generated from scratch). Further, TGIE represents a substantial opportunity to improve training of foundational models themselves. Multimodal models require diverse data to train properly, and TGIE editing can enable the generation and recombination of high-quality and scalable synthetic data that, perhaps most importantly, can provide methods to optimize the distribution of training data along any given axis. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2212.06909&quot;>;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/a>;”, to be presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we introduce &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;Imagen Editor&lt;/a>;, a state-of-the-art solution for the task of masked &lt;a href=&quot;https://en.wikipedia.org/wiki/Inpainting&quot;>;inpainting&lt;/a>; — ie, when a user provides text instructions alongside an overlay or “mask” (usually generated within a drawing-type interface) indicating the area of the image they would like to modify. We also introduce &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;EditBench&lt;/a>;, a method that gauges the quality of image editing models. EditBench goes beyond the &lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;commonly&lt;/a>; &lt;a href=&quot;https://openreview.net/pdf?id=bKBhQhPeKaF&quot;>;used&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1910.13321&quot;>;coarse-grained&lt;/a>; “does this image match this text” methods, and drills down to various types of attributes, objects, and scenes for a more fine-grained understanding of model performance. In particular, it puts strong emphasis on the faithfulness of image-text alignment without losing sight of image quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s1261/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;306&quot; data-original-width=&quot;1261&quot; src=&quot;https://1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an image, a user-defined mask, and a text prompt, Imagen Editor makes localized edits to the designated areas. The model meaningfully incorporates the user&#39;s intent and performs photorealistic edits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Imagen Editor&lt;/h2>; &lt;p>; Imagen Editor is a &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/940392f5f32a7ade1cc201767cf83e31-Abstract.html&quot;>;diffusion-based model&lt;/a>; fine-tuned on &lt;a href=&quot;https://arxiv.org/abs/2205.11487&quot;>;Imagen&lt;/a>; for editing. It targets improved representations of linguistic inputs, fine-grained control and high-fidelity outputs. Imagen Editor takes three inputs from the user: 1) the image to be edited, 2) a binary mask to specify the edit region, and 3) a text prompt — all three inputs guide the output samples. &lt;/p>; &lt;p>; Imagen Editor depends on three core techniques for high-quality text-guided image inpainting. First, unlike prior inpainting models (eg, &lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;Palette&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1801.07892&quot;>;Context Attention&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1806.03589&quot;>;Gated Convolution&lt;/a>;) that apply random box and stroke masks, Imagen Editor employs an object detector masking policy with an &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;>;object detector module&lt;/a>; that produces object masks during training. Object masks are based on detected objects rather than random patches and allow for more principled alignment between edit text prompts and masked regions. Empirically, the method helps the model stave off the prevalent issue of the text prompt being ignored when masked regions are small or only partially cover an object (eg, &lt;a href=&quot;https://arxiv.org/abs/2204.14217&quot;>;CogView2&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH6oxBxwafDPfSWVkID53dhTkkrv_kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/s648/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;332&quot; data-original-width=&quot;648&quot; height=&quot;328&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH6oxBxwafDPfSWVkID53dhTkkrv_kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/w640-h328/image9.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Random masks (&lt;strong>;left&lt;/strong>;) frequently capture background or intersect object boundaries, defining regions that can be plausibly inpainted just from image context alone. Object masks (&lt;strong>;right&lt;/strong>;) are harder to inpaint from image context alone, encouraging models to rely more on text inputs during training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Next, during training and inference, Imagen Editor enhances high resolution editing by conditioning on full resolution (1024×1024 in this work), channel-wise concatenation of the input image and the mask (similar to &lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887996&quot;>;SR3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;Palette&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2112.10741&quot;>;GLIDE&lt;/a>;). For the base diffusion 64×64 model and the 64×64→256×256 super-resolution models, we apply a parameterized downsampling convolution (eg, convolution with a stride), which we empirically find to be critical for high fidelity. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6QQhPfmkF2m6fw-KaH4j7woDh1RgXTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s646/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;365&quot; data-original-width=&quot;646&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6QQhPfmkF2m6fw-KaH4j7woDh1RgXTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Imagen is fine-tuned for image editing. All of the diffusion models, ie, the base model and super-resolution (SR) models, are conditioned on high-resolution 1024×1024 image and mask inputs. To this end, new convolutional image encoders are introduced.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Finally, at inference we apply &lt;a href=&quot;https://arxiv.org/abs/2207.12598&quot;>;classifier-free guidance&lt;/a>; (CFG) to bias samples to a particular conditioning, in this case, text prompts. CFG interpolates between the text-conditioned and unconditioned model predictions to ensure strong alignment between the generated image and the input text prompt for text-guided image inpainting. We follow &lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;>;Imagen Video&lt;/a>; and use high guidance weights with guidance oscillation (a guidance schedule that oscillates within a value range of guidance weights). In the base model (the stage-1 64x diffusion), where ensuring strong alignment with text is most critical, we use a guidance weight schedule that oscillates between 1 and 30. We observe that high guidance weights combined with oscillating guidance result in the best trade-off between sample fidelity and text-image alignment. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EditBench&lt;/h2>; &lt;p>; The EditBench dataset for text-guided image inpainting evaluation contains 240 images, with 120 generated and 120 natural images. Generated images are synthesized by &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>; and natural images are drawn from the &lt;a href=&quot;https://arxiv.org/abs/1602.07332&quot;>;Visual Genome&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1811.00982&quot;>;Open Images&lt;/a>; datasets. EditBench captures a wide variety of language, image types, and levels of text prompt specificity (ie, simple, rich, and full captions). Each example consists of (1) a masked input image, (2) an input text prompt, and (3) a high-quality output image used as reference for automatic metrics. To provide insight into the relative strengths and weaknesses of different models, EditBench prompts are designed to test fine-grained details along three categories: (1) attributes (eg, material, color, shape, size, count); (2) object types (eg, common, rare, text rendering); and (3) scenes (eg, indoor, outdoor, realistic, or paintings). To understand how different specifications of prompts affect model performance, we provide three text prompt types: a single-attribute (Mask Simple) or a multi-attribute description of the masked object (Mask Rich) – or an entire image description (Full Image). Mask Rich, especially, probes the models&#39; ability to handle complex attribute binding and inclusion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau9ISKFksXjJNZwm_LrLXzEqwsi4Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s531/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;321&quot; data-original-width=&quot;531&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau9ISKFksXjJNZwm_LrLXzEqwsi4Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The full image is used as a reference for successful inpainting. The mask covers the target object with a free-form, non-hinting shape. We evaluate Mask Simple, Mask Rich and Full Image prompts, consistent with conventional text-to-image models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Due to the intrinsic weaknesses in existing automatic evaluation metrics (&lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;CLIPScore&lt;/a>; and &lt;a href=&quot;https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/0a09c8844ba8f0936c20bd791130d6b6-Paper-round1.pdf&quot;>;CLIP-R-Precision&lt;/a>;) for TGIE, we hold human evaluation as the gold standard for EditBench. In the section below, we demonstrate how EditBench is applied to model evaluation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation &lt;/h2>; &lt;p>; We evaluate the Imagen Editor model — with object masking (IM) and with random masking (IM-RM) — against comparable models, &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;Stable Diffusion&lt;/a>; (SD) and &lt;a href=&quot;https://cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>; (DL2). Imagen Editor outperforms these models by substantial margins across all EditBench evaluation categories. &lt;/p>; &lt;p>; For Full Image prompts, &lt;em>;single-image human evaluation&lt;/em>; provides binary answers to confirm if the image matches the caption. For Mask Simple prompts, single-image human evaluation confirms if the object and attribute are properly rendered, and bound correctly (eg, for a red cat, a white cat on a red table would be an incorrect binding). &lt;em>;Side-by-side human evaluation&lt;/em>; uses Mask Rich prompts only for side-by-side comparisons between IM and each of the other three models (IM-RM, DL2, and SD), and indicates which image matches with the caption better for text-image alignment, and which image is most realistic. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1VYKL-leNOiKNayiQkwzXlRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/s800/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;703&quot; data-original-width=&quot;800&quot; height=&quot;562&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1VYKL-leNOiKNayiQkwzXlRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/w640-h562/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Human evaluation. Full Image prompts elicit annotators&#39; overall impression of text-image alignment; Mask Simple and Mask Rich check for the correct inclusion of particular attributes, objects and attribute binding.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For single-image human evaluation, IM receives the highest ratings across-the-board (10–13% higher than the 2nd-highest performing model). For the rest, the performance order is IM-RM &amp;gt; DL2 &amp;gt; SD (with 3–6% difference) except for with Mask Simple, where IM-RM falls 4-8% behind. As relatively more semantic content is involved in Full and Mask Rich, we conjecture IM-RM and IM are benefited by the higher performing &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5 XXL&lt;/a>; text encoder. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJyEX698wRZGUWN3TyxR_CzR9CEQtIBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s1117/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1117&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJyEX698wRZGUWN3TyxR_CzR9CEQtIBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Single-image human evaluations of text-guided image inpainting on EditBench by prompt type. For Mask Simple and Mask Rich prompts, text-image alignment is correct if the edited image accurately includes every attribute and object specified in the prompt, including the correct attribute binding. Note that due to different evaluation designs, Full vs. Mask-only prompts, results are less directly comparable.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; EditBench focuses on fine-grained annotation, so we evaluate models for object and attribute types. For object types, IM leads in all categories, performing 10–11% better than the 2nd-highest performing model in common, rare, and text-rendering. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Qh-ea7hd1axd91Eckn-w_LbwaxHEkE0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s1086/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;579&quot; data-original-width=&quot;1086&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Qh-ea7hd1axd91Eckn-w_LbwaxHEkE0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Single-image human evaluations on EditBench Mask Simple by object type. As a cohort, models are better at object rendering than text-rendering.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For attribute types, IM is rated much higher (13–16%) than the 2nd highest performing model, except for in count, where DL2 is merely 1% behind. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-185GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s1086/image10.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;607&quot; data-original-width=&quot;1086&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-185GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Single-image human evaluations on EditBench Mask Simple by attribute type. Object masking improves adherence to prompt attributes across-the-board (IM vs. IM-RM).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Side-by-side compared with other models one-vs-one, IM leads in text alignment with a substantial margin, being preferred by annotators compared to SD, DL2, and IM-RM. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-0U0o8UJa8jM/ZINpLHAJa6I/AAAAAAAAMWg/1ieetbzjYAsdLSvNspPcH6RtpMgWdqgwQCNcBGAsYHQ/s636/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;319&quot; data-original-width=&quot;636&quot; src=&quot;https://1.bp.blogspot.com/-0U0o8UJa8jM/ZINpLHAJa6I/AAAAAAAAMWg/1ieetbzjYAsdLSvNspPcH6RtpMgWdqgwQCNcBGAsYHQ/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Side-by-side human evaluation of image realism &amp;amp; text-image alignment on EditBench Mask Rich prompts. For text-image alignment, Imagen Editor is preferred in all comparisons.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Finally, we illustrate a representative side-by-side comparative for all the models. See the &lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;paper&lt;/a>; for more examples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kSu6aolr7haZS4Oggyv76YC7VwqBkg7QnLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s766/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;766&quot; data-original-width=&quot;638&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kSu6aolr7haZS4Oggyv76YC7VwqBkg7QnLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example model outputs for Mask Simple vs. Mask Rich prompts. Object masking improves Imagen Editor&#39;s fine-grained adherence to the prompt compared to the same model trained with random masking.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented Imagen Editor and EditBench, making significant advancements in text-guided image inpainting and the evaluation thereof. Imagen Editor is a text-guided image inpainting fine-tuned from Imagen. EditBench is a comprehensive systematic benchmark for text-guided image inpainting, evaluating performance across multiple dimensions: attributes, objects, and scenes. Note that due to concerns in relation to responsible AI, we are not releasing Imagen Editor to the public. EditBench on the other hand is &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;released in full&lt;/a>; for the benefit of the research community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments &lt;/h2>; &lt;p>; &lt;em>;Thanks to Gunjan Baid, Nicole Brichtova, Sara Mahdavi, Kathy Meier-Hellstern, Zarana Parekh, Anusha Ramesh, Tris Warkentin, Austin Waters, and Vijay Vasudevan for their generous support. We give thanks to Igor Karpov, Isabel Kraus-Liang, Raghava Ram Pamidigantam, Mahesh Maddinala, and all the anonymous human annotators for their coordination to complete the human evaluation tasks. We are grateful to Huiwen Chang, Austin Tarango, and Douglas Eck for providing paper feedback. Thanks to Erica Moreira and Victor Gomes for help with resource coordination. Finally, thanks to the authors of DALL-E 2 for giving us permission to use their model outputs for research purposes.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2015281937814138473/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2015281937814138473&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2015281937814138473&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot; rel=&quot;alternate&quot; title=&quot;Imagen Editor and EditBench: Advancing and evaluating text-guided image inpainting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCOhgOtXQGmv1getpW3oHgD-4dCvqg9Q_Srs4qnD-FtydmHFb6XEesvUNGkf1eXRemuok7hb58ikl8tSw4xoNSwDd_YZkrdxVgj-6Fb5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s72-c/Imagen%20Editor%20&amp;%20EditBench%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-779195312945173416&lt;/id>;&lt;published>;2023-06-07T11:07:00.001-07:00&lt;/published>;&lt;updated>;2023-06-07T11:07:21.399-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Evaluating speech synthesis in many languages with SQuId&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Thibault Sellam, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheQlvvXBg5s0pU2XQwb7gxDhd_sNEHcE2EifW94vv-gYgpKF8V8BG5hKqw91qF5IzbZFnp2vZr1Am0OIQLAl29XK-MAh3XpOP05CzqCSLy1arpJ5gC4mJ4OMs0CGxc0iE4I9Rn5xcZaLFitBEK0lFQF1yURehcGbPYBsnKmpqE21VQHKovgAz76av4pg/s320/SQuId%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Previously, we presented the &lt;a href=&quot;https://blog.google/technology/ai/ways-ai-is-scaling-helpful/&quot;>;1,000 languages initiative&lt;/a>; and the &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; with the goal of making speech and language technologies available to billions of users around the world. Part of this commitment involves developing high-quality speech synthesis technologies, which build upon projects such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/vdtts-visually-driven-text-to-speech.html&quot;>;VDTTS&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;, for users that speak many different languages. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcw2d97jYi825Wuq60uagpE5XMlgPhb8pgmoZ7-f6tUZwYJtvcgbqdewHceeRHM1S4M64awkxgdBjCvVENZEDd6CweDk0fKiNJjGUd7cjgtfT9ddHQqbYDOZcZ_q5qdIZcoaMYFHX5lIF4cEP1L-DguwbLqvF6rsE6msyPzw1WdPCyXy21q520QkcSuA/s234/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;159&quot; data-original-width=&quot;234&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcw2d97jYi825Wuq60uagpE5XMlgPhb8pgmoZ7-f6tUZwYJtvcgbqdewHceeRHM1S4M64awkxgdBjCvVENZEDd6CweDk0fKiNJjGUd7cjgtfT9ddHQqbYDOZcZ_q5qdIZcoaMYFHX5lIF4cEP1L-DguwbLqvF6rsE6msyPzw1WdPCyXy21q520QkcSuA/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; After developing a new model, one must evaluate whether the speech it generates is accurate and natural: the content must be relevant to the task, the pronunciation correct, the tone appropriate, and there should be no acoustic artifacts such as cracks or signal-correlated noise. Such evaluation is a major bottleneck in the development of multilingual speech systems. &lt;/p>; &lt;p>; The most popular method to evaluate the quality of speech synthesis models is human evaluation: a text-to-speech (TTS) engineer produces a few thousand utterances from the latest model, sends them for human evaluation, and receives results a few days later. This evaluation phase typically involves &lt;em>;listening tests&lt;/em>;, during which dozens of annotators listen to the utterances one after the other to determine how natural they sound. While humans are still unbeaten at detecting whether a piece of text sounds natural, this process can be impractical — especially in the early stages of research projects, when engineers need rapid feedback to test and restrategize their approach. Human evaluation is expensive, time consuming, and may be limited by the availability of raters for the languages of interest. &lt;/p>; &lt;p>; Another barrier to progress is that different projects and institutions typically use various ratings, platforms and protocols, which makes apples-to-apples comparisons impossible. In this regard, speech synthesis technologies lag behind text generation, where researchers have long complemented human evaluation with automatic metrics such as &lt;a href=&quot;https://aclanthology.org/P02-1040/&quot;>;BLEU&lt;/a>; or, more recently, &lt;a href=&quot;https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html&quot;>;BLEURT&lt;/a>;. &lt;/p>; &lt;p>; In &quot;&lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;SQuId: Measuring Speech Naturalness in Many Languages&lt;/a>;&quot;, to be presented at &lt;a href=&quot;https://2023.ieeeicassp.org/&quot;>;ICASSP 2023&lt;/a>;, we introduce SQuId (Speech Quality Identification), a 600M parameter regression model that describes to what extent a piece of speech sounds natural. SQuId is based on &lt;a href=&quot;https://arxiv.org/abs/2202.01374&quot;>;mSLAM&lt;/a>; (a pre-trained speech-text model developed by Google), fine-tuned on over a million quality ratings across 42 languages and tested in 65. We demonstrate how SQuId can be used to complement human ratings for evaluation of many languages. This is the largest published effort of this type to date. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluating TTS with SQuId&lt;/h2>; &lt;p>; The main hypothesis behind SQuId is that training a regression model on previously collected ratings can provide us with a low-cost method for assessing the quality of a TTS model. The model can therefore be a valuable addition to a TTS researcher&#39;s evaluation toolbox, providing a near-instant, albeit less accurate alternative to human evaluation. &lt;/p>; &lt;p>; SQuId takes an utterance as input and an optional locale tag (ie, a localized variant of a language, such as &quot;Brazilian Portuguese&quot; or &quot;British English&quot;). It returns a score between 1 and 5 that indicates how natural the waveform sounds, with a higher value indicating a more natural waveform. &lt;/p>; &lt;p>; Internally, the model includes three components: (1) an encoder, (2) a pooling / regression layer, and (3) a fully connected layer. First, the &lt;em>;encoder &lt;/em>;takes a spectrogram as input and embeds it into a smaller 2D matrix that contains 3,200 vectors of size 1,024, where each vector encodes a time step. The pooling / regression layer aggregates the vectors, appends the locale tag, and feeds the result into a fully connected layer that returns a score. Finally, we apply application-specific post-processing that rescales or normalizes the score so it is within the [1, 5] range, which is common for naturalness human ratings. We train the whole model end-to-end with a regression loss. &lt;/p>; &lt;p>; The encoder is by far the largest and most important piece of the model. We used &lt;a href=&quot;https://arxiv.org/abs/2202.01374&quot;>;mSLAM&lt;/a>;, a pre-existing 600M-parameter &lt;a href=&quot;http://www.interspeech2020.org/index.php?m=content&amp;amp;c=index&amp;amp;a=show&amp;amp;catid=418&amp;amp;id=1331&quot;>;Conformer&lt;/a>; pre-trained on both speech (51 languages) and text (101 languages). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYyDqxha8sXDsBRVypuCZzyZbhMvZ_kB3RCjbjloyxQWaimFn05RDXX5CfyRUEy5UMBM0kgB-GntnaVz989u4h4tblGfYmfdnbTkwRKfT8zA7zEHYKm0IHf82DpzdY8gGA65uGLlEk16FK88swG0QPG5fQaXpTF7TXuCOigK4PSf50sQ9NEvorWz34pw/s475/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;226&quot; data-original-width=&quot;475&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYyDqxha8sXDsBRVypuCZzyZbhMvZ_kB3RCjbjloyxQWaimFn05RDXX5CfyRUEy5UMBM0kgB-GntnaVz989u4h4tblGfYmfdnbTkwRKfT8zA7zEHYKm0IHf82DpzdY8gGA65uGLlEk16FK88swG0QPG5fQaXpTF7TXuCOigK4PSf50sQ9NEvorWz34pw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The SQuId model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To train and evaluate the model, we created the SQuId corpus: a collection of 1.9 million rated utterances across 66 languages, collected for over 2,000 research and product TTS projects. The SQuId corpus covers a diverse array of systems, including concatenative and neural models, for a broad range of use cases, such as driving directions and virtual assistants. Manual inspection reveals that SQuId is exposed to a vast range of of TTS errors, such as acoustic artifacts (eg, cracks and pops), incorrect prosody (eg, questions without rising intonations in English), text normalization errors (eg, verbalizing &quot;7/7&quot; as &quot;seven divided by seven” rather than &quot;July seventh&quot;), or pronunciation mistakes (eg, verbalizing &quot;tough&quot; as &quot;toe&quot;). &lt;/p>; &lt;p>; A common issue that arises when training multilingual systems is that the training data may not be uniformly available for all the languages of interest. SQuId was no exception. The following figure illustrates the size of the corpus for each locale. We see that the distribution is largely dominated by US English. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikpLHSW7usXN2qsYnUbJHNcOaANJcE1LSLLeg7ykagi60xn_fBKQNMTXCv_wOcDkCCunorkrQudsSVzOyqbt1cWaLKzpadU-dAwa3Qaj41lFSg130XnyNB2I_Bt5RXHksF0BnwfBBZ9OM5yMNBtHCXX-n_1FzDrKOKqoAKDdlllazmEk1k8Nh8EJoMjA/s3362/locales_blog.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;3362&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikpLHSW7usXN2qsYnUbJHNcOaANJcE1LSLLeg7ykagi60xn_fBKQNMTXCv_wOcDkCCunorkrQudsSVzOyqbt1cWaLKzpadU-dAwa3Qaj41lFSg130XnyNB2I_Bt5RXHksF0BnwfBBZ9OM5yMNBtHCXX-n_1FzDrKOKqoAKDdlllazmEk1k8Nh8EJoMjA/s16000/locales_blog.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Locale distribution in the SQuId dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; How can we provide good performance for all languages when there are such variations? Inspired by &lt;a href=&quot;https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html&quot;>;previous work on machine translation&lt;/a>;&lt;strong>;, &lt;/strong>;as well as &lt;a href=&quot;https://ieeexplore.ieee.org/document/6424230&quot;>;past work&lt;/a>; from the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DNN-MultiLingual-ICASSP2013.pdf&quot;>;speech literature&lt;/a>;, we decided to train one model for all languages, rather than using separate models for each language. The hypothesis is that if the model is large enough, then &lt;em>;cross-locale transfer&lt;/em>; can occur: the model&#39;s accuracy on each locale improves as a result of jointly training on the others. As our experiments show, cross-locale proves to be a powerful driver of performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; To understand SQuId&#39;s overall performance, we compare it to a custom Big-SSL-MOS model (described in the &lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;paper&lt;/a>;), a competitive baseline inspired by &lt;a href=&quot;https://arxiv.org/abs/2110.02635&quot;>;MOS-SSL&lt;/a>;, a state-of-the-art TTS evaluation system. Big-SSL-MOS is based on &lt;a href=&quot;https://arxiv.org/abs/2108.06209&quot;>;w2v-BERT&lt;/a>; and was trained on the &lt;a href=&quot;https://arxiv.org/pdf/2203.11389.pdf&quot;>;VoiceMOS&#39;22 Challenge&lt;/a>; dataset, the most popular dataset at the time of evaluation. We experimented with several variants of the model, and found that SQuId is up to 50.0% more accurate. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvZGKcIEzeMTewWeCTWpKpNiLRSL4CaT8tH5xU-lD1u4Qyt5UQDLn0CORlZ2QfwKMFZ5rc4jv56RRYj_YhNfkx7z6odaf2hEJGSXVQnzmeT-eJcycjB9nC6ybYlHfTB4-LIh1UmK19ca-XFRIhC36lK6oleJcnhMsp4L0K7-TmEXKr5taJOSQ0ZpMNhg/s1131/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;550&quot; data-original-width=&quot;1131&quot; height=&quot;311&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvZGKcIEzeMTewWeCTWpKpNiLRSL4CaT8tH5xU-lD1u4Qyt5UQDLn0CORlZ2QfwKMFZ5rc4jv56RRYj_YhNfkx7z6odaf2hEJGSXVQnzmeT-eJcycjB9nC6ybYlHfTB4-LIh1UmK19ca-XFRIhC36lK6oleJcnhMsp4L0K7-TmEXKr5taJOSQ0ZpMNhg/w640-h311/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId versus state-of-the-art baselines. We measure agreement with human ratings using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient&quot;>;Kendall Tau&lt;/a>;, where a higher value represents better accuracy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To understand the impact of cross-locale transfer, we run a series of ablation studies. We vary the amount of locales introduced in the training set and measure the effect on SQuId&#39;s accuracy. In English, which is already over-represented in the dataset, the effect of adding locales is negligible. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyCRuylkQK8ZseMHYwtfMkbIal3okcJX8UFmFwzzmqM0xkDfFcvISBKymDiNyG4liTH5mhGkpkBy4QAXSsNmDnnM5Q51fZ7kEjPCp2Ph667_mvPinxxeCocJVNuFM8h8JD6TCdAhYX_JzFRzEJFTags-0dVxGpzlb0GsyzZy2VBiRFv7NZHazE8Qqdmw/s949/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;881&quot; data-original-width=&quot;949&quot; height=&quot;371&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyCRuylkQK8ZseMHYwtfMkbIal3okcJX8UFmFwzzmqM0xkDfFcvISBKymDiNyG4liTH5mhGkpkBy4QAXSsNmDnnM5Q51fZ7kEjPCp2Ph667_mvPinxxeCocJVNuFM8h8JD6TCdAhYX_JzFRzEJFTags-0dVxGpzlb0GsyzZy2VBiRFv7NZHazE8Qqdmw/w400-h371/image5.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId&#39;s performance on US English, using 1, 8, and 42 locales during fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; However, cross-locale transfer is much more effective for most other locales: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbjhO-kHVhhdfB8_zKKPwFrV3IqJjShgGUJSzPoTYNu7gkmPtayfR_ql4HNRGFDBLi0sh4MNORtLHbkrheLeRqBxgjKAiH7qDGa-_qaI2E7FPOdiqMOrKwibw691lLHscb277kB9RtJXd7MEL2nStMSWvbtFYaR5RCVBbNiQDAwuNgw7SFwo4e_m1wdA/s1320/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;549&quot; data-original-width=&quot;1320&quot; height=&quot;266&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbjhO-kHVhhdfB8_zKKPwFrV3IqJjShgGUJSzPoTYNu7gkmPtayfR_ql4HNRGFDBLi0sh4MNORtLHbkrheLeRqBxgjKAiH7qDGa-_qaI2E7FPOdiqMOrKwibw691lLHscb277kB9RtJXd7MEL2nStMSWvbtFYaR5RCVBbNiQDAwuNgw7SFwo4e_m1wdA/w640-h266/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId&#39;s performance on four selected locales (Korean, French, Thai, and Tamil), using 1, 8, and 42 locales during fine-tuning. For each locale, we also provide the training set size.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To push transfer to its limit, we held 24 locales out during training and used them for testing exclusively. Thus, we measure to what extent SQuId can deal with languages that it has never seen before. The plot below shows that although the effect is not uniform, cross-locale transfer works. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizF9U8Wxz-zsPZvqSJM1QQL7O-8TURWdbPAkzPjthZSdejs5ySvj26kysmwl9FmiFIesgzoJ5w68Zkc7_qE-KEi4SFwXY16PBTn0zukQ7w4p9pZ2RvGxarp8zZFvv-H5cEOHx5q7vibhUp_0ZdpZVp0s2NAHll0b6FrxN9z8YW-fOJyN8jtLXjK1wqUQ/s823/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;823&quot; height=&quot;264&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizF9U8Wxz-zsPZvqSJM1QQL7O-8TURWdbPAkzPjthZSdejs5ySvj26kysmwl9FmiFIesgzoJ5w68Zkc7_qE-KEi4SFwXY16PBTn0zukQ7w4p9pZ2RvGxarp8zZFvv-H5cEOHx5q7vibhUp_0ZdpZVp0s2NAHll0b6FrxN9z8YW-fOJyN8jtLXjK1wqUQ/w640-h264/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId&#39;s performance on four &quot;zero-shot&quot; locales; using 1, 8, and 42 locales during fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; When does cross-locale operate, and how? We present many more ablations in the &lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;paper&lt;/a>;, and show that while language similarity plays a role (eg, training on Brazilian Portuguese helps European Portuguese) it is surprisingly far from being the only factor that matters. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We introduce SQuId, a 600M parameter regression model that leverages the SQuId dataset and cross-locale learning to evaluate speech quality and describe how natural it sounds. We demonstrate that SQuId can complement human raters in the evaluation of many languages. Future work includes accuracy improvements, expanding the range of languages covered, and tackling new error types. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The author of this post is now part of Google DeepMind. Many thanks to all authors of the paper: Ankur Bapna, Joshua Camp, Diana Mackinnon, Ankur P. Parikh, and Jason Riesa.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/779195312945173416/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/evaluating-speech-synthesis-in-many.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/779195312945173416&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/779195312945173416&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/evaluating-speech-synthesis-in-many.html&quot; rel=&quot;alternate&quot; title=&quot;Evaluating speech synthesis in many languages with SQuId&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheQlvvXBg5s0pU2XQwb7gxDhd_sNEHcE2EifW94vv-gYgpKF8V8BG5hKqw91qF5IzbZFnp2vZr1Am0OIQLAl29XK-MAh3XpOP05CzqCSLy1arpJ5gC4mJ4OMs0CGxc0iE4I9Rn5xcZaLFitBEK0lFQF1yURehcGbPYBsnKmpqE21VQHKovgAz76av4pg/s72-c/SQuId%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1995595447829405143&lt;/id>;&lt;published>;2023-06-06T09:58:00.000-07:00&lt;/published>;&lt;updated>;2023-06-06T09:58:41.452-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Visual captions: Using large language models to augment video conferences with dynamic visuals&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ruofei Du, Research Scientist, and Alex Olwal, Senior Staff Research Scientist, Google Augmented Reality&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuQA-vtXoNbaUU05FVACHS1vMdvtOxK5pcusTeWW2dkG9OpPiKHLtRuCbtNlAMX4TyRYYNNO0NMhCPTDsGDkVzs2FJvITsOknz8GcO4hJ7Eds57YVXdFtQW3fEzNxiLq0MQC2G3GW0ESfVqOBogCsbIDc1YfQpIgKsJm0idXET58ia266bNjb63HHuDw/s745/VisualCaptions.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Recent advances in video conferencing have significantly improved remote video communication through features like live captioning and noise cancellation. However, there are various situations where dynamic visual augmentation would be useful to better convey complex and nuanced information. For example, when discussing what to order at a Japanese restaurant, your friends could share visuals that would help you feel more confident about ordering the “Sukiyaki”. Or when talking about your recent family trip to San Francisco, you may want to show a photo from your personal album. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://research.google/pubs/pub52074/&quot;>;Visual Captions: Augmenting Verbal Communication With On-the-fly Visuals&lt;/a>;”, presented at &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/95817&quot;>;ACM CHI 2023&lt;/a>;, we introduce a system that uses verbal cues to augment synchronous video communication with real-time visuals. We fine-tuned a large language model to proactively suggest relevant visuals in open-vocabulary conversations using a &lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;dataset&lt;/a>; we curated for this purpose. We open sourced Visual Captions as part of the &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; project, which is designed for rapid prototyping of augmented communication with real-time transcription. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieLD2jgO2DgpK5t9TrF7tg0-yHiPC0gtCdhJZcKBPx5cPo8JmWDVouffM6GK8hlkszzcR_rU9i-ym3WrA1HeOomCg2vQk61Flc7P58lAPXiiBS7ugc1S3Ah-p8GudwKJ3OJPew8xURhnesV5WP84MUAej6oF1jIY6CtXgv-gk1qLhs7GDTv7N8LYMz6A/s640/image11.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;277&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieLD2jgO2DgpK5t9TrF7tg0-yHiPC0gtCdhJZcKBPx5cPo8JmWDVouffM6GK8hlkszzcR_rU9i-ym3WrA1HeOomCg2vQk61Flc7P58lAPXiiBS7ugc1S3Ah-p8GudwKJ3OJPew8xURhnesV5WP84MUAej6oF1jIY6CtXgv-gk1qLhs7GDTv7N8LYMz6A/s16000/image11.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual Captions facilitates verbal communication with real-time visuals. The system is even robust against typical mistakes that may often appear in real-time speech-to-text transcription. For example, out of context, the transcription model misunderstood the word &quot;pier&quot; as &quot;pair&quot;, but Visual Captions still recommends images of the Santa Monica Pier.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Design space for augmenting verbal communication with dynamic visuals&lt;/h2>; &lt;p>; We invited 10 internal participants, each with various technical and non-technical backgrounds, including software engineers, researchers, UX designers, visual artists, students, etc., to discuss their particular needs and desires for a potential real-time visual augmentation service. In two sessions, we introduced low-fidelity prototypes of the envisioned system, followed by video demos of the existing text-to-image systems. These discussions informed a design space with eight dimensions for visual augmentation of real-time conversations, labeled below as D1 to D8. &lt;/p>; &lt;p>; Visual augmentations could be synchronous or asynchronous with the conversation (D1: Temporal), could be used for both expressing and understanding speech content (D2: Subject), and could be applied using a wide range of different visual content, visual types, and visual sources (D3: Visual). Such visual augmentation might vary depending on the scale of the meetings (D4: Scale) and whether a meeting is in co-located or remote settings (D5: Space). These factors also influence whether the visuals should be displayed privately, shared between participants, or public to everyone (D6: Privacy). Participants also identified different ways in which they would like to interact with the system while having conversations (D7: Initiation). For example, people proposed different levels of “proactivity”, which indicates the degree to which users would like the model to take the initiative. Finally, participants envisioned different methods of interaction, for example, using speech or gestures for input. (D8: Interaction). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXvp9pvpgO87EEQo0nF8rMWRppqE1TKd6kGzNeiiSdpWuzH7gLKTzdj2_m6p-VP-1w2EjF2t6hptVhSDF4gwBpPY3Ed6m6cMGzzT358eZPVA9fyrGYRBpDY5aTh44G-ybHvzPn4VZyyHDkzvp0JpdG6-clKwVUlg56L6lobZjc5SZMiOcBlo4bIA8mMQ/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1547&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXvp9pvpgO87EEQo0nF8rMWRppqE1TKd6kGzNeiiSdpWuzH7gLKTzdj2_m6p-VP-1w2EjF2t6hptVhSDF4gwBpPY3Ed6m6cMGzzT358eZPVA9fyrGYRBpDY5aTh44G-ybHvzPn4VZyyHDkzvp0JpdG6-clKwVUlg56L6lobZjc5SZMiOcBlo4bIA8mMQ/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Design space for augmenting verbal communication with dynamic visuals.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Informed by this initial feedback, we designed Visual Captions to focus on generating &lt;em>;synchronous&lt;/em>; visuals of semantically relevant &lt;em>;visual content&lt;/em>;, &lt;em>;type&lt;/em>;, and &lt;em>;source&lt;/em>;. While participants in these initial exploratory sessions were participating in one-to-one remote conversations, deployment of Visual Captions in the wild will often be in one-to-many (eg, an individual giving a presentation to an audience) and many-to-many scenarios (eg, a discussion among multiple people in a meeting). &lt;/p>; &lt;p>; Because the visual that best complements a conversation depends strongly on the context of the discussion, we needed a training set specific to this purpose. So, we collected a dataset of 1595 quadruples of &lt;em>;language (1)&lt;/em>;, &lt;em>;visual content (2)&lt;/em>;, &lt;em>;type (3)&lt;/em>;, and &lt;em>;source (4)&lt;/em>; across a variety of contexts, including daily conversations, lectures, and travel guides. For example, “I would love to see it!” corresponds to visual content of “face smiling”, a visual type of “emoji”, and visual source of “public search”. “Did she tell you about our trip to Mexico?” corresponds to visual content of “a photo from the trip to Mexico&#39;&#39;, a visual type of “photo”, and visual source of “personal album”. We publicly released this &lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;VC1.5K dataset&lt;/a>; for the research community. &lt;/p>; &lt;br />; &lt;h2>;Visual intent prediction model&lt;/h2>; &lt;p>; To predict what visuals could supplement a conversation, we trained a visual intent prediction model based on a large language model using the VC1.5K dataset. For training, we parsed each visual intent into the format of &quot;&lt;code style=&quot;font-size: small;&quot;>;&amp;lt;Visual Type&amp;gt; of &amp;lt;Visual Content&amp;gt; from &amp;lt;Visual Source&amp;gt;&lt;/code>;&quot;. &lt;/p>; &lt;pre class=&quot;prettyprint&quot; style=&quot;font-size: small; margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;{&quot;prompt&quot;: &quot;&amp;lt;Previous Two Sentences&amp;gt; →&quot;, &quot;completion&quot;: &quot;&amp;lt;Visual Type 1&amp;gt; of &quot;&amp;lt;Visual Type 1&amp;gt; from &quot;&amp;lt;Visual Source 1&amp;gt;; &amp;lt;Visual Type 2&amp;gt; of &quot;&amp;lt;Visual Type 2&amp;gt; from &quot;&amp;lt;Visual Source 2&amp;gt;; ... \𝑛&quot;} &lt;/pre>; &lt;p>; Using this format, this system can handle open-vocabulary conversations and contextually predict visual content, visual source, and visual type. Anecdotally, we found that it outperforms keyword-based approaches, which fail to handle open-vocabulary examples like “Your aunt Amy will be visiting this Saturday,” and cannot suggest relevant visual types or visual sources.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtY2Jw884pCstWr6aMdcy4Ge6EmxUbMk7YIEiVX3PrDgteVh3Fhw9Ug74ygoBXCHOZUswYcNyNjf5Lckn8zWWCNxlJyvrWQTIWqGuZMYoeJnsTYiD4Anv8Wssu1nZ5dKQcevobH61kYSL8_UYBD3qF_aQoTSjD0GZc7fFS4ehf6yFcxhkUYZBy7EvOwQ/s2552/vc-examples.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1900&quot; data-original-width=&quot;2552&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtY2Jw884pCstWr6aMdcy4Ge6EmxUbMk7YIEiVX3PrDgteVh3Fhw9Ug74ygoBXCHOZUswYcNyNjf5Lckn8zWWCNxlJyvrWQTIWqGuZMYoeJnsTYiD4Anv8Wssu1nZ5dKQcevobH61kYSL8_UYBD3qF_aQoTSjD0GZc7fFS4ehf6yFcxhkUYZBy7EvOwQ/s16000/vc-examples.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of visual intent predictions by our model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We used 1276 (80%) examples from the VC1.5K dataset for fine-tuning the large language model and the remaining 319 (20%) examples as test data. We measured the performance of the fine-tuned model with the token accuracy metric, ie, the percentage of tokens in a batch that were correctly predicted by the model. During training, our model reached a training token accuracy of 97% and a validation token accuracy of 87%. &lt;/p>; &lt;br />; &lt;h2>;Performance&lt;/h2>; &lt;p>; To evaluate the utility of the trained Visual Captions model, we invited 89 participants to perform 846 tasks. They were asked to provide feedback on a scale of &quot;1 — Strongly Disagree&quot; to &quot;7 — Strongly Agree&quot; for six qualitative statements. Most participants preferred to have the visual during a conversation (Q1, 83% ≥ 5–Somewhat Agree). Moreover, they considered the displayed visuals to be useful and informative (Q2, 82% ≥ 5–Somewhat Agree), high-quality (Q3, 82% ≥ 5–Somewhat Agree), and relevant to the original speech (Q4, 84% ≥ 5–Somewhat Agree). Participants also found the predicted visual type (Q5, 87% ≥ 5–Somewhat Agree) and visual source (Q6, 86% ≥ 5–Somewhat Agree) to be accurate given the context of the corresponding conversation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgks8-CM1HIfvpCikBqx2EuVzdZgN0oLFZixeTcm0Fazt7vhUnR7olQqiMi12m1itphv0Pmtq0RxoPCywVBsw9OU8irUBarqTfHDJfIGpeOUrCmIeBtd5AsUVfPlbPxGXkYzgmMYu9l1JqzhHJy8Sld7rSi-DyuQeSOytHtP54rBOrbuLbCB2ndNozKLg/s1828/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width=&quot;1828&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgks8-CM1HIfvpCikBqx2EuVzdZgN0oLFZixeTcm0Fazt7vhUnR7olQqiMi12m1itphv0Pmtq0RxoPCywVBsw9OU8irUBarqTfHDJfIGpeOUrCmIeBtd5AsUVfPlbPxGXkYzgmMYu9l1JqzhHJy8Sld7rSi-DyuQeSOytHtP54rBOrbuLbCB2ndNozKLg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Technical evaluation results of the visual prediction model rated by study participants.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With this fine-tuned visual intent prediction model, we developed Visual Captions on the &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; platform, which can add new interactive widgets directly on the camera streams of video conferencing platforms, such as &lt;a href=&quot;https://apps.google.com/meet/&quot;>;Google Meet&lt;/a>;. As shown in the system workflow below, Visual Captions automatically captures the user&#39;s speech, retrieves the last sentences, feeds them into the visual intent prediction model every 100 ms, retrieves relevant visuals, and then suggests visuals in real time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQVHhdvhqftCX1QDtTRSfAYaYMBT_0blsN_WOaRf44C-c0R2tggMy2yddOpSnjSRfhIOQ793JAZfNwSLy_XLW3_l6NAtJJ-tkrxOYrL_kuW2D5HWYbCs3dnt0OGINHOwuYE5DIK2dp8Ia_Oq3-y4mqntpZ3hK0a44pW7umjy3KEwIhUw1z46j0NsJV7A/s1756/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;662&quot; data-original-width=&quot;1756&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQVHhdvhqftCX1QDtTRSfAYaYMBT_0blsN_WOaRf44C-c0R2tggMy2yddOpSnjSRfhIOQ793JAZfNwSLy_XLW3_l6NAtJJ-tkrxOYrL_kuW2D5HWYbCs3dnt0OGINHOwuYE5DIK2dp8Ia_Oq3-y4mqntpZ3hK0a44pW7umjy3KEwIhUw1z46j0NsJV7A/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;System workflow of Visual Captions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Visual Captions provides three levels of proactivity when suggesting visuals: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Auto-display&lt;/em>; (high-proactivity): The system autonomously searches and displays visuals publicly to all meeting participants. No user interaction required. &lt;/li>;&lt;li>;&lt;em>;Auto-suggest&lt;/em>; (medium-proactivity): The suggested visuals are shown in a private scrolling view. A user then clicks a visual to display it publicly. In this mode, the system is proactively recommending visuals, but the user decides when and what to display. &lt;/li>;&lt;li>;&lt;em>;On-demand-suggest &lt;/em>;(low-proactivity): The system will only suggest visuals if a user presses the spacebar. &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Quantitative and qualitative evaluation: User studies&lt;/h2>; &lt;p>; We evaluated Visual Captions in both a controlled lab study (&lt;em>;n &lt;/em>;= 26) and in-the-wild deployment studies (&lt;em>;n&lt;/em>; = 10). Participants found that real-time visuals facilitated live conversations by helping explain unfamiliar concepts, resolve language ambiguities, and make conversations more engaging. Participants also reported different preferences for interacting with the system in-situ, and that varying levels of proactivity were preferred in different social scenarios. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitIGdmw1Po1EpiimJnsXctKJIVpeptyNRJYmf-fSIBkEymdJCcvI9muj5Ov_ohIKdIeScv3gvpm0RJli8V2J4Li4mjiAeqyjfqgdB0GhfkB96_sZcYjApSk9Cj9yHWyINEskK5Fic-9ET0kaZGtSGXXdzdjOJDKHcE52pzrGixYPjpZcYpNFHPzxazfw/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;590&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitIGdmw1Po1EpiimJnsXctKJIVpeptyNRJYmf-fSIBkEymdJCcvI9muj5Ov_ohIKdIeScv3gvpm0RJli8V2J4Li4mjiAeqyjfqgdB0GhfkB96_sZcYjApSk9Cj9yHWyINEskK5Fic-9ET0kaZGtSGXXdzdjOJDKHcE52pzrGixYPjpZcYpNFHPzxazfw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Participants&#39; &lt;a href=&quot;https://en.wikipedia.org/wiki/NASA-TLX&quot;>;Task Load Index&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale&quot;>;Likert scale&lt;/a>; ratings (from 1 - Strongly Disagree to 7 - Strongly Agree) of four conversations without Visual Captions (“No VC”) and the three Visual Captions modes: auto-display, auto-suggest, and on-demand suggest.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusions and future directions&lt;/h2>; &lt;p>; This work proposes a system for real-time visual augmentation of verbal communication, called Visual Captions, that was trained using a dataset of 1595 visual intents collected from 246 participants, covering 15 topic categories. We publicly release the training dataset, &lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;VC1.5K&lt;/a>; to the research community to support further research in this space. We have also deployed Visual Captions in &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>;, which facilitates video conferences in Google Meet by transcribing meetings and augmenting the camera video streams. &lt;/p>; &lt;p>; Visual Captions represents a significant step towards enhancing verbal communication with on-the-fly visuals. By understanding the importance of visual cues in everyday conversations, we can create more effective communication tools and improve how people connect. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Key contributors to the project include Xingyu “Bruce” Liu, Vladimir Kirilyuk, Xiuxiu Yuan, Peggy Chi, Alex Olwal, and Ruofei Du.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;We would like to extend our thanks to those on the ARChat team who provided assistance, including Jason Mayes, Max Spear, Na Li, Jun Zhang, Jing Jin, Yuan Ren, Adarsh Kowdle, Ping Yu, Darcy Philippon, and Ezgi Oztelcan. We would also like to thank the many people with whom we&#39;ve had insightful discussions and those who provided feedback on the manuscript, including Eric Turner, Yinda Zhang, Feitong Tan, Danhang Tang, and Shahram Izadi. We would also like to thank our CHI reviewers for their insightful feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1995595447829405143/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/visual-captions-using-large-language.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1995595447829405143&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1995595447829405143&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/visual-captions-using-large-language.html&quot; rel=&quot;alternate&quot; title=&quot;Visual captions: Using large language models to augment video conferences with dynamic visuals&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuQA-vtXoNbaUU05FVACHS1vMdvtOxK5pcusTeWW2dkG9OpPiKHLtRuCbtNlAMX4TyRYYNNO0NMhCPTDsGDkVzs2FJvITsOknz8GcO4hJ7Eds57YVXdFtQW3fEzNxiLq0MQC2G3GW0ESfVqOBogCsbIDc1YfQpIgKsJm0idXET58ia266bNjb63HHuDw/s72-c/VisualCaptions.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5739375622760902222&lt;/id>;&lt;published>;2023-06-02T10:02:00.000-07:00&lt;/published>;&lt;updated>;2023-06-02T10:02:37.777-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Automatic Speech Recognition&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AVFormer: Injecting vision into frozen speech models for zero-shot AV-ASR&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Arsha Nagrani and Paul Hongsuck Seo, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEpnVot9nisUn-B5GierK8WfqI_mhVwKmJ1mjj_FOdOr2_74gIwvwSJgobi8K_a1uq9n5B3pO4Zi5-mXoE0mzOw32WW-ny6kBThTgHIv3wtQVrpjdMdhMYCSqustzlN7ArP3y2jSqEy_2rlz_zXfsfp05Z3svQ7bu8rYJAN6V1ellHjbL-SJOHraUHEQ/s1150/AVFormer.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_recognition&quot;>;Automatic speech recognition&lt;/a>; (ASR) is a well-established technology that is widely adopted for various applications such as conference calls, streamed video transcription and voice commands. While the challenges for this technology are centered around noisy &lt;em>;audio&lt;/em>; inputs, the &lt;em>;visual&lt;/em>; stream in multimodal videos (eg, TV, online edited videos) can provide strong cues for improving the robustness of ASR systems — this is called audiovisual ASR (AV-ASR). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Although lip motion can provide strong signals for speech recognition and is the most common area of focus for AV-ASR, the mouth is often not directly visible in &lt;em>;videos in the wild&lt;/em>; (eg, due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Egocentric_vision&quot;>;egocentric viewpoints&lt;/a>;, face coverings, and low resolution) and therefore, a new emerging area of research is &lt;em>;unconstrained&lt;/em>; AV-ASR (eg, &lt;a href=&quot;https://arxiv.org/abs/2206.07684&quot;>;AVATAR&lt;/a>;), which investigates the contribution of entire visual frames, and not just the mouth region. &lt;/p>; &lt;p>; Building audiovisual datasets for training AV-ASR models, however, is challenging. Datasets such as &lt;a href=&quot;https://srvk.github.io/how2-dataset/&quot;>;How2&lt;/a>; and &lt;a href=&quot;https://gabeur.github.io/avatar-visspeech&quot;>;VisSpeech&lt;/a>; have been created from instructional videos online, but they are small in size. In contrast, the models themselves are typically large and consist of both visual and audio encoders, and so they tend to overfit on these small datasets. Nonetheless, there have been a number of recently released large-scale audio-only models that are heavily optimized via large-scale training on massive &lt;em>;audio-only&lt;/em>; data obtained from audio books, such as &lt;a href=&quot;https://github.com/facebookresearch/libri-light&quot;>;LibriLight&lt;/a>; and &lt;a href=&quot;https://www.openslr.org/12&quot;>;LibriSpeech&lt;/a>;. These models contain billions of parameters, are readily available, and show strong generalization across domains. &lt;/p>; &lt;p>; With the above challenges in mind, in “&lt;a href=&quot;https://arxiv.org/pdf/2303.16501.pdf&quot;>;AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR&lt;/a>;”, we present a simple method for augmenting existing large-scale audio-only models with visual information, at the same time performing lightweight domain adaptation. AVFormer injects visual embeddings into a frozen ASR model (similar to how Flamingo &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;injects visual information&lt;/a>; into large language models for vision-text tasks) using lightweight trainable adaptors that can be trained on a small amount of weakly labeled video data with minimum additional training time and parameters. We also introduce a simple curriculum scheme during training, which we show is crucial to enable the model to jointly process audio and visual information effectively. The resulting AVFormer model achieves state-of-the-art zero-shot performance on three different AV-ASR benchmarks (How2, VisSpeech and &lt;a href=&quot;https://ego4d-data.org/&quot;>;Ego4D&lt;/a>;), while also crucially preserving decent performance on traditional audio-only speech recognition benchmarks (ie, &lt;a href=&quot;https://www.openslr.org/12&quot;>;LibriSpeech&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxMFN8ianu3Y1iJ4tB5Bt7xSnYsHXt3-oYl3LnZjpHt9bPrUHFgQ14msppOwZ7TmOFMUSXc86l5tcuB1W6SroOQiiT__IB7ZyeiHRxTf51xY-f_i3iWCRzIDmB2ciAlvjsNATTDVBt0nvuEnWBQSoKou0PNBF7UYfkI2xM0MxPZa1JlxtOSHePEgxiVw/s958/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;286&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxMFN8ianu3Y1iJ4tB5Bt7xSnYsHXt3-oYl3LnZjpHt9bPrUHFgQ14msppOwZ7TmOFMUSXc86l5tcuB1W6SroOQiiT__IB7ZyeiHRxTf51xY-f_i3iWCRzIDmB2ciAlvjsNATTDVBt0nvuEnWBQSoKou0PNBF7UYfkI2xM0MxPZa1JlxtOSHePEgxiVw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Unconstrained audiovisual speech recognition. We inject vision into a frozen speech model (&lt;a href=&quot;https://arxiv.org/pdf/2202.01855.pdf&quot;>;BEST-RQ&lt;/a>;, in grey) for zero-shot audiovisual ASR via lightweight modules to create a parameter- and data-efficient model called AVFormer (blue). The visual context can provide helpful clues for robust speech recognition especially when the audio signal is noisy (the visual loaf of bread helps correct the audio-only mistake “clove” to “loaf” in the generated transcript).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Injecting vision using lightweight modules&lt;/h2>; &lt;p>; Our goal is to add visual understanding capabilities to an existing audio-only ASR model while maintaining its generalization performance to various domains (both AV and audio-only domains). &lt;/p>; &lt;p>; To achieve this, we augment an existing state-of-the-art ASR model (&lt;a href=&quot;https://arxiv.org/pdf/2202.01855.pdf&quot;>;Best-RQ&lt;/a>;) with the following two components: (i) linear visual projector and (ii) lightweight adapters. The former projects visual features in the audio token embedding space. This process allows the model to properly connect separately pre-trained visual feature and audio input token representations. The latter then minimally modifies the model to add understanding of multimodal inputs from videos. We then train these additional modules on unlabeled web videos from the &lt;a href=&quot;https://www.di.ens.fr/willow/research/howto100m/&quot;>;HowTo100M dataset&lt;/a>;, along with the outputs of an ASR model as pseudo ground truth, while keeping the rest of the Best-RQ model frozen. Such lightweight modules enable data-efficiency and strong generalization of performance. &lt;/p>; &lt;p>; We evaluated our extended model on AV-ASR benchmarks in a zero-shot setting, where the model is never trained on a manually annotated AV-ASR dataset. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Curriculum learning for vision injection&lt;/h2>; &lt;p>; After the initial evaluation, we discovered empirically that with a naïve single round of joint training, the model struggles to learn both the adapters and the visual projectors in one go. To mitigate this issue, we introduced a two-phase &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/8403835/&quot;>;curriculum learning strategy&lt;/a>; that decouples these two factors — domain adaptation and visual feature integration — and trains the network in a sequential manner. In the first phase, the adapter parameters are optimized without feeding visual tokens at all. Once the adapters are trained, we add the visual tokens and train the visual projection layers alone in the second phase while the trained adapters are kept frozen. &lt;/p>; &lt;p>; The first stage focuses on audio domain adaptation. By the second phase, the adapters are completely frozen and the visual projector must simply learn to generate visual prompts that project the visual tokens into the audio space. In this way, our curriculum learning strategy allows the model to incorporate visual inputs as well as adapt to new audio domains in AV-ASR benchmarks. We apply each phase just once, as an iterative application of alternating phases leads to performance degradation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_hr_fd1iI5JofcM_c_4rRIayzvF5dlQ7T-keHxvHuLRlqmlKhs_Yy-IOuiYrj8GrnscXNB_vN1oXqlTnxvzL0dyl9PraXfuIQ91lQttaY4-e019DmRCxRllEIJj6T3RGn5J-RyTpALh1xgpVugcYQBy20rQMbjLg2CKLTazNJLM37lAWL3kiHH8pmGA/s1318/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;486&quot; data-original-width=&quot;1318&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_hr_fd1iI5JofcM_c_4rRIayzvF5dlQ7T-keHxvHuLRlqmlKhs_Yy-IOuiYrj8GrnscXNB_vN1oXqlTnxvzL0dyl9PraXfuIQ91lQttaY4-e019DmRCxRllEIJj6T3RGn5J-RyTpALh1xgpVugcYQBy20rQMbjLg2CKLTazNJLM37lAWL3kiHH8pmGA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall architecture and training procedure for AVFormer. The architecture consists of a frozen &lt;a href=&quot;https://arxiv.org/pdf/2005.08100.pdf&quot;>;Conformer&lt;/a>; encoder-decoder model, and a frozen &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>; encoder (frozen layers shown in gray with a lock symbol), in conjunction with two lightweight trainable modules - (i) visual projection layer (orange) and bottleneck adapters (blue) to enable multimodal domain adaptation. We propose a two-phase curriculum learning strategy: the adapters (blue) are first trained without any visual tokens, after which the visual projection layer (orange) is tuned while all the other parts are kept frozen.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The plots below show that without curriculum learning, our AV-ASR model is worse than the audio-only baseline across all datasets, with the gap increasing as more visual tokens are added. In contrast, when the proposed two-phase curriculum is applied, our AV-ASR model performs significantly better than the baseline audio-only model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxlm_YiD6Bdma_rFDzaZL4lnvmSamUWD5l6jLB8F4411UV1UL2Pe3Ot3iBdNhJpXhYMsa_2m_3VZ6VnZCWmK4K50h2LfGdMBP-_TqTWBAFKX-Aqq9rdpo9P1n48TV3zPTIBBwfyMEbzHMdTvU5rq1OZVkao5K-Gjl3kk7O9zsAFGk56aaMS2xrOr8T8w/s1374/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1053&quot; data-original-width=&quot;1374&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxlm_YiD6Bdma_rFDzaZL4lnvmSamUWD5l6jLB8F4411UV1UL2Pe3Ot3iBdNhJpXhYMsa_2m_3VZ6VnZCWmK4K50h2LfGdMBP-_TqTWBAFKX-Aqq9rdpo9P1n48TV3zPTIBBwfyMEbzHMdTvU5rq1OZVkao5K-Gjl3kk7O9zsAFGk56aaMS2xrOr8T8w/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effects of curriculum learning. Red and blue lines are for audiovisual models and are shown on 3 datasets in the zero-shot setting (lower &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;WER&lt;/a>; % is better). Using the curriculum helps on all 3 datasets (for How2 (a) and Ego4D (c) it is crucial for outperforming audio-only performance). Performance improves up until 4 visual tokens, at which point it saturates.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results in zero-shot AV-ASR&lt;/h2>; &lt;p>; We compare AVFormer to BEST-RQ, the audio version of our model, and AVATAR, the state of the art in AV-ASR, for zero-shot performance on the three AV-ASR benchmarks: How2, VisSpeech and Ego4D. AVFormer outperforms AVATAR and BEST-RQ on all, even outperforming both AVATAR and BEST-RQ when they are trained on LibriSpeech and the full set of HowTo100M. This is notable because for BEST-RQ, this involves training 600M parameters, while AVFormer only trains 4M parameters and therefore requires only a small fraction of the training dataset (5% of HowTo100M). Moreover, we also evaluate performance on LibriSpeech, which is audio-only, and AVFormer outperforms both baselines. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNT2kWtP5VgTsln7LWDMpxkLacsXHRAaM8rHxnUkL3o3mh5UAt3CFZ02wX-AzllqEw7V5fDYFC5yDe-QVO9oMjFPx6b2Lw9qyCsgXUVQm4JQPqbN52V4D9u9SwaR79aF7vXsGHMzxN4StK0YZe059gxa_pUXRwea44zz8wyIs6drTrxyk8Qa48CGr1g/s3475/AVFormer_results%20(1).png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2129&quot; data-original-width=&quot;3475&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNT2kWtP5VgTsln7LWDMpxkLacsXHRAaM8rHxnUkL3o3mh5UAt3CFZ02wX-AzllqEw7V5fDYFC5yDe-QVO9oMjFPx6b2Lw9qyCsgXUVQm4JQPqbN52V4D9u9SwaR79aF7vXsGHMzxN4StK0YZe059gxa_pUXRwea44zz8wyIs6drTrxyk8Qa48CGr1g/s16000/AVFormer_results%20(1).png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison to state-of-the-art methods for zero-shot performance across different AV-ASR datasets. We also show performances on LibriSpeech which is audio-only. Results are reported as WER % (lower is better). AVATAR and BEST-RQ are finetuned end-to-end (all parameters) on HowTo100M whereas AVFormer works effectively even with 5% of the dataset thanks to the small set of finetuned parameters.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduce AVFormer, a lightweight method for adapting existing, frozen state-of-the-art ASR models for AV-ASR. Our approach is practical and efficient, and achieves impressive zero-shot performance. As ASR models get larger and larger, tuning the entire parameter set of pre-trained models becomes impractical (even more so for different domains). Our method seamlessly allows both domain transfer and visual input mixing in the same, parameter efficient model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Paul Hongsuck Seo, Arsha Nagrani and Cordelia Schmid.&lt;/em>; &lt;/p>; &lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5739375622760902222/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5739375622760902222&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5739375622760902222&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html&quot; rel=&quot;alternate&quot; title=&quot;AVFormer: Injecting vision into frozen speech models for zero-shot AV-ASR&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEpnVot9nisUn-B5GierK8WfqI_mhVwKmJ1mjj_FOdOr2_74gIwvwSJgobi8K_a1uq9n5B3pO4Zi5-mXoE0mzOw32WW-ny6kBThTgHIv3wtQVrpjdMdhMYCSqustzlN7ArP3y2jSqEy_2rlz_zXfsfp05Z3svQ7bu8rYJAN6V1ellHjbL-SJOHraUHEQ/s72-c/AVFormer.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-791000644276225005&lt;/id>;&lt;published>;2023-06-01T10:25:00.000-07:00&lt;/published>;&lt;updated>;2023-06-01T10:25:21.235-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Retrieval-augmented visual-language pre-training&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ziniu Hu, Student Researcher, and Alireza Fathi, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrAmMW5q9tM1RpbnrDdMiw-kXeP1kkCNH0Vj_ZmqxTxoT45GYgBjIKXBZ2eF7_mtQ-ijE7h5g-O69tcbfmEoqifjP6ssE12hsRzIE-fl64ZdaFBL0f0Ruu-Ix3R-dy1jhBibQXoxZCpF0CVsdLWwK1aS-JJGu2wcPdyZkPMeRQFP-AqZq5nYUq0KTOAQ/s320/REVEAL%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Large-scale models, such as &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;, &lt;a href=&quot;https://openai.com/research/language-models-are-few-shot-learners&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;, have demonstrated the ability to store substantial amounts of knowledge when scaled to tens of billions of parameters and trained on large text and image datasets. These models achieve state-of-the-art results on downstream tasks, such as image captioning, visual question answering and open vocabulary recognition. Despite such achievements, these models require a massive volume of data for training and end up with a tremendous number of parameters (billions in many cases), resulting in significant computational requirements. Moreover, the data used to train these models can become outdated, requiring re-training every time the world&#39;s knowledge is updated. For example, a model trained just two years ago might yield outdated information about the current president of the United States. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In the fields of natural language processing (&lt;a href=&quot;https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens&quot;>;RETRO&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html&quot;>;REALM&lt;/a>;) and computer vision (&lt;a href=&quot;https://arxiv.org/abs/2112.08614&quot;>;KAT&lt;/a>;), researchers have attempted to address these challenges using retrieval-augmented models. Typically, these models use a backbone that is able to process a single modality at a time, eg, only text or only images, to encode and retrieve information from a knowledge corpus. However, these retrieval-augmented models are unable to leverage all available modalities in a query and knowledge corpora, and may not find the information that is most helpful for generating the model&#39;s output. &lt;/p>; &lt;p>; To address these issues, in “&lt;a href=&quot;https://arxiv.org/abs/2212.05221&quot;>;REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory&lt;/a>;”, to appear at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we introduce a visual-language model that learns to utilize a multi-source multi-modal “memory” to answer knowledge-intensive queries. REVEAL employs &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;>;neural representation learning&lt;/a>; to encode and convert diverse knowledge sources into a memory structure consisting of key-value pairs. The keys serve as indices for the memory items, while the corresponding values store pertinent information about those items. During training, REVEAL learns the key embeddings, value tokens, and the ability to retrieve information from this memory to address knowledge-intensive queries. This approach allows the model parameters to focus on reasoning about the query, rather than being dedicated to memorization. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWSYZK0R2z8mvSezb9KbBJB8CFtBMQkfTamLXMUNTjOIMQJb8-4-VAeMmboZLusdXtY6MwPgAV_9B4Um23lYa9llZnYa1xLpXKrPmF_wwMPK-Orlw3t5BNJZzpHR8P6R4f7OZK46E5arqJZApeMnjD2OdBWaclvviiKVDB2yQD1YYMs5yiS9ix83NBdA/s1440/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;579&quot; data-original-width=&quot;1440&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWSYZK0R2z8mvSezb9KbBJB8CFtBMQkfTamLXMUNTjOIMQJb8-4-VAeMmboZLusdXtY6MwPgAV_9B4Um23lYa9llZnYa1xLpXKrPmF_wwMPK-Orlw3t5BNJZzpHR8P6R4f7OZK46E5arqJZApeMnjD2OdBWaclvviiKVDB2yQD1YYMs5yiS9ix83NBdA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We augment a visual-language model with the ability to retrieve multiple knowledge entries from a diverse set of knowledge sources, which helps generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Memory construction from multimodal knowledge corpora&lt;/h2>; &lt;p>; Our approach is similar to &lt;a href=&quot;https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html&quot;>;REALM&lt;/a>; in that we precompute key and value embeddings of knowledge items from different sources and index them in a unified knowledge memory, where each knowledge item is encoded into a key-value pair. Each key is a &lt;em>;d&lt;/em>;-dimensional embedding vector, while each value is a sequence of token embeddings representing the knowledge item in more detail. In contrast to previous work, REVEAL leverages a diverse set of multimodal knowledge corpora, including the &lt;a href=&quot;https://research.google/pubs/pub42240/&quot;>;WikiData knowledge graph&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2103.01913&quot;>;Wikipedia passages and images&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2102.08981&quot;>;web image-text pairs&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1612.00837&quot;>;visual question answering data&lt;/a>;. Each knowledge item could be text, an image, a combination of both (eg, pages in Wikipedia) or a relationship or attribute from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_graph&quot;>;knowledge graph&lt;/a>; (eg, Barack Obama is 6&#39; 2” tall). During training, we continuously re-compute the memory key and value embeddings as the model parameters get updated. We update the memory asynchronously at every thousand training steps. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Scaling memory using compression &lt;/h3>; &lt;p>; A naïve solution for encoding a memory value is to keep the whole sequence of tokens for each knowledge item. Then, the model could fuse the input query and the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot;>;top-k&lt;/a>; retrieved memory values by concatenating all their tokens together and feeding them into a &lt;a href=&quot;https://huggingface.co/blog/encoder-decoder&quot;>;transformer encoder-decoder&lt;/a>; pipeline. This approach has two issues: (1) storing hundreds of millions of knowledge items in memory is impractical if each memory value consists of hundreds of tokens and (2) the transformer encoder has a quadratic complexity with respect to the total number of tokens times &lt;em>;k&lt;/em>; for &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;self-attention&lt;/a>;. Therefore, we propose to use the &lt;a href=&quot;https://www.deepmind.com/publications/perceiver-general-perception-with-iterative-attention&quot;>;Perceiver architecture&lt;/a>; to encode and compress knowledge items. The Perceiver model uses a transformer decoder to compress the full token sequence into an arbitrary length. This lets us retrieve top-&lt;em>;k&lt;/em>; memory entries for &lt;em>;k&lt;/em>; as large as a hundred. &lt;/p>; &lt;p>; The following figure illustrates the procedure of constructing the memory key-value pairs. Each knowledge item is processed through a multi-modal visual-language encoder, resulting in a sequence of image and text tokens. The key head then transforms these tokens into a compact embedding vector. The value head (perceiver) condenses these tokens into fewer ones, retaining the pertinent information about the knowledge item within them. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEis_OAb1NNR8mM6vUH-Q0CU56sxDLX0FGfUI6sOGGY2STGzlwFh4Jk3O6nYyrVtc4JsxE9OJQo4EvJOoVK7ZDMiqt5BJ5VDO87CbqnOfcHJXg6czk7rGbgkwHzOApsRVgybqLUr8f8h9Lo1SGdd4eT7zMLgACejH3LHEyT_tcgLKmIdMKLv2_N4gVZRKQ/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;776&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEis_OAb1NNR8mM6vUH-Q0CU56sxDLX0FGfUI6sOGGY2STGzlwFh4Jk3O6nYyrVtc4JsxE9OJQo4EvJOoVK7ZDMiqt5BJ5VDO87CbqnOfcHJXg6czk7rGbgkwHzOApsRVgybqLUr8f8h9Lo1SGdd4eT7zMLgACejH3LHEyT_tcgLKmIdMKLv2_N4gVZRKQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We encode the knowledge entries from different corpora into unified key and value embedding pairs, where the keys are used to index the memory and values contain information about the entries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large-scale pre-training on image-text pairs&lt;/h2>; &lt;p>; To train the REVEAL model, we begin with the large-scale corpus, collected from the public Web with three billion image alt-text caption pairs, introduced in &lt;a href=&quot;https://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html&quot;>;LiT&lt;/a>;. Since the dataset is noisy, we add a filter to remove data points with captions shorter than 50 characters, which yields roughly 1.3 billion image caption pairs. We then take these pairs, combined with the text generation objective used in &lt;a href=&quot;https://ai.googleblog.com/2021/10/simvlm-simple-visual-language-model-pre.html&quot;>;SimVLM&lt;/a>;, to train REVEAL. Given an image-text example, we randomly sample a prefix containing the first few tokens of the text. We feed the text prefix and image to the model as input with the objective of generating the rest of the text as output. The training goal is to condition the prefix and autoregressively generate the remaining text sequence. &lt;/p>; &lt;p>; To train all components of the REVEAL model end-to-end, we need to &lt;a href=&quot;https://arxiv.org/abs/1910.08475&quot;>;warm start&lt;/a>; the model to a good state (setting initial values to model parameters). Otherwise, if we were to start with random weights (cold-start), the retriever would often return irrelevant memory items that would never generate useful training signals. To avoid this cold-start problem, we construct an initial retrieval dataset with pseudo–ground-truth knowledge to give the pre-training a reasonable head start. &lt;/p>; &lt;p>; We create a modified version of the &lt;a href=&quot;https://ai.googleblog.com/2021/09/announcing-wit-wikipedia-based-image.html&quot;>;WIT&lt;/a>; dataset for this purpose. Each image-caption pair in WIT also comes with a corresponding Wikipedia passage (words surrounding the text). We put together the surrounding passage with the query image and use it as the pseudo ground-truth knowledge that corresponds to the input query. The passage provides rich information about the image and caption, which is useful for initializing the model. &lt;/p>; &lt;p>; To prevent the model from relying on low-level image features for retrieval, we apply random data augmentation to the input query image. Given this modified dataset that contains pseudo-retrieval ground-truth, we train the query and memory key embeddings to warm start the model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;REVEAL workflow&lt;/h2>; &lt;p>; The overall workflow of REVEAL consists of four primary steps. First, REVEAL encodes a multimodal input into a sequence of token embeddings along with a condensed query embedding. Then, the model translates each multi-source knowledge entry into unified pairs of key and value embeddings, with the key being utilized for memory indexing and the value encompassing the entire information about the entry. Next, REVEAL retrieves the top-&lt;em>;k&lt;/em>; most related knowledge pieces from multiple knowledge sources, returns the pre-processed value embeddings stored in memory, and re-encodes the values. Finally, REVEAL fuses the top-&lt;em>;k&lt;/em>; knowledge pieces through an attentive knowledge fusion layer by injecting the retrieval score (dot product between query and key embeddings) as a prior during attention calculation. This structure is instrumental in enabling the memory, encoder, retriever and the generator to be concurrently trained in an end-to-end fashion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbkJJ8hgi1MUjDVf-rgpLu4DS60Mg8VZAPFw_XVL_9y3y5UGonUDSBZPygZklrVw6_K1iT7z3N27uaHIObxyWCVnVSj0InGjvHjEOtDUgg0CO79qStbibrvLtQIcY2VTM9qbVoOO0b5Cv70F-gQwuvfOr3CRWVPzqqj9tmn2VhIVV1JPJLCkPTISDaiQ/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1003&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbkJJ8hgi1MUjDVf-rgpLu4DS60Mg8VZAPFw_XVL_9y3y5UGonUDSBZPygZklrVw6_K1iT7z3N27uaHIObxyWCVnVSj0InGjvHjEOtDUgg0CO79qStbibrvLtQIcY2VTM9qbVoOO0b5Cv70F-gQwuvfOr3CRWVPzqqj9tmn2VhIVV1JPJLCkPTISDaiQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall workflow of REVEAL.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate REVEAL on knowledge-based visual question answering tasks using &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; and &lt;a href=&quot;https://allenai.org/project/a-okvqa/home&quot;>;A-OKVQA&lt;/a>; datasets. We fine-tune our pre-trained model on the VQA tasks using the same generative objective where the model takes in an image-question pair as input and generates the text answer as output. We demonstrate that REVEAL achieves better results on the A-OKVQA dataset than earlier attempts that incorporate a fixed knowledge or the works that utilize large language models (eg, GPT-3) as an implicit source of knowledge. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgo8UvZJGoDe0yRpLv6qTqnwiOh3yLHBJO1oc2pZ8qqhsCcNDUkpyf95SVQo-eOU0ryPO7IGXCbr4zWG6rHqtKzz103bV8akDbegrgiVwO7y8u72UPQzfXL3Pdzb1QTTjw9lWNRYF2zrqlW5E6dr5NI3uSaB_0uYeljCErOKukwCfdsyjwAQf_14_qhcQ/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgo8UvZJGoDe0yRpLv6qTqnwiOh3yLHBJO1oc2pZ8qqhsCcNDUkpyf95SVQo-eOU0ryPO7IGXCbr4zWG6rHqtKzz103bV8akDbegrgiVwO7y8u72UPQzfXL3Pdzb1QTTjw9lWNRYF2zrqlW5E6dr5NI3uSaB_0uYeljCErOKukwCfdsyjwAQf_14_qhcQ/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual question answering results on A-OKVQA. REVEAL achieves higher accuracy in comparison to previous works including &lt;a href=&quot;https://arxiv.org/abs/1908.02265&quot;>;ViLBERT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1908.07490&quot;>;LXMERT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2111.09734&quot;>;ClipCap&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>; and &lt;a href=&quot;https://prior.allenai.org/projects/gpv2&quot;>;GPV-2&lt;/a>;. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also evaluate REVEAL on the image captioning benchmarks using &lt;a href=&quot;https://cocodataset.org/#home&quot;>;MSCOCO&lt;/a>; and &lt;a href=&quot;https://nocaps.org/&quot;>;NoCaps&lt;/a>; dataset. We directly fine-tune REVEAL on the MSCOCO training split via the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross_entropy&quot;>;cross-entropy&lt;/a>; generative objective. We measure our performance on the MSCOCO test split and NoCaps evaluation set using the &lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>;CIDEr&lt;/a>; metric, which is based on the idea that good captions should be similar to reference captions in terms of word choice, grammar, meaning, and content. Our results on MSCOCO caption and NoCaps datasets are shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqCkPNUn_sWhogsBQQsYpSA0pTXcNwZriZmok6PNLmlhuYc-mxjsU1nfDthGf9CAncBg1TJ1KzMHndKe2zkiV35lL4NTqXkFuysHH2ulmT69snG0rWqlptujIvCJz3wowb3CRrSUubiIgSQZqy_ixloVE_zMof5kpWz1xcSordWkZU1LZWf8LHmY4BDA/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqCkPNUn_sWhogsBQQsYpSA0pTXcNwZriZmok6PNLmlhuYc-mxjsU1nfDthGf9CAncBg1TJ1KzMHndKe2zkiV35lL4NTqXkFuysHH2ulmT69snG0rWqlptujIvCJz3wowb3CRrSUubiIgSQZqy_ixloVE_zMof5kpWz1xcSordWkZU1LZWf8LHmY4BDA/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Image Captioning results on MSCOCO and NoCaps using the CIDEr metric. REVEAL achieves a higher score in comparison to &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2108.10904&quot;>;SimVLM&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/image-text-pre-training-with.html&quot;>;CoCa&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Below we show a couple of qualitative examples of how REVEAL retrieves relevant documents to answer visual questions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV11nORwmRcQrKjx2Md2T01gN_jus3BbtDlx5DZt4OXZSenEKlpjvtFTYBOC3-XusalvcpQwQEeHy3-UBUtrpYrsEMUyIS6l2SdXcGgJC4IS5KHbts9NC0x6M0a3XENFaRvME3my3yrj_nDO09yNBIuPzJZ4cZXAC4hY65VsFeEKWlBi_YYupmHNfz8Q/s931/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;338&quot; data-original-width=&quot;931&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV11nORwmRcQrKjx2Md2T01gN_jus3BbtDlx5DZt4OXZSenEKlpjvtFTYBOC3-XusalvcpQwQEeHy3-UBUtrpYrsEMUyIS6l2SdXcGgJC4IS5KHbts9NC0x6M0a3XENFaRvME3my3yrj_nDO09yNBIuPzJZ4cZXAC4hY65VsFeEKWlBi_YYupmHNfz8Q/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REVEAL can use knowledge from different sources to correctly answer the question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present an end-to-end retrieval-augmented visual language (REVEAL) model, which contains a knowledge retriever that learns to utilize a diverse set of knowledge sources with different modalities. We train REVEAL on a massive image-text corpus with four diverse knowledge corpora, and achieve state-of-the-art results on knowledge-intensive visual question answering and image caption tasks. In the future we would like to explore the ability of this model for attribution, and apply it to a broader class of multimodal tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross and Alireza Fathi.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/791000644276225005/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/791000644276225005&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/791000644276225005&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot; rel=&quot;alternate&quot; title=&quot;Retrieval-augmented visual-language pre-training&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrAmMW5q9tM1RpbnrDdMiw-kXeP1kkCNH0Vj_ZmqxTxoT45GYgBjIKXBZ2eF7_mtQ-ijE7h5g-O69tcbfmEoqifjP6ssE12hsRzIE-fl64ZdaFBL0f0Ruu-Ix3R-dy1jhBibQXoxZCpF0CVsdLWwK1aS-JJGu2wcPdyZkPMeRQFP-AqZq5nYUq0KTOAQ/s72-c/REVEAL%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8031728070441281505&lt;/id>;&lt;published>;2023-05-31T10:13:00.003-07:00&lt;/published>;&lt;updated>;2023-06-12T15:09:48.808-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Large sequence models for software development activities&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Petros Maniatis and Daniel Tarlow, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8_SbpjaNoBFB_ymmlPa_9YTKL4tOAPfWFHajgnphxoPc3dTXyORaOPk1kN1TP8gVgdjiyPtqGfEYmVCyYIIBdd2ICEIROmNVMCbJmcmT0wLoRWow8djNms5ejc-pfk2Bx_79P7FCnN5PGAQqnXq8YWkn5sX_oLwP0NSoq3IvaV9WyiYhYk1wYK8cTEg/s1200/DIDACT-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Software isn&#39;t created in one dramatic step. It improves bit by bit, one little step at a time — editing, running unit tests, fixing build errors, addressing code reviews, editing some more, appeasing &lt;a href=&quot;https://en.wikipedia.org/wiki/Lint_(software)&quot;>;linters&lt;/a>;, and fixing more errors — until finally it becomes good enough to merge into a code repository. Software engineering isn&#39;t an isolated process, but a dialogue among human developers, code reviewers, bug reporters, software architects and tools, such as compilers, unit tests, linters and static analyzers. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today we describe DIDACT (​​Dynamic Integrated Developer ACTivity), which is a methodology for training large machine learning (ML) models for software development. The novelty of DIDACT is that it uses &lt;em>;the process of software development &lt;/em>;as the source of training data for the model, rather than just &lt;em>;the polished end state &lt;/em>;of that process, the finished code. By exposing the model to the contexts that developers see as they work, paired with the actions they take in response, the model learns about the dynamics of software development and is more aligned with how developers spend their time. We leverage instrumentation of Google&#39;s software development to scale up the quantity and diversity of developer-activity data beyond previous works. Results are extremely promising along two dimensions: usefulness to professional software developers, and as a potential basis for imbuing ML models with general software development skills. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJD9kDvfSCOudyA3cXrYr3eYIKzuqNfndwpoZVHsENIIFUTARGBFoqn_IE627Zk2iGuQg3NFOGcPw2pCA8fBSYv-iBzYNEw4yoOGghZrNvo1AJY1K0o9IvzMCqKjKPEKupP7NL8yQqBs3BUzoizgePEBgZN5vN9Ni7B0a2_eCs4GzFEJYDUR2xF4TIYg/s1475/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;687&quot; data-original-width=&quot;1475&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJD9kDvfSCOudyA3cXrYr3eYIKzuqNfndwpoZVHsENIIFUTARGBFoqn_IE627Zk2iGuQg3NFOGcPw2pCA8fBSYv-iBzYNEw4yoOGghZrNvo1AJY1K0o9IvzMCqKjKPEKupP7NL8yQqBs3BUzoizgePEBgZN5vN9Ni7B0a2_eCs4GzFEJYDUR2xF4TIYg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DIDACT is a multi-task model trained on development activities that include editing, debugging, repair, and code review.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We built and deployed internally three DIDACT tools, &lt;a href=&quot;https://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html&quot;>;Comment Resolution&lt;/a>; (which we recently announced), Build Repair, and Tip Prediction, each integrated at different stages of the development workflow. All three of these tools received enthusiastic feedback from thousands of internal developers. We see this as the ultimate test of usefulness: do professional developers, who are often experts on the code base and who have carefully honed workflows, leverage the tools to improve their productivity? &lt;/p>; &lt;p>; Perhaps most excitingly, we demonstrate how DIDACT is a first step towards a general-purpose developer-assistance agent. We show that the trained model can be used in a variety of surprising ways, via prompting with prefixes of developer activities, and by chaining together multiple predictions to roll out longer activity trajectories. We believe DIDACT paves a promising path towards developing agents that can generally assist across the software development process. &lt;/p>; &lt;br />; &lt;h2>;A treasure trove of data about the software engineering process&lt;/h2>; &lt;p>; Google&#39;s software engineering toolchains store every operation related to code as a log of interactions among tools and developers, and have done so for decades. In principle, one could use this record to replay in detail the key episodes in the “software engineering video” of how Google&#39;s codebase came to be, step-by-step — one code edit, compilation, comment, variable rename, etc., at a time. &lt;/p>; &lt;p>; Google code lives in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Monorepo&quot;>;monorepo&lt;/a>;, a single repository of code for all tools and systems. A software developer typically experiments with code changes in a local &lt;a href=&quot;https://en.wikipedia.org/wiki/Copy-on-write&quot;>;copy-on-write&lt;/a>; workspace managed by a system called &lt;a href=&quot;https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext&quot;>;Clients in the Cloud&lt;/a>; (CitC). When the developer is ready to package a set of code changes together for a specific purpose (eg, fixing a bug), they create a changelist (CL) in &lt;a href=&quot;https://abseil.io/resources/swe-book/html/ch19.html&quot;>;Critique&lt;/a>;, Google&#39;s code-review system. As with other types of code-review systems, the developer engages in a dialog with a peer reviewer about functionality and style. The developer edits their CL to address reviewer comments as the dialog progresses. Eventually, the reviewer declares “LGTM!” (“looks good to me”), and the CL is merged into the code repository. &lt;/p>; &lt;p>; Of course, in addition to a dialog with the code reviewer, the developer also maintains a “dialog” of sorts with a plethora of other software engineering tools, such as the compiler, the testing framework, linters, static analyzers, fuzzers, etc. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhdDAeePURbX0syZUQcLifMLS9uMw-ufpwg8jUXQoQMyHa0UNstR_vA7mqan_fjqzlXLuSlwVZItU-dqZne3Bk4Adsb2DLTi__wX0vmfk_JnEjRC_tmE72e7woRNCsZO6znn3OCQsZLZvZrlU55byBsm3oi5ubHBvDizqvz3N83je01r7XtZ6cuvEZZA/s1877/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1751&quot; data-original-width=&quot;1877&quot; height=&quot;597&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhdDAeePURbX0syZUQcLifMLS9uMw-ufpwg8jUXQoQMyHa0UNstR_vA7mqan_fjqzlXLuSlwVZItU-dqZne3Bk4Adsb2DLTi__wX0vmfk_JnEjRC_tmE72e7woRNCsZO6znn3OCQsZLZvZrlU55byBsm3oi5ubHBvDizqvz3N83je01r7XtZ6cuvEZZA/w640-h597/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the intricate web of activities involved in developing software: small actions by the developer, interactions with a code reviewer, and invocations of tools such as compilers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A multi-task model for software engineering&lt;/h2>; &lt;p>; DIDACT utilizes interactions among engineers and tools to power ML models that assist Google developers, by suggesting or enhancing actions developers take — in context — while pursuing their software-engineering tasks. To do that, we have defined a number of tasks about individual developer activities: repairing a broken build, predicting a code-review comment, addressing a code-review comment, renaming a variable, editing a file, etc. We use a common formalism for each activity: it takes some &lt;em>;State&lt;/em>; (a code file), some &lt;em>;Intent&lt;/em>; (annotations specific to the activity, such as code-review comments or compiler errors), and produces an &lt;em>;Action&lt;/em>; (the operation taken to address the task). This Action is like a mini programming language, and can be extended for newly added activities. It covers things like editing, adding comments, renaming variables, marking up code with errors, etc. We call this language &lt;em>;DevScript&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsd4dFYSNVxcg20eqgJqdr3u9S2hHsBlqOelnix5XZd5zIhrZIemDLaF3CcaMk09Y_9kyDkhAuAq2-STjKDOP1Tb9ShsE91FpNgnLRqOxIzCKrTj2_WaAJUlRxikaLmQK2iv7YfpnQtQeaUeIXNCRE9efYju6KxGBEu4zwDA0vQ6qla6dNvpResnch2w/s1200/DIDACT.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;762&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsd4dFYSNVxcg20eqgJqdr3u9S2hHsBlqOelnix5XZd5zIhrZIemDLaF3CcaMk09Y_9kyDkhAuAq2-STjKDOP1Tb9ShsE91FpNgnLRqOxIzCKrTj2_WaAJUlRxikaLmQK2iv7YfpnQtQeaUeIXNCRE9efYju6KxGBEu4zwDA0vQ6qla6dNvpResnch2w/s16000/DIDACT.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The DIDACT model is prompted with a task, code snippets, and annotations related to that task, and produces development actions, eg, edits or comments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This state-intent-action formalism enables us to capture many different tasks in a general way. What&#39;s more, DevScript is a concise way to express complex actions, without the need to output the whole state (the original code) as it would be after the action takes place; this makes the model more efficient and more interpretable. For example, a rename might touch a file in dozens of places, but a model can predict a single rename action. &lt;/p>; &lt;br />; &lt;h2>;An ML peer programmer&lt;/h2>; &lt;p>; DIDACT does a good job on individual assistive tasks. For example, below we show DIDACT doing code clean-up after functionality is mostly done. It looks at the code along with some final comments by the code reviewer (marked with “human” in the animation), and predicts edits to address those comments (rendered as a &lt;em>;diff&lt;/em>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNMPCHlZwRpo4EKM-HZt7qVII342P6kFgSK18H2sYqftvqmrEC-SZp9FsmgvpZtafgCid3WF905_ooJrA8vEFNs7M4B9Ox5pObqzvCltaAUXouIkiVpLWKRs-VEY0Dhpg11RbH7iJfhdinkNegpK0rOhTPR7rdzleQJzpIErVIZBkPi3WjisexB0Uklw/s897/DIDACT2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;761&quot; data-original-width=&quot;897&quot; height=&quot;543&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNMPCHlZwRpo4EKM-HZt7qVII342P6kFgSK18H2sYqftvqmrEC-SZp9FsmgvpZtafgCid3WF905_ooJrA8vEFNs7M4B9Ox5pObqzvCltaAUXouIkiVpLWKRs-VEY0Dhpg11RbH7iJfhdinkNegpK0rOhTPR7rdzleQJzpIErVIZBkPi3WjisexB0Uklw/w640-h543/DIDACT2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an initial snippet of code and the comments that a code reviewer attached to that snippet, the Pre-Submit Cleanup task of DIDACT produces edits (insertions and deletions of text) that address those comments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The multimodal nature of DIDACT also gives rise to some surprising capabilities, reminiscent of &lt;a href=&quot;https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html&quot;>;behaviors emerging with scale&lt;/a>;. One such capability is &lt;em>;history augmentation&lt;/em>;, which can be enabled via prompting. Knowing what the developer did recently enables the model to make a better guess about what the developer should do next. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgwd-Dvh_esu_26f2rO6DKwMpGyDF0wKHzasGuD3RhXNJiJR5cM613KLSuabNp5hvKV9dF2vzHd7XkUOKD4d7l1cOyV2ovYwxWLECmudUKDk5bmVNOg5eImsqUJbkUkPWk7yOiCogafub7eHb9B3cytjGInIICcMoWFrTlsgqfN3B9IvHFPbWbwfIkZQ/s1044/CitCMonster.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;633&quot; data-original-width=&quot;1044&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgwd-Dvh_esu_26f2rO6DKwMpGyDF0wKHzasGuD3RhXNJiJR5cM613KLSuabNp5hvKV9dF2vzHd7XkUOKD4d7l1cOyV2ovYwxWLECmudUKDk5bmVNOg5eImsqUJbkUkPWk7yOiCogafub7eHb9B3cytjGInIICcMoWFrTlsgqfN3B9IvHFPbWbwfIkZQ/s16000/CitCMonster.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of history-augmented code completion in action.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A powerful such task exemplifying this capability is &lt;em>;history-augmented code completion&lt;/em>;. In the figure below, the developer adds a new function parameter (1), and moves the cursor into the documentation (2). Conditioned on the history of developer edits and the cursor position, the model completes the line (3) by correctly predicting the docstring entry for the new parameter. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_ao8R6Rv9t0sz_O27BPC0Wy3dpbllP101ovPL1yklPN4SqJu8B94EZyhKumBZLsbmuiLdNVlA92wmOgnBQ-nglk2FcNRc4B2J2hFsR3x0Zy2XWcDGylowaxI2hDJH2cAyIzAEcZqCeg8PsLWCNkKzhr-jnMPr4_aGjjZguCogRalV2582jqshece5bA/s1048/DIDACT4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1048&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_ao8R6Rv9t0sz_O27BPC0Wy3dpbllP101ovPL1yklPN4SqJu8B94EZyhKumBZLsbmuiLdNVlA92wmOgnBQ-nglk2FcNRc4B2J2hFsR3x0Zy2XWcDGylowaxI2hDJH2cAyIzAEcZqCeg8PsLWCNkKzhr-jnMPr4_aGjjZguCogRalV2582jqshece5bA/s16000/DIDACT4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of edit prediction, over multiple chained iterations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In an even more powerful history-augmented task, &lt;em>;edit prediction&lt;/em>;, the model can choose where to edit next in a fashion that is historically consistent&lt;strong>;. &lt;/strong>;If the developer deletes a function parameter (1), the model can use history to correctly predict an update to the docstring (2) that removes the deleted parameter (without the human developer manually placing the cursor there) and to update a statement in the function (3) in a syntactically (and — arguably — semantically) correct way. With history, the model can unambiguously decide how to continue the “editing video” correctly. Without history, the model wouldn&#39;t know whether the missing function parameter is intentional (because the developer is in the process of a longer edit to remove it) or accidental (in which case the model should re-add it to fix the problem). &lt;/p>; &lt;p>; The model can go even further. For example, we started with a blank file and asked the model to successively predict what edits would come next until it had written a full code file. The astonishing part is that the model developed code in a step-by-step way that would seem natural&lt;strong>; &lt;/strong>;to a developer: It started by first creating a fully working skeleton with imports, flags, and a basic main function. It then incrementally added new functionality, like reading from a file and writing results, and added functionality to filter out some lines based on a user-provided regular expression, which required changes across the file, like adding new flags. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; DIDACT turns Google&#39;s software development process into training demonstrations for ML developer assistants, and uses those demonstrations to train models that construct code in a step-by-step fashion, interactively with tools and code reviewers. These innovations are already powering tools enjoyed by Google developers every day. The DIDACT approach complements the great strides taken by large language models at Google and elsewhere, towards technologies that ease toil, improve productivity, and enhance the quality of work of software engineers. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a multi-year collaboration among Google Research, Google Core Systems and Experiences, and DeepMind. We would like to acknowledge our colleagues Jacob Austin, Pascal Lamblin, Pierre-Antoine Manzagol, and Daniel Zheng, who join us as the key drivers of this project. This work could not have happened without the significant and sustained contributions of our partners at Alphabet (Peter Choy, Henryk Michalewski, Subhodeep Moitra, Malgorzata Salawa, Vaibhav Tulsyan, and Manushree Vijayvergiya), as well as the many people who collected data, identified tasks, built products, strategized, evangelized, and helped us execute on the many facets of this agenda (Ankur Agarwal, Paige Bailey, Marc Brockschmidt, Rodrigo Damazio Bovendorp, Satish Chandra, Savinee Dancs,&amp;nbsp;&lt;/em>;&lt;i>;Denis Davydenko,&amp;nbsp;&lt;/i>;&lt;em>;Matt Frazier, Alexander Frömmgen, Nimesh Ghelani, Chris Gorgolewski, Chenjie Gu, Vincent Hellendoorn, Franjo Ivančić, Marko Ivanković, Emily Johnston, Luka Kalinovcic, Lera Kharatyan, Jessica Ko, Markus Kusano, Kathy Nix, Christian Perez, Sara Qu, Marc Rasi, Marcus Revaj, Ballie Sandhu, Michael Sloan, Tom Small, Gabriela Surita, Maxim Tabachnyk, David Tattersall, Sara Toth, Kevin Villela, Sara Wiltberger, and Donald Duo Zhao) and our extremely supportive leadership (Martín Abadi, Joelle Barral, Jeff Dean, Madhura Dudhgaonkar, Douglas Eck, Zoubin Ghahramani, Hugo Larochelle, Chandu Thekkath, and Niranjan Tulpule). Thank you!&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8031728070441281505/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/large-sequence-models-for-software.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031728070441281505&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031728070441281505&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/large-sequence-models-for-software.html&quot; rel=&quot;alternate&quot; title=&quot;Large sequence models for software development activities&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8_SbpjaNoBFB_ymmlPa_9YTKL4tOAPfWFHajgnphxoPc3dTXyORaOPk1kN1TP8gVgdjiyPtqGfEYmVCyYIIBdd2ICEIROmNVMCbJmcmT0wLoRWow8djNms5ejc-pfk2Bx_79P7FCnN5PGAQqnXq8YWkn5sX_oLwP0NSoq3IvaV9WyiYhYk1wYK8cTEg/s72-c/DIDACT-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;