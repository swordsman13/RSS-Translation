<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-12-15T12:51:04.281-08:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Health"></category><category term="Robotics"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="AI for Social Good"></category><category term="Security and Privacy"></category><category term="Computer Science"></category><category term="HCI"></category><category term="MOOC"></category><category term="Quantum AI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="accessibility"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Audio"></category><category term="Android"></category><category term="EMNLP"></category><category term="ICML"></category><category term="Physics"></category><category term="TPU"></category><category term="Awards"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Search"></category><category term="Structured Data"></category><category term="video"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Responsible AI"></category><category term="TTS"></category><category term="User Experience"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="Google Accelerated Science"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="Video Analysis"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Diversity"></category><category term="Interspeech"></category><category term="RAI-HCT Highlights"></category><category term="Systems"></category><category term="UI"></category><category term="Vision Research"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Cloud Platform"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Differential Privacy"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Climate"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="Large Language Models"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="Weather"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Generative AI"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="Graphs"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1319&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-6479608460479432217&lt;/id>;&lt;发布>;2023-12-15T11:40:00.000-08:00&lt;/发布>;&lt;更新>;2023-12- 15T12:18:41.742-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;NeurIPS&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;风格转移&quot; >;&lt;/category>;&lt;title type=&quot;text&quot;>;StyleDrop：任意样式的文本到图像生成&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Kihyuk Sohn 发布和 Dilip Krishnan，Google 研究院研究科学家&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bqg9UcqVUFC -uKMbsFvxRmRWkokKR4VinfuZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9to_P8/s1600/StyleDrop%20hero.gif&quot;样式=“显示：无；” />; &lt;p>; 在大量图像-文本对上训练的文本到图像模型使得能够创建涵盖许多流派和主题的丰富多样的图像。此外，诸如“动漫”或“蒸汽朋克”之类的流行风格，当添加到输入文本提示时，可以转换为特定的视觉输出。虽然我们在&lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;即时工程&lt;/a>;方面付出了很多努力，但由于配色方案、照明和其他特征的细微差别。例如，“水彩画”可能指多种风格，并且使用简单地说“水彩画风格”的文本提示可能会导致一种特定风格或几种不可预测的混合。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjNi6hwWTqFdzKvJ9qxXf5ppyfhixq1qERPyXZqBmdrPVP4ObR94N1pVVIJAGZseKbsLtOcf 6qm2M0LgRtNKqXN -7qjC7Isz1keA8Pm4_ADYuzywpXLb3h7W4H5p5X3f7Y_A21uLNQWceazq-Ex6YOLCDzacl0Umk-EhqZvMUz0aZHG9nvxxb52gw-zKz/s959/image5.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img 边框=&quot;0&quot; 数据原始高度= “403”数据原始宽度=“959”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjNi6hwWTqFdzKvJ9qxXf5ppyfhixq1qERPyXZqBmdrPVP4ObR94N1pVVIJAGZseKbsLtOcf6qm2M0LgRtNK qXN-7qjC7Isz1keA8Pm4_ADYuzywpXLb3h7W4H5p5X3f7Y_A21uLNQWceazq-Ex6YOLCDzacl0Umk-EhqZvMUz0aZHG9nvxxb52gw-zKz/s16000/image5.gif&quot;/>;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;当我们提到“水彩绘画风格”时，我们指的是什么？ StyleDrop 不是用自然语言指定样式，而是通过引用样式参考图像来生成样式一致的图像&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt; span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在此博客中我们引入了“&lt;a href=&quot;https://arxiv.org/abs/2306.00983&quot;>;StyleDrop：任何样式的文本到图像生成&lt;/a>;”，这是一个允许显着更高水平的风格化文本的工具 -到图像合成。 StyleDrop 不寻找文本提示来描述样式，而是使用一个或多个样式参考图像来描述文本到图像生成的样式。通过这样做，StyleDrop 能够以与参考一致的风格生成图像，同时有效地规避文本提示工程的负担。这是通过对一些样式的&lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;>;适配器调整&lt;/a>;有效地微调预训练的文本到图像生成模型来完成的参考图像。此外，通过对其生成的一组图像迭代地微调 StyleDrop，它实现了从文本提示生成风格一致的图像。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;方法概述&lt;/h2>; &lt;p>; StyleDrop是一种文本到图像的生成模型，允许生成视觉风格与用户提供的风格参考图像一致的图像。这是通过对预先训练的文本到图像生成模型进行几次参数高效微调的迭代来实现的。具体来说，我们在 &lt;a href=&quot;https://muse-model.github.io/&quot;>;Muse&lt;/a>; 上构建 StyleDrop，这是一个文本到图像生成视觉转换器。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Muse：文本到图像生成视觉转换器&lt;/h3>; &lt;p>; &lt;a href=&quot; https://muse-model.github.io/&quot;>;Muse&lt;/a>; 是一种基于蒙版生成图像转换器的最先进的文本到图像生成模型（&lt;a href=&quot;https: //openaccess.thecvf.com/content/CVPR2022/papers/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf&quot;>;MaskGIT&lt;/a>;）。与扩散模型不同，例如 &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>; 或 &lt;a href=&quot;https://github.com/CompVis/stable-diffusion&quot;>;Stable Diffusion&lt;/a>;，Muse 将图像表示为一系列离散标记，并使用变压器架构对其分布进行建模。与扩散模型相比，Muse 速度更快，同时实现了具有竞争力的发电质量。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;参数高效的适配器调整&lt;/h3>; &lt;p>; StyleDrop 是通过微调预调整来构建的在一些风格参考图像及其相应的文本提示上训练 Muse 模型。关于变压器参数高效微调的工作有很多，包括&lt;a href=&quot;https://arxiv.org/abs/2104.08691&quot;>;提示调整&lt;/a>;和&lt;a href=&quot;https:// arxiv.org/abs/2106.09685&quot;>;大型语言模型的低阶适应&lt;/a>;（LoRA）。其中，我们选择适配器调整，它被证明可以有效地针对&lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;>;语言&lt;/a>;微调大型变压器网络，并且以参数高效的方式执行&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf&quot;>;图像生成&lt;/a>;任务。例如，它引入了不到一百万个可训练参数来微调 3B 参数的 Muse 模型，并且只需要 1000 个训练步骤即可收敛。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6 so7Abf52d3Kyu-77Di6YvRncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s1632/image2.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1632&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6so7Abf52d3Kyu-77Di6Yv RncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Muse 的参数高效适配器调整。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height :40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;带反馈的迭代训练&lt;/h3>; &lt;p>; 虽然 StyleDrop 在从一些风格参考图像中学习风格方面很有效，但从单个风格参考图像中学习仍然具有挑战性风格参考图像。这是因为模型可能无法有效地分解&lt;em>;内容&lt;/em>;（即图像中的内容）和&lt;em>;风格&lt;/em>;（即图像的呈现方式），从而导致减少生成中的&lt;em>;文本可控性&lt;/em>;。例如，如下文步骤 1 和 2 所示，根据单个风格参考图像训练的 StyleDrop 生成的吉娃娃图像显示了风格参考图像中内容（即房屋）的泄漏。此外，生成的寺庙图像看起来与参考图像中的房屋太相似（概念崩溃）。 &lt;/p>; &lt;p>; 我们通过在由用户或图像文本对齐模型选择的合成图像子集上训练新的 StyleDrop 模型来解决这个问题（例如，&lt;a href=&quot;https://arxiv.org /abs/2103.00020&quot;>;CLIP&lt;/a>;)，其图像是由在单个图像上训练的第一轮 StyleDrop 模型生成的。通过对多个合成的图文对齐图像进行训练，该模型可以轻松地将风格与内容分开，从而实现改进的图文对齐。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw4 16CwaV4R3GvkMSa3CVubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s1295/image3 .gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;825&quot; data-original-width=&quot;1295&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw416CwaV4R3GvkMSa3CV ubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s16000/image3.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;带反馈的迭代训练&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>;。第一轮StyleDrop可能会由于内容风格解缠的困难而导致文本可控性降低，例如内容泄漏或概念崩溃。使用由前几轮 StyleDrop 模型生成并由人类或图像文本对齐模型选择的合成图像进行迭代训练，提高了风格化文本到图像生成的文本依从性。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;StyleDrop gallery&lt;/h3>; &lt;p>; 我们通过在 24 个不同风格的参考图像上运行实验来展示 StyleDrop 的有效性。如下所示，StyleDrop 生成的图像在风格上彼此高度一致，并且与风格参考图像高度一致，同时描绘了各种上下文，例如小企鹅、香蕉、钢琴等。此外，该模型可以使用以下方式渲染字母图像一致的风格。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2 ldPyRTLwUFp8tDyQgt3EQTl8g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/s1632/image6.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1632&quot; data-original-width=&quot;1536&quot; height=&quot;640&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2ldPyRTLwUFp8tDyQgt3EQTl8 g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/w602-h640/image6.gif&quot; width=&quot;602&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;风格化的文本到图像生成。样式参考图片&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt; /sup>; 位于黄色框内的左侧。使用的文本提示为：&lt;br>;第一行：小企鹅、香蕉、长凳。&lt;br>;第二行：蝴蝶、F1 赛车、圣诞树。&lt;br>;第三行：咖啡机、咖啡机帽子，驼鹿。&lt;br>;第四行：机器人，毛巾，木屋。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing =&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt; a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpAyPPdjBT2pS6Bgq2CAppCGBU-sw4QmUp DrN-1lrlpmtxCBRIN3AQN07IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s1526/image1.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;767&quot; data-original-width=&quot;1526&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpayPPdjBT2pS6Bgq2CAppCGBU-sw4QmUpDrN-1lrlpmtxCBRIN3AQN07 IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;风格化的视觉角色生成。样式参考图片&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt; /sup>; 位于黄色框内的左侧。使用的文本提示为：（第一行）字母“A”、字母“B”、字母“C”，（第二行）字母“E”、字母“F”、字母“G”。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;以我的风格生成对象的图像&lt;/h3>; &lt;p>; 下面我们通过从两个个性化生成分布中采样来显示生成的图像，一个用于对象，另一个用于样式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lr​​XSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurR p8-rmsqoLkj-oXWqi4xe4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s1006/image8.gif “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;412&quot; data-original-width=&quot;1006&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lr​​XSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurRp8-rmsqoLkj-oXWqi4xe 4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s16000/image8.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;蓝色边框顶部的图像是来自 DreamBooth 数据集的对象参考图像（茶壶、花瓶、狗和猫），左侧的图像红色边框底部是样式参考图像*。紫色边框中的图像（即右下四张图像）是根据特定对象的样式图像生成的。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40 %;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;定量结果&lt;/h3>; &lt;p>; 为了进行定量评估，我们从 &lt;a href=&quot;https://github.com/google- Research/parti/blob/main/PartiPrompts.tsv&quot;>;Parti 提示&lt;/a>;并测量&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;图像到图像 CLIP 分数&lt;/a >; 用于风格一致性，&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;图像到文本 CLIP 分数&lt;/a>;用于文本一致性。我们研究 Muse 和 Imagen 的非微调模型。在微调模型中，我们与 &lt;a href=&quot;https://imagen.research.google/&quot; 上的 &lt;a href=&quot;https://dreambooth.github.io/&quot;>;DreamBooth&lt;/a>; 进行了比较>;Imagen&lt;/a>;，最先进的个性化主题文本到图像方法。我们展示了 StyleDrop 的两个版本，一个是根据单个风格参考图像进行训练的，另一个是“StyleDrop (HF)”，它是使用如上所述的人类反馈的合成图像进行迭代训练的。如下所示，与未微调的对应项（0.694 vs. 0.556）以及 Imagen 上的 DreamBooth（0.694 vs. 0.644）相比，StyleDrop (HF) 的风格一致性得分显着提高。我们观察到，与 StyleDrop 相比，StyleDrop (HF) 的文本一致性得分有所提高（0.322 vs. 0.313）。此外，在 Imagen 上的 DreamBooth 和 Muse 上的 StyleDrop 之间的人类偏好研究中，我们发现，就风格参考图像的一致性而言，86% 的人类评分者更喜欢 Muse 上的 StyleDrop，而不是 Imagen 上的 DreamBooth。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiID8cTIDI32_6aRRoVhwxHDBJnPrnk_etBx904nBw0kFBDD6z99ttJqzz2574I-irFJ4_5dg0xHPqHPdpgeFi68Tl 1gu4pciiChyphenhyphenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCICKNpmhiJNvYDvI4ux5/s2980 /image16.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;第2980章phenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCIcKNpmhiJNvYDvI4ux5/s16000/image16.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height :40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; StyleDrop 使用一些样式参考图像在文本到图像生成中实现了样式一致性。 Google 的人工智能原则指导了我们 Style Drop 的开发，我们敦促负责任地使用该技术。 StyleDrop 经过改编，可&lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/image/fine-tune-style&quot;>;在 Vertex AI 中创建自定义样式模型&lt;/a>; ，我们相信它对于艺术总监和平面设计师（他们可能想要以自己的风格集思广益或制作视觉资产原型，以提高他们的生产力和创造力）或想要生成新媒体资产的企业来说可能是一个有用的工具体现一个特定的品牌。与其他生成式人工智能功能一样，我们建议从业者确保其与所使用的任何媒体资产的版权保持一致。更多结果请访问我们的&lt;a href=&quot;https://styledrop.github.io/&quot;>;项目网站&lt;/a>;和&lt;a href=&quot;https://youtu.be/gNHD0_hkJ9A&quot;>;YouTube 视频&lt;/一个>;。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Kihyuk Sohn、Nataniel Ruiz 进行， Kimin Lee、Daniel Castro Chin、Irina Blok、Huiwen Chang、Jarred Barber、Lu Jiang、Glenn Entis、Yuanzhen Li、Yuanhao、Irfan Essa、Michael Rubinstein 和 Dilip Krishnan。 &lt;/em>;我们感谢实验中使用的图像的所有者（&lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/main/images/assets/data.md&quot;>;链接&lt; /a>; 归属）分享他们的宝贵资产。 &lt;/p>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt; super>;&lt;a name=&quot;fn1&quot;>;&lt;b>;*&lt;/b>;&lt;/a>;&lt;/sup>;请参阅&lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/ main/images/assets/data.md&quot;>;图片来源&lt;/a>;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt; /p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6479608460479432217/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://blog.research.google/2023/12/styledrop-text-to-image- Generation-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// blog.research.google/2023/12/styledrop-text-to-image- Generation-in.html&quot; rel=&quot;alternate&quot; title=&quot;StyleDrop：任何样式的文本到图像生成&quot; type=&quot;text/html &quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bqg9UcqV UFC-UKMbsFvxRmRWkokKR4VinfuZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9tO_P8/s72- c/StyleDrop%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8223613466421639113&lt;/id>;&lt;已发布>;2023-12-10T06:11:00.000-08:00&lt;/已发布>;&lt;更新>;2023-12-13T14:11:22.885-08:00&lt;/更新>;&lt;title type=&quot;text&quot;>;Google 在 NeurIPS 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-作者&quot;>;发布者：Google 项目经理 Catherine Armato&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7 vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s1100/google_research_sticker-hero.jpg “样式=“显示：无；” />; &lt;p>; 本周，第 37 届年度&lt;a href=&quot;https://neurips.cc/Conferences/2023&quot;>;神经信息处理系统会议&lt;/a>; (NeurIPS 2023) 召开，这是全球最大的机器学习会议今年，在洛杉矶新奥尔良拉开帷幕。 Google 很荣幸成为今年 NeurIPS 的&lt;a href=&quot;https://neurips.cc/Conferences/2023/Sponsors&quot;>;钻石级赞助商&lt;/a>;，并且将在超过 170 篇被接受的论文中占据一席之地，两次主题演讲，并通过组织支持和参与超过 20 个研讨会和教程为更广泛的研究界做出额外贡献。 Google 还很荣幸成为&lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66602&quot;>;机器学习女性&lt;/a>;和&lt;a href=&quot;的白金赞助商https://nips.cc/virtual/2023/affinity-workshop/66607&quot;>;人工智能中的拉丁语&lt;/a>;研讨会。我们期待分享我们的一些广泛的机器学习研究，并扩大我们与更广泛的机器学习研究社区的合作伙伴关系。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 亲自参加 NeurIPS 2023？欢迎参观 Google 研究展位，详细了解我们为解决该领域一些最有趣的挑战而所做的令人兴奋的工作。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) 帐户，了解 Google 展位活动（例如演示和问答环节）。 &lt;/p>; &lt;p>; 您可以在下面的列表中详细了解我们在会议上展示的最新前沿工作（Google 附属机构以&lt;strong>;粗体&lt;/strong>;突出显示）。请参阅 &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023&quot;>;Google DeepMind 博客&lt;/a>;，详细了解他们参加 NeurIPS 2023 的信息。&lt;/p >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;板和线组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; NeurIPS 董事会：&lt;strong>;Corinna Cortes&lt;/strong>;&lt;br />; 顾问委员会：&lt;strong>;John C. Platt&lt;/strong>; strong>;&lt;br />; 高级领域主席：&lt;strong>;Inderjit S. Dhillon&lt;/strong>;&lt;br />; 创意人工智能主席：&lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; 项目主席：&lt;strong>;Amir Globerson &lt;/strong>;&lt;br />; 数据集和基准主席：&lt;b>; Remi Denton&lt;/b>;&lt;br />;博览会主席：&lt;b>;叶文明&lt;/b>;&lt;/p>; &lt;/div>; &lt;div style= &quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展位演示/问答时间表&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;h4>;该时间表可能会发生变化。请访问 Google 展位 (#215) 了解更多信息。&lt;/h4>; &lt;p>; 所见即所读？改进文本-图像对齐评估&lt;br />; 演讲者：&lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; 12 月 11 日星期一 | &lt;em>;12:15PM - 1:45PM&lt;/em>; &lt;/p>; &lt;p>; 像图一样说话：大型语言模型的图编码&lt;br />; 演讲者：&lt;strong>;Bahar Fatemi&lt;/strong>;、&lt;strong >; Jonathan Halcrow&lt;/strong>;、&lt;strong>; Bryan Perozzi&lt;/strong>;&lt;br />; 12 月 11 日星期一 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; VisIT-Bench：受实际使用启发的视觉语言教学基准&lt;br />; 演讲者：&lt;strong>;Yonatan Bitton &lt;/strong>;&lt;br />; 12 月 11 日星期一 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; MLCommons Croissant&lt;br />; 演讲者：&lt;strong>;Omar Benjelloun&lt;/strong>;、&lt;strong>; Meg Risdal&lt;/strong>;、&lt; strong>;Lora Aroyo&lt;/strong>;&lt;br />; 12 月 12 日，星期二 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; DaTaSeg：驯服通用多数据集多任务分割模型&lt;br />; 演讲者：&lt;strong>;顾秀野&lt;/strong>;&lt; br />; 12 月 12 日星期二 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; 嵌入大图&lt;br />; 演讲者：&lt;strong>;Bryan Perozzi&lt;/strong>;、&lt;strong>;Anton Tsitsulin&lt;/strong>;&lt; br />; 12 月 12 日星期二 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; 相关噪声被证明可以击败独立噪声，实现差异化的私人学习&lt;br />; 演讲者：&lt;strong>;Krishna Pillutla&lt;/strong>;&lt;br />; 12 月 12 日星期二 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; Med-PaLM&lt;br />; 演讲者：&lt;strong>;涂涛&lt;/strong>;&lt;br />; 12 月 12 日，星期二 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; StyleDrop：任意样式的文本到图像生成&lt;br />; 演讲者：&lt;strong>;Kihyuk Sohn&lt;/strong>;、&lt;strong >;Lu Jiang&lt;/strong>;、&lt;strong>;Irfan Essa&lt;/strong>;&lt;br />; 12 月 12 日，星期二 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; DICES 数据集：对话式 AI 安全评估的多样性&lt;br />; 演讲者：&lt;strong>;Lora Aroyo&lt;/strong>;、&lt;strong>; Alicia Parrish&lt;/strong>;、&lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; 谐振器：基于游戏的可扩展大型模型评估&lt;br />; 演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong >;Michal Todorovic&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Adversarial Nibbler&lt;br />; 演讲者：&lt;strong>;Lora Aroyo&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; 迈向通才生物医学人工智能&lt;br />; 演讲者：&lt;strong>;涂涛&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; 条件适配器&lt;br />; 演讲者：&lt;strong>;Junwen Bai&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; 通过多模式 RAG 进行患者援助&lt;br />; 演讲者：&lt;strong>;Ryan Knuffman&lt;/strong>;、&lt;strong>;Milica Cvetkovic&lt;/strong >;&lt;br />; 12 月 13 日星期三 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;p>; Hessian 结构如何解释锐度正则化的奥秘&lt;br />; 演讲者：&lt;strong>;Hossein Mobahi&lt;/strong>;&lt;br />; 星期三， 12 月 13 日 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;主题演讲嘉宾&lt; /h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73993&quot;>;负责任的人工智能的多面性&lt; /a>;&lt;br />; 演讲者：&lt;strong>;Lora Aroyo&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73987&quot;>;草图：核心工具、学习增强和自适应鲁棒性&lt;/a>;&lt;br />; 演讲者：&lt;strong>;Jelani Nelson&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%; &quot;>; &lt;br />; &lt;/div>; &lt;h2>;亲和力研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/ 2023/affinity-workshop/66602&quot;>;机器学习领域的女性&lt;/a>;&lt;br />; &lt;strong>;Google 赞助 - 白金级&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc /virtual/2023/affinity-workshop/66607&quot;>;AI 中的拉丁语&lt;/a>;&lt;br />; &lt;strong>;Google 赞助 - 白金级&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// nips.cc/virtual/2023/affinity-workshop/66605&quot;>;机器学习新功能&lt;/a>;&lt;br />; 组织者：&lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot; line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https:// nips.cc/virtual/2023/workshop/66541&quot;>;人工智能加速材料设计&lt;/a>; (AI4Mat-2023)&lt;br />; 炉边聊天：&lt;strong>;Gowoon Cheon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66524&quot;>;联想记忆和联想记忆Hopfield Networks 2023 年&lt;/a>;&lt;br />; 小组成员：&lt;strong>;Blaise Agüera y Arcas&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop /66535&quot;>;认知系统中的信息理论原理&lt;/a>; (InfoCog)&lt;br />;演讲者：&lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips .cc/virtual/2023/workshop/66518&quot;>;机器学习和物理科学&lt;/a>;&lt;br />;演讲者：&lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://nips.cc/virtual/2023/workshop/66494&quot;>;UniReps：统一神经模型中的表示&lt;/a>;&lt;br />;组织者：&lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://nips.cc/virtual/2023/workshop/66517&quot;>;基础模型中零/少样本学习的鲁棒性&lt;/a>; (R0-FoMo)&lt;br />; 演讲者：&lt;strong>; Partha Talukdar&lt;/strong>;&lt;br />; 组织者：&lt;strong>;Ananth Balashankar&lt;/strong>;、&lt;strong>;姚勤&lt;/strong>;、&lt;strong>;Ahmad Beirami&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66539&quot;>;扩散模型研讨会&lt;/a>;&lt;br />;演讲者：&lt;strong>;Tali Dekel&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66502&quot;>;时间视角下的算法公平性&lt;/a>;&lt;br />;圆桌会议负责人：&lt;strong>;Stephen Pfohl&lt;/strong>;&lt; br />; 组织者：&lt;strong>;Golnoosh Farnadi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66550&quot;>;深度学习中的后门：好处、坏人和丑人&lt;/a>;&lt;br />;组织者：&lt;strong>;Eugene Bagdasaryan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/ Workshop/66549&quot;>;OPT 2023：机器学习优化&lt;/a>;&lt;br />; 组织者：&lt;strong>;Cristóbal Guzmán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc /virtual/2023/workshop/66545&quot;>;机器学习促进创造力和设计&lt;/a>;&lt;br />;演讲者：&lt;strong>;Aleksander Holynski&lt;/strong>;、&lt;strong>;Alexander Mordvintsev&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66511&quot;>;机器人学习研讨会：大规模模型的预训练、微调和泛化&lt;/a>;&lt;br />;演讲者： &lt;strong>;Matt Barnes&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66516&quot;>;音频机器学习&lt;/a>;&lt;br />;组织者：&lt;strong>;Shrikanth Narayanan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66531&quot;>;基础模型时代的联邦学习&lt;/ a>; (FL@FM-NeurIPS&#39;23)&lt;br />; 演讲嘉宾：&lt;strong>;谢卓瑞&lt;/strong>;、&lt;strong>;徐峥&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://nips.cc/virtual/2023/workshop/66526&quot;>;社会责任语言建模研究&lt;/a>; (SoLaR)&lt;br />;小组成员：&lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66506&quot;>;我不敢相信这不是更好（ICBINB）：基础模型时代的失败模式&lt;/a>;&lt;br / >; 顾问委员会：&lt;strong>;Javier Antorán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66501&quot;>;系统机器学习&lt;/a>; &lt;br />; 主办方：&lt;strong>;王亚文&lt;/strong>;&lt;br />; 竞赛组委会：&lt;strong>;Bryan Perozzi&lt;/strong>;、&lt;strong>;Sami Abu-el-haija&lt;/strong>;&lt;br />; 指导委员会：&lt;strong>;Milad Hashemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66514&quot;>;自我监督学习：理论与实践&lt;/ a>;&lt;br />; 组织者：&lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66496&quot;>;将模型行为归因于比例&lt;/a>; (ATTRIB 2023)&lt;br />; 组织者：Kevin Guu*、Tolga Bolukbasi* &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;竞赛&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/competition/66581&quot;>;NeurIPS 2023机器遗忘竞赛&lt;/a>;&lt;br />; 组织者：&lt;b>;Isabelle Guyon&lt;/b>;、&lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips. cc/virtual/2023/competition/66593&quot;>;Lux AI 挑战赛第二季 NeurIPS 版&lt;/a>;&lt;br />; 组织者：&lt;strong>;Bovard Doerschuk-Tiberi&lt;/strong>;、&lt;strong>;Addison Howard&lt;/strong>; &lt; /p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/tutorial/73949&quot;>;以数据为中心的人工智能，打造可靠且负责任的人工智能：从理论到实践&lt;/a>;&lt;br />; &lt;strong>; Isabelle Guyon&lt;/strong>;、Nabeel Seedat、Mihaela va der Schaar &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;创意 AI 赛道&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 创意人工智能表演 1 和 20px 2&lt;br />; 演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; 组织者：&lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; &lt;em>;&lt; a href=&quot;https://nips.cc/virtual/2023/session/74093&quot;>;性能 1&lt;/a>;：12 月 11 日星期一 |下午 6:30 - 晚上 8:30，大堂舞台&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74094&quot;>;表演 2&lt;/a>;： 12 月 14 日，星期四 | 7:00PM - 9:00PM，大堂舞台&lt;/em>; &lt;/p>; &lt;p>; 创意人工智能会议 1 – 3&lt;br />; 演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong>;Yonatan Bitton&lt; /strong>;&lt;br />; 组织者：&lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74090&quot;>;会议 1&lt; /a>;：12 月 12 日，星期二 |下午 3:05 - 3:40，D2 厅&lt;/em>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74091&quot;>;会议 2&lt;/a>;： 12 月 13 日，星期三 |上午 10:45 - 下午 2:15，D2 厅&lt;/em>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74092&quot;>;第 3 场会议&lt;/a>;： 12 月 14 日，星期四 |上午 10:45 - 下午 2:15，D2 厅&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/session/75889&quot;>;创意 AI 视频&lt;/a >;&lt;br />; 主办方：&lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;博览会讲座&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78252&quot;>;图学习会面人工智能&lt;/a>;&lt;br />;演讲者：&lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel /78243&quot;>;共鸣器：音乐空间&lt;/a>;&lt;br />;演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong>;Michal Todorovic&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://nips.cc/Expo/Conferences/2023/talk%20panel/78237&quot;>;ML 中的经验严谨性是一项大规模可并行化的挑战&lt;/a>;&lt;br />; 演讲者：&lt;strong>;Megan Risdal (Kaggle)&lt;/ strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;口头演讲&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sW8yGZ4uVJ&quot;>;基于有序条件的策略梯度方法全局收敛&lt;/a>;&lt;br />; 梅金成，博Dai，&lt;strong>;Alekh Agarwal&lt;/strong>;，&lt;strong>; &lt;/strong>;Mohammad Ghavamzadeh*，Csaba Szepesvari，Dale Schuurmans &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum? id=y8UAQQHVTX&quot;>;私人永恒预测&lt;/a>;&lt;br />; Moni Naor、Kobbi Nissim、&lt;strong>;Uri Stemmer&lt;/strong>;、Chao Yan &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=PITeSdYQkv&quot;>;用户级差异隐私，每个用户的示例很少&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;、&lt;strong>;Pritish Kamath&lt;/strong>;、&lt; strong>;Ravi Kumar&lt;/strong>;、&lt;strong>;Pasin Manurangsi&lt;/strong>;、Raghu Meka、&lt;strong>;张驰远&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /forum?id=dVaWCDMBof&quot;>;DataComp：寻找下一代多模态数据集&lt;/a>;&lt;br />; Samir Yitzhak Gadre、Gabriel Ilharco、Alex Fang、Jonathan Hayase、Georgios Smyrnis、Thao Nguyen、Ryan Marten、Mitchell Wortsman、Dhruba Ghosh、张洁宇、Eyal Orgad、Rahim Entezari、Giannis Daras、Sarah Pratt、Vivek Ramanujan、Yonatan Bitton、Kalyani Marathe、Stephen Mussmann、Richard Vencu、Mehdi Cherti、Ranjay Krishna、Pang Wei Koh&lt;/strong >;、Olga Saukh、Alexander Ratner、Shuran Song、Hannaneh Hajishirzi、Ali Farhadi、Romain Beaumont、Sewoong Oh、Alex Dimakis、Jenia Jitsev、Yair Carmon、Vaishaal Shankar、Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/forum?id=w116w62fxH&quot;>;可实现回归的最佳学习者：PAC 学习和在线学习&lt;/a>;&lt;br />; Idan Attias、Steve Hanneke、Alkis Kalavasis、&lt;strong>;Amin Karbasi&lt;/strong >;, Grigoris Velegkas &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jDIlzSU8wJ&quot;>;扩散模型对于光流和单目深度估计的惊人有效性&lt;/a>;&lt;br />; Saurabh Saxena、&lt;strong>;Charles Herrmann&lt;/strong>;、&lt;strong>;Junhwa Hur&lt;/strong>;、&lt;strong>;Abhishek Kar&lt;/strong>;、&lt;strong>; &lt;/strong>;Mohammad Norouzi*、&lt;strong>;德清太阳&lt;/strong>;，&lt;strong>; &lt;/strong>;大卫·J·弗利特 &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;期刊轨道&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://jmlr.org/papers/v24/20-998.html&quot;>;使用图进行图聚类神经网络&lt;/a>;&lt;br />; &lt;strong>;Anton Tsitsulin&lt;/strong>;、&lt;strong>;John Palowitch&lt;/strong>;、&lt;strong>;Bryan Perozzi&lt;/strong>;、Emmanuel Müller &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;聚焦论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href =&quot;https://openreview.net/forum?id=1p6teT6F73&quot;>;高效变压器的交替更新&lt;/a>;（请参阅博客文章）&lt;br />; &lt;strong>;Cenk Baykal&lt;/strong>;、&lt;strong>;Dylan Cutler &lt;/strong>;、&lt;strong>;Nishanth Dikkala&lt;/strong>;、Nikhil Ghosh*、&lt;strong>;Rina Panigrahy&lt;/strong>;、&lt;strong>;王鑫&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/forum?id=EldbUlZtbd&quot;>;本地化是否有助于编辑？语言模型中基于因果关系的本地化与知识编辑的惊人差异&lt;/a>;&lt;br />; &lt;strong>;Peter Hase&lt;/strong>;、Mohit Bansal、&lt;strong>;Been Kim&lt;/strong>;、&lt;strong>; Asma Ghandeharioun &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jR2FkqW6GB&quot;>;在游戏中学习对学习者有好处吗？&lt;/a>;&lt;br />; William Brown ,&lt;strong>; Jon Schneider&lt;/strong>;,&lt;strong>; Kiran Vodrahalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Bj1QSgiBPP&quot;>;参与式个性化分类&lt;/a>;&lt;br />; Hailey Joren、&lt;strong>;Chirag Nagpal&lt;/strong>;、&lt;strong>;Katherine Heller&lt;/strong>;、&lt;strong>; &lt;/strong>;Berk Ustun &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=c2eedxSlPJ&quot;>;可分离数据上梯度下降的严格风险界限&lt;/a>;&lt;br />; Matan Schliserman，&lt;strong>;Tomer Koren&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=67o9UQgTD0&quot;>;神经语言模型中的反事实记忆&lt;/a>;&lt;br />; &lt;strong>;张驰远&lt;/strong>;,&lt;strong >; &lt;/strong>;Daphne Ippolito、Katherine Lee、Matthew Jagielski、Florian Tramèr、Nicholas Carlini &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5NxJuc0T1P&quot;>;Debias Coarsely，示例有条件地：通过最优输运和概率扩散模型进行统计降尺度&lt;/a>;&lt;br />; &lt;strong>;万忠一&lt;/strong>;、Ricardo Baptista、&lt;strong>; Anudhyan Boral&lt;/strong>;、&lt;strong>;陈一凡&lt;/strong>;,&lt;strong>;约翰·安德森&lt;/strong>;,&lt;strong>;沙飞&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;莱昂纳多·泽佩达-努涅斯&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=vTug54Uunq&quot;>;通用优化方法的更快利润最大化&lt;/a>;&lt;br />;Guanghui Wang、Zihao Hu、Vidya Muthukumar、&lt;strong>;Jacob Abernethy &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3PjCt4kmRx&quot;>;从像素到 UI 操作：学习通过图形用户界面遵循说明&lt;/a>;&lt; br />; Peter Shaw、Mandar Joshi、&lt;strong>;James Cohan&lt;/strong>;、&lt;strong>; &lt;/strong>;Jonathan Berant、Panupong Pasupat、胡鹤翔、Urvashi Khandelwal、Kenton Lee、Kristina N Toutanova &lt;/p>; &lt;p >; &lt;a href=&quot;https://openreview.net/forum?id=5Gw9YkJkFF&quot;>;PAC 根据标签比例学习线性阈值&lt;/a>;&lt;br />; &lt;strong>;Anand Brahmbhatt&lt;/strong>;、&lt;strong>; Rishi Saket&lt;/strong>;,&lt;strong>; Aravindan Raghuveer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CXPUg86A1D&quot;>;SPAE：用于多模式生成的语义金字塔自动编码器与冻结法学硕士&lt;/a>;&lt;br />;于丽君*、&lt;strong>;程勇&lt;/strong>;、&lt;strong>; &lt;/strong>;王志若、&lt;strong>;Vivek Kumar&lt;/strong>;、&lt;strong>; Wolfgang Macherey &lt;/strong>;、&lt;strong>;黄艳萍&lt;/strong>;、&lt;strong>;大卫·罗斯&lt;/strong>;、&lt;strong>;伊尔凡·埃萨&lt;/strong>;、&lt;strong>; &lt;/strong>;Yonatan Bisk、&lt;strong>;明-宣扬&lt;/strong>;、&lt;strong>;凯文·墨菲&lt;/strong>;、&lt;strong>; &lt;/strong>;亚历山大·豪普特曼、&lt;strong>;陆江&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=QatZNssk7T&quot;>;平衡对抗模型中的自适应数据分析&lt;/a>;&lt;br />; Kobbi Nissim，&lt;strong>; Uri Stemmer&lt;/strong>;，&lt;strong>; &lt;/strong>;Eliad Tsfadia &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NiQTy0NW1L&quot;>;Lexinvariant 语言模型&lt;/a>;&lt;br/>;Qian Huang、Eric Zelikman、Sarah Chen、&lt;strong >; Yuhuai Wu&lt;/strong>;、Gregory Valiant、Percy Liang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=HF6bnhfSqH&quot;>;论量子反向传播、信息重用和作弊测量折叠&lt;/a>;&lt;br />; &lt;strong>;阿米拉·阿巴斯&lt;/strong>;、&lt;strong>; &lt;/strong>;罗比·金、黄心源、&lt;strong>;威廉·J·哈金斯&lt;/strong>;、&lt;strong>;雷米斯莫瓦萨格、&lt;strong>;达尔吉尔博亚&lt;/strong>;、&lt;strong>;&lt;/strong>;&lt;strong>;贾罗德·麦克林&lt;/strong>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/forum?id=Pya0kCEpDk&quot;>;随机块模型和混合模型的私有估计算法&lt;/a>;&lt;br />;陈宏杰，&lt;strong>; Vincent Cohen-Addad&lt;/strong>;，&lt;strong>; &lt;/strong>;Tommaso d&#39;Orsi、&lt;strong>; Alessandro Epasto&lt;/strong>;、Jacob Imola、David Steurer、Stefan Tiegel &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DBz9E5aZey&quot;>;可证明通过虚拟粒子随机逼近的 SVGD 快速有限粒子变体&lt;/a>;&lt;br />; &lt;strong>;Aniket Das&lt;/strong>;、&lt;strong>;Dheeraj Nagaraj&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/forum?id=e0pRF9tOtm&quot;>;重新审视私人（随机）非凸优化：二阶平稳点和过度风险&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/strong>; ,&lt;strong>; &lt;/strong>;刘道高*、Sewoong Oh、Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=bKqrWLCMrX&quot;>;揭示视频分布变化下的自监督学习&lt;/a>;&lt;br />; Pritam Sarkar，&lt;strong>;Ahmad Beirami&lt;/strong>;，&lt;strong>;Ali Etemad&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/forum?id=ikkdTD3hQJ&quot;>;AIMS：无所不包的多层次细分&lt;/a>;&lt;br />; Lu Qi, Jason Kuen, Weidong Gui, Jiuxiang Gu, Zhe Lin, Bo杜宇旭、&lt;strong>;杨明轩&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rheCTpRrxI&quot;>;DreamHuman：来自文本的可动画 3D 头像&lt;/a>;&lt;br />; &lt;strong>;尼科斯·科洛图罗斯&lt;/strong>;、&lt;strong>;蒂莫·阿尔迪克&lt;/strong>;、&lt;strong>;安德烈·赞菲尔&lt;/strong>;、&lt;strong>;爱德华·加布里埃尔·巴扎万&lt;/strong>;、&lt; strong>;Mihai Fieraru&lt;/strong>;、&lt;strong>;Cristian Sminchisescu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=oaCDiKoJ2w&quot;>;后续行动也很重要: 通过任职后情境改善情境强盗&lt;/a>;&lt;br />;王超奇、叶子宇、&lt;strong>;冯哲&lt;/strong>;、&lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;、徐海峰&lt;/p>; &lt; p>; &lt;a href=&quot;https://openreview.net/forum?id=m21rQusNgb&quot;>;学习用于排名的列表级域不变表示&lt;/a>;&lt;br />; 西安睿成*，&lt;strong>;庄红雷&lt; /strong>;、&lt;strong>;秦珍&lt;/strong>;、Hamed Zamani*、&lt;strong>;路静&lt;/strong>;、&lt;strong>;吉马&lt;/strong>;、&lt;strong>;凯辉&lt;/strong>;、韩昭, &lt;strong>;Xuanhui Wang&lt;/strong>;, &lt;strong>;Michael Bendersky&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=hCdqDkA25J&quot;>;最优保证凸优化中的算法再现性和梯度复杂性&lt;/a>;&lt;br />; 张亮，杨俊驰，&lt;strong>;Amin Karbasi&lt;/strong>;，鸟何&lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=hJzEoQHfCe&quot;>;统一嵌入：网络规模机器学习系统经过实战检验的特征表示&lt;/a>;&lt;br />; Benjamin Coleman、Wang-Cheng Kang、&lt;strong>;Matthew Fahrbach&lt;/strong>; ,&lt;strong>; &lt;/strong>;Ruoxi Wang、Lichan Hong、Ed Chi、Derek Cheng &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xOJUmwwlJc&quot;>;接近通知校准深度神经网络&lt;/a>;&lt;br />; 熊苗、邓爱琳、&lt;strong>;Pang Wei Koh&lt;/strong>;、Jiaying Wu、Shen Li、Jianqing Xu、Bryan Hooi &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https ://openreview.net/forum?id=WfsWy59bX2&quot;>;通过相似聚类进行匿名学习：模型泛化的精确分析&lt;/a>;&lt;br />; &lt;strong>;Adel Javanmard&lt;/strong>;,&lt;strong>; Vahab Mirrokni&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EUiIbwV379&quot;>;通过更好的私有特征选择实现更好的私有线性回归&lt;/a>;&lt;br />; &lt;特拉维斯·迪克，詹妮弗·吉伦沃特*，马修·约瑟夫&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=XAyPlfmWpu&quot;>;二值化神经机器翻译&lt;/a>;&lt;br />; 张一驰、Ankush Garg、曹元、&lt;strong>;Łukasz Lew&lt;/strong>;、Behrooz Ghorbani*、张志如、Orhan Firat &lt;/p>; &lt;p>; &lt;a href= &quot;https://nips.cc/virtual/2023/poster/73665&quot;>;BoardgameQA：具有矛盾信息的自然语言推理数据集&lt;/a>;&lt;br />; &lt;strong>;Mehran Kazemi&lt;/strong>;，&lt;strong>;袁泉&lt;/strong>;、&lt;strong>;Deepti Bhatia&lt;/strong>;、&lt;strong>;Najoung Kim&lt;/strong>;、&lt;strong>;徐鑫&lt;/strong>;、&lt;strong>;Vaiva Imbrasaite&lt;/strong>;、&lt;strong>; Deepak Ramachandran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1vvsIJtnnr&quot;>;通过缓和的指数测量进行提升&lt;/a>;&lt;br />; &lt;strong>;Richard诺克，&lt;strong>; &lt;/strong>;埃桑·阿米德，&lt;strong>;曼弗雷德·沃穆斯&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SGlrCuwdsB &quot;>;（基于分数的）文本控制生成模型的概念代数&lt;/a>;&lt;br />; Zihao Wang，Lin Gui，Jeffrey Negrea，&lt;strong>;Victor Veitch&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q8mH2d6uw2&quot;>;通过不连续网络进行深度合约设计&lt;/a>;&lt;br />; Tonghan Wang、&lt;strong>;Paul Dütting&lt;/strong>;、Dmitry Ivanov、Inbal Talgam -Cohen，David C. Parkes &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=YoghyvSG0H&quot;>;Diffusion-SS3D：半监督 3D 物体检测的扩散模型&lt;/a >;&lt;br />; 何正如、戴振轩、林彦雨、&lt;strong>;杨明轩&lt;/strong>;、&lt;strong>;蔡艺轩&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PYASzxr2OP&quot;>;通过比较反馈获取用户偏好以进行个性化多目标决策&lt;/a>;&lt;br />; Han Shao、Lee Cohen、Avrim Blum、 &lt;strong>;Yshay Mansour&lt;/strong>;、Aadirupa Saha、Matthew Walter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qCglMj6A4z&quot;>;线性相关噪声的梯度下降：理论差异隐私及其应用&lt;/a>;&lt;br />; Anastasia Koloskova*、&lt;strong>;Ryan McKenna&lt;/strong>;、&lt;strong>;Zachary Charles&lt;/strong>;、&lt;strong>;J Keith Rush&lt;/strong>;、&lt;strong >;Hugh Brendan McMahan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCwToX315b&quot;>;Entrywise 变换矩阵乘积的低秩近似的难度&lt;/a>;&lt; br />; &lt;strong>;Tamas Sarlos&lt;/strong>;、&lt;strong>; &lt;/strong>;宋星友、David P. Woodruff、张秋仪 &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/forum?id=JhQP33aMx2&quot;>;多模态基础模型的逐模块自适应蒸馏&lt;/a>;&lt;br />;&lt;br />; 陈亮，&lt;strong>;于家辉&lt;/strong>;，&lt;strong>;明轩杨&lt;/strong>;、&lt;strong>;Matthew Brown&lt;/strong>;、崔寅、赵拓、&lt;strong>;宫伯清&lt;/strong>;、周天一&lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=zGRWp7yRqd&quot;>;多重交换 k-Means++&lt;/a>;&lt;br />; Lorenzo Beretta、&lt;strong>;Vincent Cohen-Addad&lt;/strong>;、&lt;strong>;Silvio Lattanzi&lt;/strong>;、 &lt;strong>; Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8vuDHCxrmy&quot;>;OpenMask3D：开放词汇 3D 实例分割&lt;/a>;&lt;br />; Ayça Takmaz、Elisabetta Fedele、Robert Sumner、Marc Pollefeys、&lt;strong>;Federico Tombari&lt;/strong>;、&lt;strong>;Francis Engelmann&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/forum?id=7RMGI4slcb&quot;>;多语言学习中数据集不平衡的情况下的顺序问题&lt;/a>;&lt;br />; Dami Choi*, &lt;strong>;Derrick Xin&lt;/strong>;, &lt;strong>;Hamid Dadkhahi&lt;/a>;&lt;br />;强>;、Justin Gilmer、Ankush Garg、Orhan Firat、Chih-Kuan Yeh、Andrew M. Dai、Behrooz Ghorbani &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=yEf8NSqTPu&quot; >;PopSign ASL v1.0：通过智能手机收集的独立美国手语数据集&lt;/a>;&lt;br />; Thad Starner、Sean Forbes、Matthew So、David Martin、Rohit Sridhar、Gururaj Deshpande、&lt;strong>;Sam Sepah&lt;/strong >;、Sahir Shahryar、Khushi Bhardwaj、Tyler Kwok、Daksh Sehgal、Saad Hassan、Bill Neubauer、Sofia Vempala、Alec Tan、Jocelyn Heath、Unnathi Kumar、Priyanka Mosur、Tavenner Hall、Rajandeep Singh、Christopher Cui、&lt;strong>;Glenn Cameron&lt; /strong>;、&lt;strong>;索希尔·戴恩&lt;/strong>;、&lt;strong>;加勒特·坦泽&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gaktiSjatl&quot;>;半隐式去噪扩散模型（SIDDMs）&lt;/a>;&lt;br />; 徐彦武*、龚明明、谢少安、&lt;strong>;魏伟&lt;/strong>;、&lt;strong>; Matthias Grundmann&lt;/strong>;、&lt;strong>; &lt;/strong>;Kayhan Batmanghelich，&lt;strong>;侯廷波&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xGz0wAIJrS&quot;>;State2Explanation：基于概念的解释有益于代理学习和用户理解&lt;/a>;&lt;br />; Devleena Das、Sonia Chernova、&lt;strong>;Been Kim&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum ?id=AwhpBEqmyo&quot;>;StoryBench：连续故事可视化的多方面基准&lt;/a>;&lt;br />; Emanuele Bugliarello*、Hernan Moraldo、Ruben Villegas、Mohammad Babaeizadeh、Mohammad Taghi Saffar、Han Zhang、Dumitru Erhan、&lt;strong>; Vittorio Ferrari&lt;/strong>;、Pieter-Jan Kindermans、&lt;strong>;Paul Voigtlaender&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=wv3bHyQbX7&quot;>;主题驱动通过学徒学习实现文本到图像生成&lt;/a>;&lt;br />;陈文虎、胡鹤翔、&lt;strong>;李彦东&lt;/strong>;、&lt;strong>;Nataniel Ruiz&lt;/strong>;、&lt;strong>;贾旭辉&lt;/strong>; strong>;、Ming-Wei Chang、William W. Cohen &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=plAix1NxhU&quot;>;TpuGraphs：大张量计算图上的性能预测数据集&lt;/a>;&lt;br />; &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;、&lt;strong>;Sami Abu-El-Haija&lt;/strong>;、Kaidi Cao*、&lt;strong>;Bahare Fatemi&lt;/strong>;、&lt;strong>; Mike Burrows&lt;/strong>;、Charith Mendis*、&lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=a147pIS2Co&quot;>;培训链-通过潜在变量推理进行思想&lt;/a>;&lt;br />; &lt;strong>;Du Phan&lt;/strong>;、&lt;strong>;Matthew D. Hoffman&lt;/strong>;、David Dohan*、Sholto Douglas、&lt;strong>; Tuan Anh Le&lt;/strong>;、Aaron Parisi、&lt;strong>;Pavel Sountsov&lt;/strong>;、Charles Sutton、Sharad Vikram、&lt;strong>; Rif A. Saurous&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=1ZzG6td0el&quot;>;信息约束下交互式高维估计的统一下界&lt;/a>;&lt;br />; Jayadev Acharya, Clement L. Canonne, &lt;strong>;孙子腾&lt;/strong>; , Himanshu Tyagi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=j5AoleAIru&quot;>;所见即所读？改进文本-图像对齐评估&lt;/a>;&lt;br />; &lt;strong>;Michal Yarom&lt;/strong>;、&lt;strong>;Yonatan Bitton&lt;/strong>;、&lt;strong>;Soravit Changpinyo&lt;/strong>;、&lt;strong>;Roee Aharoni&lt; /strong>;、&lt;strong>;乔纳森·赫齐格&lt;/strong>;、&lt;strong>;奥兰·朗&lt;/strong>;、&lt;strong>;&lt;/strong>;&lt;strong>;埃兰·奥菲克&lt;/strong>;、&lt;strong>;伊丹·斯佩克托&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=4KZhZJSPYU&quot;>;什么时候基于置信度的级联延迟就足够了？&lt;/a>;&lt;br />; &lt;strong>;Wittawat Jitkrittum&lt; /strong>;、&lt;strong>;内哈·古普塔&lt;/strong>;、&lt;strong>;阿迪亚·克里希纳·梅农&lt;/strong>;、&lt;strong>;Harikrishna Narasimhan&lt;/strong>;、&lt;strong>;安基特·辛格·拉瓦特&lt;/strong>;、&lt;strong>;桑吉夫Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=A18PgVSUgf&quot;>;通过知识蒸馏加速分子图神经网络&lt;/a>;&lt;br />; Filip Ekström Kelvinius、Dimitar Georgiev、Artur Petrov Toshev、&lt;strong>;Johannes Gasteiger&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7EMphtUgCI&quot;>;AVIS：自主视觉信息使用大语言模型智能体进行搜索&lt;/a>;&lt;br />;胡紫牛*、&lt;strong>;Ahmet Iscen&lt;/strong>;、&lt;strong>;孙晨&lt;/strong>;、张凯伟、孙一舟、&lt;strong>;David罗斯&lt;/strong>;、&lt;strong>;科迪莉亚·施密德&lt;/strong>;、&lt;strong>;阿里雷扎·法蒂&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=9mJXDcr17V &quot;>;超越不变性：解决“虚假”相关性的测试时标签转移适应&lt;/a>;&lt;br />; Qingyao Sun、Kevin Patrick Murphy、&lt;strong>;Sayna Ebrahimi&lt;/strong>;、Alexander D&#39;Amour &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0tEjORCGFD&quot;>;协作分数蒸馏以实现一致的视觉编辑&lt;/a>;&lt;br />; Subin Kim、Kyungmin Lee、June Suk Choi、Jongheon Jeong、&lt;strong>;Kihyuk Sohn&lt;/strong>;、Jinwoo Shin&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1SF2tiopYJ&quot;>;CommonScenes：使用 Scene 生成常识性 3D 室内场景图表&lt;/a>;&lt;br />;Guangyao Zhai、Evin Pınar Örnek、Shun-Cheng Wu、Yan Di、&lt;strong>;Federico Tombari&lt;/strong>;、Nassir Navab、Benjamin Busam &lt;/p>; &lt;p>; &lt;a href= &quot;https://openreview.net/forum?id=65aDEXIhih&quot;>;学习神经网络的计算复杂性：平滑性和简并性&lt;/a>;&lt;br />; &lt;strong>;Amit Daniely&lt;/strong>;、Nathan Srebro、Gal Vardi &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BopG5dhH7L&quot;>;一种计算高效的稀疏在线牛顿法&lt;/a>;&lt;br />; Fnu Devvrit*, Sai Surya Duvvuri,&lt; &lt;/strong>;Rohan Anil、&lt;strong>;Vineet Gupta&lt;/strong>;、&lt;strong>;谢卓瑞&lt;/strong>;、&lt;strong>;Inderjit S Dhillon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6EDHfVHicP&quot;>;DDF-HO：基于条件定向距离场的手持物体重建&lt;/a>;&lt;br />; 张晨阳光，严迪，张瑞达，光耀翟，&lt;strong>;Fabian Manhardt&lt;/strong>;，&lt;strong>;Federico Tombari&lt;/strong>;，纪向阳&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=2nTpPxJ5Bs&quot; >;双向强盗反馈的双重拍卖&lt;/a>;&lt;br />; &lt;strong>;Soumya Basu&lt;/strong>;，Abiishek Sankararaman &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs /2305.19234&quot;>;使用大型语言模型生成特定领域语言的语法提示&lt;/a>;&lt;br />;Bailin Wang, Zi Wang, Xuzhi Wang, &lt;strong>;Yuan Cao&lt;/strong>;, Rif A. Saurous, Yoon Kim &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5SIz31OGFV&quot;>;深度神经网络训练的不一致、不稳定和泛化差距&lt;/a>;&lt;br />; Rie Johnson，张桐* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3YDukx2cpr&quot;>;通过图段训练进行大图属性预测&lt;/a>;&lt;br />;曹凯迪*, &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;、&lt;strong>;Sami Abu-El-Haija&lt;/strong>;、&lt;strong>;Dustin Zelle&lt;/strong>;、&lt;strong>;周艳琪&lt;/strong>;、&lt;strong>;&lt;/ strong>;Charith Mendis*、Jure Leskovec、&lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1GxKVprbwM&quot;>;关于计算成对统计量本地差异隐私&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;、&lt;strong>;Pritish Kamath&lt;/strong>;、&lt;strong>;Ravi Kumar&lt;/strong>;、&lt;strong>;Pasin Manurangsi&lt;/strong>; , &lt;strong>;Adam Sealfon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7UdVPRmpif&quot;>;关于蒸馏中的师生偏差：不服从是否值得？ &lt;/a>;&lt;br />; &lt;strong>;Vaishnavh Nagarajan&lt;/strong>;、&lt;strong>;Aditya Krishna Menon&lt;/strong>;、&lt;strong>;Srinadh Bhojanapalli&lt;/strong>;、&lt;strong>;Hossein Mobahi&lt;/strong>;、 &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CzkOzKWpMa&quot;>;具有未知上下文分布的上下文强盗的最佳交叉学习&lt;/a >;&lt;br />; &lt;strong>;乔恩·施奈德&lt;/strong>;、&lt;strong>;朱利安·齐默特&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8XRMbNAP6Z&quot; >;滑动窗口模型中的近最优 k 聚类&lt;/a>;&lt;br />; David Woodruff、&lt;strong>;钟培林&lt;/strong>;、Samson Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/forum?id=3H37XciUEv&quot;>;语言模型的事后解释可以改进语言模型&lt;/a>;&lt;br />; Satyapriya Krishna、Jiaqi Ma、Dylan Z Slack、&lt;strong>;Asma Ghandeharioun&lt;/strong>;、 Sameer Singh，Himabindu Lakkaraju &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BJ0fQUU32w&quot;>;具有生成检索的推荐系统&lt;/a>;&lt;br />; Shashank Rajput*&lt;strong >;、&lt;/strong>;Nikhil Mehta、Anima Singh、&lt;strong>;Raghunandan Hulikal Keshavan&lt;/strong>;、&lt;strong>;Trung Vu、Lukasz Heldt&lt;/strong>;、&lt;strong>; &lt;/strong>;Lichan Hong、Yi Tay&lt;strong>; >;、Vinh Q. Tran&lt;/strong>;、&lt;strong>;Jonah Samost&lt;/strong>;、Maciej Kula、Ed H. Chi、Maheswaran Sathiamoorthy &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /forum?id=8OTPepXzeh&quot;>;用于微调文本到图像扩散模型的强化学习&lt;/a>;&lt;br />; &lt;strong>;范颖&lt;/strong>;、Olivia Watkins、杜雨晴、刘浩、&lt;strong >;Moonkyung Ryu&lt;/strong>;、&lt;strong>;Craig Boutilier&lt;/strong>;、Pieter Abbeel、Mohammad Ghavamzadeh*、Kangwook Lee、Kimin Lee* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /forum?id=5VQFAvUHcd&quot;>;可复制集群&lt;/a>;&lt;br />; &lt;strong>;Hossein Esfandiari&lt;/strong>;、&lt;strong>;Amin Karbasi&lt;/strong>;、&lt;strong>;Vahab Mirrokni&lt;/strong>;、Grigoris Velegkas , Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5cPz5hrjy6&quot;>;强化学习的可复制性&lt;/a>;&lt;br />; &lt;strong>;Amin Karbasi&lt;/strong >;、Grigoris Velegkas、林杨、Felix Zhou&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=szFqlNRxeS&quot;>;黎曼投影免费在线学习&lt;/a>;&lt;br / >; Zihao Hu、Guanghui Wang、&lt;strong>;Jacob Abernethy&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=29WbraPk8U&quot;>;锐度感知最小化导致低-排名特征&lt;/a>;&lt;br />; Maksym Andriushchenko、&lt;strong>;Dara Bahri&lt;/strong>;、&lt;strong>;Hossein Mobahi&lt;/strong>;、Nicolas Flammarion &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=2hQ7MBQApp&quot;>;什么是平坦度正则化的归纳偏差？深度矩阵分解模型研究&lt;/a>;&lt;br />;Khashayar Gatmiry、李志远、庄庆耀、&lt;strong>;Sashank Reddi&lt;/strong>;、马腾宇、Stefanie Jegelka&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=JzQlGqBm8d&quot;>;具有随机优化共享基础的块低阶预处理器&lt;/a>;&lt;br />; Jui-Nan Yen、Sai Surya Duvvuri、&lt;strong>;Inderjit S Dhillon&lt;/strong>;、&lt;strong>;Cho-Jui Hsieh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Ntd6X7uWYF&quot;>;阻止合作匪徒：在线具有每项预算约束的协作过滤&lt;/a>;&lt;br />; &lt;strong>;Soumyabrata Pal&lt;/strong>;、&lt;strong>;Arun Sai Suggala&lt;/strong>;、&lt;strong>;Karthikeyan Shanmugam&lt;/strong>;、&lt;strong>; Prateek Jain&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NIrTSCiIZ7&quot;>;具有扩散模型的边界引导无学习语义控制&lt;/a>;&lt;br / >; 叶朱、吴宇、&lt;strong>;邓志伟&lt;/strong>;、Olga Russakovsky、严岩&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=IyYyKov0Aj&quot;>;有条件适配器：具有快速推理的参数高效迁移学习&lt;/a>;&lt;br />; 雷涛、&lt;strong>;Junwen Bai&lt;/strong>;、Siddhartha Brahma、&lt;strong>; Joshua Ainslie&lt;/strong>;、Kenton Lee、Yanqi Zhou、杜楠*、&lt;strong>;赵云&lt;/strong>;、&lt;strong>;吴跃新&lt;/strong>;、李波、&lt;strong>;张宇&lt;/strong>;、张明伟&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=KTRwpWCMsC&quot;>;利用现代 Hopfield 网络对时间序列进行共形预测&lt;/a>;&lt;br />; Andreas Auer、&lt;strong>;Martin Gauch&lt;/strong>;、 Daniel Klotz、Sepp Hochreiter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PzYAMXmIT3&quot;>;视觉预训练有助于端到端推理吗？&lt;/a>;&lt;br / >; &lt;strong>;孙晨&lt;/strong>;、罗志祥、&lt;strong>;周兴一&lt;/strong>;、&lt;strong>;Anurag Arnab&lt;/strong>;、&lt;strong>;Cordelia Schmid&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PAYXfIUKWY&quot;>;针对不同训练数据的模型的自然分布变化的有效鲁棒性&lt;/a>;&lt;br />;周兴石*，&lt;strong>;Nicholas Carlini&lt; /strong>;、&lt;strong>;Ananth Balashankar&lt;/strong>;、Ludwig Schmidt、&lt;strong>;谢卓瑞&lt;/strong>;、Alex Beutel*、&lt;strong>;姚勤&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/forum?id=Nh5dp6Uuvx&quot;>;使用人类相似性判断改进神经网络表示&lt;/a>;&lt;br />; Lukas Muttenthaler*、Lorenz Linhardt、Jonas Dippel、Robert A. Vandereulen、 Katherine Hermann、Andrew K. Lampinen、Simon Kornblith &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=N6FhEMnxCU&quot;>;标记鲁棒且差分私有线性回归：计算和统计效率&lt; /a>;&lt;br />; 刘希阳、&lt;strong>;Prateek Jain&lt;/strong>;、&lt;strong>;孔伟豪&lt;/strong>;、&lt;strong>;Sewoong Oh&lt;/strong>;、&lt;strong>;Arun Sai Suggala&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Fdfyga5i0A&quot;>;Mnemosyne：学习用 Transformer 训练 Transformer&lt;/a>;&lt;br />; Deepali Jain、Krzysztof Choromanski、&lt; strong>;Avinava Dubey&lt;/strong>;、Sumeet Singh、Vikas Sindhwani、Tingnan 张、Jie Tan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=MCkUS1P3Sh&quot;>;纳什后悔保证线性强盗&lt;/a>;&lt;br />; Ayush Sawarni，&lt;strong>;Soumyabrata Pal&lt;/strong>;，Siddharth Barman &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2307.03043&quot; >;倒角距离的近线性时间算法&lt;/a>;&lt;br />; Ainesh Bakshi、Piotr Indyk、&lt;strong>;Rajesh Jayaram&lt;/strong>;、Sandeep Silwal、Erik Waingarten。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=FviF8vuz5B&quot;>;关于高斯分布和乘积分布的差分隐私采样&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt; /strong>;, 小胡*, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id =LelK6Mfoey&quot;>;马尔可夫决策过程中静态风险度量的动态规划分解&lt;/a>;&lt;br />; Jia Lin Hau, Erick Delage, Mohammad Ghavamzadeh*, Marek Petrik &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/forum?id=HFQFAyNucq&quot;>;ResMem：学你能学的，记住其余的&lt;/a>;&lt;br />; Zitong Yang、&lt;strong>;Michal Lukasik&lt;/strong>;、&lt;strong>; Vaishnavh Nagarajan &lt;/strong>;、&lt;strong>;李宗林&lt;/strong>;、&lt;strong>;Ankit Singh Rawat&lt;/strong>;、&lt;strong>;Manzil Zaheer、&lt;/strong>;&lt;strong>;Aditya Krishna Menon&lt;/strong>;、&lt;strong>; Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PcNpL9Q39p&quot;>;负责任的人工智能 (RAI) 游戏和集成&lt;/a>;&lt;br />; Yash Gupta、Runtian Zhai、&lt;strong>;Arun Suggala&lt;/strong>;、Pradeep Ravikumar &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DVlawv2rSI&quot;>;RoboCLIP：一个演示就足够了学习机器人策略&lt;/a>;&lt;br />; Sumedh A Sontakke、Jesse Zhang、&lt;strong>;Sébastien MR Arnold&lt;/strong>;、Karl Pertsch、Erdem Biyik、Dorsa Sadigh、Chelsea Finn、Laurent Itti &lt;/p>; &lt;p >; &lt;a href=&quot;https://openreview.net/forum?id=I6aOjhpcNQ&quot;>;通过核化率失真最大化实现稳健的概念擦除&lt;/a>;&lt;br />; Somnath Basu Roy Chowdhury、Nicholas Monath、&lt;strong>;Kumar Avinava Dubey&lt;/strong>;、&lt;strong>;Amr Ahmed&lt;/strong>;、Sniigdha Chaturvedi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=FmZVRe0gn8&quot;>;稳健的多代理基于对抗性正则化的强化学习：理论基础和稳定算法&lt;/a>;&lt;br />; Alexander Bukharin, Yan Li, Yue Yu, Qingru Zhu, &lt;strong>;Zhehui Chen&lt;/strong>;, Simiao Zuo, Chao Zhang, Songan Zhang,赵佗&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PTvxck0QDE&quot;>;1 隐藏层神经网络中的简单性偏差&lt;/a>;&lt;br />; Depen Morwani*, Jatin Batra、&lt;strong>;Prateek Jain&lt;/strong>;、&lt;strong>; Praneeth Netrapalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2302.03806&quot;>;SLaM：学生- 与未标记示例进行蒸馏的标签混合&lt;/a>;&lt;br />; Vasilis Kontonis、&lt;strong>; Fotis Iliopoulos&lt;/strong>;、&lt;strong>;Khoa Trinh&lt;/strong>;、&lt;strong>;Cenk Baykal&lt;/strong>;、&lt; strong>;Gaurav Menghani&lt;/strong>;、&lt;strong>;Erik Vee&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCHmP68Gtj&quot;>;SNAP：自我监督用于视觉定位和语义理解的神经图&lt;/a>;&lt;br />; Paul-Edouard Sarlin*、&lt;strong>;Eduard Trulls&lt;/strong>;、Marc Pollefeys、&lt;strong>;Jan Hosang&lt;/strong>;、&lt;strong>;Simon Lynen &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QvIvWMaQdX&quot;>;SOAR：改进了近似最近邻搜索的索引&lt;/a>;&lt;br />; &lt;strong >;Philip Sun&lt;/strong>;、&lt;strong>;David Simcha&lt;/strong>;、&lt;strong>;Dave Dopson&lt;/strong>;、&lt;strong>;郭瑞琪&lt;/strong>;、&lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=KoaFh16uOc&quot;>;StyleDrop：任何样式的文本到图像合成&lt;/a>;&lt;br />; &lt;strong>;Kihyuk Sohn&lt;/ strong>;、&lt;strong>;蒋路&lt;/strong>;、&lt;strong>;贾里德·巴伯&lt;/strong>;、Kimin Lee*、&lt;strong>;纳塔尼尔·鲁伊斯&lt;/strong>;、&lt;strong>;迪利普·克里希南&lt;/strong>;、张慧文* ,&lt;strong>;李元珍&lt;/strong>;,&lt;strong>;伊尔凡·埃萨&lt;/strong>;,&lt;strong>;迈克尔·鲁宾斯坦&lt;/strong>;,&lt;strong>;郝元&lt;/strong>;,&lt;strong>;格伦·恩蒂斯&lt;/strong>; 、&lt;strong>;伊琳娜·布洛克&lt;/strong>;、&lt;strong>;丹尼尔·卡斯特罗·钦&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LSYQB4CwD3&quot;>;三塔：使用预训练图像模型进行灵活对比学习&lt;/a>;&lt;br />; Jannik Kossen*、&lt;strong>; Mark Collier&lt;/strong>;、Basil Mustafa、Xiao Wang、Xiaohua Zhai、Lucas Beyer、Andreas Steiner、&lt;strong>;Jesse Berent &lt;/strong>;,&lt;strong>; &lt;/strong>;Rodolphe Jenatton，&lt;strong>; Efi Kokiopoulou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=GIlsH0T4b2&quot; >;多位专家的两阶段延迟学习&lt;/a>;&lt;br />;毛安琪、Christopher Mohri、&lt;strong>;Mehryar Mohri&lt;/strong>;、钟玉涛&lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=ZBzYWP2Gpl&quot;>;AdANNS：自适应语义搜索框架&lt;/a>;&lt;br />; Aniket Rege、&lt;strong>;Aditya Kusupati&lt;/strong>;、Sharan Ranjit S、Alan Fan、Qingqing Cao、Sham Kakade、&lt;strong>;Prateek Jain&lt;/strong>;、Ali Farhadi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Srt1hhQgqa&quot;>;Cappy：超越并提升大型企业具有小评分器的多任务 LM&lt;/a>;&lt;br />; Bowen Tan*、&lt;strong>;Yun Zhu&lt;/strong>;、&lt;strong>;Lijuan Liu&lt;/strong>;、Eric Xing、Zhiting Hu、&lt;strong>;Jindong Chen&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dJZ3MvDw86&quot;>;文本 OOD 泛化的因果结构驱动增强&lt;/a>;&lt;br />; &lt; strong>;Amir Feder&lt;/strong>;、Yoav Wald、Claudia Shi、Suchi Saria、David Blei &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=S0xrBMFihS&quot;>;稠密指数随机特征：高斯核的锐利正估计&lt;/a>;&lt;br />; Valerii Likhosherstov、Krzysztof Choromanski、&lt;strong>;Avinava Dubey&lt;/strong>;、&lt;strong>;Frederick Liu&lt;/strong>;、&lt;strong>;Tamas Sarlos&lt; /strong>;, Adrian Weller &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Vm1zeYqwdc&quot;>;扩散超特征：穿越时空搜索语义对应&lt;/a>;&lt;br />; Grace Luo、Lisa Dunlap、Dong Huk Park、&lt;strong>;Aleksander Holynski&lt;/strong>;、Trevor Darrell &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qgv56R2YJ7&quot; >;用于可控图像生成的扩散自引导&lt;/a>;&lt;br />; &lt;strong>;Dave Epstein&lt;/strong>;、Allan Jabri、&lt;strong>;Ben Poole&lt;/strong>;、Alexei A Efros、&lt;strong>; Aleksander Holynski&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dnGEPkmnzO&quot;>;Õ(k) 更新时间的完全动态 k 聚类&lt;/a>;&lt;br />;萨扬·巴塔查亚 (Sayan Bhattacharya)、马丁·尼古拉斯·科斯塔 (Martin Nicolas Costa)、&lt;strong>;Silvio Lattanzi&lt;/strong>;、&lt;strong>;Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id= SVjDiiVySh&quot;>;通过语言重写改进 CLIP 训练&lt;/a>;&lt;br />; &lt;strong>;Lijie Fan&lt;/strong>;、&lt;strong>;Dilip Krishnan&lt;/strong>;、Phillip Isola、Dina Katabi、&lt;strong>;Yonglong Tian&lt;/strong>;强>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=UzUhiKACmS&quot;>;基于距离的隐私的 k 均值聚类&lt;/a>;&lt;br />; &lt;strong>;Alessandro Epasto&lt;/strong>;、&lt;strong>;Vahab Mirrokni&lt;/strong>;、Shyam Narayanan、&lt;strong>;钟培林&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum? id=Xu8aG5Q8M3&quot;>;LayoutGPT：大型语言模型的组合视觉规划和生成&lt;/a>;&lt;br />; Weixi Feng, Wanrong Zhu, Tsu-Jui Fu,&lt;strong>; Varun Jampani&lt;/strong>;, &lt;strong>;Arjun Reddy Akula&lt;/strong>;、何学海、&lt;strong>;Sugato Basu&lt;/strong>;、Xin Eric Wang、William Yang Wang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id= SxXN3kNTsV&quot;>;用于专家混合对话管理的离线强化学习&lt;/a>;&lt;br />; Dhawal Gupta*、&lt;strong>;Yinlam Chow&lt;/strong>;、&lt;strong>; Azamat Tulepbergenov&lt;/strong>;、Mohammad Ghavamzadeh*、 &lt;strong>;Craig Bouutilier&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=aG6xOP9QY7&quot;>;用于标签差分隐私回归的最佳无偏随机发生器&lt;/a>;&lt; br />; &lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;、&lt;strong>;Badih Ghazi&lt;/strong>;、&lt;strong>;Pritish Kamath&lt;/strong>;、&lt;strong>;Ravi Kumar&lt;/strong>;、&lt;strong>;Ethan Jacob Leeman&lt; /strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Avinash V Varadarajan&lt;/strong>;, &lt;strong>;张驰远&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=WbFhFvjjKj&quot;>;释义可以逃避人工智能生成文本的检测，但检索是一种有效的防御&lt;/a>;&lt;br />; &lt;strong>;Kalpesh Krishna&lt;/strong>; 、Yixiao Song、Marzena Karpinska、John Wieting、Mohit Iyyer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SaMrN9tnxE&quot;>;ReMaX：放松以更好地训练高效全景分割&lt; /a>;&lt;br />;孙书阳*、&lt;strong>;王伟军&lt;/strong>;、于启航*、&lt;strong>;安德鲁·霍华德&lt;/strong>;、菲利普·托尔、陈良杰* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SouroWC5Un&quot;>;稳健且主动安全的无服务器协作学习&lt;/a>;&lt;br />; Nicholas Franzese、Adam Dziedzic、&lt;strong>;Christopher A. Choquette-Choo &lt;/strong>;、Mark R. Thomas、Muhammad Ahmad Kaleem、Stephan Rabanser、Congyu Fang、&lt;strong>;Somesh Jha&lt;/strong>;、Nicolas Papernot、Xiao Wang &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/forum?id=SdYHLTCC5J&quot;>;SpecTr：通过最优传输进行快速推测解码&lt;/a>;&lt;br />; &lt;strong>;Ziteng Sun&lt;/strong>;, &lt;strong>;Ananda Theertha Suresh&lt;/strong>;, &lt; strong>;Jae Hun Ro&lt;/strong>;、&lt;strong>;艾哈迈德·贝拉米&lt;/strong>;、&lt;strong>;Himanshu Jain&lt;/strong>;、&lt;strong>;Felix Yu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= &quot;https://openreview.net/forum?id=YZ7ip645Ra&quot;>;具有更强一致性保证的结构化预测&lt;/a>;&lt;br />;毛安琪，&lt;strong>;Mehryar Mohri&lt;/strong>;，钟玉涛&lt;/p>; &lt; p>; &lt;a href=&quot;https://openreview.net/forum?id=qgiG7WZohZ&quot;>;亲和感知图网络&lt;/a>;&lt;br />; &lt;strong>;Ameya Velingker&lt;/strong>;，&lt;strong>; Ali Kemal Sinop&lt;/strong>;、Ira Ktena、Petar Veličković、&lt;strong>;Sreenivas Gollapudi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rJc5Lsn5QU&quot;>;ARTIC3D：从嘈杂的网络图像集中学习鲁棒的铰接式 3D 形状&lt;/a>;&lt;br />; Chun-Han Yao*, &lt;strong>;Amit Raj&lt;/strong>;, Wei-Chih Hung,&lt;strong>; Yuanzhen Li&lt;/strong>;, &lt;迈克尔·鲁宾斯坦，&lt;strong>;杨明轩&lt;/strong>;，&lt;strong>; &lt;/strong>;&lt;strong>;瓦伦·詹帕尼&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/forum?id=eoDNaH3pfB&quot;>;交互式机器学习的黑盒差异隐私&lt;/a>;&lt;br />; &lt;strong>;Haim Kaplan&lt;/strong>;、&lt;strong>;Yishay Mansour&lt;/strong>;、 &lt;strong>;Shay Moran&lt;/strong>;、Kobbi Nissim、&lt;strong>;Uri Stemmer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=orh4e0AO9R&quot;>;绕过模拟器：近乎最优的对抗性线性上下文强盗&lt;/a>;&lt;br />; Haolin Liu、Chen-Yu Wei、&lt;strong>;Julian Zimmert&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/forum?id=kXOXrVnwbb&quot;>;DaTaSeg：驯服通用多数据集多任务分割模型&lt;/a>;&lt;br />; &lt;strong>;Xiuye​​ Gu&lt;/strong>;，Yin Cui*，&lt;strong>;乔纳森·黄&lt;/strong>;、&lt;strong>;阿卜杜拉·拉什万&lt;/strong>;、&lt;strong>;杨轩&lt;/strong>;、&lt;strong>;周兴一&lt;/strong>;、&lt;strong>;Golnaz Ghiasi&lt;/strong>;、&lt;strong>;郭伟成&lt;/strong>;、&lt;strong>;陈慧忠&lt;/strong>;、陈良杰*、&lt;strong>;David Ross&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/forum?id=kqBUgrkm1c&quot;>;从标签比例轻松学习&lt;/a>;&lt;br />; &lt;strong>;Robert Busa-Fekete&lt;/strong>;、Heejin Choi*、&lt;strong>;Travis Dick&lt;/strong>;、&lt;strong >;Claudio Gentile&lt;/strong>;、&lt;strong>;Andres Munoz Medina&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q3fCWoC9l0&quot;>;高效的数据子集选择跨模型的泛化训练：传导网络和归纳网络&lt;/a>;&lt;br />; Eeshaan Jain、Tushar Nandy、&lt;strong>;Gaurav Aggarwal&lt;/strong>;、&lt;strong>;Ashish Tendulkar&lt;/strong>;、Rishabh Iyer、Abir De &lt;/ p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=h2lkx9SQCD&quot;>;通过二阶方法实现更快的差分隐私凸优化&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/ strong>;、Mahdi Haghifam*、Thomas Steinke、Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gdVcFOvxT3&quot;>;寻找马尔可夫决策过程策略的安全区&lt;/ a>;&lt;br />;Lee Cohen、&lt;strong>;Yishay Mansour&lt;/strong>;、Michal Moshkovitz &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=s1FjXzJ0jy&quot;>;聚焦变压器：情境缩放的对比训练&lt;/a>;&lt;br/>;Szymon Tworkowski、Konrad Staniszewski、Mikołaj Pacek、Yuhuai Wu*、Henryk Michalewski、Piotr Miłoś &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/forum?id=h3kuB4z2G9&quot;>;利用有限的图知识进行超越马尔可夫等价的前门调整&lt;/a>;&lt;br />; Abhin Shah, &lt;strong>;Karthikeyan Shanmugam&lt;/strong>;, Murat Kocaoglu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nI7EmXq2PL&quot;>;H 一致性界限：表征和扩展&lt;/a>;&lt;br />; Anqi Mao、&lt;strong>;Mehryar Mohri&lt;/strong>;、Yutao钟&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kjMGHTo8Cs&quot;>;逆动力学预训练学习多任务模仿的良好表示&lt;/a>;&lt;br />; David Brandfonbrener，&lt;strong >;Ofir Nachum&lt;/strong>;，Joan Bruna &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pvPujuvjQd&quot;>;大多数神经网络几乎是可以学习的&lt;/a>;&lt;br / >; &lt;strong>;Amit Daniely&lt;/strong>;、Nathan Srebro、Gal Vardi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nQ84YY9Iut&quot;>;多类提升：简单直观的弱学习标准&lt;/a>;&lt;br />; Nataly Brukhim、&lt;strong>;Amit Daniely&lt;/strong>;、&lt;strong>;Yishay Mansour&lt;/strong>;、&lt;strong>;Shay Moran&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/forum?id=gJHAT79cZU&quot;>;NeRF 重温：修复体渲染中的正交不稳定性&lt;/a>;&lt;br />; Mikaela Angelina Uy、Kiyohiro Nakayama、Guandao Yang、Rahul Krishna Thomas、 Leonidas Guibas，&lt;strong>; Ke Li&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=izNfcaHJk0&quot;>;通过压缩增强隐私：实现最佳隐私准确性-分布式均值估计中的通信权衡&lt;/a>;&lt;br />; Wei-Ning Chen, Dan Song, Ayfer Ozgur, &lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/forum?id=rzDBoh1tBh&quot;>;私有联合频率估计：适应实例的硬度&lt;/a>;&lt;br />; Jingfeng Wu*, &lt;strong>;Wennan Zhu&lt;/strong>;, &lt;strong >;Peter Kairouz&lt;/strong>;、Vladimir Braverman &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pVlC0reMKq&quot;>;RETVec：弹性且高效的文本矢量化器&lt;/a>;&lt;br />; &lt;strong>;Elie Bursztein&lt;/strong>;、&lt;strong>;Marina 张&lt;/strong>;、&lt;strong>;Owen Skipper Vallis&lt;/strong>;、&lt;strong>;贾新宇&lt;/strong>;、&lt;strong>;Alexey Kurakin&lt;/strong>; strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ne6zeqLFCZ&quot;>;优化算法的符号发现&lt;/a>;&lt;br />; 陈向宁*, &lt;strong>;陈梁&lt;/strong>;、&lt;strong>;黄大&lt;/strong>;、&lt;strong>;Esteban Real&lt;/strong>;、&lt;strong>;王开元&lt;/strong>;、&lt;strong>;范晓&lt;/strong>;、&lt;strong>;宣仪董&lt;/strong>;、&lt;strong>;Thang Luong&lt;/strong>;、Cho-Jui Hsieh、&lt;strong>;卢一峰&lt;/strong>;、&lt;strong>;Quoc V. Le&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/forum?id=lds9D17HRd&quot;>;两个特征的故事：稳定扩散补充 DINO 的零样本语义对应&lt;/a>;&lt;br />; Junyi 张，&lt;strong>;Charles Herrmann&lt;/strong>;、&lt;strong>;Junhwa Hur&lt;/strong>;、&lt;strong>;Luisa F. Polania&lt;/strong>;、&lt;strong>;Varun Jampani&lt;/strong>;、&lt;strong>;孙德庆&lt;/strong>;、&lt;strong>; >; Ming-Hsuan Yang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=iSd8g75QvP&quot;>;转化式在线学习的三分法&lt;/a>;&lt;br />; Steve Hanneke、Shay Moran、Jonathan Shafer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=i2H2sEiq2T&quot;>;DP 的统一快速梯度裁剪框架-SGD&lt;/a>;&lt;br />; &lt;strong>;William Kong&lt;/strong>;、&lt;strong>;Andres Munoz Medina&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ forum?id=mlbes5TAAg&quot;>;在审计差异化私有机器学习中释放随机化的力量&lt;/a>;&lt;br />; &lt;strong>;Krishna Pillutla&lt;/strong>;、&lt;strong>;Galen Andrew&lt;/strong>;、&lt;strong>;Peter Kairouz &lt;/strong>;，&lt;strong>;H.布伦丹·麦克马汉，&lt;strong>;阿丽娜·奥普雷亚&lt;/strong>;，&lt;strong>;吴世雄&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id= zEm6hF97Pz&quot;>;（放大）带状矩阵分解：私人培训的统一方法&lt;/a>;&lt;br />; Christopher A Choquette-Choo、&lt;strong>; Arun Ganesh&lt;/strong>;、&lt;strong>;Ryan McKenna&lt;/strong>;、 &lt;strong>;H Brendan McMahan&lt;/strong>;、&lt;strong>;基思·拉什&lt;/strong>;、&lt;strong>; &lt;/strong>;Abhradeep Guha Thakurta、&lt;strong>;徐峥&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xcGhx9FdxM&quot;>;通过弃权进行顺序预测中的对抗弹性&lt;/a>;&lt;br />; Surbhi Goel、Steve Hanneke、&lt;strong>;Shay Moran&lt;/strong>;、Abhishek Shetty &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uTlKUAm68H&quot;>;交替梯度下降和专家混合用于集成多模态感知&lt;/a>;&lt;br />; &lt; strong>;哈桑·阿克巴里&lt;/strong>;、&lt;strong>;丹·康德拉图克&lt;/strong>;、&lt;strong>;崔银&lt;/strong>;、&lt;strong>;雷切尔·霍农&lt;/strong>;、&lt;strong>;王慧生&lt;/strong>;、&lt; strong>;Hartwig Adam&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf/0083ab45a583fe1992b3c4f9222081c50b9d30c3.pdf&quot;>;Android in the Wild：用于 Android 设备控制的大规模数据集&lt; /a>;&lt;br />; &lt;strong>;克里斯托弗·罗尔斯&lt;/strong>;、&lt;strong>;爱丽丝·李&lt;/strong>;、&lt;strong>;丹尼尔·罗德里格斯&lt;/strong>;、&lt;strong>;奥丽安娜·里瓦&lt;/strong>;、蒂莫西·利利克拉普 &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CiRHWaRbp0&quot;>;对抗性图像混淆的鲁棒性基准测试&lt;/a>;&lt;br />; Florian Stimberg，&lt;strong>;Ayan Chakrabarti&lt;/ strong>;、&lt;strong>;卢春达&lt;/strong>;、&lt;strong>;Hussein Hazimeh&lt;/strong>;、&lt;strong>;Otilia Stretcu&lt;/strong>;、&lt;strong>;乔伟&lt;/strong>;、&lt;strong>;刘银涛&lt;/strong>;、&lt;strong>;梅尔维·卡亚&lt;/strong>;、&lt;strong>;赛勒斯·拉什奇安&lt;/strong>;、&lt;strong>;阿里尔·福克斯曼&lt;/strong>;、&lt;strong>;穆罕默德·泰克&lt;/strong>;、斯文·高瓦尔&lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uIj1jDc8k6&quot;>;通过社区参与构建社会文化包容性刻板印象资源&lt;/a>;&lt;br />; &lt;strong>;Sunipa Dev&lt;/strong >;、贾亚·戈亚尔、&lt;strong>;Dinesh Tewari&lt;/strong>;、&lt;strong>;Shachi Dave&lt;/strong>;、&lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=L9I9FhHfS3&quot;>;机器学习公平性肤色标注的共识和主观性&lt;/a>;&lt;br />; &lt;strong>;Candice Schumann&lt;/strong>;、&lt;strong>;Gbolahan O Olanubi&lt;/strong>;、 &lt;strong>;奥瑞尔·赖特&lt;/strong>;、小埃利斯·蒙克*、&lt;strong>;考特尼·赫尔德雷斯&lt;/strong>;、&lt;strong>; &lt;/strong>;&lt;strong>;苏珊娜·里科&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zdli6OxpWd&quot;>;在个人级差异隐私下计算不同元素&lt;/a>;&lt;br />; &lt;strong>;Alexander Knop&lt;/strong>;，Thomas Steinke &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=GjNvvswoUL&quot;>;DICES 数据集：对话式人工智能安全评估的多样性&lt;/a>;&lt;br />; &lt;strong>;Lora Aroyo&lt;/strong >;、亚历克斯·S·泰勒、&lt;strong>;马克·迪亚兹&lt;/strong>;、&lt;strong>;克里斯托弗·M·霍曼&lt;/strong>;、&lt;strong>;艾丽西亚·帕里什&lt;/strong>;、格雷格·塞拉皮奥-加西亚、&lt;strong>;Vinodkumar Prabhakaran&lt; /strong>;, &lt;strong>;Ding Wang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SS3CK3yx5Z&quot;>;ImageNet 迁移到真实世界数据集方面取得进展吗?&lt;/a>;&lt;br />; Alex Fang、&lt;strong>;Simon Kornblith&lt;/strong>;、Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=AA2uO0HHmr&quot; >;根据 2D 注释估计通用 3D 房间结构&lt;/a>;&lt;br />; Denys Rozumnyi*、&lt;strong>;Stefan Popov&lt;/strong>;、&lt;strong>;Kevis-kokitsi Maninis&lt;/strong>;、Matthias Nießner、&lt;strong>;Vittorio Ferrari&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6hZIfAY9GD&quot;>;大型语言模型作为属性训练数据生成器：多样性和偏见的故事&lt;/a >;&lt;br />; 余悦、庄雨辰、张洁宇、孟雨、Alexander Ratner、Ranjay Krishna、&lt;strong>;沉家明&lt;/strong>;、&lt;strong>; &lt;/strong>;张超&lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/forum?id=Y45ZCxslFx&quot;>;MADLAD-400：多语言和文档级大型审核数据集&lt;/a>;&lt;br />; Sneha Kudugunta，&lt;strong>;Isaac Caswell&lt;/ strong>;、张彪、Xavier Garcia、Derrick Xin、&lt;strong>;Aditya Kusupati&lt;/strong>;、Romi Stella、Ankur Bapna、Orhan Firat &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ forum?id=uhKtQMn21D&quot;>;机制：学习率调节器&lt;/a>;&lt;br />; Ashok Cutkosky、Aaron Defazio、&lt;strong>;Harsh Mehta&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=8TMhs2pIfG&quot;>;NAVI：具有高质量 3D 形状和姿势注释的类别无关图像集合&lt;/a>;&lt;br />; &lt;strong>;Varun Jampani&lt;/strong>;，&lt;strong>; Kevis-kokitsi Maninis&lt;/strong>;、&lt;strong>;安德烈亚斯·恩格尔哈特&lt;/strong>;、&lt;strong>;阿琼·卡普尔&lt;/strong>;、&lt;strong>;Karen Truong&lt;/strong>;、&lt;strong>;凯尔·萨金特&lt;/strong>;、&lt;斯特凡·波波夫、&lt;strong>;安德烈·阿劳霍&lt;/strong>;、&lt;strong>;里卡多·马丁·布鲁阿拉&lt;/strong>;、&lt;strong>;考沙尔·帕特尔&lt;/strong>;、&lt;strong>;丹尼尔·弗拉西奇&lt;/strong>;、 &lt;strong>;维托里奥·法拉利&lt;/strong>;、&lt;strong>;阿米什·马卡迪亚&lt;/strong>;、刘策*、&lt;strong>;李元振&lt;/strong>;、&lt;strong>;周浩&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=x6cOcxRnxG&quot;>;神经理想大涡模拟：用神经随机微分方程模拟湍流&lt;/a>;&lt;br />; &lt;strong>;Anudhyan Boral&lt;/strong>; , &lt;strong>;万钟一&lt;/strong>;, &lt;strong>;Leonardo Zepeda-Nunez&lt;/strong>;, &lt;strong>;James Lottes&lt;/strong>;, &lt;strong>;王清&lt;/strong>;, &lt;strong>;范亦凡陈&lt;/strong>;、&lt;strong>;约翰·罗伯茨·安德森&lt;/strong>;、&lt;strong>;飞沙&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id= wFuemocyHZ&quot;>;重新启动采样以改进生成过程&lt;/a>;&lt;br />;徐一伦、邓明阳、程翔、&lt;strong>;田永龙&lt;/strong>;、刘子明、Tommi Jaakkola&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xUyBP16Q5J&quot;>;重新思考推荐系统中的激励措施：单调奖励总是有益的吗？&lt;/a>;&lt;br />; Fan Yao, Chuanhao Li, Karthik Abinav Sankararaman, Yiming Liao , &lt;strong>;朱彦&lt;/strong>;, 王起凡, 王红宁, 徐海峰 &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jGyMUum1Lq&quot;>;重新审视评估指标语义分割：细粒度交集优化与评估&lt;/a>;&lt;br />; Zifu Wang, &lt;strong>;Maxim Berman&lt;/strong>;, Amal Rannen-Triki, Philip Torr, Devis Tuia, Tinne Tuytelaars, Luc Van Gool、Jiaqian Yu、Matthew B. Blaschko &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0H5fRQcpQ7&quot;>;RoboHive：机器人学习的统一框架&lt;/a>;&lt;br />; &lt;strong>;Vikash Kumar&lt;/strong>;、Rutav Shah、周高跃、Vincent Moens、Vittorio Caggiano、Abhishek Gupta、&lt;strong>;Aravind Rajeswaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=Vn5qZGxGj3&quot;>;SatBird：利用遥感和公民科学数据进行鸟类物种分布建模&lt;/a>;&lt;br />;Mélisande Teng、Amna Elmustafa、Benjamin Akera、Yoshua Bengio、Hager Radi、&lt; strong>;Hugo Larochelle&lt;/strong>;，David Rolnick &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sqTcCXkG4P&quot;>;大型嵌入模型的稀疏性保持差分隐私训练&lt;/ a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;、Yangsibo Huang*、&lt;strong>;Pritish Kamath&lt;/strong>;、&lt;strong>;Ravi Kumar&lt;/strong>;、&lt;strong>;Pasin Manurangsi&lt;/strong>;、 &lt;strong>;Amer Sinha&lt;/strong>;、&lt;strong>; &lt;/strong>;&lt;strong>;张驰远&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id= xpjsOQtKqx&quot;>;StableRep：从文本到图像模型的合成图像形成强大的视觉表示学习者&lt;/a>;&lt;br />; &lt;strong>;Yonglong Tian&lt;/strong>;、&lt;strong>;Lijie Fan&lt;/strong>;、Phillip Isola、 &lt;strong>;Huiwen Chang&lt;/strong>;、&lt;strong>;Dilip Krishnan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EPz1DcdPVE&quot;>;迈向联邦基金会模型：用于小组结构学习的可扩展数据集管道&lt;/a>;&lt;br />; &lt;strong>;Zachary Charles&lt;/strong>;、&lt;strong>;Nicole Mitchell&lt;/strong>;、&lt;strong>;Krishna Pillutla&lt;/strong>;、&lt;strong>; Michael Reneer&lt;/strong>;、&lt;strong>;Zachary Garrett&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zWxKYyW9ik&quot;>;提示调整的普遍性和局限性&lt; /a>;&lt;br />; 王一涵、Jatin Chauhan、王伟、&lt;strong>;谢卓瑞&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id =sovxUzPzLN&quot;>;使用稳定扩散的无监督语义对应&lt;/a>;&lt;br />; Eric Hedlin、Gopal Sharma、Shweta Mahajan、&lt;strong>;Hossam Isack&lt;/strong>;、&lt;strong>;Abhishek Kar&lt;/strong>;、&lt;strong>; Andrea Tagliasacchi&lt;/strong>;、&lt;strong>; &lt;/strong>;Kwang Moo Yi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QEDjXv9OyY&quot;>;YouTube-ASL：大型-规模、开放域美国手语-英语平行语料库&lt;/a>;&lt;br />; &lt;strong>;Dave Uthus&lt;/strong>;、&lt;strong>;Garrett Tanzer&lt;/strong>;、&lt;strong>;Manfred Georg&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=swNtr6vGqg&quot;>;具有相关数据的线性回归中的噪声水平&lt;/a>;&lt;br />; Ingvar Ziemann，&lt;strong>; Stephen Tu&lt;/strong>;、George J. Pappas、Nikolai Matni &lt;/p>; &lt;/div>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple- style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http:// /blog.research.google/feeds/8223613466421639113/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2023/12/google-at-neurips-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/8223613466421639113&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/8223613466421639113&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-neurips-2023.html &quot; rel=&quot;alternate&quot; title=&quot;Google at NeurIPS 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https: //img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWgu TDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s72-c/google_research_sticker-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>; &lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6514019106482066697&lt;/id>;&lt;已发布>; 2023-12-08T10:34:00.000-08:00&lt;/发布>;&lt;更新>;2023-12-08T14:25:05.067-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger. com/atom/ns#&quot; term=&quot;算法&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;稀疏性保持差分隐私训练&lt;/stitle>;&lt;content type =&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：Yangsibo Huang，Google 研究实习生； Chiyuan 张，Google 研究科学家&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t4 3a4BhICFwU7EqAs9AZI6aMHLKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkaa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s1600/稀疏%20DP-SGD .png&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;em>;大型嵌入模型&lt;/em>;已成为推荐系统中各种应用的基本工具 [&lt;a href=&quot;https://research.google/pubs/pub45530/&quot;>;1&lt;/em>; a>;、&lt;a href=&quot;https://arxiv.org/abs/2104.05158&quot;>;2&lt;/a>;] 和自然语言处理 [&lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;>; 3&lt;/a>;、&lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;>;4&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/1810.04805 ”>;5&lt;/a>;]。此类模型可以通过映射分类或&lt;a href=&quot;https://en.wikipedia.org/wiki/String_(computer_science)&quot;>;字符串&lt;/a>;值的输入属性，将非数值数据集成到深度学习模型中使用嵌入层将大词汇量转换为固定长度的表示向量。这些模型广泛部署在个性化推荐系统中，并在语言任务中实现了最先进的性能，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;>;语言建模&lt;/a >;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;情绪分析&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;问题回答&lt;/a>;。在许多此类场景中，部署这些模型时隐私是同样重要的功能。因此，人们提出了各种技术来实现私有数据分析。其中，&lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;差分隐私&lt;/a>;（DP）是一种广泛采用的定义，它限制个人用户信息的暴露，同时仍然允许分析人口水平模式。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 对于训练具有 DP 保证的深度神经网络，最广泛使用的算法是 &lt;a href=&quot;https://arxiv.org/abs/1607.00133 &quot;>;DP-SGD&lt;/a>;（DP &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;随机梯度下降&lt;/a>;）。 DP-SGD 的一个关键组成部分是在训练期间向梯度向量的每个坐标添加高斯噪声。然而，当应用于大型嵌入模型时，这会带来可扩展性挑战，因为它们依赖梯度稀疏性来进行有效训练，但向所有坐标添加噪声会破坏稀疏性。 &lt;/p>; &lt;p>; 为了缓解这种梯度稀疏问题，请参阅“&lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;大型嵌入模型的稀疏性保持差分隐私训练&lt;/a>;”（将在 &lt;a href=&quot;https://nips.cc/Conferences/2023&quot;>;NeurIPS 2023&lt;/a>; 上展示），我们提出了一种名为&lt;em>;支持自适应过滤的稀疏训练&lt;/em>;的新算法（ DP-AdaFEST）。在较高层面上，该算法通过仅选择在每次迭代时添加噪声的特征行子集来保持梯度的稀疏性。关键是使这种选择具有差分隐私性，从而在隐私成本、训练效率和模型效用之间实现三向平衡。我们的实证评估表明，DP-AdaFEST 实现了显着稀疏的梯度，与标准 DP-SGD 产生的密集梯度相比，梯度大小减少了 10&lt;sup>;5&lt;/sup>;X 以上，同时保持了相当的准确度水平。这种梯度大小的减小可以转化为 20 倍的挂钟时间改进。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;概述&lt;/h2>; &lt;p>; 更好地了解梯度稀疏问题的挑战和我们的解决方案，让我们首先概述一下 DP-SGD 在训练过程中的工作原理。如下图所示，DP-SGD 的运行方式是剪切当前随机样本子集中每个示例的梯度贡献（称为小批量），并按坐标添加 &lt;a href=&quot;https://en. wikipedia.org/wiki/Gaussian_noise&quot;>;高斯噪声&lt;/a>;到&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;随机梯度下降&lt;/a>;每次迭代期间的平均梯度（新加坡元）。 DP-SGD 已证明其在保护用户隐私方面的有效性，同时在各种应用中保持模型实用性 [&lt;a href=&quot;https://arxiv.org/pdf/2204.13650.pdf&quot;>;6&lt;/a>;，&lt;a href =&quot;https://arxiv.org/pdf/2110.06500.pdf&quot;>;7&lt;/a>;]。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2k AmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s1999/image5.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71 quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;DP-SGD 工作原理的说明。在每个训练步骤中，都会对一小批示例进行采样，并用于计算每个示例的梯度。这些梯度通过高斯噪声的裁剪、聚合和求和来处理，以产生最终的私有化梯度。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 将 DP-SGD 应用于大型嵌入模型的挑战主要来自 1）非数字特征字段，如用户/产品 ID 和类别，以及 2）通过嵌入层转换为密集向量的单词和标记。由于这些特征的词汇量大小，该过程需要具有大量参数的大型嵌入表。与参数数量相比，梯度更新通常非常稀疏，因为每个小批量示例仅激活一小部分嵌入行（下图可视化了零值坐标的比率，即稀疏性）不同批量大小下的梯度）。这种稀疏性在有效处理大规模嵌入训练的工业应用中得到了充分利用。例如，&lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Google Cloud TPU&lt;/a>; 是一种定制设计的 AI 加速器，针对大型 AI 模型的训练和推理进行了优化，具有&lt;a href =&quot;https://cloud.google.com/blog/topics/developers-practitioners/building-large-scale-recommenders-using-cloud-tpus&quot;>;专用 API&lt;/a>; 用于处理具有稀疏更新的大型嵌入。与 GPU 上的训练相比，这可以&lt;a href=&quot;https://eng.snap.com/training-models-with-tpus&quot;>;显着提高训练吞吐量&lt;/a>;，而 GPU 目前还没有专门的优化用于稀疏嵌入查找。另一方面，DP-SGD 完全破坏了梯度稀疏性，因为它需要向&lt;em>;所有&lt;/em>;坐标添加独立的高斯噪声。这为大型嵌入模型的私人训练设置了障碍，因为与非私人训练相比，训练效率将显着降低。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4 T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDhrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s1814/image3.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1058&quot; data-original-width=&quot;1814&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEE IIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;在 &lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge&quot; 中嵌入梯度稀疏性（零值梯度坐标的分数） -dataset&quot;>;Criteo pCTR&lt;/a>; 模型（见下文）。该图报告了桶数最多的前 5 个分类特征（总共 26 个）的梯度稀疏度（超过 50 个更新步骤的平均值），以及所有分类特征的稀疏度。随着更多的示例命中嵌入表中的更多行，从而创建非零梯度，稀疏性随着批量大小而减小。然而，即使对于非常大的批量大小，稀疏度也高于 0.97。所有五个功能都一致观察到这种模式。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt; h2>;算法&lt;/h2>; &lt;p>;我们的算法是通过在每次迭代时使用额外的机制扩展标准DP-SGD来构建的，以私下选择“热门特征”，这些特征是由当前的多个训练示例激活的特征。小批量。如下图所示，该机制分几个步骤工作：&lt;/p>; &lt;ol>; &lt;li>;计算有多少示例对每个特征桶做出了贡献（我们将分类特征的每个可能值称为“桶”）。 &lt;/li>;&lt;li>;通过削减每个示例的计数来限制其总贡献。 &lt;/li>;&lt;li>;将高斯噪声添加到每个特征桶的贡献计数中。 &lt;/li>;&lt;li>;仅选择要包含在梯度更新中且计数高于给定阈值（稀疏性控制参数）的特征，从而保持稀疏性。该机制是差分隐私的，并且可以通过将其与标准 DP-SGD 迭代组合来轻松计算隐私成本。 &lt;/li>; &lt;/ol>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto；margin-right：auto；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFR zUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s1600 /image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;743&quot; data-original-width=&quot;1600&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJv QenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;说明具有 20 个桶的合成分类特征的算法过程。我们计算对每个桶做出贡献的示例数量，根据每个示例的总贡献（包括对其他特征的贡献）调整值，添加高斯噪声，并仅保留那些噪声贡献超过（噪声）梯度阈值的桶更新。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;理论动机&lt;/h2>; &lt; p>; 我们通过将 DP-AdaFEST 视为使用随机&lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_complexity_(optimization)&quot;>;梯度预言&lt;/a>;的优化来提供 DP-AdaFEST 背后的理论动机。理论设置中随机梯度下降的标准分析将模型的测试误差分解为“偏差”和“方差” ”条款&lt;/a>;。 DP-AdaFEST 的优点可以被视为以稍微增加偏差为代价来减少方差。这是因为与 DP-SGD 相比，DP-AdaFEST 向较小的一组坐标添加了噪声，DP-SGD 向所有坐标添加了噪声。另一方面，DP-AdaFEST 给梯度带来了一些偏差，因为嵌入特征上的梯度以一定的概率被丢弃。我们建议感兴趣的读者参阅&lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;论文&lt;/a>;的第3.4节以了解更多详细信息。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>; 我们通过大型嵌入模型应用来评估算法的有效性，公共数据集，包括一个广告预测数据集 (&lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset&quot;>;Criteo-Kaggle&lt;/a>;) 和一个语言理解数据集 (&lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;SST-2&lt;/a>;)。我们使用&lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;带有指数选择的DP-SGD&lt;/a>;作为基线比较。 &lt;/p>; &lt;p>; DP-AdaFEST 的有效性如下图所示，它比基线实现了显着更高的梯度大小缩减（即梯度稀疏性），同时保持相同水平的效用（即只有最低的性能）降解）。 &lt;/p>; &lt;p>; 具体而言，在 Criteo-Kaggle 数据集上，DP-AdaFEST 将常规 DP-SGD 的梯度计算成本降低了 5x10&lt;sup>;5&lt;/sup>; 倍以上，同时保持了可比的&lt;a href= “https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;>;AUC&lt;/a>;（我们定义为小于 0.005 的损失）。这种减少转化为更高效、更具成本效益的培训过程。相比之下，如下图绿线所示，基线方法无法在如此小的效用损失阈值内实现合理的成本降低。 &lt;/p>; &lt;p>; 在语言任务中，减少梯度大小的潜力不大，因为使用的词汇通常较小并且已经相当紧凑（如下右图所示）。然而，采用保持稀疏性的DP-SGD有效地避免了密集的梯度计算。此外，根据理论分析中提出的偏差-方差权衡，我们注意到当梯度大小的减小最小时，DP-AdaFEST 偶尔会表现出比 DP-SGD 更优越的实用性。相反，当结合稀疏性时，基线算法在维持效用方面面临挑战。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPA UV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s1860/image6.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1860&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6 rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;DP 在 ε =1.0 下实现的最佳梯度大小缩减（常规 DP-SGD 和稀疏性保持算法之间的非零梯度值计数的比率）的比较-AdaFEST（我们的算法）和基线算法（&lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;采用指数选择的 DP-SGD&lt;/a>;）与不同效用阈值下的 DP-SGD 进行比较不同之处。曲线越高表示效用/效率之间的权衡越好。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在实践中，大多数广告预测模型都在不断地训练和评估。为了模拟这种在线学习设置，我们还使用时间序列数据进行评估，这些数据由于非平稳而极具挑战性。我们的评估使用 &lt;a href=&quot;https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/&quot;>;Criteo-1TB&lt;/a>; 数据集，其中包含真实世界的用户点击24 天内收集的数据。一致的是，DP-AdaFEST 将常规 DP-SGD 的梯度计算成本降低了 10&lt;sup>;4&lt;/sup>; 倍以上，同时保持了相当的 AUC。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4 WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s2500/image7 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;891&quot; data-original-width=&quot;2500&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10u SiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DP-AdaFEST（我们的算法）和具有指数选择的 DP-SGD（之前的算法）在 ε =1.0 下实现的最佳梯度大小减小的比较与 DP-SGD 在不同阈值下的效用差异进行比较。曲线越高表示效用/效率权衡越好。 DP-AdaFEST 始终优于以前的方法。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了一种新算法 DP-AdaFEST，用于在差分隐私训练中保持梯度稀疏性，特别是在涉及大型嵌入模型的应用中，大型嵌入模型是推荐系统和自然语言处理中各种应用的基本工具。我们的算法显着减小了梯度大小，同时保持了现实世界基准数据集的准确性。此外，它提供了通过稀疏控制参数平衡效用和效率的灵活选项，而我们的建议提供了更好的隐私效用损失。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是与 Pritish 的 Badih Ghazi 合作完成的Kamath、Ravi Kumar、Pasin Manurangsi 和 Amer Sinha。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6514019106482066697/comments/default&quot; rel=&quot;replies &quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving- Differentially.html#comment-form&quot; rel =&quot;回复&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;编辑&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving- Differentially.html&quot; rel=&quot;alternate&quot; title=&quot;稀疏性保留差分私人训练&quot; type=&quot;text/html&quot; />;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image高度=“16” rel=“http://schemas.google.com/g/2005#thumbnail” src=“https://img1.blogblog.com/img/b16-rounded.gif” 宽度=“16” >;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV 4t43a4BhICFwU7EqAs9AZI6aMHLKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s72 -c/Sparse%20DP-SGD.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr ：总计>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-5912569868305742102&lt;/id>;&lt;发布>;2023-12-07T09:51:00.000-08:00&lt;/已发布>;&lt;更新>;2023-12-07T09:53:23.920-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;增强现实&quot;>;&lt; /类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“HCI”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/” ns#&quot; term=&quot;Virtual Reality&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;有效：经过感知验证的虚拟化身库，具有包容性和多样性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class= &quot;byline-author&quot;>;发布者：Mar Gonzalez-Franco，Google AR 和 Google 研究科学家VR&lt;/span>;&lt;p>; 随着虚拟现实 (VR) 和增强现实 (AR) 技术的不断普及，虚拟化身正成为我们数字交互中越来越重要的一部分。特别是，虚拟化身是许多社交 VR 和 AR 交互的中心，因为它们是代表远程参与者和促进协作的关键。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在过去的十年中，跨学科科学家付出了大量的努力来更好地理解化身的使用，并做出了许多有趣的观察，包括用户&lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frvir.2020.575943/full&quot;>;体现他们的化身&lt;/a>;的能力（即，认为化身身体是他们自己的错觉） ）和&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9089510&quot;>;自我头像追随者效果&lt;/a>;，它在头像和用户的动作之间创建了足够强的绑定化身实际上可以影响用户的行为。 &lt;/p>; &lt;p>; 在实验中使用化身不仅是为了研究用户在 VR 空间中的互动和行为方式，也是为了发现人类感知和神经科学的局限性。事实上，一些 VR 社交实验通常依赖于重新创建在现实世界中无法轻松重现的场景，例如酒吧爬行到 &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0052766&quot; >;探索内群体与外群体效应&lt;/a>;，或欺骗实验，例如&lt;a href=&quot;https://doi.org/10.1371/journal.pone.0209704&quot;>;米尔格拉姆对虚拟现实中权威的服从&lt;/一个>;。其他研究试图探索深层神经科学现象，例如&lt;a href=&quot;https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2021.0453&quot;>;人类运动控制机制&lt;/a>;。这或许遵循大脑可塑性的&lt;a href=&quot;https://www.nature.com/articles/35784&quot;>;橡胶手错觉&lt;/a>;，一个人可以开始感觉自己拥有一块橡胶手，而他们真正的手隐藏在窗帘后面。使用个性化&lt;a href=&quot;https://doi.org/10.1186/s13063-022-06683-1&quot;>;化身&lt;/a>;进行精神治疗的可能疗法也越来越多。在这些情况下，VR 成为一种&lt;a href=&quot;https://en.wikipedia.org/wiki/Ecological_validity&quot;>;生态上有效的&lt;/a>;工具，使科学家能够探索或治疗人类行为和感知。 &lt;/p>; &lt;p>; 如果没有能够方便地进行实验的研究工具和库，这些实验和疗法都不可能存在。因此，近年来围绕头像创建和动画发布了多个系统和开源工具。然而，现有的头像库尚未在多样性谱上进行系统验证。与化身互动时，社会&lt;a href=&quot;https://doi.org/10.1016/j.concog.2013.04.016&quot;>;偏见和动态&lt;/a>;也会转移到 VR/AR，这可能会导致得出不完整的结论VR/AR 中人类行为的研究。 &lt;/p>; &lt;p>; 为了部分解决这个问题，我们与中佛罗里达大学合作创建并发布了开源&lt;a href=&quot;https://github.com/google/valid-avatar-library&quot; >;实现包容性和多样性的虚拟头像库&lt;/a>;（有效）。 &lt;a href=&quot;https://doi.org/10.3389/frvir.2023.1248915&quot;>;我们最近的论文&lt;/a>;中进行了描述，发表于&lt;a href=&quot;https://www.frontiersin.org/journals/virtual -reality&quot;>;&lt;i>;虚拟现实前沿&lt;/i>;&lt;/a>;，该化身库可随时用于 VR/AR 实验，其中包括美国人口普查局认可的 7 个不同种族和民族的 210 个化身。这些化身经过感知验证，旨在促进虚拟化身研究的多样性和包容性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0 LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s1717/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1413&quot; data-original-width=&quot;1717&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8 IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;VALID 库中提供的所有 42 个基本头像的头像都是在与来自 &lt;a href=&quot;https://www 的 7 个民族和种族群体的成员的广泛互动中创建的.federalregister.gov/documents/2023/01/27/2023-01635/initial-proposals-for-updating-ombs-race-and-ethnicity-statistical-standards&quot;>;联邦登记册&lt;/a>;，其中包括（AIAN、亚洲人、黑人、西班牙裔、中东和北非地区、NHPI 和白人）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;库的创建和验证&lt;/h2>; &lt;p>; 我们的多样化头像库的种族和民族的初步选择遵循&lt;a href=&quot;https://www.npr.org/2023/01/26/1151608403/mena-race-categories-us-census的最新指南-middle-eastern-latino-hispanic&quot;>;美国人口普查局&lt;/a>;建议，截至 2023 年，使用代表美国社会大部分人口的 7 个民族和种族群体，这也可以推断到全球人口。这些群体包括西班牙裔或拉丁裔、美洲印第安人或阿拉斯加原住民 (AIAN)、亚洲人、黑人或非裔美国人、夏威夷原住民或其他太平洋岛民 (NHPI)、&lt;/em>; &lt;em>;白人、中东或北非&lt;/em>; (MENA)。我们预计图书馆将继续发展，通过未来添加的头像带来更多的多样性和代表性。 &lt;/p>; &lt;p>; 化身是手工建模和创建的，使用的过程将平均面部特征与每个种族群体的代表利益相关者的广泛合作相结合，他们的反馈用于艺术地修改化身的面部网格。然后我们对来自 33 个国家的参与者进行了一项在线研究，以确定图书馆中每个头像的种族和性别是否可识别。除了头像之外，我们还提供通过观察用户对所有 42 个基本头像的种族和性别进行统计验证的标签（见下文）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG 8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V /s1999/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;736&quot; data-original-width=&quot;1999&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanvDq yQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V/s16000/image3.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在库验证期间向参与者展示的黑人/非裔美国人头像的头像示例。&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 我们发现所有亚洲人、黑人和白人化身都被所有参与者普遍认为是他们的模仿种族，而我们的美洲印第安人或阿拉斯加原住民（AIAN） ）、西班牙裔、中东或北非 (MENA) 化身通常只能由同一种族的参与者识别。这也表明，参与者种族可以提高对同一种族的虚拟角色的识别度。图书馆发布的论文强调了在研究 VR 中的化身行为时如何考虑这种群体内的熟悉度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLlj kA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s1999/image1.jpg&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1509&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1Mvk KiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;由其他种族参与者和同种族参与者分开的 42 个基本化身的同意率的混淆矩阵热图。此矩阵中可见的一个有趣的方面是，参与者在识别自己种族的化身方面明显优于其他种族。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;数据集详细信息&lt;/h2>; &lt;p>; 我们的模型采用 &lt;a href=&quot;https://www.autodesk.com/products/fbx/overview&quot;>;FBX 格式&lt;/a>;，与以前的头像库兼容，例如常用的&lt;a href=&quot;https://github.com/microsoft/Microsoft-Rocketbox&quot;>;Rocketbox&lt;/a>;，并且可以轻松集成到大多数游戏引擎中，例如&lt;a href=&quot;https://unity. com/&quot;>;Unity&lt;/a>; 和 &lt;a href=&quot;https://www.unrealengine.com/&quot;>;Unreal&lt;/a>;。此外，头像还配有 69 块骨骼和 65 种面部混合形状，使研究人员和开发人员能够轻松创建和应用动态面部表情和动画。这些头像被故意制作成部分卡通化，以避免极端相似的场景，在这种情况下，一个人可能被模仿，但仍然具有足够的代表性，能够进行可靠的用户研究和社会实验。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgN WC12kdwkvcDLaUSuq1lod1RUh67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s1999/image4.jpg &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;986&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh67 -lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;有效头像中包含的骨架索具（允许动画的骨骼）和一些面部混合形状的图像。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;br />; &lt;p>; 头像可以进一步组合休闲装和五种职业装，包括医疗、军事、工人和商务。这是对先前库的有意改进，在某些情况下，在化身服装中再现了陈规定型的性别和种族偏见，并为某些专业化身提供了非常有限的多样性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7 PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s1537/image5.jpg&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1023&quot; data-original-width=&quot;1537&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNHWd t1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;有效头像中包含一些示例服装的图像。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;开始使用 VALID&lt;/h2 >; &lt;p>; 我们相信，&lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;促进包容性和多样性的虚拟头像库&lt;/a>;（VALID）将成为宝贵的资源致力于 VR/AR 应用的研究人员和开发人员。我们希望它将有助于创造更加包容和公平的虚拟体验。为此，我们邀请您探索我们在开源 &lt;a href=&quot;https://en.wikipedia.org/wiki/MIT_License&quot;>;MIT 许可证&lt;/a>;下发布的头像库。您可以免费下载头像并在各种设置中使用它们。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这个化身库是与来自纽约大学的 Tiffany D. Do、Steve Zelenty 和 Ryan P McMahan 教授合作诞生的。佛罗里达州中部。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5912569868305742102/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html#comment-form&quot; rel=&quot;replies&quot; title= “0 条评论” type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;edit&quot; type=&quot;application/atom+ xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html&quot; rel=&quot;alternate&quot; title=&quot;VALID：一个经过感知验证的虚拟化身库，具有包容性和多样性&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1 Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s72 -c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8414954450937764241&lt;/id>;&lt;已发布>;2023-12-05T17:32:00.000-08:00&lt;/已发布>;&lt;已更新>;2023-12-06T19:51:14.038-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= &quot;EMNLP&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 在 EMNLP 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：项目经理 Malaya Jules，谷歌&lt;/span>; &lt;img src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz _O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s320/EMNLP%202023.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google 很荣幸成为 &lt;a href=&quot;https://2023.emnlp.org/sponsors/&quot;>;钻石赞助商&lt;/a>;。 org/&quot;>;自然语言处理的经验方法&lt;/a>; (EMNLP 2023)，这是一个重要的年度会议，本周在新加坡圣淘沙举行。 Google 在今年的会议上表现强劲，收到了超过 65 篇论文，并积极参与了 11 个研讨会和教程。 Google 也很高兴成为&lt;a href=&quot;https://www.winlp.org/winlp-2023-workshop/winlp-2023-sponsors/&quot;>;主要赞助商&lt;/a>; ://www.google.com/url?q=https://www.winlp.org/winlp-2023-workshop/&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1701403043253017&amp;amp;usg=AOvVaw2oPPnFe0AEcdP1iSblNV91&quot;>;拓宽 NLP &lt;/a>; 研讨会 (WiNLP)，旨在强调人工智能和机器学习领域的人员、观点和文化的全球代表性。我们期待分享我们一些广泛的 NLP 研究，并扩大我们与更广泛的研究界的合作伙伴关系。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 我们希望您能够参观 Google 展位，与积极追求 NLP 最新创新的研究人员交谈，并查看一些预定的展位活动（例如，下面列出的演示和问答环节）。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) 和 &lt;a href=&quot;https://www.linkedin.com/showcase/googleresearch/?viewAsMember =true&quot;>;LinkedIn&lt;/a>; 帐户，了解有关 EMNLP 2023 上 Google 展位活动的更多信息。&lt;/p>; &lt;p>; 请查看下文，了解有关 EMNLP 2023 上展示的 Google 研究的更多信息（Google 附属机构&lt;strong>;粗体&lt;/strong>;）。 &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;板和线高度组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 赞助主席：&lt;strong>;&lt;em>;Shyam Upadyay&lt;/em>;&lt;/strong>; &lt;br />; 行业分会主席：&lt; strong>;&lt;em>;Imed Zitouni&lt;/em>;&lt;/strong>; &lt;br />; 高级项目委员会：&lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt;em>;&lt;strong>;Annie Louis&lt;/ &lt;strong>;&lt;/em>;、&lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Brian Roark&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展台活动&lt;/h2>; &lt;p>; &lt;i>;此时间表可能会发生变化。请访问 Google 展位了解更多信息。&lt;/i>;&lt;/p>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 开发和利用机器翻译和翻译的评估指标。改进多语言 NLP &lt;br />; 演讲者：&lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dan Deutch&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jan -Thorsten Peter&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;David Vilar Torres&lt;/em>;&lt;/strong>; &lt;br />; 12 月 8 日星期五 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; 可微搜索索引和索引生成检索&lt;br />;演讲者：&lt;strong>;&lt;em>;Sanket Vaibhav Mehta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vinh Tran&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; Kai Hui&lt;/em>;&lt;/strong>;、&lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;br />; 12 月 8 日星期五 | 3:30PM -4:00PM SST &lt;/p>; &lt;p>; 单次检索和生成&lt;br />; 演讲者：&lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;Livio Baldini Soares&lt;/em>;&lt;/strong>; &lt;br />; 12 月 9 日，星期六 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; 放大对抗性攻击&lt;br />; 演讲者：&lt;strong>; &lt;em>;Anu Sinha&lt;/em>;&lt;/strong>; &lt;br />; 12 月 9 日，星期六 | 12:30PM -1:45PM SST &lt;/p>; &lt;p>; 自动提示设计：通用自适应提示（请参阅&lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive -prompting-of-large.html&quot;>;博文&lt;/a>;) &lt;br />; 演讲者：&lt;strong>;&lt;em>;钱星辰&lt;sup>;*&lt;/sup>;&lt;/em>;&lt;/strong>;, &lt;strong>; >;&lt;em>;孙若曦&lt;/em>;&lt;/strong>; &lt;br />; 12 月 9 日，星期六 | 3:30PM -4:00PM SST &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;已接受论文&lt;/h2>; &lt;div style =&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03291.pdf&quot;>;SynJax：JAX 的结构化概率分布&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;米洛什·斯塔诺耶维奇&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;洛朗·萨特兰&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2311.11077.pdf&quot;>;适配器：用于参数高效和模块化迁移学习的统一库&lt;/a>; &lt;br />; &lt;em>;Clifton Poth&lt;/em>;、&lt;em>;Hannah Sterz&lt;/em>;、&lt; em>;Indraneil Paul&lt;/em>;、&lt;em>;Sukannya Purkayastha&lt;/em>;、&lt;em>;Leon Engländer&lt;/em>;、&lt;em>;Timo Imhof&lt;/em>;、&lt;em>;Ivan Vulić&lt;/em>;、&lt;强>;&lt;em>;塞巴斯蒂安·鲁德&lt;/em>;&lt;/strong>;、&lt;em>;伊琳娜·古列维奇&lt;/em>;、&lt;strong>;&lt;em>;乔纳斯·菲佛&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://arxiv.org/pdf/2306.08937.pdf&quot;>;DocumentNet：弥合文档预训练中的数据差距&lt;/a>; &lt;br />; &lt;em>;Lijun Yu&lt;/em>;，&lt;strong >;&lt;em>;苗金&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;孙晓宇&lt;/em>;&lt;/strong>;、&lt;em>;陈嘉仪&lt;/em>;、&lt;em>;Alexander Hauptmann&lt;/em>; >;, &lt;strong>;&lt;em>;戴汉俊&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;魏巍&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2311.08592.pdf&quot;>;AART：人工智能辅助红队为新的法学硕士支持的应用程序提供多样化的数据生成&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Bhaktipriya Radharapu&lt;/em>; &lt;/strong>;、&lt;strong>; &lt;em>;凯文·罗宾逊&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Preethi Lahoti&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.15239.pdf&quot;>;CRoW：现实世界任务中常识推理的基准测试&lt;/a>; &lt;br />; &lt;em>;Mete Ismayilzada&lt;/em>;、&lt;em>;Debjit Paul&lt;/em>;、&lt;em>;Syrielle Montariol&lt;/em>;、&lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;、&lt;em>; Antoine Bosselut&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11610.pdf&quot;>;大型语言模型可以自我改进&lt;/a>; &lt;br />; &lt;em >;黄嘉欣&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;顾世祥&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;侯乐&lt;/em>;&lt;/strong>; ,&lt;strong>; &lt;em>;吴跃新&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;王学智&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;于鸿坤&lt;/em>;&lt;/strong>; >;, &lt;em>;Jiawei Han&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14767.pdf&quot;>;剖析自回归语言模型中事实关联的回忆&lt; /a>; &lt;br />; &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Katja Filippova&lt; /em>;&lt;/strong>;，&lt;strong>;&lt;em>;阿米尔·格洛伯森&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10160.pdf&quot; >;停止以纯文本形式上传测试数据：通过评估基准减轻数据污染的实用策略&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/强>;，&lt;em>;奥马尔·戈德曼&lt;/em>;，&lt;em>;约夫·戈德堡&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.16391.pdf&quot;>;选择性标签：如何从根本上降低文档提取模型的数据标签成本&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;周一超&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;James Bradley Wendt&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;Navneet Potti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;谢静&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sandeep Tata&lt; /em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2112.12870.pdf&quot;>;测量自然语言生成模型中的归因&lt;/a>; &lt;br />; &lt;强>;&lt;em>;汉娜·拉什金&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;维塔利·尼古拉耶夫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马修·拉姆&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迈克尔·柯林斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dipanjan&lt;/em>; &lt;em>;Das&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;斯拉夫·彼得罗夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;高拉夫·辛格·托马尔&lt;/em>;&lt;/strong>;、&lt;em>; &lt;strong>;尤利亚图尔克&lt;/strong>;&lt;/em>;，&lt;strong>;&lt;em>;大卫·雷特&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.02011。 pdf&quot;>;逆缩放可以变成 U 形&lt;/a>; &lt;br />; &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/ strong>;、&lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;Quoc Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://arxiv.org/pdf/2305.14282.pdf&quot;>;INSTRUCTSCORE：通过自动反馈进行可解释文本生成评估&lt;/a>; &lt;br />; &lt;em>;Wenda Xu&lt;/em>;，&lt;em>;Danqing Wang&lt;/em>; >;、&lt;em>;潘亮明&lt;/em>;、&lt;em>;宋振桥&lt;/em>;、&lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;、&lt;em>;王威廉杨&lt;/em>;、 &lt;em>;李雷&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.14796.pdf&quot;>;论会话问答中对话历史表征的稳健性：综合研究和新的基于提示的方法&lt;/a>; &lt;br />; &lt;em>;Zorik Gekhman&lt;/em>;，&lt;em>;Nadav Oved&lt;/em>;，&lt;strong>; &lt;em>;Orgad Keller&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;伊丹·斯佩克托&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Roi Reicart&lt;/em>;&lt;/strong>;&lt;/p>;&lt;p>;&lt;a href=&quot;https: //arxiv.org/pdf/2208.04347.pdf&quot;>;研究有效扩展 Transformer 以实现长输入汇总&lt;/a>; &lt;br />; &lt;em>;Jason Phang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong >;&lt;em>;赵耀&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Peter J Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/2212.09744.pdf&quot;>;DSI++：使用新文档更新变压器内存&lt;/a>; &lt;br />; &lt;em>;Sanket Vaibhav Mehta&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>;&lt;em>; Jai Gupta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; >;Vinh Q. Tran&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;饶金峰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Marc Najork&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;艾玛·斯特鲁贝尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;唐纳德·梅茨勒&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2305.12029.pdf&quot;>;MultiTurnCleanup：多轮口语会话记录清理基准&lt;/a>; &lt;br />; &lt;em>;沉华&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>;&lt; em>;Vicky Zayats&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;约翰 C 罗霍尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丹尼尔·大卫·沃克&lt;/em>;&lt;/strong>;、&lt;强>; &lt;em>;德克·帕德菲尔德&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14318.pdf&quot;>;q2d：将问题转化为对话进行教学模特如何搜索&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shlomi Cohen-Ganor&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>; Ido Hakimi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yoad Lewenberg&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt; strong>; &lt;em>;Enav Weinreb&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02171.pdf&quot;>;具体序列中抽象状态表示的出现造型&lt;/a>; &lt;br />; &lt;em>;田云&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>;曾子来&lt;/em>;、&lt;em>;Kunal Handa&lt;/em>;、&lt;strong>; &lt;em>;Ashish V Thapliyal&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;庞波&lt;/em>;&lt;/strong>;、&lt;em>;Ellie Pavlick&lt;/em>;、&lt;em>;陈孙&lt;/em>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14332.pdf&quot;>;跨语言问答的归因评估和建模&lt;/a>; &lt;br />; &lt;em>;Benjamin Muller&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安·鲁德&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;利维奥·巴尔迪尼·苏亚雷斯&lt;/em>;&lt;/strong>; >;、&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;王欣怡&lt;/em>;&lt;/强>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2305.14281.pdf&amp;sa=D&amp;amp;source=docs&amp;amp; ust=1701762357021319&amp;amp;usg=AOvVaw2Q_4he8TIMmr8URRV1w4uX&quot;>;多模态预训练中视觉关系的弱监督学习&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Emanuele Bugliarello&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Aida Nematzadeh&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;丽莎·安妮·亨德里克斯&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/ pdf/2305.13286.pdf&quot;>;语言如何相互影响？研究 LM 微调过程中的跨语言数据共享&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni&lt;/em>;、&lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>;、&lt;em>;Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14214.pdf&quot;>;CompoundPiece：评估和提高语言模型的分解性能&lt;/a>; &lt;br>; &lt;本杰明·米尼克斯霍夫 (Benjamin Minixhofer)、&lt;strong>;&lt;em>;乔纳斯·菲佛 (Jonas Pfeiffer)&lt;/em>;&lt;/strong>;、&lt;em>;伊万·武利奇 (Ivan Vulić)&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2302.01328.pdf&quot;>;IC3：委员会共识的图像说明&lt;/a>; &lt;br />; &lt;em>;David Chan&lt;/em>;，&lt;strong>;&lt;em>;Austin Myers&lt;/em>;&lt; /strong>;,&lt;strong>;&lt;em>;Sudheendra Vijayanarasimhan&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;David A Ross&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;John Canny&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11877.pdf&quot;>;幻觉（无法）回答的奇怪案例：在隐藏的状态中寻找真相- 自信的大型语言模型&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;、&lt;em>;Omer Goldman&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、 &lt;em>;Ido Dagan&lt;/em>;、&lt;em>;Shauli Ravfogel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14542.pdf&quot;>;评估大型语言模型受控生成任务研究&lt;/a>; &lt;br />; &lt;em>;孙焦&lt;/em>;、&lt;em>;田宇飞&lt;/em>;、&lt;em>;周望春树&lt;/em>;、&lt;em>;徐楠&lt;/em>; >;, &lt;em>;胡钱&lt;/em>;, &lt;em>;拉胡尔·古普塔&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;em>;彭南云&lt;/em>;, &lt; em>;马学哲&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14324.pdf&quot;>;关系很重要：通过成对精度和关系校准对现代指标进行元评估&lt; /a>; &lt;br />; &lt;strong>;&lt;em>;丹尼尔·多伊奇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔治·福斯特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;马库斯·弗雷塔格&lt; /em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11399.pdf&quot;>;以 0.1% 的额外计算超越缩放定律&lt;/a>; &lt;br />; &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>;郑亨元&lt;sup>;*&lt;/sup>; &lt;/em>;、&lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;、&lt;em>;David R. So&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>; Siamak Shakeri&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;郑怀秀&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;饶金峰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;唐纳德&lt;/em>; &lt;em>;梅茨勒&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;斯拉夫·彼得罗夫&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;尼尔·霍尔斯比&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;Quoc V. Le&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2311.09006.pdf&quot;>;数据相似性不足以解释语言模型的性能&lt;/a>; &lt;br />; &lt;em>;Gregory Yauney&lt;sup>;*&lt;/sup>;&lt;/ em>;、&lt;strong>;&lt;em>;艾米丽·赖夫&lt;/em>;&lt;/strong>;、&lt;em>;大卫·米姆诺&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf /2311.00913.pdf&quot;>;语言模型预训练的自我影响引导数据重新加权&lt;/a>; &lt;br />; &lt;em>;Megh Thakkar&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>;&lt;em>; Tolga Bolukbasi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sriram Ganapathy&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shikhar Vashishth&lt;/em>;&lt;/strong>;、&lt;em>; Sarath Chandar &lt;/em>;，&lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11826.pdf&quot;>;重新标记：推理感知表到分析文本生成&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Deepanway Ghosal&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Preksha Nema&lt;/em>;&lt;/strong>;，&lt; strong>; &lt;em>;Aravindan Raghuveer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15265v1.pdf&quot;>;GATITOS：使用新的多语言词典适用于低资源机器翻译&lt;/a>; &lt;br />; &lt;em>;Alex Jones&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;、&lt; strong>; &lt;em>;Ishank Saxena&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.20201v1.pdf&quot;>;视频有用的多模式机器翻译&lt; /a>; &lt;br />; &lt;em>;李一航、清水秀一郎&lt;/em>;、&lt;em>;褚晨辉&lt;/em>;、&lt;em>;黑桥贞雄&lt;/em>;、&lt;strong>;&lt;em>;李伟&lt;/em>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.08298.pdf&quot;>;符号调优改善语言模型中的情境学习&lt;/a>; &lt;br / >; &lt;em>;Jerry Wei&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;乐厚&lt;/em>;&lt;/strong>;、&lt;em>; &lt;strong>;Andrew Kyle Lampinen&lt;/strong>;&lt; /em>;、&lt;em>;陈向宁&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;大黄&lt;/em>;&lt;/strong>;、&lt;em>;Yi Tay&lt;sup>;*&lt;/ super>;&lt;/em>;、&lt;strong>;&lt;em>;新云&lt;/em>; &lt;em>;陈&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;卢一峰&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;、&lt;em>;马腾宇&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Quoc V Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14755.pdf&quot;>;“不要断章取义！”文体重写对语境模型和评估的需求&lt;/a>; &lt;br />; &lt;em>;Akhila Yerukola&lt;/em>;、&lt;em>;周旭辉&lt;/em>;、&lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>; >;&lt;/strong>;、&lt;em>;Maarten Sap&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08264.pdf&quot;>;QAmeleon：只有 5 个示例的多语言 QA &lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Priyanka Agrawal&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;克里斯·阿尔贝蒂&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Fantine Huot &lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;约书亚·梅内斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;吉马&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安Ruder&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;库兹曼·甘切夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; Mirella Lapata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03540.pdf&quot;>;说、读和提示：高保真文本到-在最低限度监督下的演讲&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;尤金·哈里托诺夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;达米安·文森特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Zalán Borsos&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Raphaël Marinier&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sertan Girgin&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马特·沙里菲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Marco Tagliasacchi&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;Neil Zeghidour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09939.pdf&quot;>;AnyTOD：面向任务的可编程对话系统&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;赵杰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;曹源&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;拉加夫·古普塔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Harrison Lee&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;明秋王&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;哈根索尔陶&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;伊扎克·沙夫兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;吴永辉&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14613.pdf&quot;>;有选择地回答模棱两可的问题&lt;/a>; &lt;br />; &lt;强>;&lt;em>;杰里米·R.科尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;张俊泉&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;丹尼尔·吉利克&lt;/em>;&lt;/ &lt;strong>;、&lt;strong>;&lt;em>;朱利安·马丁·艾森施洛斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Bhuwan Dhingra&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;雅各布·爱森斯坦&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08954.pdf&quot;>;PRESTO：用于解析现实的面向任务的对话框的多语言数据集&lt;/a>;（参见&lt; a href=&quot;https://blog.research.google/2023/03/presto-multilingual-dataset-for-parsing.html&quot;>;博客文章&lt;/a>;）&lt;br />; &lt;strong>;&lt;em>;Rahul Goel &lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;瓦利德·阿马尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿迪亚·古普塔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Siddharth Vashishtha&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;佐野元树&lt;/em>;&lt;/strong>;、&lt;em>; Faiz Surani&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em >;Max Chang&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;HyunJeong Choe&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;大卫·格林&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Chuan He&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rattima Nitisaroj&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Anna&lt;/em>; &lt;em>;Trukhina&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shachi Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pararth Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rushin Shah&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;周瑜&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13281.pdf&quot;>;LM vs LM：通过交叉检查检测事实错误&lt;/a>; &lt;br />; &lt;em>;Roi Cohen&lt;/em>;、&lt;em>;May Hamri&lt;/em>;、&lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.03668.pdf&quot;>;生成套件多级多模式网页理解任务&lt;/a>; &lt;br />; &lt;em>;Andrea Burns&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Krishna Srinivasan&lt;/em>;&lt;/strong>; ,&lt;strong>;&lt;em>;约书亚·安斯利&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杰夫·布朗&lt;/em>;&lt;/strong>;、&lt;em>;布莱恩 A. 普卢默&lt;/em>;、&lt;em>; Kate&lt;/em>; &lt;em>;Saenko&lt;/em>;、&lt;strong>;&lt;em>;倪建模&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;郭曼迪&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2302.08956.pdf&amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701763216883702&amp;amp;usg =AOvVaw1QisabqC-Gy-U4ou1jbHAY&quot;>;AfriSenti：非洲语言的 Twitter 情绪分析基准&lt;/a>; &lt;br />; &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;、&lt;em>;Idris Abdulmumin&lt;/em>;、&lt;em>; Abinew Ali Ayele&lt;/em>;、&lt;em>;Nedjma Ousidhoum&lt;/em>;、&lt;em>;David Ifeoluwa Adelani&lt;/em>;、&lt;em>;Seid Muhie Yimam&lt;/em>;、&lt;em>;Ibrahim Said Ahmad&lt;/em>; , &lt;em>;Meriem Beloucif&lt;/em>;, &lt;em>;Saif M.&lt;/em>; &lt;em>;Mohammad&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>; Oumaima Hourrane&lt;/em>;、&lt;em>;Alipio Jorge&lt;/em>;、&lt;em>;Pavel Brazdil&lt;/em>;、&lt;em>;Felermino D. M&lt;/em>;。 &lt;em>;A. Ali&lt;/em>;、&lt;em>;Davis David&lt;/em>;、&lt;em>;Salomey Osei&lt;/em>;、&lt;em>;Bello Shehu-Bello&lt;/em>;、&lt;em>;Falalu Ibrahim Lawan&lt;/em>;、&lt; em>;Tajuddeen&lt;/em>; &lt;em>;Gwadabe&lt;/em>;、&lt;em>;Samuel Rutunda&lt;/em>;、&lt;em>;Tadesse Destaw Belay&lt;/em>;、&lt;em>;Wendimu Baye Messelle&lt;/em>;、&lt;em>; >; Hailu Beshada&lt;/em>; &lt;em>;Balcha&lt;/em>;、&lt;em>;Sisay Adugna Chala&lt;/em>;、&lt;em>;Hagos Tesfahun Gebrmichael&lt;/em>;、&lt;em>; Bernard Opoku&lt;/em>;、&lt;em>; >;Stephen Arthur&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13682.pdf&quot;>;通过令牌消除优化检索增强阅读器模型&lt;/a>; &lt;br / >; &lt;em>;Moshe Berchansky&lt;/em>;、&lt;em>;Peter Izsak&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、&lt;em>;Ido Dagan&lt;/em>;、&lt;em>; >;Moshe Wasserblat&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13194.pdf&quot;>;SEAHORSE：用于摘要评估的多语言、多方面数据集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;伊丽莎白·克拉克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安·格尔曼&lt;/em>;&lt;/ &lt;strong>;、&lt;strong>; &lt;em>;约书亚·梅内斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伊·阿哈罗尼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;维塔利·尼古拉耶夫&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Thibault Sellam&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Aditya Siddhan&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Dipanjan Das&lt;/em>; &lt;/strong>;，&lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13245.pdf&quot;>;GQA ：从多头检查点训练广义多查询变压器模型&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;James Lee-Thorp&lt;/a>; &lt;em>; em>;&lt;/strong>;、&lt;em>;米契尔·德容&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;尤里·泽姆良斯基&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;费德里科勒布朗&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;苏米特桑海&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09752。 pdf&quot;>;CoLT5：具有条件计算的更快的远程变压器&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;陶雷&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;米歇尔·德容&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;圣地亚哥·翁塔农&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;悉达多·梵天&lt;/em>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;尤里·泽姆良斯基&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;David Uthus&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;郭曼迪&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;詹姆斯·李-索普&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;宋云轩&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf /2310.16523.pdf&quot;>;通过集体批评和自我投票提高大型语言模型中人口表征的多样性&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>;,&lt;strong >; &lt;em>;尼古拉斯·布鲁姆&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;小马&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Raghavendra Kotikalapudi&lt;/em>;&lt;/strong>;、&lt;强>;&lt;em>;Sahitya Potluri&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Qijun Tan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Hansa Srinivasan&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;本·帕克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;艾哈迈德·贝拉米&lt;/em>;&lt;/strong>;、&lt;em>;亚历克斯·博伊特尔&lt;/em>;、&lt;strong>;&lt;em>; Jilin Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14926.pdf&quot;>;通用自适应提示&lt;/a>;（参见&lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;博客文章&lt;/a>;) &lt;br />; &lt;em>;万星辰&lt;sup >;*&lt;/sup>;&lt;/em>;、&lt;em>; &lt;strong>;孙若曦、Hootan Nakhost&lt;/strong>;&lt;/em>;、&lt;strong>;&lt;em>;戴汉俊&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;朱利安·马丁·艾森施洛斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sercan O. Arik&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;托马斯·普菲斯特&lt;/em>;&lt;/strong>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11171.pdf&quot;>;TrueTeacher：使用大型语言模型学习事实一致性评估&lt;/a>; &lt;br />; &lt;strong>;&lt; em>;佐里克·格赫曼&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·赫齐格&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伊·阿哈罗尼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Chen Elkind&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/ pdf/2310.07871.pdf&quot;>;多模态电子健康记录分层预训练&lt;/a>; &lt;br />; &lt;em>;Xiaochen Wang&lt;/em>;，&lt;em>;Junyu Luo&lt;/em>;，&lt;em>;Jiaqi Wang&lt; /em>;、&lt;em>;尹子仪&lt;/em>;、&lt;em>;崔素涵&lt;/em>;、&lt;em>;袁忠&lt;/em>;、&lt;strong>;&lt;em>;王亚庆&lt;/em>;&lt;/strong>; , &lt;em>;马凤龙&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14499.pdf&quot;>;NAIL：具有高效非自回归解码器的词汇检索索引&lt;/ a>; &lt;br />; &lt;strong>;&lt;em>;利维奥·巴尔迪尼·苏亚雷斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;丹尼尔·吉利克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杰里米·R.科尔&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;汤姆·科维亚特科斯基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11841。 pdf&quot;>;生成检索如何扩展到数百万个段落？&lt;/a>; &lt;br />; &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Kai Hui&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;庄红雷&lt;/em>;&lt;/strong>;、&lt;em>;林志颖&lt;/em>;、&lt;strong>;&lt;em>;唐纳德·梅茨勒&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.13959.pdf&quot;>;让每个例子都有意义：论自我影响力从嘈杂的 NLP 中学习的稳定性和实用性数据集&lt;/a>; &lt;br />; &lt;em>;Irina Bejan&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Artem Sokolov&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; Katja Filippova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EMNLP 的发现&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11689.pdf&quot;>;适应自我评估以提高法学硕士的选择性预测&lt;/a >; &lt;br />; &lt;em>;陈杰峰&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;尹金成&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sayna Ebrahimi&lt;/em>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sercan O Arik&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Somesh Jha &lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.10062.pdf&quot;>;工具辅助生成策略的综合评估&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mor Geva&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16568.pdf&quot;>;1-PAGER：一次性答案生成和证据检索&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05401.pdf&quot;>;MaXM：迈向多语言视觉问答&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;薛林婷、Michal Yarom&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ashish V. Thapliyal&lt;/em>;&lt;/strong>;、&lt; strong>;&lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Julien&lt;/em>;&lt;em>;Amelot&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陈曦&lt;/em>; em>;&lt;/strong>;,&lt;strong>; &lt;em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.18431v1.pdf&quot; >;SDOH-NLI：从临床记录推断健康社会决定因素的数据集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;、&lt;em>;Eric Loreaux&lt;sup >;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;塔尔舒斯特&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陈明君&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alvin Rajkomar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14815.pdf&quot;>;使用基于案例的推理进行机器阅读理解&lt;/ a>; &lt;br />; &lt;em>;Dung Ngoc Thai&lt;/em>;、&lt;em>;Dhruv Agarwal&lt;/em>;、&lt;em>;Mudit Chaudhary&lt;/em>;、&lt;em>;赵文龙&lt;/em>;、&lt;em>; Rajarshi Das&lt;/em>;、&lt;em>; Jay-Yoon Lee&lt;/em>;、&lt;em>;Hannaneh Hajishirzi&lt;/em>;、&lt;strong>;&lt;em>;Manzil Zaheer&lt;/em>;&lt;/strong>;、&lt;em>;安德鲁McCallum&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.06897.pdf&quot;>;非洲语言跨语言开放检索问答&lt;/a>; &lt;br / >; &lt;em>;Odunayo Ogundepo&lt;/em>;、&lt;em>;Tajuddeen Gwadabe&lt;/em>;、&lt;strong>;&lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jonathan H. Clark &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;、&lt;em>;David Ifeoluwa Adelani&lt;/em>;、&lt;em>;Bonaventure FP Dossou&lt;/em>;、&lt;em>; >;Abdou Aziz DIOP&lt;/em>;、&lt;em>;Claytone Sikasote&lt;/em>;、&lt;em>;Gilles HACHEME&lt;/em>;、&lt;em>;快乐 Buzaaba&lt;/em>;、&lt;em>;Ignatius Ezeani&lt;/em>;、&lt; Rooweither Mabuya&lt;/em>;、Salomey Osei&lt;/em>;、Chris&lt;/em>;、Chinenye Emezue&lt;/em>;、Albert Kahira&lt;/em>;、&lt;em>; Shamsuddeen Hassan Muhammad&lt;/em>;、&lt;em>;Akintunde Oladipo&lt;/em>;、&lt;em>;Abraham Toluwase Owodunni&lt;/em>;、&lt;em>;Atnafu Lambebo Tonja&lt;/em>;、&lt;em>;Iyanuoluwa Shode&lt;/em>;、 &lt;em>;Akari Asai&lt;/em>;、&lt;em>;Anuoluwapo Aremu&lt;/em>;、&lt;em>;Ayodele Awokoya&lt;/em>;、&lt;em>;Bernard Opoku&lt;/em>;、&lt;em>;Chiamaka Ijeoma Chukwuneke&lt;/em>; 、&lt;em>;Christine Mwase&lt;/em>;、&lt;em>;Clemencia Siro&lt;/em>;、&lt;em>;Stephen Arthur&lt;/em>;、&lt;em>;Tunde Oluwaseyi Ajayi&lt;/em>;、&lt;em>;Verrah Akinyi Otiende&lt;/em>; em>;、&lt;em>;Andre Niyongabo Rubungo&lt;/em>;、&lt;em>;Boyd Sinkala&lt;/em>;、&lt;em>;Daniel Ajisafe&lt;/em>;、&lt;em>;Emeka Felix Onwuegbuzia&lt;/em>;、&lt;em>;Falalu Ibrahim Lawan&lt;/em>;、&lt;em>;Ibrahim Said Ahmad&lt;/em>;、&lt;em>;Jesujoba Oluwadara Alabi&lt;/em>;、&lt;em>;CHINEDU EMMANUEL&lt;/em>; &lt;em>;MBONU&lt;/em>;、&lt;em>;Mofetoluwa Adeyemi&lt;/em>;、&lt;em>;Mofya Phiri&lt;/em>;、&lt;em>;Orevaoghene Ahia&lt;/em>;、&lt;em>;Ruqayya Nasir Iro&lt;/em>;、&lt;em>;Sonia Adhiambo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.08653.pdf&quot;>;概率神经总结中的不确定性校准和选择性生成：基准研究&lt;/a>; &lt;br />; &lt;strong>;&lt;em >;Polina Zablotskaia&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;杜潘&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;约书亚·梅内斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt; em>;Shashi Narayan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;任杰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;刘哲刘&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.09860.pdf&quot;>;Epsilon 采样摇滚：研究机器翻译最小贝叶斯风险解码的采样策略&lt;/a>; &lt;br />; &lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;、&lt;em>;Behrooz Ghorbani&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>;Patrick Fernandes&lt;sup>;*&lt;/sup>;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14552.pdf&quot;>;大型语言模型对推理任务的幻觉来源&lt;/a>; &lt;br />; &lt;em>;Nick McKenna&lt; /em>;、&lt;em>;李天一&lt;/em>;、&lt;em>;梁程&lt;/em>;、&lt;strong>;&lt;em>;Mohammad Javad Hosseini&lt;/em>;&lt;/strong>;、&lt;em>;马克·约翰逊&lt;/em>; >;, &lt;em>;Mark Steedman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.09017v2.pdf&quot;>;不要添加，不要错过：有效保留从预选文本跨度生成的内容&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、&lt;em>;Eran Hirsch&lt; /em>;, &lt;em>;Ido Dagan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07686.pdf&quot;>;什么使思想链提示有效？反事实研究&lt;/a>; &lt;br />; &lt;em>;Aman Madaan&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>; &lt;strong>;凯瑟琳·赫尔曼&lt;/strong>;&lt;/em>;、&lt;strong>;&lt; em>;Amir Yazdanbakhsh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03945.pdf&quot;>;使用大型语言模型理解 HTML&lt;/a>; &lt; br/>; &lt;strong>; &lt;em>; izzeddin gur &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; ofir nachum &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; yingjie miao &lt;/em>; &lt;/em>; &lt; /strong>;，&lt;strong>; &lt;em>; Mustafa Safdari &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; austin huang &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; aakanksha &lt;/em>; &lt; em>; chowdhery &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; sharan narang &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; noah fiedel &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;>; EM>; Aleksandra Faust &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09928.pdf&quot;>;通过检测和移除输入输入模型来改善摘要模型的鲁棒性噪声&lt;/a>; &lt;br />; &lt;em>; kundan krishna &lt;sup>;*&lt;/sup>; &lt;/em>;，&lt;strong>; &lt;em>; yao zhao &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; Jie Ren &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; balaji lakshminarayanan &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; >; Mohammad &lt;/em>; &lt;em>; saleh &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; ：//arxiv.org/pdf/2310.15916.pdf“>;在上下文学习创建任务矢量&lt;/a>; &lt;br />; &lt;br />; &lt;em>; &lt;em>; >; &lt;/strong>;，&lt;strong>; &lt;em>; amir globerson &lt;/em>; &lt;/em>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/pdf/2212.10544.pdf&quot;>; pre - 训练而无需注意&lt;/a>; &lt;br />; &lt;em>; junxiong wang &lt;/em>;，&lt;em>; &lt;strong>; &lt;em>; Alexander M Rush &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12441.pdf&quot;>; mux-plms：数据多路复用对于高通量语言模型&lt;/a>; &lt;br />; &lt;em>; vishvak murahari &lt;/em>;，&lt;em>; &lt;em>; Ameet Deshpande &lt;/em>;，&lt;em>; carlos e jimenez &lt;/em>;，&lt;em>; &lt;em>; &lt;strong >; izhak shafran &lt;/strong>; &lt;/em>;，&lt;strong>; &lt;em>; mingqiu wang &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; yuan &lt;/em>; &lt;/em>; &lt;em>; cao &lt;/em>; &lt;/em>; &lt;/em>; &lt;/ strong>;，&lt;em>; karthik r narasimhan &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/pdf/2310.14408.pdf&quot;>; parade parade：Parade：PARADE：通过LLMS &lt;/llms &lt;/ a>; &lt;br />; &lt;em>; andrew drozdov &lt;sup>;*&lt;/sup>; &lt;/em>;，&lt;strong>; &lt;em>; honglei zhuang &lt;/em>; &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; zhuyun dai &lt;em>; /em>; &lt;/strong>;，&lt;strong>; &lt;em>; zhen Qin &lt;/em>; &lt;/strong>;，&lt;em>; razieh rahimi &lt;/em>;，&lt;strong>; &lt;em>; xuanhui wang &lt;/em>; &lt;/em>; &lt;/strong >;，&lt;strong>; &lt;em>; dana alon &lt;/em>; &lt;/strong>;，&lt;em>; mohit iyyer &lt;/em>;，&lt;em>; andrew McCallum &lt;/em>;，&lt;em>; donald Metzler &lt;sup>;*&lt;/ sup>; &lt;/em>;，&lt;strong>; &lt;em>; kai hui &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/pdf/2310.13678.pdf&quot;>;大型语言模型上的有限状态解码约束&lt;/a>; &lt;br />; &lt;em>; Arya D. McCarthy &lt;/em>;，&lt;strong>; &lt;emt>; &lt;em>; &lt;em>; hao zhang &lt;/em em>; &lt;/em>; &lt;/em>; &lt; /strong>;，&lt;strong>; &lt;em>; shankar kumar &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; felix stahlberg &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; ke wu &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07496.pdf&quot;>;使用近似地球学&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;br />; Roy Chowdhury &lt;sup>;*&lt;/sup>; &lt;/em>;，&lt;strong>; &lt;em>; nicholas Monath &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; kumar avinava dubey &lt;/em>; &lt;/em>; &lt;/em>; &lt;/strong>;， &lt;strong>; &lt;em>; amr ahmed &lt;/em>; &lt;/strong>;，&lt;em>; snigdha chaturvedi &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://arxiv.org/pdf/pdf/2311.02883。 pdf“>; sqlprompt：具有最小标记的数据&lt;/a>; &lt;br />; &lt;br />; &lt;strong>; &lt;em>; ruoxi sun &lt;/em>; &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;emn>; &lt;em>; &lt;em>; sercan o 。 >; hanjun dai &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; pengcheng yin &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; tomas pfister &lt;/em>; &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/p>; &lt;/p>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://zi-lin.com/pdf/emnlp_2023_retrieval.pdf&quot;>;通过利用结构和不确定性&lt;/a>; &lt;br />; /em>;，&lt;strong>; &lt;em>; quan yuan &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; panupong pasupat &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; jeremiah zhe liu &lt;/em em &lt;/em em &lt;/em em >; &lt;/strong>;，&lt;strong>; &lt;em>; jingbo shang &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/pdf/2310.08740.pdf&quot;>; a带有结构化反射的计算机控制的零拍语言&lt;/a>; &lt;br />; &lt;strong>; &lt;em>; tao li &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; gang li &lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; strong>;，&lt;strong>; &lt;em>; Zhiwei Deng &lt;/em>; &lt;/strong>;，&lt;em>; bryan wang &lt;sup>;*&lt;/sup>; &lt;/em>;，&lt;strong>; &lt;emn>; &lt;em>; yang yang li &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08371.pdf&quot;>;语言基础上的实用主义者：现象，任务和建模方法&lt;/a>; &lt;/a>; &lt;br/>; &lt;br/>; >; &lt;em>; Daniel Fried &lt;/em>;，&lt;em>; nicholas tomlin &lt;/em>;，&lt;em>; jennifer hu &lt;/em>;，&lt;strong>; &lt;em>; roma patel patel &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong >; &lt;em>; aida nematzadeh &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13535.pdf&quot;>;通过成对的相反表面的积极产生的分类器鲁棒性提高分类器的鲁棒性&lt;/a>; &lt;br />; &lt;strong>; &lt;em>; ananth balashankar &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; xUezhi wang &lt;/em>; &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; yao Qin在Chen &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; ed h。 strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14224224.pdf&quot;>; mmt5：模块化多语言的预训练求解源语言幻觉&lt;/a>; &lt;br />; >; &lt;em>; jonas pfeiffer &lt;/em>; &lt;/strong>;，&lt;em>; &lt;strong>; francesco piccinno &lt;/strong>; &lt;/em>;，&lt;strong>; &lt;em>; &lt;em>; massimo nicosia &lt;/em>; &lt;/em>; &lt;/em>; &lt;/strong>;，&lt; strong>; &lt;em>; xinyi wang &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; Machel Reid &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; sebastian &lt;/em>; &lt;em>; ruder &lt;/em>; &lt;em>; em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.10551.pdf&quot;>;扩展法律与模型架构：电感偏见如何影响扩展？&lt;/a>; &lt;/a>;在&lt;/strong>;，&lt;strong>; &lt;em>; hyung won chung &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; william fedus &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; jinfeng rao &lt;/ em>; &lt;/strong>;，&lt;strong>; &lt;em>; sharan narang &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; vinh Q. tran &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;emn>; &lt;em>; dani Yogatama &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; donald Metzler &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =“ https://arxiv.org/pdf/pdf/2211.00142。 pdf“>; tata：非洲语言的多语言表与文本数据集&lt;/a>; &lt;br />; &lt;br />; &lt;strong>; &lt;em>; sebastian gehrmann &lt;/em>; &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;strong>; &lt;em>; &lt;em>; &lt;em>; &lt;em>; &lt;em>; &lt;em>; &lt;em>; /em>; &lt;/strong>;，&lt;strong>; &lt;em>; Vitaly Nikolaev，&lt;/em>; &lt;em>; Jan A. Botha &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; Michael Chavinda &lt;/em em>; &lt; /strong>;，&lt;strong>; &lt;em>; ankur p parikh &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; clara E. rivera &lt;/em>; &lt;/em>; &lt;/em>; &lt;/pr>; &lt;/p>; &lt;p>; &lt;a href =“ https://arxiv.org/pdf/2305.11938.pdf”>; xtreme-up：用于代表性不足的语言的用户 -  centric-centric scentric-data基准&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; ，&lt;/em>; &lt;/strong>; &lt;strong>; &lt;em>; Jonathan H. Clark &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; Alexander Gutkin &lt;/em>; &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;>; &lt;em >; mihir kale &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; min ma &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; massimo nicosia &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;>; &lt; em>; shruti rijhwani &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>;帕克·莱利（Parker Riley） strong>; &lt;em>; Xinyi Wang &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; John Frederick &lt;/em>; &lt;em>; wieting &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;emn>; &lt;em>; &lt;em>; &lt;em>; nitish gupta &lt; /em>; &lt;/strong>;，&lt;strong>; &lt;em>; anna katanova &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; christo kirov &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;emn>; &lt;em>; dana l Dickinson &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; brian roark &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; bidisha samanta &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; Connie Tao &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; David Ifeoluwa Adelani &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; vera axelrod &lt;/em>; &lt;/em>; &lt;/strong>; em>; isaac rayburn &lt;/em>; &lt;em>; caswell &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; colin cherry &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; dan garrette &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; reeve ingle &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>;梅尔文·约翰逊>; &lt;/strong>;，&lt;strong>; &lt;em>; partha talukdar &lt;/em>; &lt;/strong>; &lt;/p>; &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; Div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style =“ Margin-Left：20px;”>; &lt;p>; &lt;a href =“ https://www.google.com/url?q=https:///wwww .winlp.org /＆amp; sa = d＆amp; source = docs＆amp; ust = 1701752203060961＆amg = aovvaw3srozmnvyuwvyxelluogki>; nlp Workshop &lt;/a>;（winlp）&lt;br /&lt;br /&lt;br /&lt;br /&lt;br /&lt; / >; &lt;em>; sunipa dev &lt;/em>; &lt;/strong>; &lt;br />;小组成员：&lt;strong>; &lt;em>; preethi lahoti &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ https：https： //sites.google.com/corp/view/crac2023/?pli=1&quot;>;第六次有关参考，Anaphora和Coreference的计算模型&lt;/a>;（CRAC）&lt;br />; &lt;br />; &lt;br />; &lt;strong>; em>; bernd bohnet &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nlposs.github.io/2023/index.html&quot;>;自然语言处理开源的第三个研讨会软件&lt;/a>;（NLP-SOSS）&lt;br />;组织者：&lt;strong>; &lt;em>; geeticka chauhan &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ Robonlp-2023.github.io/&quot;>; combined关于机器人技术的空间语言理解和接地沟通的研讨会&lt;/a>;（splu-robonlp） /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gem-benchmark.com/workshop&quot;>;自然语言生成，评估和公制&lt;/a>;（GEM）&lt;br />;组织者：&lt; strong>; &lt;em>;伊丽莎白·克拉克（Elizabeth Clark）&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arabicnlp20233.sigarab.org/&quot;>;第一次阿拉伯语自然语言加工会议&lt;/a>; （Arabicnlp）&lt;br />;组织者：&lt;strong>; &lt;em>; imed Zitouni &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://wwwww.bigpictureworkshop.com/&quot;>;大局：制作研究叙事&lt;/a>;（BigPicture）&lt;br />;组织者：&lt;strong>; &lt;em>; nora Kassner &lt;/em>; &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>; &lt;em>; &lt;em>; sebastian ruder &lt;/em>; &lt;/em>; &lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://blackboxnlp.github.io/&quot;>; blackboxnlp 2023：第六届关于NLP的分析和解释神经网络的研讨会：&lt;strong>; &lt;em>; najoung kim &lt;/em>; &lt;/strong>; &lt;br />;小组成员：&lt;strong>; &lt;em>; &lt;em>; neel nanda &lt;/em>; &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href = “ https://www.conll.org/2023&quot;>; signll计算自然语言学习会议/strong>; &lt;br />;区域和ACS：&lt;strong>; &lt;em>; kyle Gorman &lt;/em>; &lt;/strong>;（语音和语音），&lt;strong>; &lt;em>; fei liu &lt;/em>; &lt;/em>; &lt;/strong>;（自然语言生成）&lt;/p>; &lt;p>; &lt;a href=&quot;https://sigtyp.github.io/ws2023-mrl.html&quot;>;关于多语言表示学习的第三次研讨会&lt;/a>;（mrl）&lt; BR/>;组织者：&lt;strong>; &lt;em>; Omer Goldman &lt;/em>; &lt;/strong>;，&lt;strong>; &lt;em>;塞巴斯蒂安ruder &lt;/em>; &lt;/em>; &lt;/strong>; &lt;br/>; &lt;br/>;受邀演讲者：&lt;strong>; &lt;em &lt;em >; Orhan Firat &lt;/em>; &lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;&lt;/h2>; &lt;&lt;/h2>; &lt; div style =“ margin-left：20px;”>; &lt;p>; &lt;p>; &lt;a href=&quot;https://emnlp2023-creative-nlg.github.io/&quot;>;创意自然语言生成&lt;/a>; &lt;em>; tuhin chakrabarty &lt;sup>;*&lt;/sup>; &lt;/em>; &lt;/p>; &lt;/div>; &lt;！ - 脚注 - >; &lt;hr width =“ 80％”/>; &lt;p>; &lt;span class = “ Apple-Style-Span” style =“ font-size：small;”>; &lt;b>;*&lt;/b>;＆nbsp;在Google上完成的工作&lt;/span>; &lt;/span>; &lt;/p>; &lt;/pents>; &lt;link href =&#39; http://blog.research.google/feeds/8414954445440937764241/comments/comments/default/default“ rel =” rel =“回复” title =“ post comment” type =“ application/atom+xml+xml”/>; 。 //www.blogger.com/feeds/8474926331452026626/posts/default/841414954440937764241“ /847492633145202626/posts/default/8414954440937764241“ rel =“ self” type =“ application/application/application/atom+xml”/>; &lt;link href =“ 2023.html“ rel =”替代” title =“ google at emnlp 2023” type =“ text/html”/>; &lt;ause>; &lt;names>; google ai &lt;/name>; &lt;uri>; &lt;uri>; http://wwwww.blogger.com /profile/12098626514775266161&lt;/uri>; &lt;email>; noreply@blogger.com&lt;/email>; &lt;gd:image height =“ 16” rel =“ http://schemas.google.com/g/g/g/g/2005#thumbnail” src = src = src = “ https://img1.blogblog.com/img/b16-rounded.gif” width =“ 16”>; &lt;/gd：image>; &lt;/ruter>; &lt;媒体：thumbnail height =“ 72” url =“ https：/https：/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s72-c/EMNLP%202023.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com /mrss/&quot;>; &lt;/media:thumbnail>; &lt;thr:total>; 0&lt;/thr:total>; &lt;/entry>; &lt;entry>; &lt;&lt;id>; tag：blogger.com，1999年：blog-847492633145202626. id>; &lt;Ebtubred>; 2023-12-04T14：41：00.000-08：00 &lt;/publined>; &lt;Updateed>; 2023-12-04T14：46：06.688-08：00 &lt;/fimeporated>; &lt;category Schemi /www.blogger.com/atom/ns#“ term =“ physics”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/atom/ns#” term =“量子ai”>; &lt; /类别>; &lt;类别scheme =“ http://www.blogger.com/atom/ns#” term =“ Quantum Computing”>; &lt;/cattory>; &lt;/cattory>; &lt;title type =“ text”>;带有带有古典机制的新量子算法指数加速&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由Robin Kothari和Rolando Somma发布，研究科学家，Google Research，Google Research，Quantum AI Team>; &lt;/span>; &lt;img src = &lt;img src = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s1100/glued-trees.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>;量子计算机有望比古典计算机更快地解决一些问题，但是只有少数示例具有如此巨大的加速，例如&lt;a href =“ https://en.wikipedia.org/wiki /shor％27S_ALGORITHM“>; SHOR的保理算法&lt;/a>;和&lt;a href=&quot;https://blog.research.google/2023/1023/10/developing-industrial-industrial-use-cases-for.html for.html&quot;>;量子模拟&lt;/一个>;。在这几个示例中，大多数涉及模拟固有量子机械的物理系统 - 一种自然应用量子计算机。但是，模拟不是固有量子的系统呢？量子计算机可以为此提供指数优势吗？ &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;p>;在“ &lt;a href=&quot;https://link.aps.org/doi/10.1103/physrevx.13.041041&quot;>;模拟中的指数量子加速耦合古典振荡器&lt;/a>;”，发表在&lt;a href=&quot;https://journals.aps.org/prx/&quot;>;物理评论x &lt;/a>;（prx）中，并在&lt;a href =“ ：//focs.computer.org/2023/“>;计算机科学基础的研讨会&lt;/a>;（焦点2023），我们报告发现发现了一种新的量子算法，该算法为模拟耦合&lt;a href = &lt;a href = &lt;a href = &lt;a href = &lt;a href = &lt;a href = &lt;a href = &lt;a href = &lt;a href = “ https://en.wikipedia.org/wiki/harmonic_oscillator&quot;>; classical谐波振荡器&lt;/a>;。这些是本质上最基本的，无处不在的系统，可以描述从电路到分子振动再到桥梁力学的无数天然系统的物理。与麦格理大学的Dominic Berry和多伦多大学的Nathan Wiebe合作，我们找到了一个映射，可以将涉及耦合振荡器的任何系统转换为描述量子系统时间演变的问题。给定某些限制，可以用量子计算机指数级解决此问题的速度比经典计算机更快。此外，我们使用此映射来证明量子算法有效解决的任何问题都可以被重铸为一个涉及耦合振荡器网络的问题，尽管其中许多是指数为指数。除了解锁以前未知的量子计算机应用程序外，该结果还提供了一种新的方法来设计新的量子算法，这是通过纯粹针对经典系统来设计的。 &lt;/p>; &lt;br />; &lt;h2>;模拟耦合振荡器&lt;/h2>; &lt;p>;我们认为的系统由经典的谐波振荡器组成。单个谐波振荡器的一个示例是附着在弹簧上的质量（例如球）。如果您将质量从其休息位置取代，那么弹簧将诱导恢复力，将质量推向相反方向。这种恢复力导致质量来回振荡。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/s359/oscillator.gif “ style =”边距 - 左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-foriginal-height =“ 116” data-eriginal-width =“ 359” height =“ 129” src = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/w400-h129/oscillator.gif&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;谐波振荡器的一个简单示例是弹簧到墙上连接到墙的质量。 [图像来源：&lt;a href=&quot;https://commons.wikimedia.org/wiki/file:simple_harmonic_oscillator.gif&quot;>; wikimedia &lt;/a>;] &lt;p>;现在考虑&lt;em>;耦合&lt;/em>;谐波振荡器，其中&lt;em>;多个质量通过弹簧彼此附着。取代一个质量，它将引起一波振荡，以通过系统脉冲。正如人们可能期望的那样，模拟古典计算机上大量质量的振荡变得越来越困难。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1 /s1129/image4.png“ style =” Margin-Left：auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 832” data-Original-width =“ 1129”高度=“ 295” src =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxseh-jwiedm-jwiedm-ytyxexllq6wnxehuuxhuuxhuxhing-vwlouze-plyze2gplbbbbbbekshmrfe6qupy4qjippy4qjimbbbbbbbbbbbbbbbbbbm ime4qjime4quig4quig4quig4quig4quig4quig4quig4quig4 Quikiknize4quikik umbbn ime4q.nize4quikik um兰3Z2DM8SVXDM5LLRXFID3XON38BLXBHHHEVLNF28XITJWDERMHHUAGJYLI4YMT17YTZMDKUKE-MFXJ3E9SDM-CT3LU81GS1GS1GS1/W400-H295/W400-H295/IMAGE 4.PNG400-H295/pim.4.png = />; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;一个可以通过弹簧连接的群体系统的示例系统量子算法。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;启用大量耦合的谐波振荡器的模拟，我们提出了一个编码的映射，该映射编码所有的位置和速度质量和弹簧进入Qubits系统的量子波函数。由于描述Qubits系统波函数的参数数量随量子数的数量成倍增长，因此我们可以将&lt;em>; n &lt;/em>;球的信息编码为仅有关log的量子机械系统（&lt;em>; n &lt;/em>;）Qubits。只要对系统有一个紧凑的描述（即，质量和弹簧的属性），我们就可以在以后的时间里进化波力以学习球和弹簧的坐标，而资源的资源要少得多。一种幼稚的古典方法，用于模拟球和弹簧。 &lt;/p>; &lt;p>;我们表明，可以在量子计算机上有效地模拟某些类别的耦合经典振荡器系统。但是，仅此一项并不能排除某些尚未尚未尚未熟悉的聪明古典算法的可能性，这些算法同样有效地使用了资源。为了证明我们的量子算法可以通过任何可能的经典算法实现指数加速，我们提供了另外两种证据。 &lt;/p>; &lt;br />; &lt;h2>;胶合树问题和量子甲骨文&lt;/h2>; &lt;p>;对于第一条证据，我们使用映射表明量子算法可以有效地解决著名问题关于已知的图形很难经典地解决，称为&lt;a href=&quot;https://arxiv.org/abs/quant-ph/0209131&quot;>; glued-trees reses roods &lt;/a>;。该问题需要两个分支树 - 一个图形，其节点为每个分支到另外两个节点，类似于树的分支路径 - 并将其分支通过随机的边缘粘合在一起，如下图所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s930/image2 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr caption”样式=“ text-align：center;”>;胶合树问题的视觉表示。在这里，我们从标有入口的节点开始，并允许局部探索该图，该图是通过将两个二进制树随机粘合在一起而获得的。目标是找到标有出口的节点。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;胶合树问题的目标是找到出口节点 - 第二棵树 - 尽可能有效。但是，胶合树的节点和边缘的确切配置最初隐藏在我们身上。要了解系统，我们必须查询&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/oracle_machine&quot;>; oracle &lt;/a>;，可以回答有关设置的特定问题。这个甲骨文允许我们探索树木，但仅在本地探索。几十年前，&lt;a href=&quot;https://doi.org/10.1145/780542.780552&quot;>;显示出&lt;/a>; &lt;/a>; &lt;/a>;在经典计算机上找到出口节点所需的查询数与一个多义因子成正比&lt;em>; n &lt;/em>;，节点的总数。 &lt;/p>; &lt;p>;，但是将其重新塑造为球和弹簧的问题，我们可以想象每个节点都是一个球，并且两个节点之间的每个连接都是弹簧。拔出入口节点（第一棵树的根），振荡将脉动穿过树木。到达出口节点时，只需花费时间就会用树的&lt;em>; &lt;em>;深度&lt;/em>;缩放。因此，通过将胶状树球和弹簧系统映射到量子系统并将其映射到那段时间，我们可以检测出出口节点的振动并比使用经典计算机更快地确定其指数速度。 &lt;/p>; &lt;br />; &lt;h2>; bqp completeness &lt;/h2>; &lt;p>;第二个也是最强的证据表明，我们的算法比任何可能的经典算法都更有效地揭示了通过检查一组问题A量子计算机可以有效地求解（即，可在&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/time_complexity#polynomial_time&quot;>; polynomial time &lt;/a>;中解决），称为&lt;a href =“ href =” https：https：： //en.wikipedia.org/wiki/bqp&quot;>; bounded-error量子多项式时间&lt;/a>;或bqp。 BQP中最困难的问题称为“ BQP完整”。 &lt;/p>; &lt;p>;虽然人们普遍认为存在一些量子算法可以有效解决的问题，而经典算法则无法解决，但尚未证明这一点。因此，我们可以提供的最好的证据是我们的问题是BQP完整的，也就是说，它是BQP中最困难的问题之一。如果有人要找到一种有效的经典算法来解决我们的问题，那么有效地通过量子计算机解决的每个问题都可以在经典上解决！甚至都没有&lt;a href=&quot;https://en.wikipedia.org/wiki/integer_factorization&quot;>;保理问题&lt;/a>;（找到给定的大数量的主要因素），这构成了&lt;a href =的基础“ https://en.wikipedia.org/wiki/rsa_(cryptosystem）“>; Modern Encryptight &lt;/a>;，并被Shor的算法著名地解决，预计将是BQP complete。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s574/image1 。 src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s320/image1.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;显示了BPP和BQP的可信赖关系的图，这是可以有效地有效的问题分别在古典计算机和量子计算机上解决。 BQP结合问题是BQP中最困难的问题。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>; &lt;p>;表明我们模拟球和弹簧的问题确实是bqp complete，我们从标准的BQP完整问题模拟通用量子电路，并表明每个量子电路都可以表示为许多球的系统，并结合弹簧。因此，我们的问题也是bqp complete。 &lt;/p>; &lt;br />; &lt;h2>;含义和未来的工作&lt;/h2>; &lt;p>;这项工作还阐明了2002年的工作，当时理论计算机科学家Lov K. Grover和他的同事Anirvan M. Sengupta使用了An &lt;a href=&quot;https://journals.ops.org/pra/abstract/10.1103/physreva.65.032319&quot;>;类似于耦合&lt;/a>; pendulums以说明格罗弗（Grover en.wikipedia.org/wiki/grover%27S_ALGORITHM&quot;>; search算法&lt;/a>;可以在未分类的数据库中找到正确的元素，比经典可以更快地完成。有了适当的设置和初始条件，可以判断系统在系统演变后的时间后，&lt;em>; n &lt;/em>; pendulums之一与其他&lt;em>; n &lt;/em>;那只是〜√（&lt;em>; n）&lt;/em>;。尽管这暗示了某些经典振荡系统与量子算法之间的联系，但它却没有解释为什么Grover的量子算法具有量子优势。 &lt;/p>; &lt;p>;我们的结果使该连接精确。我们表明，任何经典的谐波振荡器系统的动力学确实可以等效地理解为相应的尺寸相应量子系统的动力学。通过这种方式，我们可以在log（&lt;em>; n &lt;/em>;）量子的量子计算机上模拟Grover和Sengupta的摆系统系统，并找到可以在时间〜√（&lt;em>; &lt;em>;中找到正确元素的不同量子算法） n &lt;/em>;）。我们在古典系统和量子系统之间发现的类比可以用于构建提供指数加速的其他量子算法，在这种量子的原因中，从经典波传播的方式来看，加速的原因现在更加明显。 &lt;/p>; &lt;p>;我们的工作还表明，每个量子算法都可以等效地理解为在耦合振荡器系统中经典波的传播。这意味着，例如，我们原则上可以构建一个经典系统，该系统在其演变为时间后，该系统呈指数呈指数级的时间，而其呈指数级，而不是任何已知的经典算法的运行时间。这可能看起来像是用于保理的有效的经典算法，但是捕获的是振荡器的数量呈指数级，这使其成为解决分解的不切实际的方法。 &lt;/p>; &lt;p>;耦合的谐波振荡器本质上是普遍存在的，描述了从电路到分子链再到桥梁等结构的广泛系统。尽管我们这里的工作着重于这个广泛的问题的基本复杂性，但我们希望它将指导我们寻找谐波振荡器问题的现实示例，其中量子计算机可以提供指数优势。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢我们的量子计算科学传播者凯蒂·麦考密克（Katie McCormick）帮助撰写此博客文章。&lt;/em>; &lt;/ p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/366658657220987689888/comments/comments/default/default” rel =“ requies” >; &lt;link href =“ http://blog.research.google/2023/2023/a-new-quantum-quantum-algorithm-for-classical.html#comment-form” =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/36665865722098768768768988链接href =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/36665865722098768988” 。 />;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image高度=“16” rel=“http://schemas.google.com/g/2005#thumbnail” src=“https://img1.blogblog.com/img/b16-rounded.gif” 宽度=“16” >;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s72 -c/glued-trees.jpg“ width =” 72“ xmlns：媒体=” http：//search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr>; &lt;thr>; &lt;thr：thr>; thr>; 0 &lt;/thr：thr：thr：总计>; &lt;/entry>; &lt;enter>; &lt;id>;标签：blogger.com，1999：Blog-8474926331452026626.POST-68038970217457115 &lt;/id>; &lt;/id>; &lt;/id>; &lt;出版>; 2023-12-04t10：00.000-08：00.000-08：00.00：00.00：00.00：00.00：00.00：00.00 &lt;/00 &lt;/00 &lt;/出版>; &lt;更新>; 2023-12-04T11：29：40.224-08：00 &lt;/updated>; &lt;类别方案=“ http://www.blogger.com/atom/atom/ns#” &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ optimization”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“安全与隐私”>; &lt;/category>; &lt;title type =“ text”>;摘要报告优化在“隐私沙框归因报告”报告中api &lt;/stitle &lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author &quot;>;Posted by Hidayet Aksu, Software Engineer, and Adam Sealfon, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN-hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s320 /Hero.jpg“ style =” display：none;” />; &lt;p>;近年来，&lt;a href=&quot;https://privacysandbox.com/&quot;>;隐私沙盒&lt;/a>;启动了倡议，以探索广告商通过瞄准广告商来衡量其运动有效性的负责任方法到&lt;a href=&quot;https://blog.chromium.org/2020/01/building-more-private-web-path-path-towards.html&quot;>;贬低第三方饼干&lt;/a>; =“ https://www.gov.uk/cma-cases/investigation-into-googles-privacy-sandbox-browser-changes-”>;解决与英国竞争和市场管理局的任何竞争问题&lt;/a>;）。 &lt;a href=&quot;https://en.wikipedia.org/wiki/http_cookie#third-party_cookie&quot;>; cookies &lt;/a>;是包含网站在用户设备上存储的用户偏好的小片段；它们可用于提供更好的浏览体验（例如，允许用户自动登录）并提供相关内容或广告。隐私沙箱试图通过提供隐私保护替代方案来解决有关使用cookie在网络上跟踪数据的问题。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;p>;许多浏览器使用&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/differential_privacy&quot;>; distinial私密&lt;/a>;（DP ）提供隐私提供的API，例如&lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/attribution-reporting&quot;>;属性报告API &lt;/a>;（ARA） t依靠cookie进行广告转换测量。 ARA对单个用户操作进行加密并在汇总&lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/&quot;>;摘要报告&lt;/a>;中，估算诸如测量目标之类的诸如测量目标之类转换的数量和价值（网站上有用的操作，例如进行购买或注册邮件列表）归因于广告系列。 &lt;/p>; &lt;p>;配置API参数的任务，例如，跨不同转换分配贡献预算，对于最大化摘要报告的实用性很重要。在“ &lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;摘要报告优化的“隐私沙箱归因报告API &lt;/a”中””，我们引入了一个正式的数学框架，用于建模摘要报告。然后，我们制定了最大化摘要报告的实用性作为优化问题的问题，以获得最佳的ARA参数。最后，我们使用实际和合成数据集评估了该方法，与基线非优化摘要报告相比，证明了实用性的明显改善。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; ara摘要报告&lt;/h2>; &lt;p>;我们使用以下示例来说明我们的符号。想象一个虚构的礼品店，称为&lt;em>; du＆amp; Penc &lt;/em>;使用数字广告来吸引客户。下表捕获了他们的假期销售，每个记录都包含（i）印象ID，（ii）广告系列以及（iii）展示广告的城市以及（i）的转换功能，购买的物品数量以及（ii）这些物品的总价值。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleuserercontent.com/img/r29vz2xl/avvz2xl/avvxsejqrhtapdapdlj8o 1ukex3C3BDB5PEJQLF0UQ_MAXUXKVA1YPKVNZ5TQERTR02ENDFJJ0NNOED_RGDDDFWXLRTSCNGXENGXOE9LRGSSSSSSUPHRGSUPHRTZ6SGQROMWROMWRJPUNJPUN1JPUN1JPUN1JDDRWWWDFNFNFNFN/s.左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-foriginal-height =“ 397” data-eriginal-width =“ 1035” height =“ 245” src =“ 。 QERTR02ENDFJJ0NNOED_RGDDFWXLRTSCNGXOE9LRGSSUPHRTZ6SGQROMWRJPUNJPUN字幕“ style =” text-align：center;“>;印象和转换功能日志du＆amp; penc。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h3>;数学模型&lt; /h3>; &lt;p>; ARA摘要报告可以通过四种算法进行建模：（1）贡献向量，（2）&lt;a href =“ https://github.com/wicg/wicg/wicg/attribution-reporting-reporting-reporting-porting-api/blob/blob/main/-main/- gentregate.md＃贡献bounding-and-budgeting“>;贡献边界&lt;/a>;，（3）&lt;a href =” https://github.com/wicg/wicg/attribution-reporting-reporting-reporting-reporting-api/blob/blob/main/main/main/main/aggregation_service_service_tee。 MD？”>;摘要报告&lt;/a>;和（4）重建值。贡献边界和摘要报告由ARA执行，而贡献向量和重建值由ADTECH提供商（使企业可以买卖数字广告的工具和系统）执行。这项工作的目的是协助ADTECHS优化摘要报告算法。 &lt;/p>; &lt;p>;贡献向量算法将测量值转换为已离散和缩放的ARA格式。缩放需要考虑到每印象的总体贡献限制。在这里，我们提出了一种剪辑并执行随机舍入的方法。该算法的结果是聚集键和值的直方图。 &lt;/p>; &lt;p>;接下来，边界算法在客户端设备上运行的贡献限制，并强制对属性报告的贡献，其中任何进一步的贡献都超过了限制。输出是归因转换的直方图。 &lt;/p>; &lt;p>;摘要报告算法在服务器端运行&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/trusted_execution_environment&quot;>;可信赖的执行环境满足DP。噪声是从离散&lt;a href=&quot;https://en.wikipedia.org/wiki/laplace_distribution&quot;>; laplace Distribution &quot;>; laplace Distribution &lt;/a>;中采样的，为了执行隐私预算，可能只查询一次报告。 &lt;/p>; &lt;p>;最后，重建值算法将测量值转换回原始比例。重建值和贡献向量算法是由ADTECH设计的，并且都影响了从摘要报告中收到的实用程序。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercentent.com/img/b/r29vz2xl/avvxseh0guxyl66s0s0s0s9xvnnrlooitvsg3llooitvsg3l9k53 expimpimqumpimpimpimpimpimpimpimpempimpimpempimpempimpempimq HBITNOOVQ1_USH9RRPEKYSIXB5ZEEBN5FS9VFPU2QHST8VFHSVDZVDZV7SPVP4SCVP4SCVP4SC3FVYH4CG3PMBIPD4GZ6ETV2-MSBRRRRRRRRRRRRBRRBRRBRBRBRBRBRBYCHGE7WP1UAA8IIDVC97LA7LA7/sight tyle style边距左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Forminal-height =“ 1014” data-Original-width =“ 1999” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>; ARA摘要报告的说明用法，其中包括贡献向量（算法A），贡献边界（算法C），摘要报告（算法S）和重建值（算法R）。算法C和S固定在API中。 ADTECH设计A和R。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;/div>; &lt; H2>;误差指标&lt;/h2>; &lt;p>;选择用于评估近似质量的误差度量时，有几个因素需要考虑。为了选择特定的度量，我们考虑了误差度量的理想属性，可以进一步用作目标函数。考虑到所需的属性，我们选择了&lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/design-design-decisisions/#rmsre&quot;>; 𝜏- thunciped to a>;（RMSRE &lt;sub>; 𝜏 &lt;/sub>;）作为其属性的错误度量。有关与其他可能的指标进行详细的讨论和比较，请参见&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>; paper &lt;/a>;。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; sub>;，我们为每个切片选择一个上限参数，&lt;em>; c &lt;/em>;和隐私预算。两者的组合决定了在ADTECH侧编码的实际测量方法（例如两个转换为$ 3），然后传递给ARA以进行贡献边界算法处理。 RMSRE &lt;sub>; 𝜏 &lt;/sub>;可以准确地计算出来，因为它可以根据剪辑和噪声分布的差异表示。遵循这些步骤，我们发现RMSRE &lt;sub>; 𝜏 &lt;/sub>;对于固定的隐私预算，𝛼，&lt;sub>; &lt;/sub>;或Capping参数c，capping参数为&lt;a href =“ https：// en。 wikipedia.org/wiki/convex_optimization&quot;>; convex &lt;/a>;（因此，可以有效地获得其他参数的误差 - 限量化值），而对于关节变量（c，𝛼），它可能不会变成非convex（因此我们可能不会始终能够选择最佳参数）。无论如何，任何现成的优化器都可用于选择隐私预算和封盖参数。在我们的实验中，我们使用&lt;a href=&quot;https://docs.scipy.org/doc/doc/scipy/reference/optimize.minimize-slslsqp.html&quot;>; slsqp &lt;/a>;来自&lt;a href =“ https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html&quot;>; scipy.optimize &lt;/a>;库。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>;转换数据集。但是，由于隐私问题，可以限制或放缓此类数据，或者根本无法使用。解决这些局限性的一种方法是使用复制真实数据特征的合成数据。 &lt;/p>; &lt;p>;我们提供了一种通过现实转换数据集的统计建模来负责任地生成综合数据的方法。我们首先对实际转换数据集进行经验分析，以发现ARA的相关特征。然后，我们设计了一个使用此分布知识来创建可以通过输入参数自定义的现实合成数据集的管道。 &lt;/p>; &lt;p>;管道首先会产生从&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/power_law&quot;>; powerlaw分布&lt;/a>;（步骤1）的印象它会产生从&lt;a href=&quot;https://en.wikipedia.org/wiki/poisson_distribution&quot;>; poisson Distribution&quot;>; poisson distribution &quot;>;（步骤2）的每种印象从&lt;a href=&quot;https://en.wikipedia.org/wiki/log-normal_distribution&quot;>; log-normal分布&lt;/a>;（步骤3）。借助数据集依赖性参数，我们发现这些分布与AD-DATASET特性紧密匹配。因此，可以从历史或公共数据集中学习参数，并生成合成数据集进行实验。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s841 /image2.png“ style =”边距 - 左：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 841” data-Original-width =“ 840” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>;整体数据集生成步骤具有插图的功能。&lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;实验评估&lt;/h2>; &lt;p>;我们在三个真实世界数据集中评估了我们的算法（&lt;a href =” https://ailab.criteo.com/criteo-ponsored-search-conversion-log-dataset/&quot;>; criteo &lt;/a>;，AdTech Real Estate和AdTech Travel）和三个合成数据集。 Criteo由1500万的点击组成，房地产由100K转换组成，旅行由30K转换组成。每个数据集都将训练集和一个测试集分配为。培训集用于选择贡献预算，剪辑阈值参数和转换计数限制（实际数据集每次点击只有一个转换），并且在测试集中评估了错误。每个数据集都使用印象功能将切片划分为切片。对于实际数据集，我们考虑每个切片的三个查询；对于合成数据集，我们考虑每个切片的两个查询。 &lt;/p>; &lt;p>;对于每个查询，我们选择RMSRE &lt;sub>; &lt;/sub>; 𝜏值是培训数据集中查询中位数的五倍。这样可以确保将误差指标的不变性到数据重新缩放，并允许我们通过使用每个功能使用𝝉来结合不同尺度的特征的错误。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s1971/image5.png&quot; style=&quot;边距 - 左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Forminal-height =“ 1028” data-Original-width =“ 1971” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>;实际数据集的散点图，说明了观察转换值的概率。拟合曲线代表了最佳的对数正态分布模型，可有效捕获数据中的基本模式。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;br />; &lt;div style =“ line-height：40 ％;“>; &lt;br />; &lt;/div>; &lt;h3>;结果&lt;/h3>; &lt;p>;我们将基于优化的算法与简单基线方法进行比较。对于每个查询，基线都使用均等的贡献预算和固定的培训数据来选择剪接阈值。在现实世界和合成数据集上，我们的算法比基线产生的误差大大低。我们基于优化的方法适应隐私预算和数据。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s1999/image3 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr caption”样式=“ text-align：center;”>; rmsre &lt;ub>;τ&lt;/sub>;用于隐私预算{1、2、2、4、8、8、16、32、64}，用于我们的算法和基线三个现实世界和三个合成数据集。我们基于优化的方法始终达到的误差要比使用固定分位数作为剪辑阈值的基线较低，并在查询中平均将贡献预算平均分配。 &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;我们研究了ARA中摘要报告的优化，该报告目前已在数百个上部署数百万个镀铬浏览器。我们为ARA提供了一种严格的预算优化问题的表述，目的是为研究人员提供鲁棒的抽象，以促进实际改进。 &lt;/p>; &lt;p>;我们的食谱利用历史数据来绑定和扩展在差异隐私下的未来数据的贡献，非常普遍，适用于广告以外的设置。一种基于这项工作的方法是使用过去的数据来了解数据分布的参数，然后将其从此分布中得出的合成数据应用于对未来数据的查询的隐私预算。请参阅&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>; paper &lt;/a>;和&lt;a href =“ https://github.com/google-research/google-research/google-research/google-research/tree/tree/主/ara_optimization“>;随附的代码&lt;/a>;用于详细的算法和证明。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;这项工作是与Badih Ghazi合作完成的Pritish Kamath，Ravi Kumar，Pasin Manurangsi和Avinash Varadarajan。我们感谢Akash Nadan的帮助。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/68038970217457115/comments/comments/comments/comments/default/default” “发表注释” type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2023/12/summary-report-eptimization-in-privacy.ht-privacy.html#comment-form” rel =“回复” title =“ 0注释” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/5474926331452026626/posts/posts/posts/default/68038970217457115 type =“ application/application/atom+xml”/>; &lt;link href =“ http://www.blogger.com/feeds/847492633145202626/posts/posts/default/68038038970217457115 />; &lt;link href =“ http://blog.research.google/2023/12/summary-report-timization-in-privacy.html” rel =“替代” title =“摘要”摘要报告摘要报告优化， API“ type =” text/html“/>; &lt;under>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147775266161 &lt;/uri>; &lt;emage>; noreply@blogger。 com &lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN- HLJ1TP-SHWMIMXYFONL9CXBW11ABP89RIL2O6LDJCT8MCJQ6HWU13VMN6CQNCNCNNWSPH9D4LGO_CISNXVQKCCYSYT_SIWN_SIWN/S72-C/MERIGE.JPG s/“>; &lt;/媒体：缩略图>; &lt;thr：总计>; 0 &lt;/ thr：thr>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626. /publubly>; &lt;updated>; 2023-12-05T09:38:21.187-08:00&lt;/updated>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“ specy”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http://www.blogger.com/atom /ns＃“ term =“ translate”>; &lt;/category>; &lt;title type =“ text”>;来自单语数据&lt;/stitle>; &lt;content type>; &lt;content type =“ html”>; &lt;span class =“ byline -author&quot;>;Posted by Eliya Nachmani, Research Scientist, and Michelle Tadmor Ramanovich, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1 -3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s1600/T3.png&quot; style=&quot;display: none;&quot; />; &lt;p>;语音到讲话翻译（S2ST）是一种&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/machine_translation&quot;>;机器翻译&lt;/a>;语言到另一个。这项技术有可能打破语言障碍，并促进来自不同文化和背景的人们之间的沟通。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;之前，我们介绍了&lt;a href =“ https://ai.googleblog.com/2019/05/introducing-translatotron-end-end-end-end-end-end-end-end-end-end-eend-to-to- end.html“>; translatotron 1 &lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2021/09/high-quality-robust--and-robust-and-responsible.html&quot;>; translatototron 2 &lt;/a>; ，有史以来第一个能够直接翻译两种语言之间的语音的模型。但是，他们在有监督的设置中使用并行语音数据进行了培训。平行语音数据的稀缺性是该领域的主要挑战，因此大多数公共数据集都是从文本中半半特定或完全合成的。这为学习文本中未表示的语音属性的翻译和重建增加了额外的障碍，因此没有反映在合成的培训数据中。 &lt;/p>; &lt;p>;在这里，我们提出&lt;a href=&quot;https://arxiv.org/abs/2305.17547&quot;>; translatotron 3 &lt;/a>;，一部小说无人监督的语音对语音转换体系结构。在Translatotron 3中，我们表明可以单独从单语数据中学习语音到语音翻译任务。 This method opens the door not only to translation between more language pairs but also towards translation of the non-textual speech attributes such as pauses, speaking rates, and speaker identity. Our method does not include any direct supervision to target languages and therefore we believe it is the right direction for paralinguistic characteristics (eg, such as tone, emotion) of the source speech to be preserved across translation. To enable speech-to-speech translation, we use &lt;a href=&quot;https://arxiv.org/abs/1511.06709&quot;>;back-translation&lt;/a>;, which is a technique from unsupervised machine translation (UMT) where a synthetic translation of the source language is used to &lt;a href=&quot;https://arxiv.org/abs/1710.11041&quot;>;translate texts without bilingual text datasets&lt;/a>;. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Translatotron 3 &lt;/h2>; &lt;p>; Translatotron 3 addresses the problem of unsupervised S2ST, which can eliminate the requirement for bilingual speech datasets. To do this, Translatotron 3&#39;s design incorporates three key aspects: &lt;/p>; &lt;ol>; &lt;li>;Pre-training the entire model as a &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/html /He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html&quot;>;masked autoencoder&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;>;SpecAugment&lt;/a>;, a simple data augmentation method for speech recognition that operates on the logarithmic &lt;a href=&quot;https://en.wikipedia.org/wiki/Mel-frequency_cepstrum&quot;>;mel spectogram&lt;/a>; of the input audio (instead of the raw audio itself) and is shown to effectively improve the generalization capabilities of编码器。 &lt;/li>;&lt;li>;Unsupervised embedding mapping based on &lt;a href=&quot;https://arxiv.org/abs/1710.04087&quot;>;multilingual unsupervised embeddings&lt;/a>; (MUSE), which is trained on unpaired languages but allows the model to learn an embedding space that is shared between the source and target languages. &lt;/li>;&lt;li>;A reconstruction loss based on back-translation, to train an encoder-decoder direct S2ST model in a fully unsupervised manner. &lt;/li>; &lt;/ol>; &lt;p>; The model is trained using a combination of the unsupervised MUSE embedding loss, reconstruction loss, and S2S back-translation loss. During inference, the shared encoder is utilized to encode the input into a multilingual embedding space, which is subsequently decoded by the target language decoder. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Architecture&lt;/h3>; &lt;p>; Translatotron 3 employs a shared encoder to encode both the source and target语言。 The decoder is composed of a linguistic decoder, an acoustic synthesizer (responsible for acoustic generation of the translation speech), and a singular attention module, like Translatotron 2. However, for Translatotron 3 there are two decoders, one for the source language and another for the target language. During training, we use monolingual speech-text datasets (ie, these data are made up of speech-text pairs; they are &lt;em>;not&lt;/em>; translations). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Encoder&lt;/h3>; &lt;p>; The encoder has the same architecture as the speech encoder in the Translatotron 2. The output of the encoder is split into two parts: the first part incorporates semantic information whereas the second part incorporates acoustic information. By using the MUSE loss, the first half of the output is trained to be the MUSE embeddings of the text of the input speech spectrogram. The latter half is updated without the MUSE loss. It is important to note that the same encoder is shared between source and target languages. Furthermore, the MUSE embedding is multilingual in nature. As a result, the encoder is able to learn a multilingual embedding space across source and target languages. This allows a more efficient and effective encoding of the input, as the encoder is able to encode speech from both languages into a common embedding space, rather than maintaining a separate embedding space for each language. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Decoder&lt;/h3>; &lt;p>; Like Translatotron 2, the decoder is composed of three distinct components, namely the linguistic decoder, the acoustic synthesizer, and the attention module. To effectively handle the different properties of the source and target languages, however, Translatotron 3 has two separate decoders, for the source and target languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Two part training&lt;/h3>; &lt;p>; The training methodology consists of two parts: (1) auto-encoding with reconstruction and (2) a back-translation term. In the first part, the network is trained to auto-encode the input to a multilingual embedding space using the MUSE loss and the reconstruction loss. This phase aims to ensure that the network generates meaningful multilingual representations. In the second part, the network is further trained to translate the input spectrogram by utilizing the back-translation loss. To mitigate the issue of &lt;a href=&quot;https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=Catastrophic%20interference%2C%20also%20known%20as,information%20upon%20learning%20new%20information.&quot;>;catastrophic forgetting&lt;/a>; and enforcing the latent space to be multilingual, the MUSE loss and the reconstruction loss are also applied in this second part of training. To ensure that the encoder learns meaningful properties of the input, rather than simply reconstructing the input, we apply SpecAugment to encoder input at both phases. It has been shown to effectively improve the generalization capabilities of the encoder by augmenting the input data. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Training objective&lt;/h3>; &lt;p>; During the back-translation training phase (illustrated in the section below), the network is trained to translate the input spectrogram to the target language and then back to the source language. The goal of back-translation is to enforce the latent space to be multilingual. To achieve this, the following losses are applied: &lt;/p>; &lt;ul>; &lt;li>;MUSE loss: The MUSE loss measures the similarity between the multilingual embedding of the input spectrogram and the multilingual embedding of the back-translated spectrogram. &lt;/li>;&lt;li>;Reconstruction loss: The reconstruction loss measures the similarity between the input spectrogram and the back-translated spectrogram. &lt;/li>; &lt;/ul>; &lt;p>; In addition to these losses, SpecAugment is applied to the encoder input at both phases. Before the back-translation training phase, the network is trained to auto-encode the input to a multilingual embedding space using the MUSE loss and reconstruction loss. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MUSE loss&lt;/h3>; &lt;p>; To ensure that the encoder generates multilingual representations that are meaningful for both decoders, we employ a MUSE loss during training. The MUSE loss forces the encoder to generate such a representation by using pre-trained MUSE embeddings. During the training process, given an input text transcript, we extract the corresponding MUSE embeddings from the embeddings of the input language. The error between MUSE embeddings and the output vectors of the encoder is then minimized. Note that the encoder is indifferent to the language of the input during inference due to the multilingual nature of the embeddings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s1999 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;999&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The training and inference in Translatotron 3. Training includes the reconstruction loss via the auto-encoding path and employs the reconstruction loss via back-translation. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Audio samples&lt;/h2>; &lt;p>; Following are examples of direct speech-to-speech translation from Translatotron 3: &lt;/p>; &lt;h3>; Spanish-to-English (on Conversational dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left ： 汽车; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 Synthesized dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS reference (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance &lt;/h2>; &lt;p>; To empirically evaluate the performance of the proposed approach, we conducted experiments on English and Spanish using various datasets, including the &lt;a href=&quot;https://aclanthology.org/2020.lrec-1.520/&quot;>;Common Voice 11&lt;/a>; dataset, as well as two synthesized datasets derived from the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;Conversational&lt;/a>; and Common Voice 11 datasets. &lt;/p>; &lt;p>; The translation quality was measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; (higher is better) on ASR (automatic speech recognition) transcriptions from the translated speech, compared to the corresponding reference translation text. Whereas, the speech quality is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>; score (higher is better). Furthermore, the speaker similarity is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;average cosine similarity&lt;/a>; (higher is better). &lt;/p>; &lt;p>; Because Translatotron 3 is an &lt;em>;unsupervised&lt;/em>; method, as a baseline we used a cascaded S2ST system that is combined from ASR, unsupervised machine translation (UMT), and TTS (text-to -演讲）。 Specifically, we employ UMT that uses the nearest neighbor in the embedding space in order to create the translation. &lt;/p>; &lt;p>; Translatotron 3 outperforms the baseline by large margins in every aspect we measured: translation quality, speaker similarity, and speech quality. It particularly excelled on the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;conversational corpus&lt;/a>;. Moreover, Translatotron 3 achieves speech naturalness similar to that of the ground truth audio samples (measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>;, higher is better). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s1200/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Translation quality (measured by BLEU, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Speech similarity (measured by average cosine similarity between input speaker and output speaker, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Mean-opinion-score (measured by average MOS metric, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; As future work, we would like to extend the work to more languages and investigate whether zero-shot S2ST can be applied with the back-translation technique. We would also like to examine the use of back-translation with different types of speech data, such as noisy speech and low-resource languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The direct contributors to this work include Eliya Nachmani, Alon Levkovitch, Yifan Ding, Chulayutsh Asawaroengchai, Heiga Zhen, and Michelle Tadmor Ramanovich. We also thank Yu Zhang, Yuma Koizumi, Soroosh Mariooryad, RJ Skerry-Ryan, Neil Zeghidour, Christian Frank, Marco Tagliasacchi, Nadav Bar, Benny Schlesinger and Yonghui Wu.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8288223977991952319/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html&quot; rel=&quot;alternate&quot; title=&quot;Unsupervised speech-to-speech translation from monolingual data&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1-3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s72-c/T3.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1055595481112584523&lt;/id>;&lt;published>;2023-11-22T08:03:00.000-08:00&lt;/published>;&lt;updated>;2023-11-30T13:46:35.804-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;High-Performance Computing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Weather&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Improving simulations of clouds and their effects on climate&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Tapio Schneider, Visiting Researcher, and Yi-fan Chen, Engineering Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Today&#39;s climate models successfully capture broad global warming trends. However, because of uncertainties about processes that are &lt;a href=&quot;https://physicstoday.scitation.org/doi/abs/10.1063/PT.3.4772&quot;>;small in scale yet globally important&lt;/a>;, such as &lt;a href=&quot;http://rdcu.be/ohot&quot;>;clouds&lt;/a>; and &lt;a href=&quot;https://doi.org/10.3389/fmars.2019.00065&quot;>;ocean turbulence&lt;/a>;, these models&#39; predictions of upcoming climate changes are not very accurate in detail. For example, predictions of the time by which the global mean surface temperature of Earth will have warmed 2℃, relative to preindustrial times, &lt;a href=&quot;https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_TS.pdf&quot;>;vary by 40–50 years&lt;/a>; (a full human generation) among today&#39;s models. As a result, we do not have the &lt;a href=&quot;https://www.nature.com/articles/s41558-020-00984-6&quot;>;accurate and geographically granular predictions&lt;/a>; we need to plan resilient infrastructure, adapt supply chains to climate disruption, and assess the risks of climate-related hazards to vulnerable communities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In large part this is because clouds dominate errors and uncertainties in climate predictions for the coming decades [&lt;a href=&quot;https://doi.org/10.1029/2005GL023851&quot;>;1&lt;/a>;, &lt;a href=&quot;https://link.springer.com/article/10.1007/s00382-013-1725-9&quot;>;2&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1038/nclimate3402&quot;>;3&lt;/a>;]. Clouds reflect sunlight and exert a &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenhouse_effect&quot;>;greenhouse effect&lt;/a>;, making them crucial for regulating Earth&#39;s energy balance and mediating the response of the climate system to changes in greenhouse gas concentrations. However, they are too small in scale to be directly resolvable in today&#39;s climate models. Current climate models resolve motions at scales of tens to a hundred kilometers, with a &lt;a href=&quot;https://link.springer.com/article/10.1186/s40645-019-0304-z&quot;>;few pushing toward&lt;/a>; the &lt;a href=&quot;https://journals.ametsoc.org/view/journals/bams/101/5/bams-d-18-0167.1.xml&quot;>;kilometer-scale&lt;/a>;. However, the turbulent air motions that sustain, for example, the low clouds that cover large swaths of tropical oceans have scales of meters to tens of meters. Because of this wide difference in scale, climate models use empirical parameterizations of clouds, rather than simulating them directly, which result in large errors and uncertainties. &lt;/p>; &lt;p>; While clouds cannot be directly resolved in global climate models, their turbulent dynamics can be simulated in limited areas by using high-resolution &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_eddy_simulation&quot;>;large eddy simulations&lt;/a>; (LES). However, the high computational cost of simulating clouds with LES has inhibited broad and systematic numerical experimentation, and it has held back the generation of large datasets for training parameterization schemes to represent clouds in coarser-resolution global climate models. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;Accelerating Large-Eddy Simulations of Clouds with Tensor Processing Units&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/journal/19422466&quot;>;Journal of Advances in Modeling Earth Systems&lt;/a>;&lt;/em>; (JAMES), and in collaboration with a &lt;a href=&quot;https://clima.caltech.edu/&quot;>;Climate Modeling Alliance&lt;/a>; (CliMA) lead who is a visiting researcher at Google, we demonstrate that &lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Tensor Processing Units&lt;/a>; (TPUs) — application-specific integrated circuits that were originally developed for machine learning (ML) applications — can be effectively used to perform LES of clouds. We show that TPUs, in conjunction with tailored software implementations, can be used to simulate particularly computationally challenging &lt;a href=&quot;https://en.wikipedia.org/wiki/Marine_stratocumulus&quot;>;marine stratocumulus clouds&lt;/a>; in the conditions observed during the &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;Dynamics and Chemistry of Marine Stratocumulus&lt;/a>; (DYCOMS) field study. This successful TPU-based LES code reveals the utility of TPUs, with their large computational resources and tight interconnects, for cloud simulations. &lt;/p>; &lt;p>; Climate model accuracy for critical metrics, like precipitation or the energy balance at the top of the atmosphere, has improved roughly 10% per decade in the last 20 years. Our goal is for this research to enable a 50% reduction in climate model errors by improving their representation of clouds. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large-eddy simulations on TPUs&lt;/h2>; &lt;p>; In this work, we focus on stratocumulus clouds, which cover ~20% of the tropical oceans and are the most prevalent cloud type on earth. Current climate models are not yet able to reproduce stratocumulus cloud behavior correctly, which has been one of the largest sources of errors in these models. Our work will provide a much more accurate ground truth for large-scale climate models. &lt;/p>; &lt;p>; Our simulations of clouds on TPUs exhibit unprecedented computational throughput and scaling, making it possible, for example, to simulate stratocumulus clouds with 10× speedup over real-time evolution across areas up to about 35 × 54 km&lt;sup>;2&lt;/sup>;. Such domain sizes are close to the cross-sectional area of typical global climate model grid boxes. Our results open up new avenues for computational experiments, and for substantially enlarging the sample of LES available to train parameterizations of clouds for global climate models.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;8&quot; cellspacing=&quot;4&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s540/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s540/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of the cloud evolution from a simulation of a 285 x 285 x 2 km&lt;sup>;3&lt;/sup>; stratocumulus cloud sheet. This is the largest cloud sheet of its kind ever simulated. &lt;strong>;Left&lt;/strong>;: An oblique view of the cloud field with the camera cruising. &lt;strong>;Right&lt;/strong>;: Top view of the cloud field with the camera gradually pulled away.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The LES code is written in TensorFlow, an open-source software platform developed by Google for ML applications. The code takes advantage of TensorFlow&#39;s graph computation and &lt;a href=&quot;https://www.tensorflow.org/xla&quot;>;Accelerated Linear Algebra&lt;/a>; (XLA) optimizations, which enable the full exploitation of TPU hardware, including the high-speed, low-latency &lt;a href=&quot;https://patents.google.com/patent/US9372800&quot;>;inter-chip interconnects&lt;/a>; (ICI) that helped us achieve this unprecedented performance. At the same time, the TensorFlow code makes it easy to incorporate ML components directly within the physics-based fluid solver. &lt;/p>; &lt;p>; We validated the code by simulating canonical test cases for atmospheric flow solvers, such as a buoyant bubble that rises in neutral stratification, and a negatively buoyant bubble that sinks and impinges on the surface. These test cases show that the TPU-based code faithfully simulates the flows, with increasingly fine turbulent details emerging as the resolution increases. The validation tests culminate in simulations of the conditions during the DYCOMS field campaign. The TPU-based code reliably reproduces the cloud fields and turbulence characteristics observed by aircraft during a field campaign — a feat that is &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;notoriously difficult to achieve for LES&lt;/a>; because of the &lt;a href=&quot;https://doi.org/10.1029/2018MS001312&quot;>;rapid changes in temperature and other thermodynamic properties&lt;/a>; at the top of the stratocumulus decks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/s1023/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;815&quot; data-original-width=&quot;1023&quot; height=&quot;510&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/w640-h510/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;One of the test cases used to validate our TPU Cloud simulator. The fine structures from the density current generated by the negatively buoyant bubble impinging on the surface are much better resolved with a high resolution grid (10m, bottom row) compared to a low resolution grid (200 m, top row).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Outlook&lt;/h2>; &lt;p>; With this foundation established, our next goal is to substantially enlarge existing &lt;a href=&quot;https://doi.org/10.1029/2021MS002631&quot;>;databases&lt;/a>; of high-resolution cloud simulations that researchers building climate models can use to develop better cloud parameterizations — whether these are for physics-based models, ML models, or hybrids of the two. This requires additional physical processes beyond that described in the &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;paper&lt;/a>;; for example, the need to integrate radiative transfer processes into the code. Our goal is to generate data across a variety of cloud types, eg, thunderstorm clouds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s540/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;303&quot; data-original-width=&quot;540&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of a thunderstorm simulation using the same simulator as the stratocumulus simulation work. Rainfall can also be observed near the ground.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; This work illustrates how advances in hardware for ML can be surprisingly effective when repurposed in other research areas — in this case, climate modeling. These simulations provide detailed training data for processes such as in-cloud turbulence, which are not directly observable, yet are crucially important for climate modeling and prediction. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank the co-authors of the paper: Sheide Chammas, Qing Wang, Matthias Ihme, and John Anderson. We&#39;d also like to thank Carla Bromberg, Rob Carver, Fei Sha, and Tyler Russell for their insights and contributions to the work.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1055595481112584523/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and.html&quot; rel=&quot;alternate&quot; title=&quot;Improving simulations of clouds and their effects on climate&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3970135078050750650&lt;/id>;&lt;published>;2023-11-21T10:09:00.000-08:00&lt;/published>;&lt;updated>;2023-11-21T10:09:33.541-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Open sourcing Project Guideline: A platform for computer vision accessibility technology&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dave Hawkey, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Two years ago we &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;announced Project Guideline&lt;/a>;, a collaboration between Google Research and &lt;a href=&quot;https://www.guidingeyes.org/&quot;>;Guiding Eyes for the Blind&lt;/a>; that enabled people with visual impairments (eg, blindness and low-vision) to walk, jog, and run independently. Using only a Google Pixel phone and headphones, Project Guideline leverages on-device machine learning (ML) to navigate users along outdoor paths marked with a painted line. The technology has been &lt;a href=&quot;https://projectguidelinejp.withgoogle.com/intl/en/&quot;>;tested all over the world&lt;/a>; and even demonstrated during the &lt;a href=&quot;https://www.youtube.com/live/2cW1-plwqeQ?si=MTIX2uJkyWuLluht&amp;amp;t=7334&quot;>;opening ceremony at the Tokyo 2020 Paralympic Games&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Since the original announcement, we set out to improve Project Guideline by embedding new features, such as obstacle detection and advanced path planning, to safely and reliably navigate users through more complex scenarios (such as sharp turns and nearby pedestrians). The early version featured a simple frame-by-frame image segmentation that detected the position of the path line relative to the image frame. This was sufficient for orienting the user to the line, but provided limited information about the surrounding environment. Improving the navigation signals, such as alerts for obstacles and upcoming turns, required a much better understanding and mapping of the users&#39; environment. To solve these challenges, we built a platform that can be utilized for a variety of spatially-aware applications in the accessibility space and beyond. &lt;/p>; &lt;p>; Today, we announce the &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;open source release of Project Guideline&lt;/a>;, making it available for anyone to use to improve upon and build new accessibility experiences. The release includes &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;source code&lt;/a>; for the core platform, an &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/android&quot;>;Android application&lt;/a>;, pre-trained &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/vision/models&quot;>;ML models&lt;/a>;, and a &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/unreal&quot;>;3D simulation framework&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;System design&lt;/h2>; &lt;p>; The primary use-case is an Android application, however we wanted to be able to run, test, and debug the core logic in a variety of environments in a reproducible way. This led us to design and build the system using C++ for close integration with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; and other core libraries, while still being able to integrate with Android using the &lt;a href=&quot;https://developer.android.com/ndk&quot;>;Android NDK&lt;/a>;. &lt;/p>; &lt;p>; Under the hood, Project Guideline uses &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; to estimate the position and orientation of the user as they navigate the课程。 A segmentation model, built on the &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLabV3+&lt;/a>; framework, processes each camera frame to generate a binary mask of the guideline (see the &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;previous blog post&lt;/a>; for more details). Points on the segmented guideline are then projected from image-space coordinates onto a world-space ground plane using the camera pose and lens parameters (intrinsics) provided by ARCore. Since each frame contributes a different view of the line, the world-space points are aggregated over multiple frames to build a virtual mapping of the real-world guideline. The system performs piecewise curve approximation of the guideline world-space coordinates to build a spatio-temporally consistent trajectory. This allows refinement of the estimated line as the user progresses along the path. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s800/image1.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;355&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Project Guideline builds a 2D map of the guideline, aggregating detected points in each frame (&lt;strong>;red&lt;/strong>;) to build a stateful representation (&lt;strong>;blue&lt;/strong>;) as the runner progresses along the path.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A control system dynamically selects a target point on the line some distance ahead based on the user&#39;s current position, velocity,和方向。 An audio feedback signal is then given to the user to adjust their heading to coincide with the upcoming line segment. By using the runner&#39;s velocity vector instead of camera orientation to compute the navigation signal, we eliminate noise caused by irregular camera movements common during running. We can even navigate the user back to the line while it&#39;s out of camera view, for example if the user overshot a turn. This is possible because ARCore continues to track the pose of the camera, which can be compared to the stateful line map inferred from previous camera images. &lt;/p>; &lt;p>; Project Guideline also includes obstacle detection and avoidance features. An ML model is used to estimate depth from single images. To train this monocular depth model, we used &lt;a href=&quot;https://blog.research.google/2023/10/sanpo-scene-understanding-accessibility.html&quot;>;SANPO&lt;/a>;, a large dataset of outdoor imagery from urban, park, and suburban environments that was curated in-house. The model is capable of detecting the depth of various obstacles, including people, vehicles, posts, and more. The depth maps are converted into 3D point clouds, similar to the line segmentation process, and used to detect the presence of obstacles along the user&#39;s path and then alert the user through an audio signal. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/s800/image2.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; height=&quot;480&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/w640-h480/image2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using a monocular depth ML model, Project Guideline constructs a 3D point cloud of the environment to detect and alert the user of potential obstacles along the path .&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A low-latency audio system based on the &lt;a href=&quot;https://developer.android.com/ndk/guides/audio/ aaudio/aaudio&quot;>;AAudio API&lt;/a>; was implemented to provide the navigational sounds and cues to the user. Several &lt;em>;sound packs&lt;/em>; are available in Project Guideline, including a spatial sound implementation using the &lt;a href=&quot;https://resonance-audio.github.io/resonance-audio/&quot;>;Resonance Audio API&lt; /a>;. The sound packs were developed by a team of sound researchers and engineers at Google who designed and tested many different sound models. The sounds use a combination of panning, pitch, and spatialization to guide the user along the line. For example, a user veering to the right may hear a beeping sound in the left ear to indicate the line is to the left, with increasing frequency for a larger course correction. If the user veers further, a high-pitched warning sound may be heard to indicate the edge of the path is approaching. In addition, a clear “stop” audio cue is always available in the event the user veers too far from the line, an anomaly is detected, or the system fails to provide a navigational signal. &lt;/p>; &lt;p>; Project Guideline has been built specifically for Google Pixel phones with the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Google Tensor&lt;/a>; chip. The Google Tensor chip enables the optimized ML models to run on-device with higher performance and lower power consumption. This is critical for providing real-time navigation instructions to the user with minimal delay. On a Pixel 8 there is a 28x latency improvement when running the depth model on the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Tensor Processing Unit&lt;/a>; (TPU) instead of CPU, and 9x improvement compared to GPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s1124/image3 .jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;748&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Testing and simulation&lt;/h2>; &lt;p>; Project Guideline includes a simulator that enables rapid testing and prototyping of the system in a virtual environment. Everything from the ML models to the audio feedback system runs natively within the simulator, giving the full Project Guideline experience without needing all the hardware and physical environment set up. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/s1200/image4.jpg&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1200&quot; height=&quot;480&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/w640-h480/image4.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Screenshot of Project Guideline simulator.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line- height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; To launch the technology forward, &lt;a href=&quot;https://www.wear.works&quot;>;WearWorks &lt;/a>; has become an early adopter and teamed up with Project Guideline to integrate their patented haptic navigation experience, utilizing haptic feedback in addition to sound to guide runners. WearWorks has been developing haptics for over 8 years, and previously empowered the first blind marathon runner to complete the NYC Marathon without sighted assistance. We hope that integrations like these will lead to new innovations and make the world a more accessible place.&lt;br />; &lt;/p>; &lt;p>; The Project Guideline team is also working towards removing the painted line completely, using the latest advancements in mobile ML technology, such as the &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;ARCore Scene Semantics API&lt;/a>;, which can identify sidewalks, buildings, and other objects in outdoor scenes. We invite the accessibility community to build upon and improve this technology while exploring new use cases in other fields. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of Project Guideline and the technologies behind it. We&#39;d like to thank Project Guideline team members: Dror Avalon, Phil Bayer, Ryan Burke, Lori Dooley, Song Chun Fan, Matt Hall, Amélie Jean-aimée, Dave Hawkey, Amit Pitaru, Alvin Shi, Mikhail Sirotenko, Sagar Waghmare, John Watkinson, Kimberly Wilber, Matthew Willson, Xuan Yang, Mark Zarich, Steven Clark, Jim Coursey, Josh Ellis, Tom Hoddes, Dick Lyon, Chris Mitchell, Satoru Arao, Yoojin Chung, Joe Fry, Kazuto Furuichi, Ikumi Kobayashi, Kathy Maruyama, Minh Nguyen, Alto Okamura, Yosuke Suzuki, and Bryan Tanaka. Thanks to ARCore contributors: Ryan DuToit, Abhishek Kar, and Eric Turner. Thanks to Alec Go, Jing Li, Liviu Panait, Stefano Pellegrini, Abdullah Rashwan, Lu Wang, Qifei Wang, and Fan Yang for providing ML platform support. We&#39;d also like to thank Hartwig Adam, Tomas Izo, Rahul Sukthankar, Blaise Aguera y Arcas, and Huisheng Wang for their leadership support. Special thanks to our partners Guiding Eyes for the Blind and Achilles International.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3970135078050750650/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html&quot; rel=&quot;alternate&quot; title=&quot;Open sourcing Project Guideline: A platform for computer vision accessibility technology&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6563276834599281497&lt;/id>;&lt;published>;2023-11-17T11:36:00.000-08:00&lt;/published>;&lt;updated>;2023-11-17T11:36:32.036-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research Awards&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;University Relations&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Emerging practices for Society-Centered AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Anoop Sinha, Research Director, Technology &amp;amp; Society, and Yossi Matias, Vice President, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The first of &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google&#39;s AI Principles&lt;/a>; is to “Be socially beneficial.” As AI practitioners, we&#39;re inspired by the transformative potential of AI technologies to benefit society and our shared environment at a scale and swiftness that wasn&#39;t possible before. From &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;helping address the climate crisis&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/how-were-using-ai-to-help-transform-healthcare/&quot;>;helping transform healthcare&lt;/a>;, to &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/global-accessibility-awareness-day-google-product-update/&quot;>;making the digital world more accessible&lt;/a>;, our goal is to apply AI responsibly to be helpful to more people around the globe. Achieving global scale requires researchers and communities to think ahead — and act — collectively across the AI ecosystem. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We call this approach Society-Centered AI. It is both an extension and an expansion of &lt;a href=&quot;https://hcil.umd.edu/human-centered-ai/&quot;>;Human-Centered AI, &lt;/a>;focusing on the aggregate needs of society that are still informed by the needs of individual users, specifically within the context of the larger, shared human experience. Recent AI advances offer unprecedented, societal-level capabilities, and we can now methodically address those needs — if we apply collective, multi-disciplinary AI research to society-level, shared challenges, from forecasting hunger to predicting diseases to improving productivity. &lt;/p>; &lt;p>; The &lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/AI_Opportunity_Agenda.pdf&quot;>;opportunity for AI&lt;/a>; to benefit society increases each天。 We took a look at our work in these areas and at the research projects we have supported. Recently, Google &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;announced that 70 professors were selected&lt;/a>; for the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;2023 Award for Inclusion Research Program&lt;/a>;, which supports academic research that addresses the needs of historically marginalized groups globally. Through evaluation of this work, we identified a few emerging practices for Society-Centered AI: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Understand society&#39;s needs&lt;/strong>; &lt;br />;Listening to communities and partners is crucial to understanding major issues deeply and identifying priority challenges to address. As an emerging general purpose technology, AI has the potential to address major global societal issues that can significantly impact people&#39;s lives (eg, educating workers, improving healthcare, and improving productivity). We have found the key to impact is to be centered on society&#39;s needs. For this, we focus our efforts on goals society has agreed should be prioritized, such as the United Nations&#39; &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/globalgoals&quot;>;17 Sustainable Development Goals&lt;/a>;, a set of interconnected goals jointly developed by more than 190 countries to address global challenges. &lt;/li>;&lt;li>;&lt;strong>;Collective efforts to address those needs &lt;br />;&lt;/strong>;Collective efforts bring stakeholders (eg, local and academic communities, NGOs, private-public collaborations) into a joint process of design, development, implementation, and evaluation of AI technologies as they are being developed and deployed to address societal needs. &lt;/li>;&lt;li>;&lt;strong>;Measuring success by how well the effort addresses society&#39;s needs &lt;br />;&lt;/strong>;It is important and challenging to measure how well AI solutions address society&#39;s needs. In each of our cases, we identified primary and secondary indicators of impact that we optimized through our collaborations with stakeholders. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Why is Society-Centered AI important?&lt;/h2>; &lt;p>; The case examples described below show how the Society-Centered AI approach has led to impact across topics, such as accessibility, health, and climate. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Understanding the needs of individuals with non-standard speech&lt;/h3>; &lt;p>; There are &lt;a href=&quot;https://www.nidcd.nih.gov/health/statistics/quick-statistics-voice-speech-language&quot;>;millions of people&lt;/a>; with non-standard speech (eg, impaired articulation, &lt;a href=&quot;https://en.wikipedia.org/wiki/Dysarthria&quot;>;dysarthria&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Spasmodic_dysphonia&quot;>;dysphonia&lt;/a>;) in the United States alone. In &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/impaired-speech-recognition/&quot;>;2019&lt;/a>;, Google Research launched &lt;a href=&quot;https://blog.research.google/2019/08/project-euphonias-personalized-speech.html&quot;>;Project Euphonia&lt;/a>;, a methodology that allows individual users with non-standard speech to train personalized speech recognition models. Our success began with the impact we had on each individual who is now able to use voice dictation on their mobile device. &lt;/p>; &lt;p>; Euphonia started with a Society-Centered AI approach, including collective efforts with the non-profit organizations &lt;a href=&quot;http://als.net/&quot;>;ALS Therapy Development Institute&lt;/a>; and &lt;a href=&quot;http://www.alsri.org/&quot;>;ALS Residence Initiative&lt;/a>; to understand the needs of individuals with &lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;amyotrophic lateral sclerosis&lt;/a>; (ALS) and their ability to use automatic speech recognition systems. Later, we developed &lt;a href=&quot;https://blog.research.google/2021/09/personalized-asr-models-from-large-and.html&quot;>;the world&#39;s largest corpus&lt;/a>; of non-standard speech recordings, which enabled us to train a &lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; to &lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;better recognize disordered speech by 37%&lt;/a>; on real conversation &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;word error rate&lt;/a>; (WER) measurement. This also led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/speech-accessibility-project/&quot;>;2022&lt;/a>; collaboration between the University of Illinois Urbana-Champaign, Alphabet, Apple, Meta, Microsoft, and Amazon to begin the &lt;a href=&quot;https://speechaccessibilityproject.beckman.illinois.edu/&quot;>;Speech Accessibility Project&lt;/a>;, an ongoing initiative to create a publicly available dataset of disordered speech samples to improve products and make speech recognition more inclusive of diverse speech patterns. Other technologies that use AI to help remove barriers of modality and languages, include &lt;a href=&quot;https://about.google/stories/making-conversation-more-accessible-with-live-transcribe/&quot;>;live transcribe&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/first-time-i-was-able-call-my-23-year-old-son/&quot;>;live caption&lt;/a>; and &lt;a href=&quot;https://blog.google/intl/en-in/products/explore-communicate/easier-access-to-web-pages-let/&quot;>;read aloud&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Focusing on society&#39;s health needs&lt;/h3>; &lt;p>; Access to timely maternal health information can save lives globally: &lt;a href=&quot;https://www.who.int/news/item/23-02-2023-a-woman-dies-every-two-minutes-due-to-pregnancy-or-childbirth--un-agencies&quot;>;every two minutes a woman dies during pregnancy or childbirth&lt;/a>; and &lt;a href=&quot;https://data.unicef.org/topic/child-survival/under-five-mortality/&quot;>;1 in 26 children die before reaching age five&lt;/a>;. In rural India, the education of expectant and new mothers around key health issues pertaining to pregnancy and infancy required scalable, low-cost technology solutions. Together with &lt;a href=&quot;https://armman.org/&quot;>;ARMMAN&lt;/a>;, Google Research supported &lt;a href=&quot;https://blog.research.google/2022/08/using-ml-to-boost-engagement-with.html&quot;>;a program&lt;/a>; that uses mobile messaging and machine learning (ML) algorithms to predict when women might benefit from receiving interventions (ie, targeted preventative care information) and encourages them to engage with the &lt;a href=&quot;https://armman.org/mmitra/&quot;>;mMitra&lt;/a>; free voice call program. Within a year, the mMitra program has shown a 17% increase in infants with tripled birth weight and a 36% increase in women understanding the importance of taking iron tablets during pregnancy. Over 175K mothers and growing have been reached through this automated solution, which public health workers use to improve the quality of information delivery. &lt;/p>; &lt;p>; These efforts have been successful in improving health due to the close collective partnership among the community and those building the AI technology. We have adopted this same approach via collaborations with caregivers to address a variety of medical needs. Some examples include: the use of the &lt;a href=&quot;https://health.google/caregivers/arda/&quot;>;Automated Retinal Disease Assessment&lt;/a>; (ARDA) to &lt;a href=&quot;https://blog.google/technology/health/5-myths-about-medical-ai-debunked/&quot;>;help screen for diabetic retinopathy&lt;/a>; in 250,000 patients in clinics around the world; our partnership with &lt;a href=&quot;https://www.icadmed.com/&quot;>;iCAD&lt;/a>; to bring our &lt;a href=&quot;https://blog.google/technology/ai/icad-partnership-breast-cancer-screening/&quot;>;mammography&lt;/a>; AI models to clinical settings to aid in breast cancer detection; and the development of &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>;, a medical large language model that is now being &lt;a href=&quot;https: //cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model&quot;>;tested with Cloud partners&lt;/a>; to help doctors provide better病人护理。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Compounding impact from sustained efforts for crisis response&lt;/h3>; &lt;p>; Google Research&#39;s flood prediction efforts began in 2018 with &lt;a href=&quot;https://www.blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;flood forecasting&lt;/a>; in India and &lt;a href=&quot;https://www.undp.org/bangladesh/blog/climate-change-google-and-bangladesh-floods&quot;>;expanded to Bangladesh&lt;/a>; to help combat the catastrophic damage from yearly floods. The initial efforts began with partnerships with &lt;a href=&quot;https://cwc.gov.in/&quot;>;India&#39;s Central Water Commission&lt;/a>;, local governments and communities. The implementation of these efforts used &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>; on Search and Maps, and, more recently, broadly expanded access via &lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>;. Continued &lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;collaborations&lt;/a>; and advancing an AI-based global flood forecasting model allowed us to &lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;expand this capability&lt;/a>; to &lt;a href=&quot;https://blog .google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;over 80 countries&lt;/a>; across Africa, the Asia-Pacific region, Europe, and South, Central, and North美国。 We also partnered with networks of community volunteers to further amplify flood alerts. By working with governments and communities to measure the impact of these efforts on society, we refined our approach and algorithms each year. &lt;/p>; &lt;p>; We were able to leverage those methodologies and some of the underlying technology, such as &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>;, from &lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;flood forecasting&lt;/a>; to similar societal needs, such as &lt;a href=&quot;https://blog.google/products/search/mapping-wildfires-with-satellite-data/&quot;>;wildfire forecasting&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;heat alerts&lt;/a>;. Our continued engagements with organizations led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/early-warning-system-wmo-google/&quot;>;support of additional efforts&lt;/a>;, such as the World Meteorological Organization&#39;s (WMO) &lt;a href=&quot;https://public.wmo.int/en/earlywarningsforall&quot;>;Early Warnings For All Initiative&lt;/a>;. The continued engagement with communities has allowed us to learn about our users&#39; needs on a societal level over time, expand our efforts, and compound the societal reach and impact of our efforts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Further supporting Society-Centered AI research&lt;/h2>; &lt;p>; &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;We recently funded&lt;/a>; 18 university research proposals exemplifying a Society-Centered AI approach, a new track within the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;Google Award for Inclusion Research Program&lt;/a>;. These researchers are taking the Society-Centered AI methodology and helping create beneficial applications across the world. Examples of some of the projects funded include: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;AI-Driven Monitoring of Attitude Polarization in Conflict-Affected Countries for Inclusive Peace Process and Women&#39;s Empowerment:&lt;/strong>; This project&#39;s goal is to create LLM-powered tools that can be used to monitor peace in online conversations in developing nations. The initial target communities are where peace is in flux and the effort will put a particular emphasis on mitigating polarization that impacts women and promoting harmony.&lt;/li>; &lt;li>;&lt;strong>;AI-Assisted Distributed Collaborative Indoor Pollution Meters: A Case Study , Requirement Analysis, and Low-Cost Healthy Home Solution for Indian Communities: &lt;/strong>;This project is looking at the usage of low-cost pollution monitors combined with AI-assisted methodology for identifying recommendations for communities to improve air quality and at home健康。 The initial target communities are highly impacted by pollution, and the joint work with them includes the goal of developing how to measure improvement in outcomes in the local community. &lt;/li>; &lt;li>;&lt;strong>;Collaborative Development of AI Solutions for Scaling Up Adolescent Access to Sexual and Reproductive Health Education and Services in Uganda: &lt;/strong>;This project&#39;s goal is to create LLM-powered tools to provide personalized coaching and learning for users&#39; needs on topics of sexual and reproductive health education in low-income settings in Sub-Saharan Africa. The local societal need is significant, with an estimated &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9652643/&quot;>;25% rate of teenage pregnancy&lt;/a>;, and the project aims to address the needs with a collective development process for the AI solution.&lt;strong>; &lt;/strong>; &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; Focusing on society&#39;s needs, working via multidisciplinary collective research, and measuring the impact on society helps lead to AI solutions that are relevant, long-lasting, empowering, and beneficial. See the &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/&quot;>;AI for the Global Goals&lt;/a>; to learn more about potential Society-Centered AI research problems. &lt;a href=&quot;https://blog.google/outreach-initiatives/google-org/httpsbloggoogleoutreach-initiativesgoogle-orgunited-nations-global-goals-google-ai-/&quot;>;Our efforts&lt;/a>; with non-profits in these areas is complementary to the research that we are doing and encouraging. We believe that further initiatives using Society-Centered AI will help the collective research community solve problems and positively impact society at large. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many thanks to the many individuals who have worked on these projects at Google including Shruti Sheth, Reena Jana, Amy Chung-Yu Chou, Elizabeth Adkison, Sophie Allweis, Dan Altman, Eve Andersson, Ayelet Benjamini&lt;/em>;, &lt;em>;Julie Cattiau, Yuval Carny, Richard Cave, Katherine Chou, Greg Corrado, Carlos De Segovia, Remi Denton, Dotan Emanuel, Ashley Gardner, Oren Gilon, Taylor Goddu, Brigitte Hoyer Gosselink, Jordan Green, Alon Harris&lt;/em>;, &lt;em>;Avinatan Hassidim, Rus Heywood, Sunny Jansen, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Ronit Levavi Morad, Bob MacDonald, Alicia Martin, Shakir Mohamed, Philip Nelson, Moriah Royz, Katie Seaver, Joel Shor, Milind Tambe, Aparna Taneja, Divy Thakkar, Jimmy Tobin, Katrin Tomanek, Blake Walsh, Gal Weiss, Kasumi Widner, Lihong Xi, and teams.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6563276834599281497/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot; rel=&quot;alternate&quot; title=&quot;Emerging practices for Society-Centered AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6928081948246813203&lt;/id>;&lt;published>;2023-11-16T13:11:00.000-08:00&lt;/published>;&lt;updated>;2023-12-07T08:16:11.190-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Adversarial testing for generative AI safety&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kathy Meier-Hellstern, Building Responsible AI &amp;amp; Data Systems, Director, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The&lt;em>; &lt;/em>;&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; (RAI-HCT) team within Google Research is committed to advancing the theory and practice of responsible human-centered AI through a lens of culturally-aware research, to meet the needs of billions of users today, and blaze the path forward for a better AI future. The BRAIDS (Building Responsible AI Data and Solutions) team within RAI-HCT aims to simplify the adoption of RAI practices through the utilization of scalable tools, high-quality data, streamlined processes, and novel research with a current emphasis on addressing the unique challenges posed by &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>; (GenAI). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; GenAI models have enabled unprecedented capabilities leading to a rapid surge of innovative applications. Google actively leverages &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;GenAI&lt;/a>; to &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview&quot;>;enhance&lt;/a>; its &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai-in-workspace-now-available&quot;>;products&#39; utility&lt;/a>; and to improve lives. While enormously beneficial, GenAI also presents risks for disinformation, bias, and security. In 2018, Google pioneered the &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>;, emphasizing beneficial use and prevention of harm. Since then, Google has focused on effectively implementing our principles in &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; through 1) a comprehensive risk assessment framework, 2) internal governance structures, 3) education, empowering Googlers to integrate AI Principles into their work, and 4) the development of processes and tools that identify, measure, and analyze ethical risks throughout the lifecycle of AI-powered products. The BRAIDS team focuses on the last area, creating tools and techniques for identification of ethical and safety risks in GenAI products that enable teams within Google to apply appropriate mitigations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;What makes GenAI challenging to build responsibly?&lt;/h2>; &lt;p>; The unprecedented capabilities of GenAI models have been accompanied by a new spectrum of potential failures, underscoring the urgency for a comprehensive and systematic RAI approach to understanding and mitigating potential safety concerns before the model is made broadly available. One key technique used to understand potential risks is &lt;em>;adversarial testing&lt;/em>;, which is testing performed to systematically evaluate the models to learn how they behave when provided with malicious or inadvertently harmful inputs across a range of scenarios. To that end, our research has focused on three directions: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Scaled adversarial data generation&lt;/em>;&lt;br />; Given the diverse user communities, use cases, and behaviors, it is difficult to comprehensively identify critical safety issues prior to launching a product or service. Scaled adversarial data generation with humans-in-the-loop addresses this need by creating test sets that contain a wide range of diverse and potentially unsafe model inputs that stress the model capabilities under adverse circumstances. Our unique focus in BRAIDS lies in identifying societal harms to the diverse user communities impacted by our models. &lt;/li>; &lt;li>;&lt;em>;Automated test set evaluation and community engagement&lt;/em>;&lt;br />; Scaling the testing process so that many thousands of model responses can be quickly evaluated to learn how the model responds across a wide range of potentially harmful scenarios is aided with automated test set evaluation. Beyond testing with adversarial test sets, community engagement is a key component of our approach to identify “unknown unknowns” and to seed the data generation process.&lt;/li>; &lt;li>;&lt;em>;Rater diversity&lt;/em>;&lt;br />; Safety evaluations rely on human judgment, which is shaped by community and culture and is not easily automated. To address this, we prioritize research on rater diversity.&lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scaled adversarial data generation&lt;/h2>; &lt;p>; High-quality, comprehensive data underpins many key programs across Google. Initially reliant on manual data generation, we&#39;ve made significant strides to automate the adversarial data generation process. A centralized data repository with use-case and policy-aligned prompts is available to jump-start the generation of new adversarial tests. We have also developed multiple synthetic data generation tools based on large language models (LLMs) that prioritize the generation of data sets that reflect diverse societal contexts and that integrate data quality metrics for improved dataset quality and diversity. &lt;/p>; &lt;p>; Our data quality metrics include: &lt;/p>; &lt;ul>; &lt;li>;Analysis of language styles, including query length, query similarity, and diversity of language styles.&lt;/li>; &lt;li>;Measurement across a wide range of societal and multicultural dimensions, leveraging datasets such as &lt;a href=&quot;https://github.com/google-research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>;, &lt;a href=&quot;https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;, the &lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;Societal Context Repository&lt;/a>;.&lt;/li>; &lt;li>;Measurement of alignment with Google&#39;s &lt;a href=&quot;https://policies.google.com/terms/generative-ai/use-policy&quot;>;generative AI policies&lt;/a>; and intended use cases.&lt;/li>; &lt;li>;Analysis of adversariality to ensure that we examine both explicit (the input is clearly designed to produce an unsafe output) and implicit (where the input is innocuous but the output is harmful) queries. &lt;/li>; &lt;/ul>; &lt;p>; One of our approaches to scaled data generation is exemplified in our paper on &lt;a href=&quot;https://arxiv.org/abs/2311.08592&quot;>;AI-Assisted Red Teaming&lt;/a>; (AART). AART generates evaluation datasets with high diversity (eg, sensitive and harmful concepts specific to a wide range of cultural and geographic regions), steered by AI-assisted recipes to define, scope and prioritize diversity within an application context. Compared to some state-of-the-art tools, AART shows promising results in terms of concept coverage and data quality. Separately, we are also working with MLCommons to contribute to &lt;a href=&quot;https://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot;>;public benchmarks for AI Safety&lt;/一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Adversarial testing and community insights &lt;/h2>; &lt;p>; Evaluating model output with adversarial test sets allows us to identify critical safety issues prior to deployment. Our initial evaluations relied exclusively on human ratings, which resulted in slow turnaround times and inconsistencies due to a lack of standardized safety definitions and policies. We have improved the quality of evaluations by introducing policy-aligned rater guidelines to improve human rater accuracy, and are researching additional improvements to better reflect the perspectives of diverse communities. Additionally, automated test set evaluation using LLM-based auto-raters enables efficiency and scaling, while allowing us to direct complex or ambiguous cases to humans for expert rating. &lt;/p>; &lt;p>; Beyond testing with adversarial test sets, gathering community insights is vital for continuously discovering “unknown unknowns”. To provide high quality human input that is required to seed the scaled processes, we partner with groups such as the &lt;a href=&quot;https://sites.google.com/corp/google.com/earr-external-research-group/home&quot;>;Equitable AI Research Round Table&lt;/a>; (EARR), and with our internal ethics and analysis teams to ensure that we are representing the diverse communities who use our models. The &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;Adversarial Nibbler Challenge&lt;/a>; engages external users to understand potential harms of &lt;a href=&quot;https://arxiv.org/abs/2305.14384&quot;>;unsafe, biased or violent outputs&lt;/a>; to end users at scale. Our continuous commitment to community engagement includes gathering feedback from diverse communities and collaborating with the research community, for example during &lt;a href=&quot;https://sites.google.com/view/art-of-safety&quot;>;The ART of Safety workshop&lt;/a>; at the &lt;a href=&quot;http://www.ijcnlp-aacl2023.org/&quot;>;Asia-Pacific Chapter of the Association for Computational Linguistics Conference&lt;/a>; (IJCNLP-AACL 2023) to address adversarial testing challenges for GenAI. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Rater diversity in safety evaluation&lt;/h2>; &lt;p>; Understanding and mitigating GenAI safety risks is both a technical and social challenge. Safety perceptions are intrinsically subjective and influenced by a wide range of intersecting factors. Our in-depth study on demographic influences on safety perceptions explored the &lt;a href=&quot;https://arxiv.org/abs/2306.11530&quot;>;intersectional effects of rater demographics&lt;/a>; (eg, race/ethnicity, gender, age) and content characteristics (eg, degree of harm) on safety assessments of GenAI outputs. Traditional approaches largely ignore inherent subjectivity and the systematic disagreements among raters, which can mask important cultural differences. Our &lt;a href=&quot;https://arxiv.org/abs/2311.05074&quot;>;disagreement analysis framework&lt;/a>; surfaced a variety of disagreement patterns between raters from diverse backgrounds including also with “ground truth” expert ratings. This paves the way to new approaches for assessing quality of human annotation and model evaluations beyond the simplistic use of gold labels. Our &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;NeurIPS 2023 publication&lt;/a>; introduces the &lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset&quot;>;DICES&lt;/a>; (Diversity In Conversational AI Evaluation for Safety) dataset that facilitates nuanced safety evaluation of LLMs and accounts for variance, ambiguity, and diversity in various cultural contexts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Summary&lt;/h2>; &lt;p>; GenAI has resulted in a technology transformation, opening possibilities for rapid development and customization even without coding. However, it also comes with a risk of generating harmful outputs. Our proactive adversarial testing program identifies and mitigates GenAI risks to ensure inclusive model behavior. Adversarial testing and red teaming are essential components of a Safety strategy, and conducting them in a comprehensive manner is essential. The rapid pace of innovation demands that we constantly challenge ourselves to find “unknown unknowns” in cooperation with our internal partners, diverse user communities, and other industry experts. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6928081948246813203/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Adversarial testing for generative AI safety&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1338549955376716163&lt;/id>;&lt;published>;2023-11-14T12:28:00.000-08:00&lt;/published>;&lt;updated>;2023-11-14T14:09:51.250-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Scaling multimodal understanding to long videos&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Isaac Noble, Software Engineer, Google Research, and Anelia Angelova, Research Scientist, Google DeepMind &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s1600/mirasol.png&quot; style=&quot;display: none;&quot; />; &lt;p>; When building machine learning models for real-life applications, we need to consider inputs from multiple modalities in order to capture various aspects of the world around us. For example, audio, video, and text all provide varied and complementary information about a visual input. However, building multimodal models is challenging due to the heterogeneity of the modalities. Some of the modalities might be well synchronized in time (eg, audio, video) but not aligned with text. Furthermore, the large volume of data in video and audio signals is much larger than that in text, so when combining them in multimodal models, video and audio often cannot be fully consumed and need to be disproportionately compressed. This problem is exacerbated for longer video inputs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2311.05698&quot;>;Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities&lt;/a>;”, we introduce a multimodal &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive&lt;/a>; model (Mirasol3B) for learning across audio, video, and text方式。 The main idea is to decouple the multimodal modeling into separate focused autoregressive models, processing the inputs according to the characteristics of the modalities. Our model consists of an autoregressive component for the time-synchronized modalities (audio and video) and a separate autoregressive component for modalities that are not necessarily time-aligned but are still sequential, eg, text inputs, such as a title or description. Additionally, the time-aligned modalities are partitioned in time where local features can be jointly learned. In this way, audio-video inputs are modeled in time and are allocated comparatively more parameters than prior works. With this approach, we can effortlessly handle much longer videos (eg, 128-512 frames) compared to other multimodal models. At 3B parameters, Mirasol3B is compact compared to prior &lt;a href=&quot;https://arxiv.org/abs/2204.14198&quot;>;Flamingo&lt;/a>; (80B) and &lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>;PaLI-X&lt;/a>; (55B) models. Finally, Mirasol3B outperforms the state-of-the-art approaches on &lt;a href=&quot;https://blog.research.google/2022/08/efficient-video-text-learning-with.html&quot;>;video question answering&lt;/a>; (video QA), long video QA, and audio-video-text benchmarks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s1240/image5.gif &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1240&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Mirasol3B architecture consists of an autoregressive model for the time-aligned modalities (audio and video), which are partitioned in chunks, and a separate autoregressive model for the unaligned context modalities (eg, text). Joint feature learning is conducted by the Combiner, which learns compact but sufficiently informative features, allowing the processing of long video/audio inputs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Coordinating time-aligned and contextual modalities&lt;/h2>; &lt;p>; Video, audio and text are diverse modalities with distinct characteristics. For example, video is a spatio-temporal visual signal with 30–100 frames per second, but due to the large volume of data, typically only 32–64 frames &lt;em>;per video&lt;/em>; are consumed by current models. Audio is a one-dimensional temporal signal obtained at much higher frequency than video (eg, at 16 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hertz&quot;>;Hz&lt;/a>;), whereas text inputs that apply to the whole video, are typically 200–300 word-sequence and serve as a context to the audio-video inputs. To that end, we propose a model consisting of an autoregressive component that fuses and jointly learns the time-aligned signals, which occur at high frequencies and are roughly synchronized, and another autoregressive component for processing non-aligned signals. Learning between the components for the time-aligned and contextual modalities is coordinated via cross-attention mechanisms that allow the two to exchange information while learning in a sequence without having to synchronize them in time. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Time-aligned autoregressive modeling of video and audio&lt;/h2>; &lt;p>; Long videos can convey rich information and activities happening in a sequence. However, present models approach video modeling by extracting all the information at once, without sufficient temporal information. To address this, we apply an autoregressive modeling strategy where we condition jointly learned video and audio representations for one time interval on feature representations from previous time intervals. This preserves temporal information. &lt;/p>; &lt;p>; The video is first partitioned into smaller video chunks. Each chunk itself can be 4–64 frames. The features corresponding to each chunk are then processed by a learning module, called the Combiner (described below), which generates a joint audio and video feature representation at the current step — this step extracts and compacts the most important information per chunk. Next, we process this joint feature representation with an autoregressive Transformer, which applies attention to the previous feature representation and generates the joint feature representation for the next step. Consequently, the model learns how to represent not only each individual chunk, but also how the chunks relate temporally. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s1259/image1.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;744&quot; data-original-width=&quot;1259&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use an autoregressive modeling of the audio and video inputs, partitioning them in time and learning joint feature representations, which are then autoregressively learned in sequence.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Modeling long videos with a modality combiner&lt;/h2>; &lt; p>; To combine the signals from the video and audio information in each video chunk, we propose a learning module called the Combiner. Video and audio signals are aligned by taking the audio inputs that correspond to a specific video timeframe. We then process video and audio inputs spatio-temporally, extracting information particularly relevant to &lt;em>;changes in the inputs&lt;/em>; (for videos we use &lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;sparse video tubes&lt;/a>;, and for audio we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;>;spectrogram&lt;/a>; representation, both of which are processed by a &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer)&lt;/a>;. We concatenate and input these features to the Combiner, which is designed to learn a new feature representation capturing both these inputs. To address the challenge of the large volume of data in video and audio signals, another goal of the Combiner is to reduce the dimensionality of the joint video/audio inputs, which is done by selecting a smaller number of output features to be produced. The Combiner can be implemented simply as a causal Transformer, which processes the inputs in the direction of time, ie, using only inputs of the prior steps or the current one. Alternatively, the Combiner can have a learnable memory, described below. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Combiner styles&lt;/h2>; &lt;p>; A simple version of the Combiner adapts a Transformer architecture. More specifically, all audio and video features from the current chunk (and optionally prior chunks) are input to a Transformer and projected to a lower dimensionality, ie, a smaller number of features are selected as the output “combined” features. While Transformers are not typically used in this context, we find it effective for reducing the dimensionality of the input features, by selecting the last &lt;em>;m&lt;/em>; outputs of the Transformer, if &lt;em>;m&lt;/em>; is the desired output dimension (shown below). Alternatively, the Combiner can have a memory component. For example, we use the &lt;a href=&quot;https://arxiv.org/abs/2211.09119&quot;>;Token Turing Machine&lt;/a>; (TTM), which supports a differentiable memory unit, accumulating and compressing features from all previous timesteps 。 Using a fixed memory allows the model to work with a more compact set of features at every step, rather than process all the features from previous steps, which reduces computation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s1798/image8.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;619&quot; data-original-width=&quot;1798&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a simple Transformer-based Combiner (&lt;b>;left&lt;/b>;) and a Memory Combiner (&lt;b>;right&lt;/b>;) , based on the Token Turing Machine (TTM), which uses memory to compress previous history of features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our approach on several benchmarks, &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp- content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1906.02467&quot;>;ActivityNet-QA &lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>;, for the video QA task, where a text-based question about a video is issued and the model needs to answer. This evaluates the ability of the model to understand both the text-based question and video content, and to form an answer, focusing on only relevant information. Of these benchmarks, the latter two target long video inputs and feature more complex questions. &lt;/p>; &lt;p>; We also evaluate our approach in the more challenging open-ended text generation setting, wherein the model generates the answers in an unconstrained fashion as free form text, requiring an exact match to the ground truth answer. While this stricter evaluation counts synonyms as incorrect, it may better reflect a model&#39;s ability to generalize. &lt;/p>; &lt;p>; Our results indicate improved performance over state-of-the-art approaches for most benchmarks, including all with open-ended generation evaluation — notable considering our model is only 3B parameters, considerably smaller than prior approaches, eg, Flamingo 80B. We used only video and text inputs to be comparable to other work. Importantly, our model can process 512 frames without needing to increase the model parameters, which is crucial for handling longer videos. Finally with the TTM Combiner, we see both better or comparable performance while reducing compute by 18%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s1200/image3.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/ 06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>; (video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center “ cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“边距 - 左：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJlAND23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s1200/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;860&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJlAND23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>; benchmark, which features long videos for the video QA task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results on audio-video benchmarks&lt;/h2>; &lt;p>; Results on the popular audio-video datasets &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG-Sound&lt;/a>; and &lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; are shown below. Since these benchmarks are classification-only, we treat them as an open-ended text generative setting where our model produces the text of the desired class; eg, for the class ID corresponding to the “playing drums” activity, we expect the model to generate the text “playing drums”. In some cases our approach outperforms the prior state of the art by large margins, even though our model outputs the results in the generative open-ended setting. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s1200/image6.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG -Sound&lt;/a>; (audio-video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s1200/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; (audio-video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Benefits of autoregressive modeling&lt;/h2>; &lt;p>; We conduct an ablation study comparing our approach to a set of baselines that use the same input information but with standard methods (ie, without autoregression and the Combiner). We also compare the effects of pre-training. Because standard methods are ill-suited for processing longer video, this experiment is conducted for 32 frames and four chunks only, across all settings for fair comparison. We see that Mirasol3B&#39;s improvements are still valid for relatively short videos. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s1200/image4.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Ablation experiments comparing the main components of our model. Using the Combiner, the autoregressive modeling, and pre-training all improve performance.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion &lt;/h2>; &lt;p>; We present a multimodal autoregressive model that addresses the challenges associated with the heterogeneity of multimodal data by coordinating the learning between time-aligned and time-unaligned modalities. Time-aligned modalities are further processed autoregressively in time with a Combiner, controlling the sequence length and producing powerful representations. We demonstrate that a relatively small model can successfully represent long video and effectively combine with other modalities. We outperform the state-of-the-art approaches (including some much bigger models) on video- and audio-video question answering. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research is co-authored by AJ Piergiovanni, Isaac Noble, Dahun Kim, Michael Ryoo, Victor Gomes, and Anelia Angelova. We thank Claire Cui, Tania Bedrax-Weiss, Abhijit Ogale, Yunhsuan Sung, Ching-Chung Chang, Marvin Ritter, Kristina Toutanova, Ming-Wei Chang, Ashish Thapliyal, Xiyang Luo, Weicheng Kuo, Aren Jansen, Bryan Seybold, Ibrahim Alabdulmohsin, Jialin Wu, Luke Friedman, Trevor Walker, Keerthana Gopalakrishnan, Jason Baldridge, Radu Soricut, Mojtaba Seyedhosseini, Alexander D&#39;Amour, Oliver Wang, Paul Natsev, Tom Duerig, Younghui Wu, Slav Petrov, Zoubin Ghahramani for their help and support. We also thank Tom Small for preparing the animation. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1338549955376716163/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html&quot; rel=&quot;alternate&quot; title=&quot;Scaling multimodal understanding to long videos&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s72-c/mirasol.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5546775652504697591&lt;/id>;&lt;published>;2023-11-10T10:05:00.000-08:00&lt;/published>;&lt;updated>;2023-11-10T10:05:13.301-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling large-scale health studies for the research community&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Chintan Ghate, Software Engineer, and Diana Mincu, Research Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s1100/MSSignals-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; As consumer technologies like &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/google-cloud-fitbit-haga-collaborate-on-pilot-heart-study&quot;>;fitness trackers&lt;/a>; and &lt;a href=&quot;https://blog.google/products/pixel/health-ai-better-sleep/&quot;>;mobile phones&lt;/a>; become more widely used for health-related data collection, so does the opportunity to leverage these data pathways to study and advance our understanding of medical conditions. We have &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research.html&quot;>;previously&lt;/a>; touched upon how our work explores the use of this technology within the context of chronic diseases, in particular multiple sclerosis (MS). This effort leverages the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies platform&lt;/a>;, an open-source platform used to create clinical study apps, that makes it easier for anyone to run their own studies and collect good quality healthcare data, in a trusted and safe way. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we describe the setup that we developed by expanding the FDA MyStudies platform and demonstrate how it can be used to set up a digital health study. We also present our exploratory research study created through this platform, called MS Signals, which consists of a symptom tracking app for MS patients. The goal for this app is twofold: 1) to ensure that the enhancements to the FDA MyStudies platform made for a more streamlined study creation experience; and 2) to understand how new data collection mechanisms can be used to revolutionize patients&#39; chronic disease management and tracking. We have &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open sourced&lt;/a>; our extension to the FDA MyStudies platform under the &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;>;Apache 2.0 license&lt;/a>; to provide a resource for the community to build their own studies. &lt;/p>; &lt;br />; &lt;h2>;Extending the FDA MyStudies platform&lt;/h2>; &lt;p>; The original FDA MyStudies platform allowed people to configure their own study apps, manage participants, and create separate iOS and Android apps. To simplify the study creation process and ensure increased study engagement, we made a number of accessibility changes. Some of the main improvements include: cross-platform (iOS and Android) app generation through the use of &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>;, an open source framework by Google for building multi-platform applications from a single codebase; a simplified setup, so that users can prototype their study quickly (under a day in most cases); and, most importantly, an emphasis on accessibility so that diverse patient&#39;s voices are heard. The accessibility enhancements include changes to the underlying features of the platform and to the particular study design of the MS Signals study app. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Multi-platform support with rapid prototyping&lt;/h3>; &lt;p>; We decided on the use of Flutter as it would be a single point that would generate both iOS and Android apps in one go, reducing the work required to support multiple platforms. Flutter also provides &lt;a href=&quot;https://docs.flutter.dev/tools/hot-reload&quot;>;hot-reloading&lt;/a>;, which allows developers to build &amp;amp; preview features quickly. The design-system in the app takes advantage of this feature to provide a central point from which the branding &amp;amp; theme of the app can be changed to match the tone of a new study and previewed instantly. The demo environment in the app also utilizes this feature to allow developers to mock and preview questionnaires locally on their machines. In our experience this has been a huge time-saver in A/B testing the UX and the format and wording of questions live with clinicians. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;System accessibility enhancements&lt;/h3>; &lt;p>; To improve the accessibility of the platform for more users, we made several usability enhancements: &lt;/p>; &lt;ol>; &lt;li>;Light &amp;amp; dark theme support&lt;/li>; &lt;li>;Bold text &amp;amp; variable font-sizes&lt;/li>; &lt;li>;High-contrast mode&lt;/li>; &lt;li>;Improving user awareness of accessibility settings&lt;/li>; &lt;/ol>; &lt;p>; Extended exposure to bright light themes can strain the eyes, so supporting dark theme features was necessary to make it easier to use the study app frequently. Some small or light text-elements are illegible to users with vision impairments, so we added 1) bold-text and support for larger font-sizes and 2) high-contrast color-schemes. To ensure that accessibility settings are easy to find, we placed an introductory one-time screen that was presented during the app&#39;s first launch, which would directly take users to their system accessibility settings. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study accessibility enhancements&lt;/h3>; &lt;p>; To make the study itself easier to interact with and reduce cognitive overload, we made the following changes: &lt;/p>; &lt;ol>; &lt;li>;Clarified the onboarding process&lt;/li>; &lt;li>;Improved design for questionnaires&lt;/li>; &lt;/ol>; &lt;p>; First, we clarified the on-boarding process by presenting users with a list of required steps when they first open the app in order to reduce confusion and participant drop-off. &lt;/p>; &lt;p>; The original questionnaire design in the app presented each question in a card format, which utilizes part of the screen for shadows and depth effects of the card. In many situations, this is a pleasant aesthetic, but in apps where accessibility is priority, these visual elements restrict the space available on the screen. Thus, when more accessible, larger font-sizes are used there are more frequent word breaks, which reduces readability. We fixed this simply by removing the card design elements and instead using the entire screen, allowing for better visuals with larger font-sizes. &lt;/p>; &lt;br />; &lt;h2>;The MS Signals prototype study&lt;/h2>; &lt;p>; To test the usability of these changes, we used our redesigned platform to create a prototype study app called MS Signals, which uses surveys to gather information about a participant&#39;s MS-related symptoms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/s1760/MSSignals.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1728&quot; data-original-width=&quot;1760&quot; height=&quot;628&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/w640-h628/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&lt;/em>;&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1971&quot; data-original-width=&quot;1999&quot; height=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/w640-h632/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MS Studies app design&lt;/h3>; &lt;p>; As a first step, before entering any study information, participants are asked to complete an eligibility and study comprehension questionnaire to ensure that they have read through the potentially lengthy terms of study participation. This might include, for example, questions like &quot;In what country is the study available?&quot; or “Can you withdraw from the study?&quot; A section like this is common in most health studies, and it tends to be the first drop-off point for participants. &lt;/p>; &lt;p>; To minimize study drop-off at this early stage, we kept the eligibility test brief and reflected correct answers for the comprehension test back to the participants. This helps minimize the number of times a user may need to go through the initial eligibility questionnaire and ensures that the important aspects of the study protocol are made clear to them. &lt;/p>; &lt;p>; After successful enrollment, participants are taken to the main app view, which consists of three pages: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Activities:&lt;/strong>; &lt;br />; This page lists the questionnaires available to the participant and is where the majority of their time is spent. The questionnaires vary in frequency — some are one-time surveys created to gather medical history, while others are repeated daily, weekly or monthly, depending on the symptom or area they are exploring. For the one-time survey we provide a counter above each question to signal to users how far they have come and how many questions are left, similar to the questionnaire during the eligibility and comprehension step. &lt;/li>; &lt;li>;&lt;strong>;Dashboard:&lt;/strong>; &lt;br />; To ensure that participants get something back in return for the information they enter during a study, the Dashboard area presents a summary of their responses in graph or pie chart form. Participants could potentially show this data to their care provider as a summary of their condition over the last 6 months, an improvement over the traditional pen and paper methods that many employ today. &lt;/li>; &lt;li>;&lt;strong>;Resources:&lt;/strong>; &lt;br />; A set of useful links, help articles and common questions related to MS. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Questionnaire design&lt;/h3>; &lt;p>; Since needing to frequently input data can lead to cognitive overload, participant drop off, and bad data quality, we reduced the burden in two ways: &lt;/p>; &lt;ol>; &lt;li>;We break down large questionnaires into smaller ones, resulting in 6 daily surveys, containing 3–5 questions each, where each question is multiple choice and related to a single symptom. This way we cover a total of 20 major symptoms, and present them in a similar way to how a clinician would ask these questions in an in-clinic setting.&lt;/li>; &lt;li>;We ensure previously entered information is readily available in the app, along with the time of the entry.&lt;/li>; &lt;/ol>; &lt;p>; In designing the survey content, we collaborated closely with experienced clinicians and researchers to finalize the wording and layout. While studies in this field typically use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale&quot;>;Likert scale&lt;/a>; to gather symptom information, we defined a more intuitive verbose scale to provide better experience for participants tracking their disease and the clinicians or researchers viewing the disease history. For example, in the case of vision issues, rather than asking participants to rate their symptoms on a scale from 1 to 10, we instead present a multiple choice question where we detail common vision problems that they may be experiencing. &lt;/p>; &lt;p>; This verbose scale helps patients track their symptoms more accurately by including context that helps them more clearly define their symptoms. This approach also allows researchers to answer questions that go beyond symptom correlation. For example, for vision issues, data collected using the verbose scale would reveal to researchers whether &lt;a href=&quot;https://www.webmd.com/eye-health/nystagmus&quot;>;nystagmus&lt;/a>; is more prominent in patients with MS compared to double vision. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/s1780/MSSignals-2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1754&quot; data-original-width=&quot;1780&quot; height=&quot;630&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/w640-h630/MSSignals-2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Side-by-side comparison with a Likert scale on the left, and a Verbose scale on the right.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1901&quot; data-original-width=&quot;1999&quot; height=&quot;608&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/w640-h608/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Side by side comparison with a Likert scale on the left, and a Verbose scale on the right.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;h2>;Focusing on accessibility&lt;/h2>; &lt;p>; Mobile-based studies can often present additional challenges for participants with chronic conditions: the text can be hard to read, the color contrast could make it difficult to see certain bits of information, or it may be challenging to scroll through pages. This may result in participant drop off, which, in turn, could yield a biased dataset if the people who are experiencing more advanced forms of a disease are unable to provide data. &lt;/p>; &lt;p>; In order to prevent such issues, we include the following accessibility features: &lt;/p>; &lt;ul>; &lt;li>;Throughout, we employ color blind accessible color schemes. This includes improving the contrast between crucial text and important additional information, which might otherwise be presented in a smaller font and a faded text color.&lt;/li>; &lt;li>;We reduced the amount of movement required to access crucial controls by placing all buttons close to the bottom of the page and ensuring that pop-ups are controllable from the bottom part of the screen.&lt;/li>; &lt;/ul>; &lt;p>; To test the accessibility of MS Signals, we collaborated with the &lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;National MS Society&lt;/a>; to recruit participants for a user experience study. For this, a call for participation was sent out by the Society to their members, and 9 respondents were asked to test out the various app flows. The majority indicated that they would like a better way than their current method to track their symptom data, that they considered MS Signals to be a unique and valuable tool that would enhance the accuracy of their symptom tracking, and that they would want to share the dashboard view with their healthcare providers. &lt;/p>; &lt;br />; &lt;h2>;Next steps&lt;/h2>; &lt;p>; We want to encourage everyone to make use of the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open source&lt;/a>; platform to start setting up and running their own studies. We are working on creating a set of standard study templates, which would incorporate what we learned from above, and we hope to release those soon. For any issues, comments or questions please check out our &lt;a href=&quot;https://goo.gle/ms-signals&quot;>;resource page&lt;/a>;. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5546775652504697591/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments &quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:/ /blog.research.google/2023/11/enabling-large-scale-health-studies-for.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling large-scale health studies for the research community&quot; type=&quot;text/html &quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s72- c/MSSignals-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7987071997778787277&lt;/id>;&lt;published>;2023-11-09T14:23:00.000-08:00&lt;/published>;&lt; updated>;2023-11-09T14:23:38.431-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Africa&quot;>;&lt;/category>;&lt; category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term =&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Context in AI Research (CAIR)&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot; byline-author&quot;>;Posted by Katherine Heller, Research Scientist, Google Research, on behalf of the CAIR Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s1100 /CAIR-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Artificial intelligence (AI) and related machine learning (ML) technologies are increasingly influential in the world around us, making it imperative that we consider the potential impacts on society and individuals in all aspects of the technology that we create. To these ends, the Context in AI Research (CAIR) team develops novel AI methods in the context of the entire AI pipeline:&lt;strong>; &lt;/strong>;from data to end-user feedback. The pipeline for building an AI system typically starts with &lt;em>;data&lt;/em>; collection, followed by designing a &lt;em>;model&lt;/em>; to run on that data, &lt;em>;deployment&lt;/em>; of the model in the real world, and lastly, compiling and incorporation of &lt;em>;human feedback&lt;/em>;. Originating in the health space, and now expanded to additional areas, the work of the CAIR team impacts every aspect of this pipeline. While specializing in model building, we have a particular focus on building systems with responsibility in mind, including fairness, robustness, transparency, and inclusion. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC-Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/s1050/CAIR.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;383&quot; data-original-width=&quot;1050&quot; height=&quot;234&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC-Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/w640-h234/CAIR.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Data &lt;/h2>; &lt;p>; The CAIR team focuses on understanding the data on which ML systems are built. Improving the standards for the transparency of ML datasets is instrumental in our work. First, we employ documentation frameworks to elucidate dataset and model characteristics as guidance in the development of data and model documentation techniques — &lt;a href=&quot;https://arxiv.org/abs/1803.09010&quot;>;Datasheets for Datasets&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1810.03993&quot;>;Model Cards for Model Reporting&lt;/a>;. &lt;/p>; &lt;p>; For example, health datasets are highly sensitive and yet can have high impact. For this reason, we developed &lt;a href=&quot;https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533239&quot;>;Healthsheets&lt;/a>;, a health-contextualized adaptation of a Datasheet. Our motivation for developing a health-specific sheet lies in the limitations of existing regulatory frameworks for AI and health. &lt;a href=&quot;https://www.nejm.org/doi/10.1056/NEJMp1816373&quot;>;Recent research&lt;/a>; suggests that data privacy regulation and standards (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act&quot;>;HIPAA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/General_Data_Protection_Regulation&quot;>;GDPR&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act&quot;>;California Consumer Privacy Act&lt;/a>;) do not ensure ethical collection, documentation, and use of data. Healthsheets aim to fill this gap in ethical dataset analysis. The development of Healthsheets was done in collaboration with many stakeholders in relevant job roles, including clinical, legal and regulatory, bioethics, privacy, and product. &lt;/p>; &lt;p>; Further, we studied how Datasheets and Healthsheets could serve as diagnostic tools that surface the limitations and strengths of datasets. Our aim was to start a conversation in the community and tailor Healthsheets to dynamic healthcare scenarios over time. &lt;/p>; &lt;p>; To facilitate this effort, we joined the &lt;a href=&quot;http://www.datadiversity.org/&quot;>;STANDING Together&lt;/a>; initiative, a consortium that aims to develop &lt;a href=&quot;https://www.nature.com/articles/s41591-022-01987-w&quot;>;international, consensus-based standards for documentation of diversity and representation&lt;/a>; within health datasets and to provide guidance on how to mitigate risk of bias translating to harm and health inequalities. Being part of this international, interdisciplinary partnership that spans academic, clinical, regulatory, policy, industry, patient, and charitable organizations worldwide enables us to engage in the conversation about responsibility in AI for healthcare internationally. Over 250 stakeholders from across 32 countries have contributed to refining the standards&lt;strong>;.&lt;/strong>; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s1050/Healthsheets.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;731&quot; data-original-width=&quot;1050&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s16000/Healthsheets.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Healthsheets and STANDING Together: towards health data documentation and standards.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Model&lt;/h2>; &lt;p>; When ML systems are deployed in the real world, they may fail to behave in expected ways, making poor predictions in new contexts. Such failures can occur for a myriad of reasons and can carry negative consequences, especially within the context of healthcare. Our work aims to identify situations where unexpected model behavior may be discovered, before it becomes a substantial problem, and to mitigate the unexpected and undesired consequences. &lt;/p>; &lt;p>; Much of the CAIR team&#39;s modeling work focuses on identifying and mitigating when models are &lt;a href=&quot;https://blog.research.google/2021/10/how-underspecification-presents.html&quot;>;underspecified&lt;/a>;. We show that models that perform well on held-out data drawn from a training domain are not equally robust or fair under distribution shift because the models vary in the extent to which they rely on spurious correlations. This poses a risk to users and practitioners because it can be difficult to anticipate model instability using standard model evaluation practices. &lt;a href=&quot;https://www.jmlr.org/papers/v23/20-1335.html&quot;>;We have demonstrated&lt;/a>; that this concern arises in several domains, including computer vision, natural language processing, medical imaging, and prediction from electronic health records. &lt;/p>; &lt;p>; We have also shown how to use knowledge of causal mechanisms to diagnose and mitigate fairness and robustness issues in new contexts. Knowledge of causal structure allows practitioners to anticipate &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html&quot;>;the generalizability of fairness properties under distribution shift in real-world medical settings&lt;/a>;. Further, investigating the capability for specific causal pathways, or “shortcuts”, to introduce bias in ML systems, we demonstrate how to identify cases where &lt;a href=&quot;https://www.nature.com/articles/s41467-023-39902-7&quot;>;shortcut learning&lt;/a>; leads to predictions in ML systems that are unintentionally dependent on sensitive attributes (eg, age, sex, race). We have shown how to use causal &lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_acyclic_graph&quot;>;directed acyclic graphs&lt;/a>; to &lt;a href=&quot;https://proceedings.mlr.press/v206/alabdulmohsin23a&quot;>;adapt ML systems to changing environments&lt;/a>; under complex forms of distribution shift. Our team is currently investigating how a causal interpretation of different forms of bias, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Selection_bias&quot;>;selection bias&lt;/a>;, label bias, and measurement error, &lt;a href=&quot;https://nips.cc/virtual/2022/58452&quot;>;motivates the design of techniques to mitigate bias during model development and evaluation&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s727/image6.png&quot; style=&quot; margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;442&quot; data-original-width=&quot;727&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s1078/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;382&quot; data-original-width=&quot;1078&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Shortcut Learning: For some models, age may act as a shortcut in classification when using medical images.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The CAIR team focuses on developing methodology to build more inclusive models broadly. For example, we also have &lt;a href=&quot;https://arxiv.org/abs/2302.03874&quot;>;work on the design of participatory systems&lt;/a>;, which allows individuals to choose whether to disclose sensitive attributes, such as race, when an ML system makes predictions. We hope that our methodological research positively impacts the societal understanding of inclusivity in AI method development. &lt;/p>; &lt;br />; &lt;h2>;Deployment &lt;/h2>; &lt;p>; The CAIR team aims to build technology that improves the lives of all people through the use of mobile device technology. We aim to reduce suffering from health conditions, address systemic inequality, and enable transparent device-based data collection. As consumer technology, such as fitness trackers and mobile phones, become central in data collection for health, we explored the use of these technologies within the context of chronic disease, in particular, for &lt;a href=&quot;https://en.wikipedia.org/wiki/Multiple_sclerosis&quot;>;multiple sclerosis&lt;/a>; (MS). We developed new data collection mechanisms and predictions that we hope will eventually revolutionize patient&#39;s chronic disease management, clinical trials, medical reversals and drug development. &lt;/p>; &lt;p>; First, we extended the open-source &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies platform&lt;/a>;, which is used to create clinical study apps, to make it easier for anyone to run their own studies and collect good quality data, in a trusted and safe way. Our improvements include zero-config setups, so that researchers can prototype their study in a day, cross-platform app generation through the use of &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>; and, most importantly, an emphasis on accessibility so that all patient&#39;s voices are heard. We are excited to announce this work has now been &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open sourced&lt;/a>; as an extension to the original FDA-Mystudies platform. You can start setting up your own studies today! &lt;/p>; &lt;p>; To test this platform, we built a prototype app, which we call MS Signals, that uses surveys to interface with patients in a novel consumer setting. We collaborated with the &lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;National MS Society&lt;/a>; to recruit participants for a user experience study for the app, with the goal of reducing dropout rates and improving the platform further. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/s915/MSSignals.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;915&quot; height=&quot;632&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/w640-h632/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&amp;nbsp;&lt;strong>;Left:&lt;/strong>;&amp;nbsp;Study welcome screen.&amp;nbsp;&lt;strong>;Right:&lt;/strong>;&amp;nbsp;Questionnaire.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Once data is collected, researchers could potentially use it to drive the frontier of ML research in MS. In a separate study, we established a research collaboration with the &lt;a href=&quot;https://neurology.duke.edu/&quot;>;Duke Department of Neurology&lt;/a>; and demonstrated that ML models can &lt;a href=&quot;https://www.researchsquare.com/article/rs-2547289/v1&quot;>;accurately predict the incidence of high-severity symptoms&lt;/a>; within three months using continuously collected data from mobile apps. Results suggest that the trained models can be used by clinicians to evaluate the symptom trajectory of MS participants, which may inform decision making for administering interventions. &lt;/p>; &lt;p>; The CAIR team has been involved in the deployment of many other systems, for both internal and external use. For example, we have also partnered with &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; to &lt;a href=&quot;https://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html&quot;>;build a book recommendation system&lt;/a>; for children with learning disabilities, such as dyslexia. We hope that our work positively impacts future product development. &lt;/p>; &lt;br />; &lt;h2>;Human feedback &lt;/h2>; &lt;p>; As ML models become ubiquitous throughout the developed world, it can be far too easy to leave voices in less developed countries behind. A priority of the CAIR team is to bridge this gap, develop deep relationships with communities, and work together to address ML-related concerns through community-driven approaches. &lt;/p>; &lt;p>; One of the ways we are doing this is through working with grassroots organizations for ML, such as &lt;a href=&quot;https://www.sisonkebiotik.africa/home&quot;>;Sisonkebiotik&lt;/a>;, an open and inclusive community of researchers, practitioners and enthusiasts at the intersection of ML and healthcare working together to build capacity and drive forward research initiatives in Africa. We worked in collaboration with the Sisonkebiotik community to detail limitations of historical top-down approaches for global health, and suggested complementary health-based methods, specifically those of grassroots participatory communities (GPCs). We jointly created a &lt;a href=&quot;https://openreview.net/forum?id=jHY_G91R880&quot;>;framework for ML and global health&lt;/a>;, laying out a practical roadmap towards setting up, growing and maintaining GPCs, based on common values across various GPCs such as &lt;a href=&quot;https://www.masakhane.io/&quot;>;Masakhane&lt;/a>;, Sisonkebiotik and &lt;a href=&quot;https://ro-ya-cv4africa.github.io/homepage/&quot;>;Ro&#39;ya&lt;/a>;. &lt;/p>; &lt;p>; We are engaging with open initiatives to better understand the role, perceptions and use cases of AI for health in non-western countries through human feedback, with an initial focus in Africa. Together with &lt;a href=&quot;https://ghananlp.org/&quot;>;Ghana NLP&lt;/a>;, we have worked to detail the need to &lt;a href=&quot;https://arxiv.org/abs/2304.02190&quot;>;better understand algorithmic fairness and bias in health in non-western contexts&lt;/a>;. We recently launched a study to expand on this work using human feedback. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy /s927/MLPipelineBiases.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;927&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy/s16000/MLPipelineBiases.png&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Biases along the ML pipeline and their associations with African-contextualized axes of disparities.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The CAIR team is committed to creating opportunities to hear more perspectives in AI development. We partnered with Sisonkebiotik to co-organize the &lt;a href=&quot;https://www.sisonkebiotik.africa/events/workshops/dl-indaba-2023&quot;>;Data Science for Health Workshop&lt;/a>; at &lt;a href=&quot;https://deeplearningindaba.com/2023/&quot;>;Deep Learning Indaba 2023&lt;/a>; in Ghana. Everyone&#39;s voice is crucial to developing a better future using AI technology. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Negar Rostamzadeh, Stephen Pfohl, Subhrajit Roy, Diana Mincu, Chintan Ghate, Mercy Asiedu, Emily Salkey, Alexander D &#39;Amour, Jessica Schrouff, Chirag Nagpal, Eltayeb Ahmed, Lev Proleev, Natalie Harris, Mohammad Havaei, Ben Hutchinson, Andrew Smart, Awa Dieng, Mahima Pushkarna, Sanmi Koyejo, Kerrie Kauer, Do Hee Park, Lee Hartsell, Jennifer Graves, Berk Ustun, Hailey Joren, Timnit Gebru and Margaret Mitchell for their contributions and influence, as well as our many friends and collaborators at Learning Ally, National MS Society, Duke University Hospital, STANDING Together, Sisonkebiotik, and Masakhane.&lt;/em>; &lt;/ p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7987071997778787277/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot; text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research .google/2023/11/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Context in AI Research (CAIR)&quot; type=&quot;text/html&quot;/>; &lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height= “16”rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>; &lt; /gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s72-c/CAIR-hero.jpg&quot; width =&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id >;tag:blogger.com,1999:blog-8474926331452026626.post-1303378857635363504&lt;/id>;&lt;published>;2023-11-09T11:20:00.002-08:00&lt;/published>;&lt;updated>;2023-11-09T11:59 :02.393-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Overcoming leakage on error-corrected quantum processors&lt;/stitle>;&lt;content type=&quot;html&quot;>; &lt;span class=&quot;byline-author&quot;>;Posted by Kevin Miao and Matt McEwen, Research Scientists, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidYny_9v0IyOBy7QbYVgwS5FAHzgHhVUt5FQvjXhi6WWJHyIIal3awBfaXNu9l3g47HElKNkFNf6XfpU7a_9ExGLlKFgOWvUOaT0PLhEoCfofJrazqT2IL1fBMtZMwfGJSnrpBrQZwQkYJOV6iMOmhWBWIH5OCPcsEPkgw -LuiPDBoLYToGGIN3Hjfpmwr/s320/Quantum%20leakage.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Qubit&quot;>;qubits&lt;/a>; that make up &lt;a href=&quot;https://quantumai.google/hardware&quot;>;Google quantum devices&lt;/a>; are delicate and noisy, so it&#39;s necessary to incorporate error correction procedures that identify and account for qubit errors on the way to building a useful quantum computer. Two of the most prevalent error mechanisms are &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_error_correction#Bit_flip_code&quot;>;bit-flip errors&lt;/a>; (where the energy state of the qubit changes) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_error_correction#Sign_flip_code&quot;>;phase-flip errors&lt;/a>; (where the phase of the encoded quantum information changes). &lt;a href=&quot;https://blog.research.google/2021/08/demonstrating-fundamentals-of-quantum.html&quot;>;Quantum error correction&lt;/a>; (QEC) promises to address and mitigate these two prominent errors. However, there is an assortment of other error mechanisms that challenges the effectiveness of QEC. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While we want qubits to behave as ideal &lt;a href=&quot;https://en.wikipedia.org/wiki/Two-state_quantum_system&quot;>;two-level systems&lt;/a>; with no loss mechanisms, this is not the case in reality. We use the lowest two energy levels of our qubit (which form the &lt;em>;computational basis&lt;/em>;) to carry out computations. These two levels correspond to the absence (computational ground state) or presence (computational excited state) of an excitation in the qubit, and are labeled |0⟩ (“&lt;a href=&quot;https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation&quot;>;ket&lt;/a>; zero”) and |1⟩ (“ket one”), respectively. However, our qubits also host many higher levels called &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/1509.05470&quot;>;leakage states&lt;/a>;&lt;/em>;, which can become occupied. Following the convention of labeling the level by indicating how many excitations are in the qubit, we specify them as |2⟩, |3⟩, |4⟩, and so on. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;Overcoming leakage in quantum error correction&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/nphys/&quot;>;Nature Physics&lt;/a>;&lt;/em>;, we identify when and how our qubits leak energy to higher states, and show that the leaked states can corrupt nearby qubits through our two-qubit gates. We then identify and implement a strategy that can remove leakage and convert it to an error that QEC can efficiently fix. Finally, we show that these operations lead to notably improved performance and stability of the QEC process. This last result is particularly critical, since additional operations take time, usually leading to more errors. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Working with imperfect qubits&lt;/h2>; &lt;p>; Our quantum processors are built from superconducting qubits called &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Transmon&quot;>;transmons&lt;/a>;&lt;/em>;. Unlike an ideal qubit, which only has two computational levels — a computational ground state and a computational excited state — transmon qubits have many additional states with higher energy than the computational excited state. These higher leakage states are useful for particular operations that generate &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;>;entanglement&lt;/a>;, a necessary resource in quantum algorithms, and also keep transmons from becoming too non-linear and difficult to operate. However, the transmon can also be inadvertently excited into these leakage states through a variety of processes, including imperfections in the control pulses we apply to perform operations or from the small amount of stray heat leftover in our cryogenic refrigerator. These processes are collectively referred to as &lt;em>;leakage&lt;/em>;, which describes the transition of the qubit from computational states to leakage states. &lt;/p>; &lt;p>; Consider a particular two-qubit operation that is used extensively in our QEC experiments: the &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates:~:text=%5B1%5D-,Controlled%2DZ,-%2C%0Acontrolled%20sign&quot;>;CZ gate&lt;/a>;. This gate operates on two qubits, and when both qubits are in their |1⟩ level, an interaction causes the two individual excitations to briefly “bunch” together in one of the qubits to form |2⟩, while the other qubit becomes |0⟩, before returning to the original configuration where each qubit is in |1⟩. This bunching underlies the entangling power of the CZ gate. However, with a small probability, the gate can encounter an error and the excitations do not return to their original configuration, causing the operation to leave a qubit in |2⟩, a leakage state. When we execute hundreds or more of these CZ gates, this small leakage error probability accumulates. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwtMGSiPIWruSj-S6Nleklj7YF9Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/s559/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;559&quot; height=&quot;400&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwtMGSiPIWruSj-S6Nleklj7YF9Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/w400-h400/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transmon qubits support many leakage states (|2⟩, |3⟩, |4⟩, …) beyond the computational basis (|0⟩ and |1⟩). While we typically only use the computational basis to represent quantum information, sometimes the qubit enters these leakage states, and disrupts the normal operation of our qubits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; A single leakage event is especially damaging to normal qubit operation because it induces many individual errors. When one qubit starts in a leaked state, the CZ gate no longer correctly entangles the qubits, preventing the algorithm from executing correctly. Not only that, but CZ gates applied to one qubit in leaked states can cause the other qubit to leak as well, spreading leakage through the device. Our work includes extensive characterization of how leakage is caused and how it interacts with the various operations we use in our quantum processor. &lt;/p>; &lt;p>; Once the qubit enters a leakage state, it can remain in that state for many operations before relaxing back to the computational states. This means that a single leakage event interferes with many operations on that qubit, creating operational errors that are bunched together in time (&lt;em>;time-correlated &lt;/em>;errors). The ability for leakage to spread between the different qubits in our device through the CZ gates means we also concurrently see bunches of errors on neighboring qubits (&lt;em>;space-correlated&lt;/em>; errors). The fact that leakage induces patterns of space- and time-correlated errors makes it especially hard to diagnose and correct from the perspective of QEC algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The effect of leakage in QEC&lt;/h2>; &lt;p>; We aim to mitigate qubit errors by implementing &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;surface code QEC&lt;/a>;, a set of operations applied to a collection of imperfect physical qubits to form a &lt;em>;logical qubit&lt;/em>;, which has properties much closer to an ideal qubit. In a nutshell, we use a set of qubits called &lt;em>;data qubits&lt;/em>; to hold the quantum information, while another set of &lt;em>;measure qubits&lt;/em>; check up on the data qubits, reporting on whether they have suffered any errors, without destroying the delicate quantum state of the data qubits. One of the key underlying assumptions of QEC is that errors occur independently for each operation, but leakage can persist over many operations and cause a correlated pattern of multiple errors. The performance of our QEC strategies is significantly limited when leakage causes this assumption to be violated. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2-15jJ6uX2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g -WzBDS3jM-tuGSnxOHFb8kM4shNqKGPqGtKzs8FFRCPs-hH8mjsvkKLrNza/s1588/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original- width=&quot;1588&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2-15jJ6uX2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g-WzBDS3jM-tuGSnxOHFb8kM4shNqKGPqGtKzs8FFRCPs-hH8mjsvkKLrNza/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Once leakage manifests in our surface code transmon grid, it persists for a long time relative to a single surface code QEC cycle. To make matters worse, leakage on one qubit can cause its neighbors to leak as well.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our &lt;a href=&quot;https://blog.research.google/2021/08/demonstrating-fundamentals-of-quantum.html#:~:text=the%20Sycamore%20device.-,Leaky%20Qubits,-The%20goal%20of&quot;>;previous work&lt;/a>; has shown that we can remove leakage from measure qubits using an operation called &lt;em>;multi-level reset&lt;/em>; (MLR). This is possible because once we perform a measurement on measure qubits, they no longer hold any important quantum information. At this point, we can interact the qubit with a very lossy frequency band, causing whichever state the qubit was in (including leakage states) to decay to the computational ground state |0⟩. If we picture a &lt;em>;Jenga&lt;/em>; tower representing the excitations in the qubit, we tumble the entire stack over. Removing just one brick, however, is much more challenging. Likewise, MLR doesn&#39;t work with data qubits because they &lt;em>;always&lt;/em>; hold important quantum information, so we need a new leakage removal approach that minimally disturbs the computational basis states. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Gently removing leakage&lt;/h2>; &lt;p>; We introduce a new quantum operation called&lt;em>; data qubit leakage removal &lt;/em>;(DQLR), which targets leakage states in a data qubit and converts them into computational states in the data qubit and a neighboring measure qubit. DQLR consists of a two-qubit gate (dubbed &lt;em>;Leakage iSWAP&lt;/em>; — an &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates:~:text=1%5D%5B6%5D-,Imaginary%20swap,-2&quot;>;iSWAP&lt;/a>; operation with leakage states) inspired by and similar to our CZ gate, followed by a rapid reset of the measure qubit to further remove errors. The Leakage iSWAP gate is very efficient and greatly benefits from our extensive characterization and calibration of CZ gates within the surface code experiment. &lt;/p>; &lt;p>; Recall that a CZ gate takes two single excitations on two different qubits and briefly brings them to one qubit, before returning them to their respective qubits. A Leakage iSWAP gate operates similarly, but almost in reverse, so that it takes a single qubit with two excitations (otherwise known as |2⟩) and splits them into |1⟩ on two qubits. The Leakage iSWAP gate (and for that matter, the CZ gate) is particularly effective because it does not operate on the qubits if there are fewer than two excitations present. We are precisely removing the |2⟩ &lt;em>;Jenga&lt;/em>; brick without toppling the entire tower. &lt;/p>; &lt;p>; By carefully measuring the population of leakage states on our transmon grid, we find that DQLR can reduce average leakage state populations over all qubits to about 0.1%, compared to nearly 1% without it. Importantly, we no longer observe a gradual rise in the amount of leakage on the data qubits, which was always present to some extent prior to using DQLR. &lt;/p>; &lt;p>; This outcome, however, is only half of the puzzle. As mentioned earlier, an operation such as MLR could be used to effectively remove leakage on the data qubits, but it would also completely erase the stored quantum state. We also need to demonstrate that DQLR is compatible with the preservation of a logical quantum state. &lt;/p>; &lt;p>; The second half of the puzzle comes from executing the QEC experiment with this operation interleaved at the end of each QEC cycle, and observing the logical performance. Here, we use a metric called &lt;em>;detection probability&lt;/em>; to gauge how well we are executing QEC. In the presence of leakage, time- and space-correlated errors will cause a gradual rise in detection probabilities as more and more qubits enter and stay in leakage states. This is most evident when we perform no reset at all, which rapidly leads to a transmon grid plagued by leakage, and it becomes inoperable for the purposes of QEC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s1532/image2.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1532&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;The prior state-of-the-art in our QEC experiments was to use MLR on the measure qubits to remove leakage. While this kept leakage population on the measure qubits (green circles) sufficiently low, data qubit leakage population (green squares) would grow and saturate to a few percent. With DQLR, leakage population on both the measure (blue circles) and data qubits (blue squares) remain acceptably low and stable.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; With MLR, the large reduction in leakage population on the measure qubits drastically decreases detection probabilities and mitigates a considerable degree of the gradual rise. This reduction in detection probability happens even though we spend more time dedicated to the MLR gate, when other errors can potentially occur. Put another way, the correlated errors that leakage causes on the grid can be much more damaging than the uncorrelated errors from the qubits waiting idle, and it is well worth it for us to trade the former for the latter. &lt;/p>; &lt;p>; When only using MLR, we observed a small but persistent residual rise in detection probabilities. We ascribed this residual increase in detection probability to leakage accumulating on the data qubits, and found that it disappeared when we implemented DQLR. And again, the observation that the detection probabilities end up lower compared to only using MLR indicates that our added operation has removed a damaging error mechanism while minimally introducing uncorrelated errors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s1205/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;1205&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Leakage manifests during surface code operation as increased errors (shown as error detection probabilities) over the number of cycles. With DQLR, we no longer see a notable rise in detection probability over more surface code cycles.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prospects for QEC scale-up&lt;/h2>; &lt;p>; Given these promising results, we are eager to implement DQLR in future QEC experiments, where we expect error mechanisms outside of leakage to be greatly improved, and sensitivity to leakage to be enhanced as we work with larger and larger transmon grids. In particular, our simulations indicate that scale-up of our surface code will almost certainly require a large reduction in leakage generation rates, or an active leakage removal technique over all qubits, such as DQLR. &lt;/p>; &lt;p>; Having laid the groundwork by understanding where leakage is generated, capturing the dynamics of leakage after it presents itself in a transmon grid, and showing that we have an effective mitigation strategy in DQLR, we believe that leakage and its associated errors no longer pose an existential threat to the prospects of executing a surface code QEC protocol on a large grid of transmon qubits. With one fewer challenge standing in the way of demonstrating working QEC, the pathway to a useful quantum computer has never been more promising. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work would not have been possible without the contributions of the entire Google Quantum AI Team.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1303378857635363504/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/overcoming-leakage-on-error-corrected.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1303378857635363504&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1303378857635363504&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/overcoming-leakage-on-error-corrected.html&quot; rel=&quot;alternate&quot; title=&quot;Overcoming leakage on error-corrected quantum processors&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidYny_9v0IyOBy7QbYVgwS5FAHzgHhVUt5FQvjXhi6WWJHyIIal3awBfaXNu9l3g47HElKNkFNf6XfpU7a_9ExGLlKFgOWvUOaT0PLhEoCfofJrazqT2IL1fBMtZMwfGJSnrpBrQZwQkYJOV6iMOmhWBWIH5OCPcsEPkgw-LuiPDBoLYToGGIN3Hjfpmwr/s72-c/Quantum%20leakage.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4661241136828203614&lt;/id>;&lt;published>;2023-11-07T12:34:00.003-08:00&lt;/published>;&lt;updated>;2023-11-07T12:34:21.947-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Alternating updates for efficient transformers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Xin Wang, Software Engineer, and Nishanth Dikkala, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk/s1600/altup.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Contemporary deep learning models have been remarkably successful in many domains, ranging from natural language to computer vision. &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer neural networks&lt;/a>; (transformers) are a popular deep learning architecture that today comprise the foundation for most tasks in natural language processing and also are starting to extend to applications in other domains, such as &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;computer vision&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.html&quot;>;robotics&lt;/a>;, and &lt;a href=&quot;https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/&quot;>;autonomous driving&lt;/a>;. Moreover, they form the backbone of all the current state-of-the-art &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;>;language models&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Increasing scale in Transformer networks has led to improved performance and the &lt;a href=&quot;https://arxiv.org/abs/2206.07682&quot;>;emergence of behavior&lt;/a>; not present in smaller networks. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. A natural question is whether we can reap the benefits of larger models without incurring the computational burden. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2301.13310&quot;>;Alternating Updates for Efficient Transformers&lt;/a>;”, accepted as a Spotlight at &lt;a href=&quot;https://neurips.cc/virtual/2023/poster/72994&quot;>;NeurIPS 2023&lt;/a>;, we introduce AltUp, a method to take advantage of increased token representation without increasing the computation cost. AltUp is easy to implement, widely applicable to any transformer architecture, and requires minimal &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter tuning&lt;/a>;. For instance, using a variant of AltUp on a 770M parameter T5-Large model, the addition of ~100 parameters yields a model with a significantly better quality. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; To understand how we can achieve this, we dig into how transformers工作。 First, they partition the input into a sequence of tokens. Each token is then mapped to an embedding vector (via the means of an embedding table) called the token embedding. We call the dimension of this vector the token representation dimension. The Transformer then operates on this sequence of token embeddings by applying a series of computation modules (called layers&lt;em>;)&lt;/em>; using its network parameters. The number of parameters in each transformer layer is a function of the layer&#39;s &lt;em>;width&lt;/em>;, which is determined by the token representation dimension. &lt;/p>; &lt;p>; To achieve benefits of scale without incurring the compute burden, prior works such as sparse mixture-of-experts (Sparse MoE) models (eg, &lt;a href=&quot;https://arxiv.org/abs/2101.03961&quot;>;Switch Transformer&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2022/11/mixture-of-experts-with-expert-choice.html?m=1&quot;>;Expert Choice&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2022/01/scaling-vision-with-sparse-mixture-of.html?m=1&quot;>;V-MoE&lt;/a>;) have predominantly focused on efficiently scaling up the network parameters (in the self-attention and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;>;feedforward layers&lt;/a>;) by conditionally activating a subset based on the input. This allows us to scale up network size without significantly increasing compute per input. However, there is a research gap on scaling up the token representation dimension itself by conditionally activating parts of the token representation vector. &lt;/p>; &lt;p>; Recent works (for example, &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;>;scaling laws&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2011.14522&quot;>;infinite-width networks)&lt;/a>; have empirically and theoretically established that a wider token representation helps in learning more complicated functions. This phenomenon is also evident in modern architectures of increasing capability. For instance, the representation dimension grows from 512 (small) to 768 (base) and 1024 (corresponding to models with 770M, 3B, and 11B parameters respectively) in &lt;a href=&quot;https://arxiv.org/abs/1910.10683 &quot;>;T5 models&lt;/a>;, and from 4096 (8B) to 8192 (64B) and 18432 (540B) in &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM models&lt;/a>; 。 A widened representation dimension also significantly improves performance for dual encoder retrieval models. However, naïvely widening the representation vector requires one to increase the model dimension accordingly, which quadratically&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;1&lt;/a>;&lt;/sup>; increases the amount of computation in the feedforward computation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Method&lt;/h2>; &lt;p>; AltUp works by partitioning a widened representation vector into equal sized blocks, processing only a single block at each layer, and using an efficient prediction-correction mechanism to infer the outputs of the other blocks (shown below on the right). This allows AltUp to simultaneously keep the model dimension, hence the computation cost, roughly constant and take advantage of using an increased token dimension. The increased token dimension allows the model to pack more information into each token&#39;s embedding. By keeping the width of each transformer layer constant, AltUp avoids incurring the quadratic increase in computation cost that would otherwise be present with a naïve expansion of the representation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s1367/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;823&quot; data-original-width=&quot;1367&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;An illustration of widening the token representation without (&lt;b>;left&lt;/b>;) and with AltUp (&lt;b>;right&lt;/b>;). This widening causes a near-quadratic increase in computation in a vanilla transformer due to the increased layer width. In contrast, Alternating Updates keeps the layer width constant and efficiently computes the output by operating on a sub-block of the representation at each layer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More specifically, the input to each layer is two or more blocks, one of which is passed into the 1x width transformer layer (see figure below). We refer to this block as the “activated” block. This computation results in the exact output for the activated block. In parallel, we invoke a lightweight predictor that computes a weighted combination of all the input blocks. The predicted values, along with the computed value of the activated block, are passed on to a lightweight corrector that updates the predictions based on the observed values. This correction mechanism enables the inactivated blocks to be updated as a function of the activated one. Both the prediction and correction steps only involve a limited number of vector additions and multiplications and hence are much faster than a regular transformer layer. We note that this procedure can be generalized to an arbitrary number of blocks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s957/image3.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;504&quot; data-original-width=&quot;957&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;The predictor and corrector computations: The predictor mixes sub-blocks with trainable scalar coefficients; the corrector returns a weighted average of the predictor output and the transformer output. The predictor and corrector perform scalar-vector multiplications and incur negligible computation cost compared to the transformer. The predictor outputs a linear mixing of blocks with scalar mixing coefficients p&lt;sub>;i, j&lt;/sub>; , and the corrector combines predictor output and transformer output with weights g&lt;sub>;i&lt;/sub>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; At a higher level, AltUp is similar to sparse MoE in that it is a method to add capacity to a model in the form of conditionally accessed (external) parameters. In sparse MoE, the additional parameters take the form of feed forward network (FFN) experts and the conditionality is with respect to the input. In AltUp, the external parameters come from the widened embedding table and the conditionality takes the form of alternating block-wise activation of the representation vector, as in the figure above. Hence, AltUp has the same underpinning as sparse MoE models. &lt;/p>; &lt;p>; An advantage of AltUp over sparse MoE is that it does not necessitate sharding since the number of additional parameters introduced is a factor&lt;sup id=&quot;fnref2&quot;>;&lt;a href=&quot;#fn2&quot; rel=&quot;footnote&quot;>;2&lt;/a>;&lt;/sup>; of the embedding table size, which typically makes up a small fraction of the overall model size. Moreover, since AltUp focuses on conditionally activating parts of a wider token representation, it can be applied synergistically with orthogonal techniques like MoE to obtain complementary performance gains. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; AltUp was evaluated on T5 models on various benchmark language tasks. Models augmented with AltUp are uniformly faster than the extrapolated dense models at the same accuracy. For example, we observe that a T5 Large model augmented with AltUp leads to a 27%, 39%, 87%, and 29% speedup on &lt;a href=&quot;https://gluebenchmark.com/&quot;>;GLUE&lt;/a>;, &lt;a href=&quot;https://super.gluebenchmark.com/&quot;>;SuperGLUE&lt;/a>;, &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;>;SQuAD&lt;/a>;, and &lt;a href=&quot;https://nlp.cs.washington.edu/triviaqa/&quot;>;Trivia-QA&lt;/a>; benchmarks, respectively. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s1386 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;982&quot; data-original-width=&quot;1386&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Evaluations of AltUp on T5 models of various sizes and popular benchmarks. AltUp consistently leads to sizable speedups relative to baselines at the same accuracy. Latency is measured on &lt;a href=&quot;https://cloud.google.com/tpu/docs/system-architecture-tpu-vm&quot;>;TPUv3&lt;/a>; with 8 cores. Speedup is defined as the change in latency divided by the AltUp latency (B = T5 Base, L = T5 Large, XL = T5 XL models).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; AltUp&#39;s relative performance improves as we apply it to larger models — compare the relative speedup of T5 Base + AltUp to that of T5 Large + AltUp. This demonstrates the scalability of AltUp and its improved performance on even larger models. Overall, AltUp consistently leads to models with better predictive performance than the corresponding baseline models with the same speed on all evaluated model sizes and benchmarks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Extensions: Recycled AltUp&lt;/h2>; &lt;p>; The AltUp formulation adds an insignificant amount of per-layer computation, however, it does require using a wider embedding table. In certain scenarios where the vocabulary size (ie, the number of distinct tokens the tokenizer can produce) is very large, this may lead to a non-trivial amount of added computation for the initial embedding lookup and the final &lt;a href=&quot;https://www.google.com/url?q=https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1699300629405641&amp;amp;usg=AOvVaw3tnZ18-LvY0tPP_NClQ_P-&quot;>;linear + softmax operation&lt;/a>;. A very large vocabulary may also lead to an undesirable amount of added embedding parameters. To address this, Recycled-AltUp is an extension of AltUp that avoids these computational and parameter costs by keeping the embedding table&#39;s width the same. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s989/image4.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;515&quot; data-original-width=&quot;989&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Illustration of the Architecture for Recycled-AltUp with &lt;em>;K&lt;/em>; = 2.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In Recycled-AltUp, instead of widening the initial token embeddings, we replicate the embeddings &lt;em>;K&lt;/em>; times to form a wider token representation. Hence, Recycled-AltUp adds virtually no additional parameters relative to the baseline transformer, while benefiting from a wider token representation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRoANGe48-HncQDuNpxAVSUGf2GPp1qp6KpnvqMbQ5Mejlq4gWpq96vu9FxkDub8aO3RqYyDmTki2Xqkj9drOprwqrHDyoQvHI4oziYFguhDnkkYZO4GXdhAKgIlfffHJ5Po7mEVGCaAkOH7oobnt-8qKjWlerZp4EeoAjZg6IG_CEVO3hhsQvlPaV2GHU/s1999/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;643&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRoANGe48-HncQDuNpxAVSUGf2GPp1qp6KpnvqMbQ5Mejlq4gWpq96vu9FxkDub8aO3RqYyDmTki2Xqkj9drOprwqrHDyoQvHI4oziYFguhDnkkYZO4GXdhAKgIlfffHJ5Po7mEVGCaAkOH7oobnt-8qKjWlerZp4EeoAjZg6IG_CEVO3hhsQvlPaV2GHU/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Recycled-AltUp on T5-B/L/XL compared to baselines. Recycled-AltUp leads to strict improvements in pre-training performance without incurring any perceptible slowdown.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also evaluate the lightweight extension of AltUp, Recycled-AltUp, with &lt;em>;K&lt;/em>; = 2 on T5 base, large, and XL models and compare its pre-trained accuracy and speed to those of baselines. Since Recycled-AltUp does not require an expansion in the embedding table dimension, the models augmented with it have virtually the same number of trainable parameters as the baseline models. We again observe consistent improvements compared to the dense baselines. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Why does AltUp work?&lt;/h2>; &lt;p>; AltUp increases a model&#39;s capacity by adding and &lt;em>;efficiently&lt;/em>; leveraging auxiliary parameters to the embedding table, and maintaining the higher dimensional representation across the layers. We believe that a key ingredient in this computation lies in AltUp&#39;s prediction mechanism that performs an ensemble of the different blocks. This weighted combination enables continuous message passing to the entire vector despite activating only sub-blocks of it in each layer. Recycled-AltUp, on the other hand, does not add any additional parameters to the token embeddings. However, it still confers the benefit of simulating computation in a higher dimensional representation space since a higher dimensional representation vector is maintained when moving from one transformer layer to another. We conjecture that this aids the training by augmenting the flow of information through the network. An interesting research direction is to explore whether the benefits of Recycled-AltUp can be explained entirely by more favorable training dynamics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;We thank our collaborators Cenk Baykal, Dylan Cutler, and Rina Panigrahy at Google Research, and Nikhil Ghosh at University of California, Berkeley (work done during research internship at Google).&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/ a>;&lt;/sup>;This is because the feedforward layers of a Transformer are typically scaled quadratically with the model dimension.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/ a>;&lt;/span>; &lt;br />; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn2&quot;>;&lt;b>;2&lt; /b>;&lt;/a>;&lt;/sup>;This factor depends on the user-specified expansion factor, but is typically 1, ie, we double the embedding table dimension.&amp;nbsp;&lt;a href=&quot;#fnref2&quot; rev=&quot;footnote &quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4661241136828203614/comments/default&quot; rel =&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/alternating-updates-for-efficient.html# comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4661241136828203614&quot; rel =&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4661241136828203614&quot; rel=&quot;self&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/alternating-updates-for-efficient.html&quot; rel=&quot;alternate&quot; title=&quot;Alternating updates for efficient transformers&quot; type =&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk /s72-c/altup.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total >;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8959024707515398632&lt;/id>;&lt;published>;2023-11-03T11:23:00.001-07:00&lt;/published>; &lt;updated>;2023-11-03T11:27:07.156-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>; &lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Best of both worlds: Achieving scalability and quality in text clustering&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;Posted by Sara Ahmadian and Mehran Kazemi, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR2mD5afdT_Qg9Ex4Lj94FaxB9KHiq20iLoDlOY8S8Rk5XsV6n_4dU9CFbCvSeBSONGQYqy-yMGY6-KU_y_RGICOpz76GNdzv7ecxor_rVnF31lZOd3STQF4MIE4F_EadrSI1DXY67MXnsCswY3w3X8vX8KgU_rRPs6eTZndYAbVcEezgwaUsdZEAHD59Z/s320/ hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;Clustering&lt;/a>; is a fundamental, ubiquitous problem in data mining and &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;>;unsupervised&lt;/a>; machine learning, where the goal is to group together similar items. The standard forms of clustering are metric clustering and graph clustering. In metric clustering, a given metric space defines distances between data points, which are grouped together based on their &lt;em>;separation&lt;/em>;. In graph clustering, a given graph connects similar data points through edges, and the clustering process groups data points together based on the &lt;em>;connections&lt;/em>; between them. Both clustering forms are particularly useful for large corpora where class labels can&#39;t be defined. Examples of such corpora are the ever-growing digital text collections of various internet platforms, with applications including organizing and searching documents, identifying patterns in text, and recommending relevant documents to users (see more examples in the following posts: &lt;a href=&quot;https://blog.research.google/2010/10/clustering-related-queries-based-on.html?m=1&quot;>;clustering related queries based on user intent&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2021/10/practical-differentially-private.html&quot;>;practical differentially private clustering&lt;/a>;). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The choice of text clustering method often presents a dilemma. One approach is to use embedding models, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/BERT_(language_model)&quot;>;BERT&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>;, to define a metric clustering problem. Another is to utilize &lt;a href=&quot;https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-cross-attention-3b396266d82e&quot;>;cross-attention&lt;/a>; (CA) models, such as &lt;a href=&quot;https://blog.research.google/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_pre-trained_transformer&quot;>;GPT&lt;/a>;, to define a graph clustering problem. CA models can provide highly accurate similarity scores, but constructing the input graph may require a prohibitive quadratic number of inference calls to the model. On the other hand, a metric space can efficiently be defined by distances of embeddings produced by embedding models. However, these similarity distances are typically of substantial lower-quality compared to the similarity signals of CA models, and hence the produced clustering can be of much lower-quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRHbZ9egdnFlc6ZCmLeODTdPZoOXcjjEGCLHP8Ve3aihMExIjyLiX4n0Zy9l4nmkx-3k6rxYN5696irWyubaYAqecpxFTmP1wzjBo0MKnZMnnTjQrdSxKIWGzsySs8XB-OVHyyxKJBWI0dlrYvb_S204S6aSbkPTx8_FQTJXk8f_WDIoGgqNJTACuXNewu/s835/figure%20top.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;641&quot; data-original-width=&quot;835&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRHbZ9egdnFlc6ZCmLeODTdPZoOXcjjEGCLHP8Ve3aihMExIjyLiX4n0Zy9l4nmkx-3k6rxYN5696irWyubaYAqecpxFTmP1wzjBo0MKnZMnnTjQrdSxKIWGzsySs8XB-OVHyyxKJBWI0dlrYvb_S204S6aSbkPTx8_FQTJXk8f_WDIoGgqNJTACuXNewu/s16000/figure%20top.png&quot; />;&lt;/a>;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt; a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoY4juklU42gca4fdtnUN4t5j8Z-4rEEhyphenhyphenDi4kRfkV1wfqBs_wyldX9sU2pwtMHdVZPZf2vWnDweEPzwiLseYZenC3afIuaDRf_7arpoF-xnMAwgGXs48a_3wNG2trhN0CTkHgMVa6ZQ7ToP7IE6fpIFmbTF7YYGPFgYU3cMtyyBYIfSWVLVvRXY6Dueiy/s835/figure%20bottom.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;641&quot; data-original-width=&quot;835&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjoY4juklU42gca4fdtnUN4t5j8Z-4rEEhyphenhyphenDi4kRfkV1wfqBs_wyldX9sU2pwtMHdVZPZf2vWnDweEPzwiLseYZenC3afIuaDRf_7arpoF-xnMAwgGXs48a_3wNG2trhN0CTkHgMVa6ZQ7ToP7IE6fpIFmbTF7YYGPFgYU3cMtyyBYIfSWVLVvRXY6Dueiy/s16000/figure%20bottom.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing =“ 0” class =“ tr-caption-container”样式=“边距 - 左：auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the embedding-based and cross-attention–based similarity scoring functions and their scalability vs. quality dilemma.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>; Motivated by this, in “&lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals”, presented at ICLR 2023&lt;/a>;, we describe a novel clustering algorithm that effectively combines the scalability benefits from embedding models and the quality from CA models. This graph clustering algorithm has query access to both the CA model and the embedding model, however, we apply a budget on the number of queries made to the CA model. This algorithm uses the CA model to answer edge queries, and benefits from unlimited access to similarity scores from the embedding model. We describe how this proposed setting bridges algorithm design and practical considerations, and can be applied to other clustering problems with similar available scoring functions, such as clustering problems on images and media. We demonstrate how this algorithm yields high-quality clusters with almost a linear number of query calls to the CA model. We have also &lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks.zip&quot;>;open-sourced&lt;/a>; the data used in our experiments. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The clustering algorithm&lt;/h2>; &lt;p>; The KwikBucks algorithm is an extension of the well-known &lt;a href=&quot;https://cesa-bianchi.di.unimi.it/Algo2/Note/kwik.pdf&quot;>;KwikCluster algorithm (Pivot algorithm)&lt;/a>;. The high-level idea is to first select a set of documents (ie, centers) with no similarity edge between them, and then form clusters around these centers. To obtain the quality from CA models and the runtime efficiency from embedding models, we introduce the novel &lt;em>;combo similarity oracle&lt;/em>; mechanism. In this approach, we utilize the embedding model to guide the selection of queries to be sent to the CA model. When given a set of center documents and a target document, the combo similarity oracle mechanism outputs a center from the set that is similar to the target document, if present. The combo similarity oracle enables us to save on budget by limiting the number of query calls to the CA model when selecting centers and forming clusters. It does this by first ranking centers based on their embedding similarity to the target document, and then querying the CA model for the pair (ie, target document and ranked center), as shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s1601/process.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s16000/process.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;A combo similarity oracle that for a set of documents and a target document, returns a similar document from the set, if present.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We then perform a post processing step to merge clusters if there is a strong connection between two of them, ie, when the number of connecting edges is higher than the number of missing edges between two clusters. Additionally, we apply the following steps for further computational savings on queries made to the CA model, and to improve performance at runtime: &lt;/p>; &lt;ol>; &lt;li>;We leverage &lt;a href=&quot;https://arxiv.org /pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; to form a set of centers from a set of randomly selected documents instead of selecting these centers from all the documents (in the illustration below, the center nodes are red ）。 &lt;/li>;&lt;li>;We apply the combo similarity oracle mechanism to perform the cluster assignment step in parallel for all non-center documents and leave documents with no similar center as singletons. In the illustration below, the assignments are depicted by blue arrows and initially two (non-center) nodes are left as singletons due to no assignment. &lt;/li>;&lt;li>;In the post-processing step, to ensure scalability, we use the embedding similarity scores to filter down the potential mergers (in the illustration below, the green dashed boundaries show these merged clusters). &lt;/li>; &lt;/ol>;&lt;div>;&lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1bJ65zsG-PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s1322/Kwikbux%20gif.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;704&quot; data-original-width=&quot;1322&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1bJ65zsG-PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s16000/Kwikbux%20gif.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of progress of the clustering algorithm on a given graph instance. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate the novel clustering algorithm on various datasets with different properties using different embedding-based and cross-attention–based models. We compare the clustering algorithm&#39;s performance with the two best performing baselines (see the &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;paper&lt;/a>; for more details): &lt;/p>; &lt;ul>; &lt;li>;The &lt;a href=&quot;https://arxiv.org/pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; algorithm for budgeted clustering with access to CA only. &lt;/li>;&lt;li>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_clustering&quot;>;Spectral clustering&lt;/a>; on the &lt;em>;&lt;a href=&quot;https://en.wikipedia .org/wiki/Nearest_neighbor_graph&quot;>;k-nearest neighbor graph&lt;/a>;&lt;/em>; (kNN) formed by querying the CA model for the &lt;em>;k&lt;/em>;-nearest neighbors of each vertex from embedding-based相似。 &lt;/li>; &lt;/ul>; &lt;p>; To evaluate the quality of clustering, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision and recall&lt;/a>;. Precision is used to calculate the percentage of similar pairs out of all co-clustered pairs and recall is the percentage of co-clustered similar pairs out of all similar pairs. To measure the quality of the obtained solutions from our experiments, we use the &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1-score&lt;/a>;, which is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Harmonic_mean&quot;>;harmonic mean&lt;/a>; of the precision and recall, where 1.0 is the highest possible value that indicates perfect precision and recall, and 0 is the lowest possible value that indicates if either precision or recall are zero. The table below reports the F1-score for Kwikbucks and various baselines in the case that we allow only a linear number of queries to the CA model. We show that Kwikbucks offers a substantial boost in performance with a 45% relative improvement compared to the best baseline when averaging across all datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s1574/results.jpg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;582&quot; data-original-width=&quot;1574&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s16000/results.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Comparing the clustering algorithm to two baseline algorithms using various public datasets: (1) The &lt;a href=&quot;https://arxiv.org/pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; algorithm for budgeted clustering with access to CA only, and (2) &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_clustering&quot;>;spectral clustering&lt;/a>; on the &lt;em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Nearest_neighbor_graph&quot;>;k-nearest neighbor (kNN) graph&lt;/a>;&lt;/em>; formed by querying the CA model for the &lt;em>;k&lt;/ em>;-nearest neighbors of each vertex from embedding-based similarity. Pre-processed datasets can be downloaded &lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks.zip&quot;>;here&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The figure below compares the clustering algorithm&#39;s performance with baselines using different query budgets. We observe that KwikBucks consistently outperforms other baselines at various budgets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3CboOkZ4lbp9dmH-5EkPGe1zo6JPlOQbXT1n67ke4qN0kXeZXMQKb4SUM-E6TXuyWF9UF7vvKTtl2lecg-Z5tV0mA6Hobh9NKbmy__dqGRIYcTEIzdfGjCCyih2eZW4xF33n7Y1pJGpwyDvQ75rwQsp0kZICGSPoyR8ahiBQwcZvT7ZLLeAA1BQkJtLwU/s1236/stackoverflow.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1236&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3CboOkZ4lbp9dmH-5EkPGe1zo6JPlOQbXT1n67ke4qN0kXeZXMQKb4SUM-E6TXuyWF9UF7vvKTtl2lecg-Z5tV0mA6Hobh9NKbmy__dqGRIYcTEIzdfGjCCyih2eZW4xF33n7Y1pJGpwyDvQ75rwQsp0kZICGSPoyR8ahiBQwcZvT7ZLLeAA1BQkJtLwU/s16000/stackoverflow.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A comparison of KwikBucks with top-2 baselines when allowed different budgets for querying the cross-attention model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Text clustering often presents a dilemma in the choice of similarity function: embedding models are scalable but lack quality, while cross-attention models offer quality but substantially hurt scalability. We present a clustering algorithm that offers the best of both worlds: the scalability of embedding models and the quality of cross-attention models. KwikBucks can also be applied to other clustering problems with multiple similarity oracles of varying accuracy levels. This is validated with an exhaustive set of experiments on various datasets with diverse properties. See the &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This project was initiated during Sandeep Silwal&#39;s summer internship at Google in 2022. We would like to express our gratitude to our co-authors, Andrew McCallum, Andrew Nystrom, Deepak Ramachandran, and Sandeep Silwal, for their valuable contributions to this work. We also thank Ravi Kumar and John Guilyard for assistance with this blog post.&lt;/em>; &lt;/p>;&lt;/div>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8959024707515398632/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/best-of-both-worlds-achieving.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8959024707515398632&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8959024707515398632&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/best-of-both-worlds-achieving.html&quot; rel=&quot;alternate&quot; title=&quot;Best of both worlds: Achieving scalability and quality in text clustering&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR2mD5afdT_Qg9Ex4Lj94FaxB9KHiq20iLoDlOY8S8Rk5XsV6n_4dU9CFbCvSeBSONGQYqy-yMGY6-KU_y_RGICOpz76GNdzv7ecxor_rVnF31lZOd3STQF4MIE4F_EadrSI1DXY67MXnsCswY3w3X8vX8KgU_rRPs6eTZndYAbVcEezgwaUsdZEAHD59Z/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4882962745371313901&lt;/id>;&lt;published>;2023-11-02T15:01:00.001-07:00&lt;/published>;&lt;updated>;2023-11-02T15:06:20.846-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Zero-shot adaptive prompting of large language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Xingchen Wan, Student Researcher, and Ruoxi Sun, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqTosaeIs4AMYukkmgUBEii6iQcrr9_dNKM0cHnW6m9Wi8yX0V-QCduQfkqLyQPNpVbze3OFO-nPG5Wm9DLMM8pEAfhQGq-TEJroLJGQyqNr-hlJNkToBCmgJbphrCRvlv95gkDQH0ScT7VrXu2VCTKyiWy8yYM4G8voF0kD0K2oxwg2M2xBcz1yQnU0Zb/s600/USP.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Recent advances in large language models (LLMs) are very promising as reflected in their capability for general problem-solving in &lt;em>;few-shot&lt;/em>; and &lt;em>;zero-shot&lt;/em>; setups, even without explicit training on these tasks. This is impressive because in the few-shot setup, LLMs are presented with only a few question-answer demonstrations prior to being given a test question. Even more challenging is the zero-shot setup, where the LLM is directly prompted with the &lt;em>;test question only&lt;/em>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Even though the few-shot setup has dramatically reduced the amount of data required to adapt a model for a specific use-case, there are still cases where generating sample prompts can be challenging. For example, handcrafting even a small number of demos for the broad range of tasks covered by general-purpose models can be difficult or, for unseen tasks, impossible. For example, for tasks like summarization of long articles or those that require domain knowledge (eg, medical question answering), it can be challenging to generate sample answers. In such situations, models with high zero-shot performance are useful since no manual prompt generation is required. However, zero-shot performance is typically weaker as the LLM is not presented with guidance and thus is prone to spurious output. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://aclanthology.org/2023.findings-acl.216/&quot;>;Better Zero-shot Reasoning with Self-Adaptive Prompting&lt;/a>;”, published at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, we propose &lt;em>;Consistency-Based Self-Adaptive Prompting (COSP) &lt;/em>;to address this dilemma. COSP is a zero-shot automatic prompting method for reasoning problems that carefully selects and constructs &lt;em>;pseudo-&lt;/em>;demonstrations for LLMs using only unlabeled samples (that are typically easy to obtain) and the models&#39; own predictions. With COSP, we largely close the performance gap between zero-shot and few-shot while retaining the desirable generality of zero-shot prompting. We follow this with “&lt;a href=&quot;https://arxiv.org/abs/2305.14926&quot;>;Universal Self-Adaptive Prompting&lt;/a>;“ (USP), accepted at &lt;a href=&quot;https://2023. emnlp.org/&quot;>;EMNLP 2023&lt;/a>;, in which we extend the idea to a wide range of &lt;em>;general &lt;/em>;natural language understanding (NLU) and natural language generation (NLG) tasks and demonstrate its effectiveness 。 &lt;/p>; &lt;br />; &lt;h2>;Prompting LLMs with their own outputs&lt;/h2>; &lt;p>; Knowing that LLMs benefit from demonstrations and have at least &lt;em>;some&lt;/em>; zero-shot abilities, we wondered whether the model&#39;s zero-shot outputs could serve as demonstrations for the model to prompt itself. The challenge is that zero-shot solutions are imperfect, and we risk giving LLMs poor quality demonstrations, which could be worse than no demonstrations at all. Indeed, the figure below shows that adding a correct demonstration to a question can lead to a correct solution of the test question (Demo1 with question), whereas adding an incorrect demonstration (Demo 2 + questions, Demo 3 with questions) leads to incorrect answers 。 Therefore, we need to select reliable self-generated demonstrations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5QShw0e9OdtzA9Ob5vAtWloPZGIZgCGFSL9okO-5pxIN3M2QciE1MOT73NsdaHwOz5LiH94IZRkE0SYSJ2fiiBSNR_qHCrO-URHqZPq7Fhhpvm6wtykLsySfF9l5ERsbOqtT_J1-A8pjoOa9htdvXEnL4-WRbP-Kn85aIz7G-BvWNflcaG4lUPomVlLU8 /s1051/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1051&quot; data-original-width=&quot;776&quot; height =&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5QShw0e9OdtzA9Ob5vAtWloPZGIZgCGFSL9okO-5pxIN3M2QciE1MOT73NsdaHwOz5LiH94IZRkE0SYSJ2fiiBSNR_qHCrO-URHqZPq7Fhhpvm6wtykLsySfF9l5ERsbOqtT_J1-A8pjoOa9htdvXEnL4-WRbP-Kn85aIz7G-BvWNflcaG4lUPomVlLU8/w472-h640/image6.png&quot; width=&quot;472&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>; Example inputs &amp;amp; outputs for reasoning tasks, which illustrates the need for carefully designed selection procedure for in-context demonstrations (&lt;a href=&quot;https://arxiv.org/abs/1608.01413&quot;>;MultiArith&lt;/a>;&amp;nbsp;dataset &amp;amp;&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM-62B&lt;/a>;&amp;nbsp;model): (1) zero-shot&amp;nbsp;&lt;/em>;&lt;a href=&quot;https://blog.research.google/2022/05/language-models-perform-reasoning-via.html&quot; style=&quot;text-align: left;&quot;>;chain-of-thought&lt;/a>;&lt;em style=&quot;text-align: left;&quot;>;&amp;nbsp;with no demo: correct logic but wrong answer; (2) correct demo (Demo1) and correct answer; (3) correct but repetitive demo (Demo2) leads to repetitive outputs; (4) erroneous demo (Demo3) leads to a wrong answer; but (5) combining Demo3 and Demo1 again leads to a correct answer.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; COSP leverages a key observation of LLMs: that confident and consistent predictions are more likely correct. This observation, of course, depends on how good the uncertainty estimate of the LLM is. Luckily, in large models, &lt;a href=&quot;https://arxiv.org/abs/2207.05221&quot;>;previous&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2210.11610&quot;>;works&lt;/a>; suggest that the uncertainty estimates are robust. Since measuring confidence requires only model predictions, not labels, we propose to use this as a zero-shot proxy of correctness. The high-confidence outputs and their inputs are then used as &lt;em>;pseudo&lt;/em>;-demonstrations. &lt;/p>; &lt;p>; With this as our starting premise, we estimate the model&#39;s confidence in its output based on its &lt;a href=&quot;https://arxiv.org/abs/2203.11171&quot;>;self-consistency&lt;/a>; and use this measure to select robust self-generated demonstrations. We ask LLMs the same question multiple times with zero-shot &lt;a href=&quot;https://blog.research.google/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; (CoT) prompting. To guide the model to generate a range of possible rationales and final answers, we include randomness controlled by a “temperature” hyperparameter. In an extreme case, if the model is 100% certain, it should output identical final answers each time. We then compute the entropy of the answers to gauge the uncertainty — the answers that have high self-consistency and for which the LLM is more certain, are likely to be correct and will be selected. &lt;/p>; &lt;p>; Assuming that w&lt;em>;e&lt;/em>; are presented with a collection of unlabeled questions, the COSP method is: &lt;/p>; &lt;ol>; &lt;li>;Input each unlabeled question into an LLM, obtaining multiple rationales and answers by sampling the model multiple times. The most frequent answers are highlighted, followed by a score that measures consistency of answers across multiple sampled outputs (higher is better). In addition to favoring more consistent answers, we also penalize repetition within a response (ie, with repeated words or phrases) and encourage diversity of selected demonstrations. We encode the preference towards consistent, un-repetitive and diverse outputs in the form of a scoring function that consists of a weighted sum of the three scores for selection of the self-generated pseudo-demonstrations.&lt;/li>; &lt;li>;We concatenate the pseudo-demonstrations into test questions, feed them to the LLM, and obtain a final predicted answer.&lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidn_WHASrzLG7rsqvKr5XqTi0VE0hEonQJsCRy0XHnM_DsWP4u1izMPUvfakpLd9FJvu47XXspD3vgIXrnrEhbrGR5vxSkcRRtbrU7HwHh6zLpywepMg39GUAW6uSVYxW-JzdOl5IVt9KPqLDVVCvjz86e6vQgZK79FKAF5wGdpztohbWhe7tVtVwcNBgh/s949/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;396&quot; data-original-width=&quot;949&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidn_WHASrzLG7rsqvKr5XqTi0VE0hEonQJsCRy0XHnM_DsWP4u1izMPUvfakpLd9FJvu47XXspD3vgIXrnrEhbrGR5vxSkcRRtbrU7HwHh6zLpywepMg39GUAW6uSVYxW-JzdOl5IVt9KPqLDVVCvjz86e6vQgZK79FKAF5wGdpztohbWhe7tVtVwcNBgh/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Illustration of COSP: In Stage 1 (&lt;strong>;left&lt;/strong>;), we run zero-shot CoT multiple times to generate a pool of demonstrations (each consisting of the question, generated rationale and prediction) and assign a score. In Stage 2 (&lt;strong>;right&lt;/strong>;), we augment the current test question with pseudo-demos (blue boxes) and query the LLM again. A majority vote over outputs from both stages forms the final prediction.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; COSP focuses on question-answering tasks with CoT prompting for which it is easy to measure self-consistency since the questions have unique correct answers. But this can be difficult for other tasks, such as open-ended question-answering or generative tasks that don&#39;t have unique answers (eg, text summarization). To address this limitation, we introduce USP in which we generalize our approach to other general NLP tasks: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Classification&lt;/em>; (CLS): Problems where we can compute the probability of each class using the neural network output logits of each class. In this way, we can measure the uncertainty without multiple sampling by computing the entropy of the logit distribution.&lt;/li>; &lt;li>;&lt;em>;Short-form generation&lt;/em>; (SFG): Problems like question answering where we can use the same procedure mentioned above for COSP, but, if necessary, without the rationale-generating step.&lt;/li>; &lt;li>;&lt;em>;Long-form generation&lt;/em>; (LFG):&lt;em>; &lt;/em>;Problems like summarization and translation, where the questions are often open-ended and the outputs are unlikely to be identical, even if the LLM is certain. In this case, we use an &lt;em>;overlap metric&lt;/em>; in which we compute the average of the &lt;em>;pairwise&lt;/em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE score&lt;/a>; between the different outputs to the same query.&lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjn2mgVNUmsKVPYKo3zrcQnq3nHT0xIzCk2rIOK0fSrFIOEkyCrx7MWNnTrOdwnFRlGbid1cj8OqV2xBCfOtgv5oiuUPoQjRY9CpMnjM79P0mQmoyQqluMPZsqFQUtS7AtPy5Uw-sf5UT_dV_bRbGWSRQiR5U2tDIYd2zxsk_lboJsKG4mcBZKxp5gEeT_T/s1360/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;640&quot; data-original-width=&quot;1360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjn2mgVNUmsKVPYKo3zrcQnq3nHT0xIzCk2rIOK0fSrFIOEkyCrx7MWNnTrOdwnFRlGbid1cj8OqV2xBCfOtgv5oiuUPoQjRY9CpMnjM79P0mQmoyQqluMPZsqFQUtS7AtPy5Uw-sf5UT_dV_bRbGWSRQiR5U2tDIYd2zxsk_lboJsKG4mcBZKxp5gEeT_T/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Illustration of USP in exemplary tasks (classification, QA and text summarization). Similar to COSP, the LLM first generates predictions on an unlabeled dataset whose outputs are scored with logit entropy, consistency or alignment, depending on the task type, and pseudo-demonstrations are selected from these input-output pairs. In Stage 2, the test instances are augmented with pseudo-demos for prediction.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We compute the relevant confidence scores depending on the type of task on the aforementioned set of unlabeled test samples. After scoring, similar to COSP, we pick the confident, diverse and less repetitive answers to form a model-generated pseudo-demonstration set. We finally query the LLM again in a few-shot format with these pseudo-demonstrations to obtain the final predictions on the entire test set. &lt;/p>; &lt;br />; &lt;h2>;Key Results&lt;/h2>; &lt;p>; For COSP, we focus on a set of six arithmetic and commonsense reasoning problems, and we compare against 0-shot-CoT (ie, “&lt;a href=&quot;https://arxiv.org/abs/2205.11916&quot;>;Let&#39;s think step by step&lt;/a>;“ only). We use self-consistency in all baselines so that they use roughly the same amount of computational resources as COSP. Compared across three LLMs, we see that zero-shot COSP significantly outperforms the standard zero-shot baseline. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsRevpmsBSIsIMti071v4FFnK-khtMXTbqvf92wvdvKQhyphenhyphenTCADcsIOQhAtr1kfxkmoaQ5MNKmcJiYol1JmxCchzYl9Kn_9h9eaGZAvJBrRxsvJorbngS4fLaChBk6e9wtHgE7JvPMHN1gajpnhgZaCwuYOjo-CTUR9x8PbXEEbRG3vp8elQzMib5Kv5Qh-/s1883/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1738&quot; data-original-width=&quot;1883&quot; height=&quot;369&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsRevpmsBSIsIMti071v4FFnK-khtMXTbqvf92wvdvKQhyphenhyphenTCADcsIOQhAtr1kfxkmoaQ5MNKmcJiYol1JmxCchzYl9Kn_9h9eaGZAvJBrRxsvJorbngS4fLaChBk6e9wtHgE7JvPMHN1gajpnhgZaCwuYOjo-CTUR9x8PbXEEbRG3vp8elQzMib5Kv5Qh-/w400-h369/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Key results of COSP in six arithmetic (&lt;a href=&quot;https://arxiv.org/abs/1608.01413&quot; style= &quot;text-align: left;&quot;>;MultiArith&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot; style=&quot;text-align: left;&quot;>;GSM-8K&lt;/a>; , &lt;a href=&quot;https://aclanthology.org/D14-1058/&quot; style=&quot;text-align: left;&quot;>;AddSub&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1162 /tacl_a_00160&quot; style=&quot;text-align: left;&quot;>;SingleEq&lt;/a>;) and commonsense (&lt;a href=&quot;https://doi.org/10.18653/v1/N19-1421&quot; style=&quot;text-align : left;&quot;>;CommonsenseQA&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1162/tacl_a_00370&quot; style=&quot;text-align: left;&quot;>;StrategyQA&lt;/a>;) reasoning tasks using &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM-62B, PaLM-540B&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;GPT-3 ( code-davinci-001)&lt;/a>; models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class =&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcbJaMBl3_Pc5S_mp9lDWz4v_SS6Mzd3QBxyn-tWdtnDNgJLX1YPRHkdPtJBanhADoEDTzJqOsk2x4uOFoV8e0h8Tml_Nt1kOd5eT7TyC-MIqWEVEiY_kZQZ0wkPU9T5HwIHXv2IHeLXdGz1MvWiQFxNNe7CaKDfUs0j9R6F8cgN4T0dgtgp86xsF9cK6l/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1999&quot; height=&quot;325&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcbJaMBl3_Pc5S_mp9lDWz4v_SS6Mzd3QBxyn-tWdtnDNgJLX1YPRHkdPtJBanhADoEDTzJqOsk2x4uOFoV8e0h8Tml_Nt1kOd5eT7TyC-MIqWEVEiY_kZQZ0wkPU9T5HwIHXv2IHeLXdGz1MvWiQFxNNe7CaKDfUs0j9R6F8cgN4T0dgtgp86xsF9cK6l/w640-h325/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;USP improves significantly on 0-shot performance. “CLS” is an average of 15 classification tasks; “SFG” is the average of five short-form generation tasks; “LFG” is the average of two summarization tasks. “SFG (BBH)” is an average of all BIG-Bench Hard tasks, where each question is in SFG format.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For USP, we expand our analysis to a much wider range of tasks, including more than 25 classifications, short-form generation, and long-form generation tasks. Using the state-of-the-art PaLM 2 models, we also test against the &lt;a href=&quot;https://arxiv.org/abs/2210.09261&quot;>;BIG-Bench Hard&lt;/a>; suite of tasks where LLMs have previously underperformed compared to people. We show that in all cases, USP again outperforms the baselines and is competitive to prompting with golden examples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsJh9EIxiF8m6jqOM7vYniG6LX8gFVa0W3V7_DEOwMCEZjbk_M4104Tg-3qMUAbYXaR3RqMTQsrZ8TN3Np5I9SXHBVJGjB6E6_1khoubPbbR-AjUnV37TYGBHu2DJpR50Ek34dwcFkjJB-OTqW4T4E-zjQhJqPX-f_PxSj7322nU2qH2VK0TBMO1QKUkfT/s901 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;833&quot; height=&quot; 640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsJh9EIxiF8m6jqOM7vYniG6LX8gFVa0W3V7_DEOwMCEZjbk_M4104Tg-3qMUAbYXaR3RqMTQsrZ8TN3Np5I9SXHBVJGjB6E6_1khoubPbbR-AjUnV37TYGBHu2DJpR50Ek34dwcFkjJB-OTqW4T4E-zjQhJqPX-f_PxSj7322nU2qH2VK0TBMO1QKUkfT/w592-h640/image5.png&quot; width=&quot;592&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Accuracy on BIG- Bench Hard tasks with PaLM 2-M (each line represents a task of the suite). The gain/loss of USP (green stars) over standard 0-shot (green triangles) is shown in percentages. “Human” refers to average human performance; “AutoCoT” and “Random demo” are baselines we compared against in the&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2305.14926&quot;>;paper&lt;/a>;; and “3-shot” is the few-shot performance for three handcrafted demos in CoT format.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also analyze the working mechanism of USP by validating the key observation above on the relation between confidence and correctness, and we found that in an overwhelming majority of the cases, USP picks confident predictions that are more likely better in all task types considered, as shown in the如下图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjahSci_hJqgqxS-my79ZlINe9Dpi0yaLrMTXeTu-HXjmmAmhVXtylXK7BUdJGIPMFhFvqv31Wd6ux2ti1hJLIi9Rr8j_PIjD9ElUTWx0ViaP1fLg63Q9lqvgd7U74Uqi45IOU3CsqHaqjO94iSLyPD2dZCJf5x5hsHfU_6yukbJHpG6wDzdSVp7nwBHNFx/s1474/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;507&quot; data-original-width=&quot;1474&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjahSci_hJqgqxS-my79ZlINe9Dpi0yaLrMTXeTu-HXjmmAmhVXtylXK7BUdJGIPMFhFvqv31Wd6ux2ti1hJLIi9Rr8j_PIjD9ElUTWx0ViaP1fLg63Q9lqvgd7U74Uqi45IOU3CsqHaqjO94iSLyPD2dZCJf5x5hsHfU_6yukbJHpG6wDzdSVp7nwBHNFx/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;USP picks confident predictions that are more likely better. Ground-truth performance metrics against USP confidence scores in selected tasks in various task types (blue: CLS, orange: SFG, green: LFG) with PaLM-540B.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Zero-shot inference is a highly sought-after capability of modern LLMs, yet the success in which poses unique challenges. We propose COSP and USP, a family of versatile, zero-shot automatic prompting techniques applicable to a wide range of tasks. We show large improvement over the state-of-the-art baselines over numerous task and model combinations. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was conducted by Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Martin Eisenschlos, Sercan Ö. Arık, and Tomas Pfister. We would like to thank Jinsung Yoon Xuezhi Wang for providing helpful reviews, and other colleagues at Google Cloud AI Research for their discussion and feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4882962745371313901/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4882962745371313901&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4882962745371313901&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot; rel=&quot;alternate&quot; title=&quot;Zero-shot adaptive prompting of large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqTosaeIs4AMYukkmgUBEii6iQcrr9_dNKM0cHnW6m9Wi8yX0V-QCduQfkqLyQPNpVbze3OFO-nPG5Wm9DLMM8pEAfhQGq-TEJroLJGQyqNr-hlJNkToBCmgJbphrCRvlv95gkDQH0ScT7VrXu2VCTKyiWy8yYM4G8voF0kD0K2oxwg2M2xBcz1yQnU0Zb/s72-c/USP.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4489259534129284932&lt;/id>;&lt;published>;2023-11-01T10:30:00.002-07:00&lt;/published>;&lt;updated>;2023-11-06T09:07:40.958-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MetNet-3: A state-of-the-art neural weather model available in Google products&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Samier Merchant, Google Research, and Nal Kalchbrenner, Google DeepMind&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdgnhML03N9vxEdGH1TkBATtxGpjyO5XYgZwJY5dY0-sPIAvrmCll4J8I9owyJTNOHZdq6MMZskWsYJDZivZA_zvj2atWhUsPoxWnNyifiFAm83GC2EsZ4xgre8bCk32Yzv3vlR4pGn12H7T5Vkbz5BaErZ22JRB-OqveQ7EDHsrCYjKN65Soc1FrZNwvu/s1600/metnethero1.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Forecasting weather variables such as precipitation, temperature, and wind is key to numerous aspects of society, from daily planning and transportation to energy production. As we continue to see more extreme weather events such as floods, droughts, and heat waves, accurate forecasts can be essential to preparing for and mitigating their effects. The first 24 hours into the future are especially important as they are both highly predictable and actionable, which can help people make informed decisions in a timely manner and stay safe. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today we present a new weather model called &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;MetNet-3&lt;/a>;, developed by Google Research and Google DeepMind. Building on the earlier &lt;a href=&quot;https://blog.research.google/2020/03/a-neural-weather-model-for-eight-hour.html?m=1&quot;>;MetNet&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;MetNet-2&lt;/a>; models, MetNet-3 provides high resolution predictions up to 24 hours ahead for a larger set of core variables, including precipitation, surface temperature, wind speed and direction, and dew point. MetNet-3 creates a temporally smooth and highly granular forecast, with lead time intervals of 2 minutes and spatial resolutions of 1 to 4 kilometers. MetNet-3 achieves strong performance compared to traditional methods, outperforming the best single- and multi-member physics-based &lt;a href=&quot;https://en.wikipedia.org/wiki/Numerical_weather_prediction&quot;>;numerical weather prediction&lt;/a>; (NWP) models — such as &lt;a href=&quot;https://rapidrefresh.noaa.gov/hrrr/&quot;>;High-Resolution Rapid Refresh&lt;/a>; (HRRR) and &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts#:~:text=ENS%20is%20a%20probabilistic%20forecast,high%20winds%20or%20heavy%20rain).&quot;>;ensemble forecast suite&lt;/a>; (ENS) — for multiple regions up to 24 hours ahead. &lt;/p>; &lt;p>; Finally, we&#39;ve integrated MetNet-3&#39;s capabilities across various Google &lt;a href=&quot;https://support.google.com/websearch/answer/13692898&quot;>;products and technologies&lt;/a>; where weather is relevant. Currently available in the contiguous United States and parts of Europe with a focus on 12 hour precipitation forecasts, MetNet-3 is helping bring accurate and reliable weather information to people in multiple countries and languages. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQcwPsDQPUe4Uon7vWWSewbqcWsAdfUIJ4yLLFiCvdQKu4ffT6E5qIMeiabtxK5wudSL-jjxa_fW5aOaBvDILq_dQzeT4RMSULORJZrjwkDscDxLnLflUybqHlPf1J8O7KB171g5I9kLVgRbGP0mr0HxbG0pY7J9ojoEZLl4JZHaMQH490XmUR_IUj_YMO/s904/image55.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQcwPsDQPUe4Uon7vWWSewbqcWsAdfUIJ4yLLFiCvdQKu4ffT6E5qIMeiabtxK5wudSL-jjxa_fW5aOaBvDILq_dQzeT4RMSULORJZrjwkDscDxLnLflUybqHlPf1J8O7KB171g5I9kLVgRbGP0mr0HxbG0pY7J9ojoEZLl4JZHaMQH490XmUR_IUj_YMO/s16000/image55.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixRY-kzsepNdP_arXnJbHPJFViN_N4CzjOYH_1YxfjIDI5Nben4u8BoJ-tcYrrw4a3Jp7HFBGmakeBMqKAINeVFssClJHNUjvBhYHY6vpy6nOdpEoFDhCulwIE8OM9e7fRRwXqW01AeWUJjqmnNDn32ScCeQ2S64aNvDgigDes5vWA1_RrT7oMxK8sttG7/s904/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEixRY-kzsepNdP_arXnJbHPJFViN_N4CzjOYH_1YxfjIDI5Nben4u8BoJ-tcYrrw4a3Jp7HFBGmakeBMqKAINeVFssClJHNUjvBhYHY6vpy6nOdpEoFDhCulwIE8OM9e7fRRwXqW01AeWUJjqmnNDn32ScCeQ2S64aNvDgigDes5vWA1_RrT7oMxK8sttG7/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing= “0”类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MetNet-3 precipitation output summarized into actionable forecasts in Google Search on mobile.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Densification of sparse observations&lt;/h2>; &lt;p>; Many recent machine learning weather models use the atmospheric state generated by traditional methods (eg, data assimilation from NWPs) as the primary starting point to build forecasts. In contrast, a defining feature of the MetNet models has been to use direct observations of the atmosphere for training and evaluation. The advantage of direct observations is that they often have higher fidelity and resolution. However, direct observations come from a large variety of sensors at different altitudes, including weather stations at the surface level and satellites in orbit, and can be of varying degrees of sparsity. For example, precipitation estimates derived from radar such as &lt;a href=&quot;https://mrms.nssl.noaa.gov/&quot;>;NOAA&#39;s Multi-Radar/Multi-Sensor System&lt;/a>; (MRMS) are relatively dense images, whereas weather stations located on the ground that provide measurements for variables such as temperature and wind are mere points spread over a region. &lt;/p>; &lt;p>; In addition to the data sources used in previous MetNet models, MetNet-3 includes point measurements from weather stations as both inputs and targets with the goal of making a forecast at all locations. To this end, MetNet-3&#39;s key innovation is a technique called densification, which merges the traditional two-step process of data assimilation and simulation found in physics-based models into a single pass through the neural network. The main components of densification are illustrated below. Although the densification technique applies to a specific stream of data individually, the resulting densified forecast benefits from all the other input streams that go into MetNet-3, including topographical, satellite, radar, and NWP analysis features. No NWP forecasts are included in MetNet-3&#39;s default inputs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie1m1p0i-MWhS7Ih5RGzV-AQuDDPwgao4SpmnSUTdSsy7fcEwk4Soj5IJ8FqtGjhvi4ot2HKZdaQh3Hpu4CviRsx7FujT_4bbvpV8mu15Zt5bO5KbMGaaqIZoAGUp77ltVYH-zt2HTwVxbuGZHJt-0lbXZT-ukJH_KtB3pnHdRrRpZ2r5WgMSNGXnu-H8j /s1929/image22.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1929&quot; data-original-width=&quot;1600&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie1m1p0i-MWhS7Ih5RGzV-AQuDDPwgao4SpmnSUTdSsy7fcEwk4Soj5IJ8FqtGjhvi4ot2HKZdaQh3Hpu4CviRsx7FujT_4bbvpV8mu15Zt5bO5KbMGaaqIZoAGUp77ltVYH-zt2HTwVxbuGZHJt-0lbXZT-ukJH_KtB3pnHdRrRpZ2r5WgMSNGXnu-H8j/s16000/image22.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;A&lt;/b>;) During training, a fraction of the weather stations are masked out from the input while kept in the target. &lt;b>;B&lt;/b>;) To evaluate generalization to untrained locations, a set of weather stations represented by squares is never used for training and is only used for evaluation. &lt;b>;C&lt;/b>;) Data from these held out weather stations with sparse coverage is included during evaluation to determine prediction quality in these areas. &lt;b>;D&lt;/b>;) The final forecasts use the full set of training weather stations as input and produce fully dense forecasts aided by spatial parameter sharing.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;High resolution in space and time&lt;/h2>; &lt;p>; A central advantage of using direct observations is their high spatial and temporal解决。 For example, weather stations and ground radar stations provide measurements every few minutes at specific points and at 1 km resolutions, respectively; this is in stark contrast with the assimilation state from the state-of-the-art model &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts#:~:text=ENS%20is%20a%20probabilistic%20forecast,high%20winds%20or%20heavy%20rain).&quot;>;ENS&lt;/a>;, which is generated every 6 hours at a resolution of 9 km with hour-by-hour forecasts. To handle such a high resolution, MetNet-3 preserves another of the defining features of this series of models, &lt;em>;lead time conditioning&lt;/em>;. The lead time of the forecast in minutes is directly given as input to the neural network. This allows MetNet-3 to efficiently model the high temporal frequency of the observations for intervals as brief as 2 minutes. Densification combined with lead time conditioning and high resolution direct observations produces a fully dense 24 hour forecast with a temporal resolution of 2 minutes, while learning from just 1,000 points from the &lt;a href=&quot;https://madis.ncep.noaa.gov/madis_OMO.shtml&quot;>;One Minute Observation&lt;/a>; (OMO) network of weather stations spread across the United States. &lt;/p>; &lt;p>; MetNet-3 predicts a marginal multinomial probability distribution for each output variable and each location that provides rich information beyond just the mean. This allows us to compare the probabilistic outputs of MetNet-3 with the outputs of advanced probabilistic ensemble NWP models, including the ensemble forecast ENS from the &lt;a href=&quot;https://www.ecmwf.int/&quot;>;European Centre for Medium-Range Weather Forecasts&lt;/a>; and the &lt;a href=&quot;https://www.spc.noaa.gov/exper/href/&quot;>;High Resolution Ensemble Forecast&lt;/a>; (HREF) from the &lt;a href=&quot;https://www.noaa.gov/&quot;>;National Oceanic and Atmospheric Administration of the US&lt;/a>;. Due to the probabilistic nature of the outputs of both models, we are able to compute scores such as the &lt;a href=&quot;https://confluence.ecmwf.int/display/FUG/Section+12.B+Statistical+Concepts+-+Probabilistic+Data#:~:text=The%20Continuous%20Ranked%20Probability%20Score,the%20forecast%20is%20wholly%20inaccurate.&quot;>;Continuous Ranked Probability Score&lt;/a>; (CRPS). The following graphics highlight densification results and illustrate that MetNet&#39;s forecasts are not only of much higher resolution, but are also more accurate when evaluated at the overlapping lead times. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJsf6Y6gV9VjK_rS_Bf_WLWdsJOq3sQbdaW26VSp2vX1Fq5j7VcWl4VDi3BeBFpEcH_YGrkU9ozJyuP5dh8tWWCU4yGzlmGBTfwM-kXGKZvdvI1DF17V4kSJSGGBIacqaCO4N1Oc8P4PymPWdglJbew_cjP9reFSJuHR3_ikZfZFuzN6aC8F17TAtiJPIg/s768/image44.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiJsf6Y6gV9VjK_rS_Bf_WLWdsJOq3sQbdaW26VSp2vX1Fq5j7VcWl4VDi3BeBFpEcH_YGrkU9ozJyuP5dh8tWWCU4yGzlmGBTfwM-kXGKZvdvI1DF17V4kSJSGGBIacqaCO4N1Oc8P4PymPWdglJbew_cjP9reFSJuHR3_ikZfZFuzN6aC8F17TAtiJPIg/s16000/image44.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;b>;Top&lt;/b>;: MetNet-3&#39;s forecast of wind speed for each 2 minutes over the future 24 hours with a spatial resolution of 4km. &lt;b>;Bottom&lt;/b>;: ENS&#39;s hourly forecast with a spatial resolution of 18 km. &lt;br />;The two distinct regimes in spatial structure are primarily driven by the presence of the Colorado mountain ranges. Darker corresponds to higher wind speed. More samples available here: &lt;a href=&quot;https://youtube.com/watch?v=iB1DzHNqH_o&quot;>;1&lt;/a>;, &lt;a href=&quot;https://youtube.com/watch?v=LlWB558jKJk&quot;>;2&lt;/a>;, &lt;a href=&quot;https://youtube.com/watch?v=74bFo3nkbe4&quot;>;3&lt;/a>;, &lt;a href=&quot;https://youtube.com/watch?v=MKUzYQZn9sQ&quot;>;4&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBhlSum7x274E9KQGzLnjM9iXNEhifOJjKzt1Cwa5YyABCbaB68Mkr3gFvIVUhyphenhyphenaIGOqUE78MqGTK992NK8zrdKrqKxtFlYf1qeWYNkTa4PVzD3u_9lmQAjKnbLILHAkPhIOCvyAI6qBtfyf-z_xgUys3gXRJd_GSs3-qnyq0yFbjvmxdXAbVldV-xrIRJ/s1120/image11.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;694&quot; data-original-width=&quot;1120&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBhlSum7x274E9KQGzLnjM9iXNEhifOJjKzt1Cwa5YyABCbaB68Mkr3gFvIVUhyphenhyphenaIGOqUE78MqGTK992NK8zrdKrqKxtFlYf1qeWYNkTa4PVzD3u_9lmQAjKnbLILHAkPhIOCvyAI6qBtfyf-z_xgUys3gXRJd_GSs3-qnyq0yFbjvmxdXAbVldV-xrIRJ/s16000/image11.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance comparison between MetNet-3 and NWP baseline for wind speed based on CRPS (lower is better). In the hyperlocal setting, values of the test weather stations are given as input to the network during evaluation; the results improve further especially in the early lead times.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In contrast to weather station variables, precipitation estimates are more dense as they come from ground radar. MetNet-3&#39;s modeling of precipitation is similar to that of MetNet-1 and 2, but extends the high resolution precipitation forecasts with a 1km spatial granularity to the same 24 hours of lead time as the other variables, as shown in the animation below. MetNet-3&#39;s performance on precipitation achieves a better CRPS value than ENS&#39;s throughout the 24 hour range. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidyInWvqdPpdIndLrxzykAODCeJ_p69uqvYOpasjFcBQU5o8Mtr-DiLfZXZrkJel9TD9SxZEmyIb58r6TZjRw57D8aSjl9P2jxCOsK7XZeXY0J3B8UMIFnl6aqXqhd0wft_NQGBi9KqpSUHAgw2c4JoYMdt27sKp6xcvOyMfjASpaZZzlI9o8lesj3GsrL/s720/image14.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;683&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEidyInWvqdPpdIndLrxzykAODCeJ_p69uqvYOpasjFcBQU5o8Mtr-DiLfZXZrkJel9TD9SxZEmyIb58r6TZjRw57D8aSjl9P2jxCOsK7XZeXY0J3B8UMIFnl6aqXqhd0wft_NQGBi9KqpSUHAgw2c4JoYMdt27sKp6xcvOyMfjASpaZZzlI9o8lesj3GsrL/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Case study for Thu Jan 17 2019 00:00 UTC showing the probability of instantaneous precipitation rate being above 1 mm/h on CONUS. Darker corresponds to a higher probability value. The maps also show the prediction threshold when optimized towards Critical Success Index &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;CSI&lt;/a>; (dark blue contours). This specific case study shows the formation of a new large precipitation pattern in the central US; it is not just forecasting of existing patterns. &lt;br />;&lt;b>;Top:&lt;/b>; ENS&#39;s hourly forecast. &lt;b>;Center:&lt;/b>; Ground truth, source NOAA&#39;s MRMS. &lt;b>;Bottom:&lt;/b>; Probability map as predicted by MetNet-3. &lt;a href=&quot;https://www.youtube.com/watch?v=TXqR9lL4368&quot;>;Native resolution available here.&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_4M2Sz50c_PDZkyHqZGfc5p5aRGpAS04ztN9N3s3VBn4_AD8GN7Vv6Vw-2phokpqtamutHT_6nGSsXb7271cfijLu3vJT1IV8Mmo1wlq1jfYcUPNs7TL6z0Cls3qGD1jA4Z0uRpj_rNXYLpFSbHEIqNOAA_V8VE_ZhsO7o-D64nDdmRei_hPEY7YT8lcg/s1102/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;682&quot; data-original-width=&quot;1102&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_4M2Sz50c_PDZkyHqZGfc5p5aRGpAS04ztN9N3s3VBn4_AD8GN7Vv6Vw-2phokpqtamutHT_6nGSsXb7271cfijLu3vJT1IV8Mmo1wlq1jfYcUPNs7TL6z0Cls3qGD1jA4Z0uRpj_rNXYLpFSbHEIqNOAA_V8VE_ZhsO7o-D64nDdmRei_hPEY7YT8lcg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance comparison between MetNet-3 and NWP baseline for instantaneous precipitation rate on CRPS (lower is better).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Delivering realtime ML forecasts&lt;/h2>; &lt;p>; Training and evaluating a weather forecasting model like MetNet-3 on historical data is only a part of the process of delivering ML-powered forecasts to users. There are many considerations when developing a real-time ML system for weather forecasting, such as ingesting real-time input data from multiple distinct sources, running inference, implementing real-time validation of outputs, building insights from the rich output of the model that lead to an intuitive user experience, and serving the results at Google scale — all on a continuous cycle, refreshed every few minutes. &lt;/p>; &lt;p>; We developed such a real-time system that is capable of producing a precipitation forecast every few minutes for the entire contiguous United States and for 27 countries in Europe for a lead time of up to 12 hours. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg88AA6lzoFtJd9ZOXt6AiiT_gTtFcJwsZNzUJ63kuYtq7XYs0LHUSp3q37zOPolA-rR_WQPciuDZsg-4Y3J0qrLUmNxMi1iBqyR4ICy4MKwRFXHtQhfkWdwPREd4qm9FVlN6rpLEebDC7MfBg7hToXhQvdsFoGObtu-Lqty3ZQSALf1yjna37tJY4fAptE/s1600/image6.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;367&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg88AA6lzoFtJd9ZOXt6AiiT_gTtFcJwsZNzUJ63kuYtq7XYs0LHUSp3q37zOPolA-rR_WQPciuDZsg-4Y3J0qrLUmNxMi1iBqyR4ICy4MKwRFXHtQhfkWdwPREd4qm9FVlN6rpLEebDC7MfBg7hToXhQvdsFoGObtu-Lqty3ZQSALf1yjna37tJY4fAptE/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the process of generating precipitation forecasts using MetNet-3.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The system&#39;s uniqueness stems from its use of near-continuous inference, which allows the model to constantly create full forecasts based on incoming data streams. This mode of inference is different from traditional inference systems, and is necessary due to the distinct characteristics of the incoming data. The model takes in various data sources as input, such as radar, satellite, and numerical weather prediction assimilations. Each of these inputs has a different refresh frequency and spatial and temporal resolution. Some data sources, such as weather observations and radar, have characteristics similar to a continuous stream of data, while others, such as NWP assimilations, are similar to batches of data. The system is able to align all of these data sources spatially and temporally, allowing the model to create an updated understanding of the next 12 hours of precipitation at a very high cadence. &lt;/p>; &lt;p>; With the above process, the model is able to predict arbitrary discrete probability distributions. We developed novel techniques to transform this dense output space into user-friendly information that enables rich experiences throughout Google products and technologies. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Weather features in Google products&lt;/h2>; &lt;p>; People around the world rely on Google every day to provide helpful, timely, and accurate information about the weather. This information is used for a variety of purposes, such as planning outdoor activities, packing for trips, and staying safe during severe weather events. &lt;/p>; &lt;p>; The state-of-the-art accuracy, high temporal and spatial resolution, and probabilistic nature of MetNet-3 makes it possible to create unique hyperlocal weather insights. For the contiguous United States and Europe, MetNet-3 is operational and produces real-time 12 hour precipitation forecasts that are now served across Google &lt;a href=&quot;https://support.google.com/websearch/answer/13692898&quot;>;products and technologies&lt;/a>; where weather is relevant, such as Search. The rich output from the model is synthesized into actionable information and instantly served to millions of users. &lt;/p>; &lt;p>; For example, a user who searches for weather information for a precise location from their mobile device will receive highly localized precipitation forecast data, including timeline graphs with granular minute breakdowns depending on the product. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4R591KKD1ZkmhTrjo28JovCeeo2bGjb0Tn5Ohr8KEooVqZqSNlgsrJrROaPWn5XXBzEohkhZMjaX2AV3M1RikyLgO7LfIgTFt54-uumb7xxPU6blnuFC8dN8W2SjK85tBKfZQ9Kn4oR-988YKXVUTbu-N5LWWX6JurqN6RRad7Bve59oEdZC-eMsn4HH9/s600/metnet3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;600&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4R591KKD1ZkmhTrjo28JovCeeo2bGjb0Tn5Ohr8KEooVqZqSNlgsrJrROaPWn5XXBzEohkhZMjaX2AV3M1RikyLgO7LfIgTFt54-uumb7xxPU6blnuFC8dN8W2SjK85tBKfZQ9Kn4oR-988YKXVUTbu-N5LWWX6JurqN6RRad7Bve59oEdZC-eMsn4HH9/s16000/metnet3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MetNet-3 precipitation output in weather on the Google app on Android (&lt;b>;left&lt;/b>;) and mobile web Search (&lt;b>;right&lt; /b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2 >; &lt;p>; MetNet-3 is a new deep learning model for weather forecasting that outperforms state-of-the-art physics-based models for 24-hour forecasts of a core set of weather variables. It has the potential to create new possibilities for weather forecasting and to improve the safety and efficiency of many activities, such as transportation, agriculture, and energy production. MetNet-3 is operational and its forecasts are served across several Google products where weather is relevant. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of this effort 。 We would like to especially thank those from Google DeepMind (Di Li, Jeremiah Harmsen, Lasse Espeholt, Marcin Andrychowicz, Zack Ontiveros), Google Research (Aaron Bell, Akib Uddin, Alex Merose, Carla Bromberg, Fred Zyda, Isalo Montacute, Jared Sisk, Jason Hickey, Luke Barrington, Mark Young, Maya Tohidi, Natalie Williams, Pramod Gupta, Shreya Agrawal, Thomas Turnbull, Tom Small, Tyler Russell), and Google Search (Agustin Pesciallo, Bill Myers, Danny Cheresnick, Jonathan Karsh, Lior Cohen, Maca Piombi, Maia Diamant, Max Kamenetsky, Maya Ekron, Mor Schlesinger, Neta Gefen-Doron, Nofar Peled Levi, Ofer Lehr, Or Hillel, Rotem Wertman, Tamar Shevach,Vinay Ruelius Shah, Yechie Labai).&lt;/em>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4489259534129284932/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4489259534129284932&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4489259534129284932&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html&quot; rel=&quot;alternate&quot; title=&quot;MetNet-3: A state-of-the-art neural weather model available in Google products&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdgnhML03N9vxEdGH1TkBATtxGpjyO5XYgZwJY5dY0-sPIAvrmCll4J8I9owyJTNOHZdq6MMZskWsYJDZivZA_zvj2atWhUsPoxWnNyifiFAm83GC2EsZ4xgre8bCk32Yzv3vlR4pGn12H7T5Vkbz5BaErZ22JRB-OqveQ7EDHsrCYjKN65Soc1FrZNwvu/s72-c/metnethero1.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4973261706617691472&lt;/id>;&lt;published>;2023-10-27T13:22:00.001-07:00&lt;/published>;&lt;updated>;2023-10-31T14:14:34.982-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Android Wear&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Audioplethysmography for cardiac monitoring with hearable devices&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Xiaoran &quot;Van&quot; Fan, Experimental Scientist, and Trausti Thormundsson, Director, Google &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnCItsRGs939CGCBBAi5fA-xwk9JQi7b2CQ3voKK583p4uTkQkLIXUd1lU71SLynom0Yt78bRxDflolUzzfol5UbfkPmCaNn8bIUxwAbLDatqZTPxP7SYOT45g9qJODdM1kvT6NQUsixtFTiBYr_h_Hx-pFRsmbcBKdnW3WkbcD4Bcr_cZE590VL-cSu-a/s320/hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; The market for &lt;a href=&quot;https://www.telink-semi.com/introduction-true-wireless-stereo/&quot;>;true wireless stereo&lt;/a>; (TWS) &lt;a href=&quot; https://en.wikipedia.org/wiki/Active_noise_control&quot;>;active noise canceling&lt;/a>; (ANC) hearables (headphones and earbuds) has been soaring in recent years, and the global shipment volume will nearly &lt;a href=&quot; https://www.idc.com/promo/wearablevendor&quot;>;double&lt;/a>; that of smart wristbands and watches in 2023. The on-head time for hearables has extended significantly due to the recent advances in ANC, transparency mode,和人工智能。 Users frequently wear hearables not just for music listening, but also for exercising, focusing, or simply mood adjustment. However, hearable health is still mostly uncharted territory for the consumer market. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;APG: Audioplethysmography for Cardiac Monitoring in Hearables&lt;/a>;,” presented at &lt;a href=&quot;https://sigmobile.org/mobicom/2023/&quot;>;MobiCom 2023&lt;/a>;, we introduce a novel active in-ear health sensing modality. Audioplethysmography (APG) enables ANC hearables to monitor a user&#39;s physiological signals, such as heart rate and heart rate variability, without adding extra sensors or compromising battery life. APG exhibits high resilience to motion artifacts, adheres to &lt;a href=&quot;https://www.health.belgium.be/sites/default/files/uploads/fields/fpshealth_theme_file/19099349/Canadian%20guidelines%20for%20ultrasound.pdf&quot;>;safety regulations&lt;/a>; with an 80 dB margin below the limit, remains unaffected by seal conditions, and is inclusive of all skin tones. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3if_joqkSgp5nI0YvfWgUtgi3FoOtmeNcwcDdNNaMlPizqHpbuaeVFrp1MguLPQpT0hqrQaldH0ymqwjDzsD-u3qDR-r8Z0rnB7lsDuF9iab9CdR6GRPI7OPtqLFR29mRwfHz06kwqhppe7mvI9iVIBi8DtpIYh6E-UXs1RwUffTRxKVViTuXpSLXLAXI/s1600/image4.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3if_joqkSgp5nI0YvfWgUtgi3FoOtmeNcwcDdNNaMlPizqHpbuaeVFrp1MguLPQpT0hqrQaldH0ymqwjDzsD-u3qDR-r8Z0rnB7lsDuF9iab9CdR6GRPI7OPtqLFR29mRwfHz06kwqhppe7mvI9iVIBi8DtpIYh6E-UXs1RwUffTRxKVViTuXpSLXLAXI/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;APG sends a low intensity ultrasound transmitting wave (TX wave) using an ANC headphone&#39;s speakers and collects the receiving wave (RX wave) via the on-board feedback microphones. The APG signal is a pulse-like waveform that synchronizes with heartbeat and reveals rich cardiac information, such as &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/32403992/#:~:text=The%20dicrotic%20notch%20is%20a,of%20diastole%20in%20these%20arteries.&quot;>;dicrotic notches&lt;/a>;. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Health sensing in the ear canal&lt;/h2>; &lt;p>; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Ear_canal&quot;>;auditory canal&lt;/a>; receives its blood supply from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_auricular_artery&quot;>;arteria auricularis profunda&lt;/a>;, also known as the deep ear artery. This artery forms an intricate network of smaller vessels that extensively permeate the auditory canal. Slight variations in blood vessel shape caused by the heartbeat (and &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8834671&quot;>;blood pressure&lt;/a>;) can lead to subtle changes in the volume and pressure of the ear canals, making the ear canal an ideal location for health sensing. &lt;/p>; &lt;p>; Recent research has explored using hearables for health sensing by packaging together a plethora of sensors — eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram&quot;>;photoplethysmograms&lt;/a>; (PPG) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Electrocardiography&quot;>;electrocardiograms&lt;/a>; (ECG) — with a microcontroller to enable health applications, such as sleep monitoring, heart rate and blood pressure tracking. However, this sensor mounting paradigm inevitably adds cost, weight, power consumption, acoustic design complexity, and form factor challenges to hearables, constituting a strong barrier to its wide adoption. &lt;/p>; &lt;p>; Existing ANC hearables deploy feedback and feedforward microphones to navigate the ANC function. These microphones create &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3550314&quot;>;new opportunities&lt;/a>; for various sensing applications as they can detect or record many bio-signals inside and outside the ear canal. For example, feedback microphones can be used to listen to heartbeats and feedforward microphones can hear respirations. &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3458864.3467680&quot;>;Academic research&lt;/a>; on this passive sensing paradigm has prompted many mobile applications, including heart rate monitoring, ear disease diagnosis, respiration monitoring, and body activity recognition. However, microphones in consumer-grade ANC headphones come with &lt;a href=&quot;https://research.google/pubs/pub52579/&quot;>;built-in high-pass filters&lt;/a>; to prevent saturation from body motions or strong wind噪音。 The signal quality of passive listening in the ear canal also heavily relies on the earbud seal conditions. As such, it is challenging to embed health features that rely on the passive listening of low frequency signals (≤ 50 Hz) on commercial ANC headphones. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Measuring tiny physiological signals&lt;/h2>; &lt;p>; APG bypasses the aforementioned ANC headphone hardware constraints by sending a low intensity ultrasound probing signal through an ANC headphone&#39;s speakers. This signal triggers echoes, which are received via on-board feedback microphones. We observe that the tiny ear canal skin displacement and heartbeat vibrations modulate these ultrasound echoes. &lt;/p>; &lt;p>; We build a &lt;a href=&quot;https://en.wikipedia.org/wiki/Acoustic_resonance&quot;>;cylindrical resonance&lt;/a>; model to understand APG&#39;s underlying physics. This phenomenon happens at an extremely small scale, which makes the raw pulse signal invisible in the raw received ultrasound. We adopt &lt;a href=&quot;https://www.rp-photonics.com/optical_heterodyne_detection.html&quot;>;coherent detection&lt;/a>; to retrieve this micro physiological modulation under the noise floor (we term this retrieved signal as mixed-down signal, see the &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;paper&lt;/a>; for more details). The final APG waveform looks strikingly similar to a PPG waveform, but provides an improved view of cardiac activities with more pronounced &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/32403992/#:~:text=The%20dicrotic%20notch%20is%20a,of%20diastole%20in%20these%20arteries.&quot;>;dicrotic notches&lt;/a>; (ie, pressure waveforms that provide rich insights about the central artery system, such as blood pressure). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEoN64wQBOUKAhV6SHbEMqIcAuVCk01zBki8pnV1xoEseaJCT1uNRprkjKyFLYRin4a-iSrPjg3Pr-eEgc_YrOYihU61CaKl8CN5K8ET_g8vOFxHhUKQaq7Fb9xMhGVVLMS_AouTZvfDfXYRVz0ExhCIV4je2Hxqd-CVfKmmpfGjLIGeKawPEff6Ei5gfT/s1600/image5.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEoN64wQBOUKAhV6SHbEMqIcAuVCk01zBki8pnV1xoEseaJCT1uNRprkjKyFLYRin4a-iSrPjg3Pr-eEgc_YrOYihU61CaKl8CN5K8ET_g8vOFxHhUKQaq7Fb9xMhGVVLMS_AouTZvfDfXYRVz0ExhCIV4je2Hxqd-CVfKmmpfGjLIGeKawPEff6Ei5gfT/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A cylindrical model with cardiac activities ℎ(𝑡) that modulates both the phase and amplitude of the &lt;a href=&quot;https://www.digikey.com/en/articles /the-basics-of-mixers&quot;>;mixed-down&lt;/a>; signal. Based on the simulation from our analytical model, the amplitude 𝑅(𝑡) and phase Φ(𝑡) of the mixed-down APG signals both reflect the cardiac activities ℎ(𝑡).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;APG sensing in practice&lt;/h2>; &lt;p>; During our initial experiments, we observed that APG works robustly with bad earbuds seals and with music playing. However, we noticed the APG signal can sometimes be very noisy and could be heavily disturbed by body motion. At that point, we determined that in order to make APG useful, we had to make it more robust to compete with &lt;a href=&quot;https://curiouscyborg.com/history-of-photoplethysmography/#:~:text=The%20Photoplethysmogram%20was%20first%20observed,the%20light%20interaction%20with%20tissue.&quot;>;more than 80 years&lt;/a>; of PPG development. &lt;/p>; &lt;p>; While PPGs are widely used and highly advanced, they do have some limitations. For example, PPGs sensors typically use two to four &lt;a href=&quot;https://en.wikipedia.org/wiki/Diode&quot;>;diodes&lt;/a>; to send and receive light frequencies for sensing. However, due to the ultra high-frequency nature (hundreds of &lt;a href=&quot;https://en.wikipedia.org/wiki/Terahertz_radiation&quot;>;Terahertz&lt;/a>;) of the light, it&#39;s difficult for a single diode to send multiple colors with different frequencies. On the other hand, we can easily design a low-cost and low-power system that generates and receives more than ten audio tones (frequencies). We leverage &lt;a href=&quot;https://en.wikipedia.org/wiki/Diversity_scheme&quot;>;channel diversity&lt;/a>;, a physical phenomenon that describes how wireless signals (eg, light and audio) at different frequencies have different characters (eg, different attenuation and reflection coefficients) when the signal propagates in a medium, to enable a higher quality APG signal and motion resilience. &lt;/p>; &lt;p>; Next, we experimentally demonstrate the effectiveness of using multiple frequencies in the APG signaling. We transmit three probing signals concurrently with their frequencies spanning evenly from 30 KHz to 32 KHz. A participant was asked to shake their head four times during the experiment to introduce interference. The figure below shows that different frequencies can be transmitted simultaneously to gather various information with &lt;a href=&quot;https://www.rp-photonics.com/optical_heterodyne_detection.html&quot;>;coherent detection&lt;/a>;, a unique advantage to APG 。 &lt;/p>; &lt;p>; The 30 kHz phase shows the four head movements and the magnitude (amplitude) of 31 kHz shows the pulse wave signal. This observation shows that some ultrasound frequencies might be sensitive to cardiac activities while others might be sensitive to motion. Therefore, we can use the multi-tone APG as a calibration signal to find the best frequency that measures heart rate, and use only the best frequency to get high-quality pulse waveform. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP_DLCbWWNJRvMPnIbdAbij_YKuIuYGbTiN-drwsJnEkrybDQT_yNHIqKYrqhxfq8CrWSEVZ6kvCvZ1tIUoXtH5GBqEfyQjkjChwVqL_Cxc0dO6i3vJi_sPo7hjElUuxH9f8d3E6nkDBNGNNgCRdFhfllW8OnhGXVS120-e4Yh67vECY7_lQpDOH2I42TU/s1761/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1380&quot; data-original-width=&quot;1761&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP_DLCbWWNJRvMPnIbdAbij_YKuIuYGbTiN-drwsJnEkrybDQT_yNHIqKYrqhxfq8CrWSEVZ6kvCvZ1tIUoXtH5GBqEfyQjkjChwVqL_Cxc0dO6i3vJi_sPo7hjElUuxH9f8d3E6nkDBNGNNgCRdFhfllW8OnhGXVS120-e4Yh67vECY7_lQpDOH2I42TU/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;The mixed-down amplitude (upper row) and phase (bottom row) for a customized multi-tone APG signal that spans from 30 kHz to 32 kHz. With channel diversity, the cardiac activities are captured in some frequencies (eg, magnitude of 31 kHz) and head movements are captured in other frequencies (eg, magnitude of 30 kHz, 30 kHz, and phase of 31 kHz). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; After choosing the best frequency to measure heart rate, the APG pulse waveform becomes more visible with pronounced dicrotic notches , and enables accurate &lt;a href=&quot;https://en.wikipedia.org/wiki/Heart_rate_variability&quot;>;heart rate variability&lt;/a>; measurement. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0mjyX5qTrzW9rN1qexcI3Nm0hgn03m-K_kGn_A8yfqlviJ3Rc3tod-srqQ-6vuoPEo6em7z3Qx7NpgkYTMRiNVtzUidLMc1lp48n6O8hsLMqFQc8z5CKI8mqSowmEbg5ojxdG4vaYreLbv0q2YwQAk3hKwdSd1cdwQMGVtFmgwn46nvCYRuPEi6hdpcVh/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0mjyX5qTrzW9rN1qexcI3Nm0hgn03m-K_kGn_A8yfqlviJ3Rc3tod-srqQ-6vuoPEo6em7z3Qx7NpgkYTMRiNVtzUidLMc1lp48n6O8hsLMqFQc8z5CKI8mqSowmEbg5ojxdG4vaYreLbv0q2YwQAk3hKwdSd1cdwQMGVtFmgwn46nvCYRuPEi6hdpcVh/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;The final APG signal used in the measurement phase (&lt;strong>;left&lt;/strong>;) and chest &lt;a href=&quot;https://en.wikipedia.org/wiki /Electrocardiography&quot;>;ECG&lt;/a>; signal (&lt;strong>;right&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Multi-tone translates to multiple simultaneous observations, which enable the development of &lt;a href=&quot;https://en.wikipedia.org/wiki/Array_processing&quot;>;array signal processing&lt;/a>; techniques. We demonstrate the spectrogram of a running session APG experiment before and after applying &lt;a href=&quot;https://en.wikipedia.org/wiki/Signal_separation&quot;>;blind source separation&lt;/a>; (see the &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;paper&lt;/a>; for more details). We also show the ground truth heart rate measurement in the same running experiment using a &lt;a href=&quot;https://www.polar.com/us-en/sensors/h10-heart-rate-sensor&quot;>;Polar ECG chest strap &lt;/a>;。 In the raw APG, we see the running cadence (around 3.3 Hz) as well as two dim lines (around 2 Hz and 4 Hz) that indicate the user&#39;s heart rate frequency and its harmonics. The heart rate frequencies are significantly enhanced in &lt;a href=&quot;https://en.wikipedia.org/wiki/Signal-to-noise_ratio&quot;>;signal to noise ratio&lt;/a>; (SNR) after the blind source separation, which align with the ground truth heart rate frequencies. We also show the calculated heart rate and running cadence from APG and ECG. We can see that APG tracks the growth of heart rate during the running session accurately. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbwRfWHxYHVk1ARp9gzcaRTMIUO-FjAgaTqOUtfYMzJ1lpAsWSxm1MczllG4aq-3Rbi-M6u7KXqnPOdsiLJS4le05Alnf_68701LYb0DQrpbbqAYGP7jzYivDhoeniPeTmsBU7Uhg4HRyOlYdQCYWxL-xabuULYFlGdH_rIA1b1Q8ZXH8MHIYlPQcaUpYW/s1888/image3 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1423&quot; data-original-width=&quot;1888&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbwRfWHxYHVk1ARp9gzcaRTMIUO-FjAgaTqOUtfYMzJ1lpAsWSxm1MczllG4aq-3Rbi-M6u7KXqnPOdsiLJS4le05Alnf_68701LYb0DQrpbbqAYGP7jzYivDhoeniPeTmsBU7Uhg4HRyOlYdQCYWxL-xabuULYFlGdH_rIA1b1Q8ZXH8MHIYlPQcaUpYW/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;APG tracks the heart rate accurately during the running session and also measures the running cadence.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Field study and closing thoughts&lt;/h2>; &lt;p>; We conducted two rounds of user experience (UX ) studies with 153 participants. Our results demonstrate that APG achieves consistently accurate heart rate (3.21% median error across participants in all activity scenarios) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Heart_rate_variability&quot;>;heart rate variability&lt;/a>; (2.70% median error in inter-beat interval) measurements. Unlike PPG, which exhibits variable performance across skin tones, our study shows that APG is resilient to variation in: skin tone, sub-optimal seal conditions, and ear canal size. More detailed evaluations can be found in the &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;paper&lt;/a>;. &lt;/p>; &lt;p>; APG transforms any TWS ANC headphones into smart sensing headphones with a simple software upgrade, and works robustly across various user activities. The sensing carrier signal is completely inaudible and not impacted by music playing. More importantly, APG represents new knowledge in biomedical and mobile research and unlocks new possibilities for low-cost health sensing. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>; APG is the result of collaboration across Google Health, product, UX and legal teams. We would like to thank David Pearl, Jesper Ramsgaard, Cody Wortham, Octavio Ponce, Patrick Amihood, Sam Sheng, Michael Pate, Leonardo Kusumo, Simon Tong, Tim Gladwin, Russ Mirov, Kason Walker, Govind Kannan, Jayvon Timmons, Dennis Rauschmayer, Chiong Lai, Shwetak Patel, Jake Garrison, Anran Wang, Shiva Rajagopal, Shelten Yuen, Seobin Jung, Yun Liu, John Hernandez, Issac Galatzer-Levy, Isaiah Fischer-Brown, Jamie Rogers, Pramod Rudrapatna, Andrew Barakat, Jason Guss, Ethan Grabau, Pol Peiffer, Bill Park, Helen O&#39;Connor, Mia Cheng, Keiichiro Yumiba, Felix Bors, Priyanka Jantre, Luzhou Xu, Jian Wang, Jaime Lien, Gerry Pallipuram, Nicholas Gillian, Michal Matuszak, Jakub Wojciechowski, Bryan Allen, Jane Hilario, and Phil Carmack for their invaluable insights and support. Thanks to external collaborators Longfei Shangguan and Rich Howard, Rutgers University and University of Pittsburgh.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4973261706617691472/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/audioplethysmography-for-cardiac.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4973261706617691472&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4973261706617691472&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/audioplethysmography-for-cardiac.html&quot; rel=&quot;alternate&quot; title=&quot;Audioplethysmography for cardiac monitoring with hearable devices&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnCItsRGs939CGCBBAi5fA-xwk9JQi7b2CQ3voKK583p4uTkQkLIXUd1lU71SLynom0Yt78bRxDflolUzzfol5UbfkPmCaNn8bIUxwAbLDatqZTPxP7SYOT45g9qJODdM1kvT6NQUsixtFTiBYr_h_Hx-pFRsmbcBKdnW3WkbcD4Bcr_cZE590VL-cSu-a/s72-c/hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6605352025607219737&lt;/id>;&lt;published>;2023-10-26T11:01:00.000-07:00&lt;/published>;&lt;updated>;2023-10-26T11:01:04.276-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Collaboration&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Supporting benchmarks for AI safety with MLCommons&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Anoop Sinha, Technology and Society, and Marian Croak, Google Research, Responsible AI and Human Centered Technology team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5URA9ivhxcgmfXwa0668O0HslgYQ_p_x9JJbg6nDgryyNc2QImQernPyzkLPcj1esCMOQTUnEsldIlb21E0DWPDIiE2m76qSruF0jA6jfl1sNl6mBW8JUPSWzOfE7IahHRtviFpxUTPcEZoADXHNMy3gZ2hg369y5QxhY01QRVj5kwJx4uwRKzdcBFp9v/s1600/GoogleResearch.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Standard benchmarks are agreed upon ways of measuring important product qualities, and they exist in many fields. Some standard benchmarks measure safety: for example, when a car manufacturer touts a “five-star overall safety rating,” they&#39;re citing a benchmark. Standard benchmarks already exist in machine learning (ML) and AI technologies: for instance, the &lt;a href=&quot;https://mlcommons.org/en/&quot;>;MLCommons&lt;/a>; Association operates the &lt;a href=&quot;https://mlcommons.org/en/news/mlperf-inference-storage-q323/&quot;>;MLPerf&lt;/a>; benchmarks that measure the speed of cutting edge AI hardware such as Google&#39;s TPUs. However, though there has been significant work done on &lt;a href=&quot;https://blog.google/technology/ai/our-responsible-approach-to-building-guardrails-for-generative-ai/&quot;>;AI safety&lt;/a>;, there are as yet no similar standard benchmarks for AI safety. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to support a new effort by the non-profit MLCommons Association to develop standard AI safety benchmarks. Developing benchmarks that are effective and trusted is going to require advancing AI safety testing technology and incorporating a broad range of perspectives. The MLCommons effort aims to bring together expert researchers across academia and industry to develop standard benchmarks for measuring the safety of AI systems into scores that everyone can understand. We encourage the whole community, from AI researchers to policy experts, to &lt;a href=&quot;https://mlcommons.org/ai-safety&quot;>;join us&lt;/a>; in contributing to the effort. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Why AI safety benchmarks?&lt;/h2>; &lt;p>; Like most advanced technologies, AI has the potential for tremendous benefits but could also lead to negative outcomes without appropriate care. For example, AI technology can boost human productivity in a wide range of activities (eg, &lt;a href=&quot;https://blog.google/technology/health/how-ai-can-improve-health-for-everyone-everywhere/&quot;>;improve health diagnostics&lt;/a>; and research into diseases, analyze &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-transportation-energy-emissions-reduction/&quot;>;energy usage&lt;/a>;, and more). However, without sufficient precautions, AI could also be used to support harmful or malicious activities and respond in biased or offensive ways. &lt;/p>; &lt;p>; By providing standard measures of safety across categories such as harmful use, out-of-scope responses, AI-control risks, etc., standard AI safety benchmarks could help society reap the benefits of AI while ensuring that sufficient precautions are being taken to mitigate these risks. Initially, nascent safety benchmarks could help drive AI safety research and inform responsible AI development. With time and maturity, they could help inform users and purchasers of AI systems. Eventually, they could be a valuable tool for policy makers. &lt;/p>; &lt;p>; In computer hardware, benchmarks (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Standard_Performance_Evaluation_Corporation&quot;>;SPEC&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Transaction_Processing_Performance_Council&quot;>;TPC&lt;/a>;) have shown an amazing ability to align research, engineering, and even marketing across an entire industry in pursuit of progress, and we believe standard AI safety benchmarks could help do the same in this vital area. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;What are standard AI safety benchmarks?&lt;/h2>; &lt;p>; Academic and corporate research efforts have experimented with a range of AI safety tests (eg, &lt;a href=&quot;https://arxiv.org/abs/2009.11462&quot;>;RealToxicityPrompts&lt;/a>;, &lt;a href=&quot;https://crfm.stanford.edu/2022/11/17/helm.html&quot;>;Stanford HELM&lt;/a>; fairness, bias, toxicity measurements, and &lt;a href=&quot;https://blog.google/technology/ai/our-responsible-approach-to-building-guardrails-for-generative-ai/&quot;>;Google&#39;s guardrails for generative AI&lt;/a>;). However, most of these tests focus on providing a prompt to an AI system and algorithmically scoring the output, which is a useful start but limited to the scope of the test prompts. Further, they usually use open datasets for the prompts and responses, which may already have been (often inadvertently) incorporated into training data. &lt;/p>; &lt;p>; MLCommons proposes a multi-stakeholder process for selecting tests and grouping them into subsets to measure safety for particular AI use-cases, and translating the highly technical results of those tests into scores that everyone can understand. MLCommons is proposing to create a platform that brings these existing tests together in one place and encourages the creation of more rigorous tests that move the state of the art forward. Users will be able to access these tests both through online testing where they can generate and review scores and offline testing with an engine for private testing. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;AI safety benchmarks should be a collective effort&lt;/h2>; &lt;p>; Responsible AI developers use a diverse range of safety measures, including automatic testing, manual testing, red teaming (in which human testers attempt to produce adversarial outcomes), software-imposed restrictions, data and model best-practices, and auditing. However, determining that sufficient precautions have been taken can be challenging, especially as the community of companies providing AI systems grows and diversifies. Standard AI benchmarks could provide a powerful tool for helping the community grow responsibly, both by helping vendors and users measure AI safety and by encouraging an ecosystem of resources and specialist providers focused on improving AI safety. &lt;/p>; &lt;p>; At the same time, development of mature AI safety benchmarks that are both effective and trusted is not possible without the involvement of the community. This effort will need researchers and engineers to come together and provide innovative yet practical improvements to safety testing technology that make testing both more rigorous and more efficient. Similarly, companies will need to come together and provide test data, engineering support, and financial support. Some aspects of AI safety can be subjective, and building trusted benchmarks supported by a broad consensus will require incorporating multiple perspectives, including those of public advocates, policy makers, academics, engineers, data workers, business leaders, and entrepreneurs. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Google&#39;s support for MLCommons&lt;/h2>; &lt;p>; Grounded in our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; that were &lt;a href=&quot;https://blog.google/technology/ai/ai-principles/&quot;>;announced&lt;/a>; in 2018, Google is committed to specific practices for the safe, secure, and trustworthy development and use of AI (see our &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2019-progress-update.pdf&quot;>;2019&lt;/a>;, &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2020-progress-update.pdf&quot;>;2020&lt;/a>;, &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2021-progress-update.pdf&quot;>;2021&lt;/a>;, &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2022-progress-update.pdf&quot;>;2022&lt;/a>; updates). We&#39;ve also made significant &lt;a href=&quot;https://static.googleusercontent.com/media/publicpolicy.google/en//resources/whcommitments.pdf&quot;>;progress&lt;/a>; on key commitments, which will help ensure AI is developed boldly and responsibly, for the benefit of everyone. &lt;/p>; &lt;p>; Google is supporting the MLCommons Association&#39;s efforts to develop AI safety benchmarks in a number of ways. &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Testing platform&lt;/em>;: We are joining with other companies in providing funding to support the development of a testing platform. &lt;li>;&lt;em>;Technical expertise and resources&lt;/em>;: We are providing technical expertise and resources, such as the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;Monk Skin Tone Examples Dataset&lt;/a>;, to help ensure that the benchmarks are well-designed and effective. &lt;li>;&lt;em>;Datasets&lt;/em>;: We are contributing an internal dataset for multilingual representational bias, as well as already externalized tests for stereotyping harms, such as &lt;a href=&quot;https://github.com/google-research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>; and &lt;a href=&quot;https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;. Moreover, we are sharing our datasets that focus on collecting human annotations responsibly and inclusively, like &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;DICES&lt;/a>; and &lt;a href=&quot;https://www.kaggle.com/datasets/google/jigsaw-specialized-rater-pools-dataset&quot;>;SRP&lt;/a>;. &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; We believe that these benchmarks will be very useful for advancing research in AI safety and ensuring that AI systems are developed and deployed in a responsible manner. AI safety is &lt;a href=&quot;https://blog.google/technology/ai/a-shared-agenda-for-responsible-ai-progress/&quot;>;a collective-action problem&lt;/a>;. Groups like the &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/&quot;>;Frontier Model Forum&lt;/a>; and &lt;a href=&quot;https://partnershiponai.org/&quot;>;Partnership on AI&lt;/a>; are also leading important standardization initiatives. We&#39;re pleased to have been part of these groups and MLCommons since their beginning. We look forward to additional collective efforts to promote the responsible development of new generative AI tools. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many thanks to the Google team that contributed to this work : Peter Mattson, Lora Aroyo, Chris Welty, Kathy Meier-Hellstern, Parker Barnes, Tulsee Doshi, Manvinder Singh, Brian Goldman, Nitesh Goyal, Alice Friend, Nicole Delange, Kerry Barker, Madeleine Elish, Shruti Sheth, Dawn Bloxwich, William Isaac , Christina Butterfield.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6605352025607219737/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html#comment-form&quot; rel=&quot;replies &quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6605352025607219737&quot; rel=&quot;edit&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6605352025607219737&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot; rel=&quot;alternate&quot; title=&quot;Supporting benchmarks for AI safety with MLCommons&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5URA9ivhxcgmfXwa0668O0HslgYQ_p_x9JJbg6nDgryyNc2QImQernPyzkLPcj1esCMOQTUnEsldIlb21E0DWPDIiE2m76qSruF0jA6jfl1sNl6mBW8JUPSWzOfE7IahHRtviFpxUTPcEZoADXHNMy3gZ2hg369y5QxhY01QRVj5kwJx4uwRKzdcBFp9v/s72-c/GoogleResearch.png &quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7441213378686175839&lt;/id>;&lt;published>;2023-10-26T08:57:00.003-07:00&lt;/published>;&lt;updated>;2023-11-01T17 :00:03.449-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Spoken question answering and speech continuation using a spectrogram-powered LLM&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eliya Nachmani, Research Scientist, and Alon Levkovitch, Student Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgQEWecXit-a9UwHz781_B9s9ZqxsJIGt7ZXx2Lk0bcSQxXBkmLfjhNQxSiq3gqVjiUZ81_178hArCQ8nNL0OaVyAi8mKixeWN2PvXTLL4I08ht-eCVqtrTRo36dxGBSDNMdlathtEd4g_qdU3T4ZmPMdGNSXHwlDP689sxzbI4Wwosyu9wp-mjadKqL3MN/s1100/Spectron-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The goal of natural language processing (NLP) is to develop computational models that can understand and generate natural language. By capturing the statistical patterns and structures of text-based natural language, language models can predict and generate coherent and meaningful sequences of words. Enabled by the increasing use of the highly successful &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>; model architecture and with training on large amounts of text (with proportionate compute and model size), large language models (LLMs) have demonstrated remarkable success in NLP tasks. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, modeling spoken human language remains a challenging frontier. Spoken dialog systems have conventionally been built as a cascade of &lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_recognition&quot;>;automatic speech recognition&lt;/a>; (ASR), &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural-language_understanding&quot;>;natural language understanding&lt;/a>; (NLU), response generation, and &lt;a href=&quot;https://simple.wikipedia.org/wiki/Text_to_speech&quot;>;text-to-speech&lt;/a>; (TTS) systems. However, to date there have been few capable end-to-end systems for the modeling of spoken language: ie, single models that can take speech inputs and generate its continuation as speech outputs. &lt;/p>; &lt;p>;Today we present a new approach for spoken language modeling, called Spectron, published in “&lt;a href=&quot;https://arxiv.org/abs/2305.15255&quot;>;Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM&lt;/a>;.” Spectron is the first spoken language model that is trained end-to-end to directly process spectrograms as both input and output, instead of learning discrete speech representations. Using only a pre-trained text language model, it can be fine-tuned to generate high-quality, semantically accurate spoken language. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;question answering&lt;/a>; datasets.&lt;/p>; &lt;p>; We show that a pre-trained speech encoder and a language model decoder enable end-to-end training and state-of-the-art performance without sacrificing representational fidelity. Key to this is a novel end-to-end training objective that implicitly supervises speech recognition, text continuation, and conditional speech synthesis in a joint manner. A new spectrogram regression loss also supervises the model to match the higher-order derivatives of the spectrogram in the time and frequency domain. These derivatives express information aggregated from multiple frames at once. Thus, they express rich, longer-range information about the shape of the signal. Our overall scheme is summarized in the following figure: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4yanW_7FsK-zlJ5XKgusakSaIGAEvDPuRZpAZ5NGJl8mW6-4hd06ZWOsk4IIf0hl5XwWFcjkdu9gH4VwqcWCqLUfZQXDM80MuDml8Tt_P0RT3fDq5cxfoPCPNagOuU0SZbSz8Vn7x_bLkxNFXKpakBPwCkgXh8T6bB4lfHQevGBjs-U4c5WA6RgC8gukW/s1815/Spectron.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1815&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4yanW_7FsK-zlJ5XKgusakSaIGAEvDPuRZpAZ5NGJl8mW6-4hd06ZWOsk4IIf0hl5XwWFcjkdu9gH4VwqcWCqLUfZQXDM80MuDml8Tt_P0RT3fDq5cxfoPCPNagOuU0SZbSz8Vn7x_bLkxNFXKpakBPwCkgXh8T6bB4lfHQevGBjs-U4c5WA6RgC8gukW/s16000/Spectron.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;The Spectron model connects the encoder of a speech recognition model with a pre-trained Transformer-based decoder language model. At training, speech utterances split into a prompt and its continuation. Then the full transcript (prompt and continuation) is reconstructed along with the continuation&#39;s speech features. At inference, only a prompt is provided; the prompt&#39;s transcription, text continuation, and speech continuations are all generated by the model.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Spectron architecture&lt;/h2>; &lt;p>; The architecture is initialized with a pre-trained speech encoder and a pre-trained decoder language model. The encoder is prompted with a speech utterance as input, which it encodes into continuous linguistic features. These features feed into the decoder as a prefix, and the whole encoder-decoder is optimized to jointly minimize a &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy&quot;>;cross-entropy&lt;/a>; loss (for speech recognition and transcript continuation) and a novel reconstruction loss (for speech continuation). During inference, one provides a spoken speech prompt, which is encoded and then decoded to give both text and speech continuations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Speech encoder&lt;/h3>; &lt;p>; The speech encoder is a 600M-parameter &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;conformer&lt;/a>; encoder pre-trained on &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;large-scale data&lt;/a>; (12M hours). It takes the spectrogram of the source speech as input, generating a hidden representation that incorporates both linguistic and acoustic information. The input spectrogram is first subsampled using a convolutional layer and then processed by a series of conformer blocks. Each conformer block consists of a feed-forward layer, a self-attention layer, a convolution layer, and a second feed-forward layer. The outputs are passed through a projection layer to match the hidden representations to the embedding dimension of the language model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Language model&lt;/h3>; &lt;p>; We use a 350M or 1B parameter decoder language model (for the continuation and question-answering tasks, respectively) trained in the manner of &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;PaLM 2&lt;/a>;. The model receives the encoded features of the prompt as a prefix. Note that this is the only connection between the speech encoder and the LM decoder; ie, there is no cross-attention between the encoder and the decoder. Unlike most spoken language models, during training, the decoder is teacher-forced to predict the text transcription, text continuation, and speech embeddings. To convert the speech embeddings to and from spectrograms, we introduce lightweight modules pre- and post-network. &lt;/p>; &lt;p>; By having the same architecture decode the intermediate text and the spectrograms, we gain two benefits. First, the pre-training of the LM in the text domain allows continuation of the prompt in the text domain before synthesizing the speech. Secondly, the predicted text serves as intermediate reasoning, enhancing the quality of the synthesized speech, analogous to improvements in text-based language models when using intermediate &lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpads&lt;/a>; or &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; (CoT) reasoning. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Acoustic projection layers&lt;/h3>; &lt;p>; To enable the language model decoder to model spectrogram frames, we employ a multi-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Perceptron&quot;>;perceptron&lt;/a>; “pre-net” to project the ground truth spectrogram speech continuations to the language model dimension. This pre-net compresses the spectrogram input into a lower dimension, creating a bottleneck that aids the decoding process. This bottleneck mechanism prevents the model from repetitively generating the same prediction in the decoding process. To project the LM output from the language model dimension to the spectrogram dimension, the model employs a “post-net”, which is also a multi-layer perceptron. Both pre- and post-networks are two-layer multi-layer perceptrons. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Training objective&lt;/h3>; &lt;p>; The training methodology of Spectron uses two distinct loss functions: (i) &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy&quot;>;cross-entropy loss&lt;/a>;, employed for both speech recognition and transcript continuation, and (ii) &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;regression loss&lt;/a>;, employed for speech continuation. During training, all parameters are updated (speech encoder, projection layer, LM, pre-net, and post-net). &lt;/p>; &lt;br />; &lt;h2>;Audio samples&lt;/h2>; &lt;p>; Following are examples of speech continuation and question answering from Spectron: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;&lt;h3>;Speech Continuation&lt;/h3>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/1/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/2/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/2/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/3/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/3/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/4/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/4/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td colspan=&quot;2&quot; style=&quot;text-align: left;&quot;>;&lt;h3>;Question Answering&lt;/h3>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Question:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/1/Q.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Answer:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/1/S.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Question:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/2/Q.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Answer:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/2/S.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Performance&lt;/h2>; &lt;p>; To empirically evaluate the performance of the proposed approach, we conducted experiments on the&lt;a href=&quot;https://arxiv.org/abs/1912.07875&quot;>; Libri-Light dataset&lt;/a>;. Libri-Light is a 60k hour English dataset consisting of unlabelled speech readings from LibriVox audiobooks. We utilized a frozen neural vocoder called &lt;a href=&quot;https://research.google/pubs/pub51736/&quot;>;WaveFit&lt;/a>; to convert the predicted spectrograms into raw audio. We experiment with two tasks, speech continuation and spoken question answering (QA). Speech continuation quality is tested on the LibriSpeech test set. Spoken QA is tested on the Spoken WebQuestions datasets and a new test set named LLama questions, which we created. For all experiments, we use a 3 second audio prompt as input. We compare our method against existing spoken language models: &lt;a href=&quot;https://arxiv.org/abs/2209.03143&quot;>;AudioLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2102.01192&quot;>;GSLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2305.13009&quot;>;TWIST&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2305.11000&quot;>;SpeechGPT&lt;/a>;. For the speech continuation task, we use the 350M parameter version of LM and the 1B version for the spoken QA task. &lt;/p>; &lt;p>; For the speech continuation task, we evaluate our method using three metrics. The first is &lt;a href=&quot;https://en.wikipedia.org/wiki/Perplexity&quot;>;log-perplexity&lt;/a>;, which uses an LM to evaluate the cohesion and semantic quality of the generated speech. The second is &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;mean opinion score&lt;/a>; (MOS), which measures how natural the speech sounds to human evaluators. The third, speaker similarity, uses a speaker encoder to measure how similar the speaker in the output is to the speaker in the input. Performance in all 3 metrics can be seen in the following graphs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_o_DUlZ-28VmNk20zvWBWC5FNl3GMVeigztDAtB4TR5voCdX3zzuCF-6W6DDrMVGZ2szVhyphenhyphendPz1DHvjBrALQXCMkVzuM81oBck4Wb3N1VH1ndDReAPsfbQv5x7msOD3wMiHeZM-_AcN2ekgSI_G9sF5_u0lrLKc-xxSWc2d9qi6Ubt5cutgQfpIt8uKGr/s1200/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_o_DUlZ-28VmNk20zvWBWC5FNl3GMVeigztDAtB4TR5voCdX3zzuCF-6W6DDrMVGZ2szVhyphenhyphendPz1DHvjBrALQXCMkVzuM81oBck4Wb3N1VH1ndDReAPsfbQv5x7msOD3wMiHeZM-_AcN2ekgSI_G9sF5_u0lrLKc-xxSWc2d9qi6Ubt5cutgQfpIt8uKGr/w640-h396/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Log-perplexity for completions of LibriSpeech utterances given a 3-second prompt. Lower is better.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjUncRH61ZdMzRhsx9NEXoY0P0jIMkaXhycc9AjB2I0pr8bEWxGNaY0KEF4UG0dm6ZVjRT3_aXlNKny3mmjoVgPveK4rskOgU5sApzqzIXixabv7kMeZNMxUpxs_RDqSsnij-HILgUzVZT39hoBDimw9Ecnd2lMq2ylyRCVtcU_owo8jx9ndVrLUHbGhNT/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjUncRH61ZdMzRhsx9NEXoY0P0jIMkaXhycc9AjB2I0pr8bEWxGNaY0KEF4UG0dm6ZVjRT3_aXlNKny3mmjoVgPveK4rskOgU5sApzqzIXixabv7kMeZNMxUpxs_RDqSsnij-HILgUzVZT39hoBDimw9Ecnd2lMq2ylyRCVtcU_owo8jx9ndVrLUHbGhNT/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Speaker similarity between the prompt speech and the generated speech using the speaker encoder. Higher is better.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBdPk77at7a-tSVl1cgPTlMsj4tnV391evuyYPzMXxeviG48kljCNy828jCQ5eVdOcB7ML4FHduIHCuDweZwJiv2USbNqi1EXinzyiwQYM3Sxio3g-61xTBvtb9rVt_Y4Cvns-vl3V-4Gvn1RTpBquX3a6W_NNu6u-1DrK_GT5uGXOvP0WivyO7FF2-WTf/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBdPk77at7a-tSVl1cgPTlMsj4tnV391evuyYPzMXxeviG48kljCNy828jCQ5eVdOcB7ML4FHduIHCuDweZwJiv2USbNqi1EXinzyiwQYM3Sxio3g-61xTBvtb9rVt_Y4Cvns-vl3V-4Gvn1RTpBquX3a6W_NNu6u-1DrK_GT5uGXOvP0WivyO7FF2-WTf/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MOS given by human users on speech naturalness. Raters rate 5-scale subjective mean opinion score (MOS) ranging between 0 - 5 in naturalness given a speech utterance. Higher is better.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As can be seen in the first graph, our method significantly outperforms GSLM and TWIST on the log-perplexity metric, and does slightly better than state-of-the-art methods AudioLM and SpeechGPT. In terms of MOS, Spectron exceeds the performance of all the other methods except for AudioLM. In terms of speaker similarity, our method outperforms all other methods. &lt;/p>; &lt;p>; To evaluate the ability of the models to perform question answering, we use two spoken question answering datasets. The first is the LLama Questions dataset, which uses general knowledge questions in different domains generated using the LLama2 70B LLM. The second dataset is the &lt;a href=&quot;https://huggingface.co/datasets/web_questions&quot;>;WebQuestions&lt;/a>; dataset which is a general question answering dataset. For evaluation we use only questions that fit into the 3 second prompt length. To compute accuracy, answers are transcribed and compared to the ground truth answers in text form. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi60RnurzgglmP14m9BIScZH4BmupewxydVJ5GZMKclwDgyGc3-zZdj8CK7WXmGR_-pwno1Aql0N_u0ocmftoAlTCJY0H-1cYU8YTpxZu5sdIdmG-wZAc-hIwlAbCEVDPgGz4J4a1VMcXWctbjcfCTxsDSZPD69eGfBeGyAxvyYtUX0fE2dhdT5zb71gsJ0/s1200 /image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot; 396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi60RnurzgglmP14m9BIScZH4BmupewxydVJ5GZMKclwDgyGc3-zZdj8CK7WXmGR_-pwno1Aql0N_u0ocmftoAlTCJY0H-1cYU8YTpxZu5sdIdmG-wZAc-hIwlAbCEVDPgGz4J4a1VMcXWctbjcfCTxsDSZPD69eGfBeGyAxvyYtUX0fE2dhdT5zb71gsJ0/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Accuracy for Question Answering on the LLama Questions and Spoken WebQuestions datasets. Accuracy is computed using the ASR transcripts of spoken answers.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; First, we observe that all methods have more difficulty answering questions from the Spoken WebQuestions dataset than from the LLama questions dataset. Second, we observe that methods centered around spoken language modeling such as GSLM, AudioLM and TWIST have a completion-centric behavior rather than direct question answering which hindered their ability to perform QA. On the LLama questions dataset our method outperforms all other methods, while SpeechGPT is very close in performance. On the Spoken WebQuestions dataset, our method outperforms all other methods except for SpeechGPT, which does marginally better. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;This project was conceived and initiated by Michelle Tadmor Ramanovich with additional significant contributions from&lt;/i>;&lt;em>;&amp;nbsp;Eliya Nachmani, Alon Levkovitch, Roy Hirsch (&lt;a href=&quot;https://verily.com/&quot;>;Verily&lt;/a>;), Julian Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin (&lt;a href=&quot;https://verily.com/&quot;>;Verily&lt;/a>;) and RJ Skerry-Ryan. We also thank Heiga Zhen, Yifan Ding, Yu Zhang, Yuma Koizumi, Neil Zeghidour, Christian Frank, Marco Tagliasacchi, Nadav Bar, Benny Schlesinger and Blaise Aguera-Arcas. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7441213378686175839/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/spoken-question-answering-and-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7441213378686175839&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7441213378686175839&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/10/spoken-question-answering-and-speech.html&quot; rel=&quot;alternate&quot; title=&quot;Spoken question answering and speech continuation using a spectrogram-powered LLM&quot; type=&quot;text /html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQEWecXit-a9UwHz781_B9s9ZqxsJIGt7ZXx2Lk0bcSQxXBkmLfjhNQxSiq3gqVjiUZ81_178hArCQ8nNL0OaVyAi8mKixeWN2PvXTLL4I08ht-eCVqtrTRo36dxGBSDNMdlathtEd4g_qdU3T4ZmPMdGNSXHwlDP689sxzbI4Wwosyu9wp-mjadKqL3MN/ s72-c/Spectron-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr: total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7317378303167958176&lt;/id>;&lt;published>;2023-10-25T15:10:00.002-07:00&lt;/published >;&lt;updated>;2023-10-26T14:03:20.633-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>; &lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Climate&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Looking back at wildfire research in 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author &quot;>;Posted by Yi-Fan Chen, Software Engineer, and Carla Bromberg, Program Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCepSXB1QxIgipeUbH1YGd8N3TkVoT1tAgxG0fC0PPREhbsygTQ4i58OVw6-Dt_lagwgswT0jtxUorsVmtyO9UgPEDezJHz_QiKvbJlgYF8Db8W- 68KFdyZsq_uvM7YuZo9BRo__NC4BNxD6nHZpzsimeNOaV3X8dh9aliqbAbk8ycXb25s5NfLTfNe42_/s600/WildfireModeling.gif&quot; style=&quot;display: none;&quot; />; &lt;p>;Wildfires are becoming larger and affecting more and more communities around the world, often resulting in large-scale devastation. Just this year, communities have experienced catastrophic wildfires in &lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Greece_wildfires&quot;>;Greece&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Hawaii_wildfires#Maui&quot;>;Maui&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Canadian_wildfires&quot;>;Canada&lt;/a>; to name a few. While the underlying causes leading to such an increase are complex — including changing climate patterns, forest management practices, land use development policies and many more — it is clear that the advancement of technologies can help to address the new challenges.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;At Google Research, we&#39;ve been investing in a number of &lt;a href=&quot;https://research.google/teams/climate-and-sustainability/&quot;>;climate adaptation&lt;/a>; efforts, including the application of machine learning (ML) to aid in wildfire prevention and provide information to people during these events. For example, to help map fire boundaries, our wildfire boundary &lt;a href=&quot;https://blog.research.google/2023/02/real-time-tracking-of-wildfire.html&quot;>;tracker&lt;/a>; uses ML models and satellite imagery to map large fires in near real-time with updates every 15 minutes. To advance our various research efforts, we are partnering with wildfire experts and government agencies around the world.&lt;/p>; &lt;p>;Today we are excited to share more about our ongoing collaboration with the &lt;a href=&quot;https://www.fs.usda.gov/&quot;>;US Forest Service&lt;/a>; (USFS) to advance fire modeling tools and fire spread prediction algorithms. Starting from the newly developed USFS wildfire behavior model, we use ML to significantly reduce computation times, thus enabling the model to be employed in near real time. This new model is also capable of incorporating localized fuel characteristics, such as fuel type and distribution, in its predictions. Finally, we describe an early version of our new high-fidelity 3D fire spread model.&lt;/p>; &lt;br />; &lt;h2>;Current state of the art in wildfire modeling&lt;/h2>; &lt;p>;Today&#39;s most widely used state-of-the-art fire behavior models for fire operation and training are based on the &lt;a href=&quot;https://www.fs.usda.gov/research/treesearch/55928&quot;>;Rothermel fire model&lt;/a>; developed at the US Forest Service Fire Lab, by Rothermel et al., in the 1970s. This model considers many key factors that affect fire spread, such as the influence of wind, the slope of the terrain, the moisture level, the fuel load (eg, the density of the combustible materials in the forest), etc., and provided a good balance between computational feasibility and accuracy at the time. The Rothermel model has gained widespread use throughout the fire management community across the world.&lt;/p>; &lt;p>;Various operational tools that employ the Rothermel model, such as &lt;a href=&quot;https://www.frames.gov/behaveplus/home&quot;>;BEHAVE&lt;/a>;, &lt;a href=&quot;https://www.firelab.org/project/farsite&quot;>;FARSITE&lt;/a>;, &lt;a href=&quot;https://wfdss.usgs.gov/wfdss/pdfs/FSPro.pdf&quot;>;FSPro&lt;/a>;, and &lt;a href=&quot;https://firelab.org/project/flammap&quot;>;FlamMap&lt;/a>;, have been developed and improved over the years. These tools and the underlying model are used mainly in three important ways: (1) for training firefighters and fire managers to develop their insights and intuitions on fire behavior, (2) for fire behavior analysts to predict the development of a fire during a fire operation and to generate guidance for situation awareness and resource allocation planning, and (3) for analyzing forest management options intended to mitigate fire hazards across large landscapes.&amp;nbsp; These models are the foundation of fire operation safety and efficiency today.&lt;/p>; &lt;p>; However, there are limitations on these state-of-the art models, mostly associated with the simplification of the underlying physical processes (which was necessary when these models were created). By simplifying the physics to produce steady state predictions, the required inputs for fuel sources and weather became practical but also more abstract compared to measurable quantities.&amp;nbsp; As a result, these models are typically “adjusted” and “tweaked” by experienced fire behavior analysts so they work more accurately in certain situations and to compensate for uncertainties and unknowable environmental characteristics. Yet these expert adjustments mean that many of the calculations are not repeatable. &lt;/p>; &lt;p>;To overcome these limitations, USFS researchers have been working on a new model to drastically improve the physical fidelity of fire behavior prediction. This effort represents the first major shift in fire modeling in the past 50 years. While the new model &lt;a href=&quot;https://ebooks.publish.csiro.au/content/wildland-fire-behaviour&quot;>;continues to improve in capturing fire behavior&lt;/a>;, the computational cost and inference time makes it impractical to be deployed in the field or for applications with near real-time requirements. In a realistic scenario, to make this model useful and practical in training and operations, a speed up of at least 1000x would be needed.&lt;/p>; &lt;br />; &lt;h2>;Machine learning acceleration&lt;/h2>; &lt;p>;In partnership with the USFS, we have undertaken a program to apply ML to decrease computation times for complex fire models. Researchers knew that many complex inputs and features could be characterized using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;>;deep neural network&lt;/a>;, and if successful, the trained model would lower the computational cost and latency of evaluating new scenarios. Deep learning is a branch of machine learning that uses neural networks with multiple hidden layers of nodes that do not directly correspond to actual observations. The model&#39;s hidden layers allow a rich representation of extremely complex systems — an ideal technique for modeling wildfire spread.&lt;/p>; &lt;p>; We used the USFS physics-based, numerical prediction models to generate many simulations of wildfire behavior and then used these simulated examples to train the deep learning model on the inputs and features to best capture the system behavior accurately. We found that the deep learning model can perform at a much lower computational cost compared to the original and is able to address behaviors resulting from fine-scale processes. In some cases, computation time for capturing the fine-scale features described above and providing a fire spread estimate was 100,000 times faster than running the physics-based numerical models. &lt;/p>; &lt;p>;This project has continued to make great progress since the first report at &lt;a href=&quot;https://www.adai.pt/newevent/event/home/index.php?target=home&amp;amp;event =4&amp;amp;defLang=2&quot;>;ICFFR&lt;/a>; in December 2022. The joint Google–USFS &lt;a href=&quot;http://books.uc.pt/chapter?chapter=978989262298921&quot;>;presentation at ICFFR 2022&lt;/ a>; and the &lt;a href=&quot;https://www.firelab.org/project/deep-learning-high-resolution-wildfire-modeling&quot;>;USFS Fire Lab&#39;s project page&lt;/a>; provides a glimpse into the ongoing work朝这个方向。 Our team has expanded the dataset used for training by an order of magnitude, from 40M up to 550M training examples. Additionally, we have delivered a prototype ML model that our USFS Fire Lab partner is integrating into a training app that is currently being developed for release in 2024.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdwdwG4NlOjrJlRs25p5yGRUdNDErx7I9aZYLVgoPhrlyiOOx6x8toan0DKoBm2-SG2JfWkZsOluT7g_BYJpDYCTqMRu3cUHqQtyeE-Hgli5j9J3ap2Sg9SrjEfOMXj_CYhbi66iIFwSJjzkii0kJv3VV5V01jbIYFQP-DWjBc9RJKKEoJZPpIZKKYO2TJ/s1999/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdwdwG4NlOjrJlRs25p5yGRUdNDErx7I9aZYLVgoPhrlyiOOx6x8toan0DKoBm2-SG2JfWkZsOluT7g_BYJpDYCTqMRu3cUHqQtyeE-Hgli5j9J3ap2Sg9SrjEfOMXj_CYhbi66iIFwSJjzkii0kJv3VV5V01jbIYFQP-DWjBc9RJKKEoJZPpIZKKYO2TJ/w640-h360/image2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Google researchers visiting the USFS Fire Lab in Missoula, MT, stopping by&amp;nbsp;&lt;/span>;&lt;a href=&quot;https://inciweb.nwcg.gov/incident-information/mtfha-big-knife&quot; style=&quot;text-align: left;&quot;>;Big Knife Fire&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;&amp;nbsp;Operation Command Center.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Fine-grained fuel representation&lt;/h2>; &lt;p>;Besides training, another key use-case of the new model is for operational fire prediction. To fully leverage the advantages of the new model&#39;s capability to capture the detailed fire behavior changes from small-scale differences in fuel structures, high resolution fuel mapping and representation are needed. To this end, we are currently working on the integration of high resolution satellite imagery and geo information into ML models to allow fuel specific mapping at-scale. Some of the preliminary results will be presented at the upcoming &lt;a href=&quot;https://afefirecongress.org/schedule/&quot;>;10th International Fire Ecology and Management Congress&lt;/a>; in November 2023.&lt;/p>; &lt;br />; &lt;h2>;Future work&lt;/h2>; &lt;p>; Beyond the collaboration on the new fire spread model, there are many important and challenging problems that can help fire management and safety. Many such problems require even more accurate fire models that fully consider 3D flow interactions and fluid dynamics, thermodynamics and combustion physics. Such detailed calculations usually require high-performance computers (HPCs) or supercomputers. &lt;/p>; &lt;p>; These models can be used for research and longer-term planning purposes to develop insights on extreme fire development scenarios, build ML classification models, or establish a meaningful “danger index” using the simulated results. These high-fidelity simulations can also be used to supplement physical experiments that are used in expanding the operational models mentioned above. &lt;/p>; &lt;p>;In this direction, Google research has also &lt;a href=&quot;https://www.publish.csiro.au/WF/WF22225&quot;>;developed a high-fidelity large-scale 3D fire simulator&lt;/a>; that can be run on Google TPUs. In the near future, there is a plan to further leverage this new capability to augment the experiments, and to generate data to build insights on the development of extreme fires and use the data to design a fire-danger classifier and fire-danger index protocol.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4SgYHTgu7Uv6WNEJsEJCuFWKolnL13g9BxK2cDuMq7-SRM4dIUK9LbNxy3v0VyY4dU87T_NnehBz4iXCfHCWwoOU6V8-15P55Oi6FvT5BvcLgR_vzYB1xMPWUtj3AiwSsoPZY-9K5i9IwIADDAIjG7hzZZZZ00n7N2jVnjlp65ePQDsJdNIuX6avw8kD_/s480/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4SgYHTgu7Uv6WNEJsEJCuFWKolnL13g9BxK2cDuMq7-SRM4dIUK9LbNxy3v0VyY4dU87T_NnehBz4iXCfHCWwoOU6V8-15P55Oi6FvT5BvcLgR_vzYB1xMPWUtj3AiwSsoPZY-9K5i9IwIADDAIjG7hzZZZZ00n7N2jVnjlp65ePQDsJdNIuX6avw8kD_/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;An example of 3D high-fidelity simulation. This is a controlled burn field experiment (&lt;/span>;&lt;a href=&quot;https://www.fireweather.org/fireflux2&quot; style=&quot;text-align: left;&quot;>;FireFlux II&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;) simulated using&amp;nbsp;&lt;/span>;&lt;a href=&quot;https://www.publish.csiro.au/WF/WF22225&quot;>;Google&#39;s high fidelity fire simulator&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;We thank Mark Finney, Jason Forthofer, William Chatham and Issac Grenfell from US Forest Service Missoula Fire Science Laboratory and our colleagues John Burge, Lily Hu, Qing Wang, Cenk Gazen, Matthias Ihme,&amp;nbsp;Vivian Yang, Fei Sha and John Anderson for core contributions and useful discussions. We also thank Tyler Russell for his assistance with program management and coordination.&lt;/i>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7317378303167958176/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7317378303167958176&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7317378303167958176&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html&quot; rel=&quot;alternate&quot; title=&quot;Looking back at wildfire research in 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCepSXB1QxIgipeUbH1YGd8N3TkVoT1tAgxG0fC0PPREhbsygTQ4i58OVw6-Dt_lagwgswT0jtxUorsVmtyO9UgPEDezJHz_QiKvbJlgYF8Db8W-68KFdyZsq_uvM7YuZo9BRo__NC4BNxD6nHZpzsimeNOaV3X8dh9aliqbAbk8ycXb25s5NfLTfNe42_/s72-c/WildfireModeling.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-420799196308935505&lt;/id>;&lt;published>;2023-10-25T10:45:00.000-07:00&lt;/published>;&lt;updated>;2023-10-25T10:45:37.205-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;EMNLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Search&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Grammar checking at Google Search scale&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eric Malmi, Senior Research Scientist, and Jakub Adamek, Senior Software Engineer, Google, Bard Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW7pWsW1-E7U1LH1bxmjBL_2W5fS6uN2iRb2cNPBks57ubvFNJj-SjE2Zk1lFqndKWBVAeO3g3MLeJ96QUIUNJDyLTcWjeO835NT6HfJMYQBEdGqL-X_sxQ1yxMy16a0OcOLb6gSz2lqM6VofToVtN_s_F_wGB41AZwlj146y7ZXQ4PFsdywX10QO54MJ4/s320/HeroGC.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Many people with questions about grammar turn to Google Search for guidance. While existing features, such as “Did you mean”, already handle simple typo corrections, more complex grammatical error correction (GEC) is beyond their scope. What makes the development of new Google Search features challenging is that they must have high precision and recall while outputting results &lt;a href=&quot;https://ai.googleblog.com/2009/06/speed-matters.html&quot;>;quickly&lt; /a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The conventional approach to GEC is to treat it as a &lt;a href=&quot;https://workspace.google.com/blog/ai-and-machine-learning/using-neural-machine-translation-to-correct-grammatical-in-google-docs&quot;>;translation problem&lt;/a>; and use autoregressive &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; models to decode the response token-by-token, conditioning on the previously generated tokens. However, although Transformer models have proven to be effective at GEC, they aren&#39;t particularly efficient because the generation cannot be parallelized due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive&lt;/a>; decoding. Often, only a few modifications are needed to make the input text grammatically correct, so another possible solution is to treat GEC as a &lt;a href=&quot;https://arxiv.org/abs/2206.07043&quot;>;text editing&lt;/a>;问题。 If we could run the autoregressive decoder only to generate the modifications, that would substantially decrease the latency of the GEC model. &lt;/p>; &lt;p>; To this end, in “&lt;a href=&quot;https://aclanthology.org/2022.findings-emnlp.156/&quot;>;EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start&lt;/a>;”, published at Findings of &lt;a href=&quot;https://2022.emnlp.org/&quot; target=&quot;_blank&quot;>;EMNLP 2022&lt;/a>;, we describe a novel text-editing model that is based on the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>; Transformer encoder-decoder architecture. EdiT5 powers the new Google Search &lt;a href=&quot;https://support.google.com/websearch/answer/13420782&quot;>;grammar check feature&lt;/a>; that allows you to check if a phrase or sentence is grammatically correct and provides corrections when needed. Grammar check shows up when the phrase &quot;grammar check&quot; is included in a search query, and if the underlying model is confident about the correction. Additionally, it shows up for some queries that don&#39;t contain the “grammar check” phrase when Search understands that is the likely intent. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-BAXZ3wH-FcJmFoKvppChwvfZay7xH5bhiYmKQocV47sCb3IMfwEec1OpBWY3b1fd2rnA4Vhy4EqOCZiAR4Xpv6xPok9yC-ZqLogbER2lcmaje1NfBCoPAtdW7rEPG22PtNn4dVuYtYH61D9fnx7kudLZilxzUbi01ePhirqtTwMu6g9trhFFuLCDUCgP/s1412/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;1412&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-BAXZ3wH-FcJmFoKvppChwvfZay7xH5bhiYmKQocV47sCb3IMfwEec1OpBWY3b1fd2rnA4Vhy4EqOCZiAR4Xpv6xPok9yC-ZqLogbER2lcmaje1NfBCoPAtdW7rEPG22PtNn4dVuYtYH61D9fnx7kudLZilxzUbi01ePhirqtTwMu6g9trhFFuLCDUCgP/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;Model architecture&lt;/h2>; &lt;p>; For low-latency applications at Google, Transformer models are typically run on &lt;a href=&quot;https://cloud.google.com/tpu/docs/ intro-to-tpu&quot;>;TPUs&lt;/a>;. Due to their fast matrix multiplication units (MMUs), these devices are optimized for performing large matrix multiplications quickly, for example running a Transformer encoder on hundreds of tokens in only a few milliseconds. In contrast, Transformer decoding makes poor use of a TPU&#39;s capabilities, because it forces it to process only one token at a time. This makes autoregressive decoding the most time-consuming part of a translation-based GEC model. &lt;/p>; &lt;p>; In the EdiT5 approach, we reduce the number of decoding steps by treating GEC as a text editing problem. The EdiT5 text-editing model is based on the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>; Transformer encoder-decoder architecture with a few crucial modifications. Given an input with grammatical errors, the EdiT5 model uses an encoder to determine which input tokens to keep or delete. The kept input tokens form a draft output, which is optionally reordered using a non-autoregressive &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf&quot;>;pointer network &lt;/a>;。 Finally, a decoder outputs the tokens that are missing from the draft, and uses a pointing mechanism to indicate where each new token should be placed to generate a grammatically correct output. The decoder is only run to produce tokens that were missing in the draft, and as a result, runs for much fewer steps than would be needed in the translation approach to GEC. &lt;/p>; &lt;p>;To further decrease the decoder latency, we reduce the decoder down to a single layer, and we compensate by increasing the size of the encoder. Overall, this decreases latency significantly because the extra work in the encoder is efficiently parallelized.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2_MGEcdadlbXM1zMBQzROuRbYyqmcdhcytlnwdHeIJ_M0wrtioaRixoDOxMlE3acuMZRf4KB_T5TgzUCfBBmz3eeEMnnAKHZkvFmMzFnNu_2UAI1l3jhOp2dyQyOxElXwt7iEjun_gd77uk1TMiaBLlMfxEOxcFo8W48fdD9WqbYNvUZECUEd-LHTk1-R/s1878/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1332&quot; data-original-width=&quot;1878&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2_MGEcdadlbXM1zMBQzROuRbYyqmcdhcytlnwdHeIJ_M0wrtioaRixoDOxMlE3acuMZRf4KB_T5TgzUCfBBmz3eeEMnnAKHZkvFmMzFnNu_2UAI1l3jhOp2dyQyOxElXwt7iEjun_gd77uk1TMiaBLlMfxEOxcFo8W48fdD9WqbYNvUZECUEd-LHTk1-R/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an input with grammatical errors (“Guess when was I borned”), the EdiT5 model uses an encoder to determine which input tokens to keep (K) or delete (D), a pointer network (pointer) to reorder kept tokens, and a decoder to insert any new tokens that are needed to generate a grammatically correct output.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We applied the EdiT5 model to the &lt;a href=&quot;https://aclanthology.org/W19-4406/&quot;>;public BEA grammatical error correction benchmark&lt;/a>;, comparing different model sizes. The experimental results show that an EdiT5 large model with 391M parameters yields a higher &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F0.5 score&lt;/a>;, which measures the accuracy of the corrections, while delivering a 9x speedup compared to a T5 base model with 248M parameters. The mean latency of the EdiT5 model was merely 4.1 milliseconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQys5EavCOAEs0dy3M8r5Ar-9v7n5qtFkek1VSfeGBXH1pQHNv6Ebem3vDedO60BlNcp3HLGqYtsT4v4Evz2u2ksTgFfQcQIqd87NPTxVzYVyzvA85Vg_jO8a18KrMT2hOQQA5hg2frOJx2L7S5SM88VBCJzd13ClrSzHnl5xg29TMMpvqA4Yl-d98vTc1/s1987/image1.jpeg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1311&quot; data-original-width=&quot;1987&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQys5EavCOAEs0dy3M8r5Ar-9v7n5qtFkek1VSfeGBXH1pQHNv6Ebem3vDedO60BlNcp3HLGqYtsT4v4Evz2u2ksTgFfQcQIqd87NPTxVzYVyzvA85Vg_jO8a18KrMT2hOQQA5hg2frOJx2L7S5SM88VBCJzd13ClrSzHnl5xg29TMMpvqA4Yl-d98vTc1/s16000/image1.jpeg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Performance of the T5 and EdiT5 models of various sizes on the public BEA GEC benchmark plotted against mean latency. Compared to T5, EdiT5 offers a better latency-F0.5 trade-off. Note that the x axis is logarithmic.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Improved training data with large language models&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://arxiv.org/abs/2106.03830&quot;>;earlier research&lt;/a>;, as well as the results above, show that model size plays a crucial role in generating accurate grammatical corrections. To combine the advantages of large language models (LLMs) and the low latency of EdiT5, we leverage a technique called &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;hard distillation&lt;/a>;. First, we train a teacher LLM using similar datasets used for the &lt;a href=&quot;https://ai.googleblog.com/2021/10/grammar-correction-as-you-type-on-pixel.html&quot;>;Gboard grammar model&lt;/a>;. The teacher model is then used to generate training data for the student EdiT5 model. &lt;/p>; &lt;p>; Training sets for grammar models consist of &lt;em>;ungrammatical source / grammatical target&lt;/em>; sentence pairs. Some of the training sets have noisy targets that contain grammatical errors, unnecessary paraphrasing, or unwanted artifacts. Therefore, we generate new pseudo-targets with the teacher model to get cleaner and more consistent training data. Then, we re-train the teacher model with the pseudo-targets using a technique called &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/1909.13788&quot;>;self-training&lt;/a>;&lt;/em>; 。 Finally, we found that when the source sentence contains many errors, the teacher sometimes corrects only part of the errors. Thus, we can further improve the quality of the pseudo-targets by feeding them to the teacher LLM for a second time, a technique called &lt;em>;&lt;a href=&quot;https://aclanthology.org/D19-1435/&quot;>;iterative refinement&lt;/a>;&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJzgocbj86SS8moSqWrxIKz8E3CxZn712Di2CY7pwgXZ355Z4qUnDBxuEc91AU9jAdKwq4tvyaCzIqKfcJzThrjkka5q1LwXGWNa1xPi5AU7RpJDq-XmoNHs386B2bJqKDDnN8_l1mvLCSShlGjb66RWy25Z_1QXwMDpHDgsPON7r0pUqYdXZLPsx06hOC/s1089/image3.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1089&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgJzgocbj86SS8moSqWrxIKz8E3CxZn712Di2CY7pwgXZ355Z4qUnDBxuEc91AU9jAdKwq4tvyaCzIqKfcJzThrjkka5q1LwXGWNa1xPi5AU7RpJDq-XmoNHs386B2bJqKDDnN8_l1mvLCSShlGjb66RWy25Z_1QXwMDpHDgsPON7r0pUqYdXZLPsx06hOC/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Steps for training a large teacher model for grammatical error correction (GEC). Self-training and iterative refinement remove unnecessary paraphrasing, artifacts, and grammatical errors appearing in the original targets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Putting it all together&lt;/h2>; &lt;p>; Using the improved GEC data, we train two EdiT5-based models: a grammatical error correction model, and a grammaticality分类器。 When the grammar check feature is used, we run the query first through the correction model, and then we check if the output is indeed correct with the classifier model. Only then do we surface the correction to the user. &lt;/p>; &lt;p>; The reason to have a separate classifier model is to more easily trade off between precision and recall. Additionally, for ambiguous or nonsensical queries to the model where the best correction is unclear, the classifier reduces the risk of serving erroneous or confusing corrections. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have developed an efficient grammar correction model based on the state-of-the-art EdiT5 model architecture. This model allows users to check for the grammaticality of their queries in Google Search by including the “grammar check” phrase in the query. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We gratefully acknowledge the key contributions of the other team members, including Akash R, Aliaksei Severyn, Harsh Shah, Jonathan Mallinson, Mithun Kumar SR, Samer Hassan, Sebastian Krause, and Shikhar Thakur. We&#39;d also like to thank Felix Stahlberg, Shankar Kumar, and Simon Tong for helpful discussions and pointers.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/420799196308935505/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/grammar-checking-at-google-search-scale.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/420799196308935505&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/420799196308935505&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/grammar-checking-at-google-search-scale.html&quot; rel=&quot;alternate&quot; title=&quot;Grammar checking at Google Search scale&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW7pWsW1-E7U1LH1bxmjBL_2W5fS6uN2iRb2cNPBks57ubvFNJj-SjE2Zk1lFqndKWBVAeO3g3MLeJ96QUIUNJDyLTcWjeO835NT6HfJMYQBEdGqL-X_sxQ1yxMy16a0OcOLb6gSz2lqM6VofToVtN_s_F_wGB41AZwlj146y7ZXQ4PFsdywX10QO54MJ4/s72-c/HeroGC.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;