<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-12-10T14:11:35.918-08:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Health"></category><category term="Robotics"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="AI for Social Good"></category><category term="Security and Privacy"></category><category term="Computer Science"></category><category term="HCI"></category><category term="MOOC"></category><category term="Quantum AI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="accessibility"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="ACL"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="Android"></category><category term="EMNLP"></category><category term="ICML"></category><category term="Physics"></category><category term="TPU"></category><category term="Awards"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Search"></category><category term="Structured Data"></category><category term="video"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Responsible AI"></category><category term="TTS"></category><category term="User Experience"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="Google Accelerated Science"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="Video Analysis"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Diversity"></category><category term="Interspeech"></category><category term="RAI-HCT Highlights"></category><category term="Systems"></category><category term="UI"></category><category term="Vision Research"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Cloud Platform"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Differential Privacy"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Climate"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="Large Language Models"></category><category term="NAACL"></category><category term="Networks"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="Weather"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Generative AI"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="Graphs"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1318&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8223613466421639113&lt;/id>;&lt;发布>;2023-12-10T06:11:00.000-08:00&lt;/发布>;&lt;更新>;2023-12- 10T14:11:04.080-08:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Google 在 NeurIPS 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Catherine 发布Armato，Google 项目经理&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH 4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s1100/google_research_sticker-hero.jpg&quot; style=&quot;显示:没有任何;” />; &lt;p>; 本周，第 37 届年度&lt;a href=&quot;https://neurips.cc/Conferences/2023&quot;>;神经信息处理系统会议&lt;/a>; (NeurIPS 2023) 召开，这是全球最大的机器学习会议今年，在洛杉矶新奥尔良拉开帷幕。 Google 很荣幸成为今年 NeurIPS 的&lt;a href=&quot;https://neurips.cc/Conferences/2023/Sponsors&quot;>;钻石级赞助商&lt;/a>;，并且将在超过 170 篇被接受的论文中占据一席之地，两次主题演讲，并通过组织支持和参与超过 20 个研讨会和教程为更广泛的研究界做出额外贡献。 Google 还很荣幸成为&lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66602&quot;>;机器学习女性&lt;/a>;和&lt;a href=&quot;的白金赞助商https://nips.cc/virtual/2023/affinity-workshop/66607&quot;>;人工智能中的拉丁语&lt;/a>;研讨会。我们期待分享我们的一些广泛的机器学习研究，并扩大我们与更广泛的机器学习研究社区的合作伙伴关系。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 亲自参加 NeurIPS 2023？欢迎参观 Google 研究展位，详细了解我们为解决该领域一些最有趣的挑战而所做的令人兴奋的工作。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) 帐户，了解 Google 展位活动（例如演示和问答环节）。 &lt;/p>; &lt;p>; 您可以在下面的列表中详细了解我们在会议上展示的最新前沿工作（Google 附属机构以&lt;strong>;粗体&lt;/strong>;突出显示）。请参阅 &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023&quot;>;Google DeepMind 博客&lt;/a>;，详细了解他们参加 NeurIPS 2023 的信息。&lt;/p >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;板和线组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; NeurIPS 董事会：&lt;strong>;Corinna Cortes&lt;/strong>;&lt;br />; 顾问委员会：&lt;strong>;John C. Platt&lt;/strong>; strong>;&lt;br />; 高级领域主席：&lt;strong>;Inderjit S. Dhillon&lt;/strong>;&lt;br />; 创意人工智能主席：&lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; 项目主席：&lt;strong>;Amir Globerson &lt;/strong>;&lt;br />; 数据集和基准主席：&lt;strong>; Remi Denton&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;Google 研究展位演示/问答时间表&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;h4>;此时间表可能会发生变化。请访问 Google 展位 (#215) 了解更多信息。&lt;/h4>; &lt;p>; 所见即所读？改进文本-图像对齐评估&lt;br />; 演讲者：&lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; 12 月 11 日星期一 | &lt;em>;12:15PM - 1:45PM&lt;/em>; &lt;/p>; &lt;p>; 像图一样说话：大型语言模型的图编码&lt;br />; 演讲者：&lt;strong>;Bahar Fatemi&lt;/strong>;、&lt;strong >; Jonathan Halcrow&lt;/strong>;、&lt;strong>; Bryan Perozzi&lt;/strong>;&lt;br />; 12 月 11 日星期一 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; VisIT-Bench：受实际使用启发的视觉语言教学基准&lt;br />; 演讲者：&lt;strong>;Yonatan Bitton &lt;/strong>;&lt;br />; 12 月 11 日星期一 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; MLCommons Croissant&lt;br />; 演讲者：&lt;strong>;Omar Benjelloun&lt;/strong>;、&lt;strong>; Meg Risdal&lt;/strong>;、&lt; strong>;Lora Aroyo&lt;/strong>;&lt;br />; 12 月 12 日，星期二 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; DaTaSeg：驯服通用多数据集多任务分割模型&lt;br />; 演讲者：&lt;strong>;顾秀野&lt;/strong>;&lt; br />; 12 月 12 日星期二 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; 嵌入大图&lt;br />; 演讲者：&lt;strong>;Bryan Perozzi&lt;/strong>;、&lt;strong>;Anton Tsitsulin&lt;/strong>;&lt; br />; 12 月 12 日星期二 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; 相关噪声被证明可以击败独立噪声，实现差异化的私人学习&lt;br />; 演讲者：&lt;strong>;Krishna Pillutla&lt;/strong>;&lt;br />; 12 月 12 日星期二 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; Med-PaLM&lt;br />; 演讲者：&lt;strong>;涂涛&lt;/strong>;&lt;br />; 12 月 12 日，星期二 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; StyleDrop：任意样式的文本到图像生成&lt;br />; 演讲者：&lt;strong>;Kihyuk Sohn&lt;/strong>;、&lt;strong >;Lu Jiang&lt;/strong>;、&lt;strong>;Irfan Essa&lt;/strong>;&lt;br />; 12 月 12 日，星期二 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; DICES 数据集：对话式 AI 安全评估的多样性&lt;br />; 演讲者：&lt;strong>;Lora Aroyo&lt;/strong>;、&lt;strong>; Alicia Parrish&lt;/strong>;、&lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; 谐振器：基于游戏的可扩展大型模型评估&lt;br />; 演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong >;Michal Todorovic&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Adversarial Nibbler&lt;br />; 演讲者：&lt;strong>;Lora Aroyo&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; 迈向通才生物医学人工智能&lt;br />; 演讲者：&lt;strong>;涂涛&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; 条件适配器&lt;br />; 演讲者：&lt;strong>;Junwen Bai&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; 通过多模式 RAG 进行患者援助&lt;br />; 演讲者：&lt;strong>;Ryan Knuffman&lt;/strong>;、&lt;strong>;Milica Cvetkovic&lt;/strong >;&lt;br />; 12 月 13 日星期三 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;p>; Hessian 结构如何解释锐度正则化的奥秘&lt;br />; 演讲者：&lt;strong>;Hossein Mobahi&lt;/strong>;&lt;br />; 星期三， 12 月 13 日 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;主题演讲嘉宾&lt; /h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73993&quot;>;负责任的人工智能的多面性&lt; /a>;&lt;br />; 演讲者：&lt;strong>;Lora Aroyo&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73987&quot;>;草图：核心工具、学习增强和自适应鲁棒性&lt;/a>;&lt;br />; 演讲者：&lt;strong>;Jelani Nelson&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%; &quot;>; &lt;br />; &lt;/div>; &lt;h2>;亲和力研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/ 2023/affinity-workshop/66602&quot;>;机器学习领域的女性&lt;/a>;&lt;br />; &lt;strong>;Google 赞助 - 白金级&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc /virtual/2023/affinity-workshop/66607&quot;>;AI 中的拉丁语&lt;/a>;&lt;br />; &lt;strong>;Google 赞助 - 白金级&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// nips.cc/virtual/2023/affinity-workshop/66605&quot;>;机器学习新功能&lt;/a>;&lt;br />; 组织者：&lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot; line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https:// nips.cc/virtual/2023/workshop/66541&quot;>;人工智能加速材料设计&lt;/a>; (AI4Mat-2023)&lt;br />; 炉边聊天：&lt;strong>;Gowoon Cheon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66524&quot;>;联想记忆和联想记忆Hopfield Networks 2023 年&lt;/a>;&lt;br />; 小组成员：&lt;strong>;Blaise Agüera y Arcas&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop /66535&quot;>;认知系统中的信息理论原理&lt;/a>; (InfoCog)&lt;br />;演讲者：&lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips .cc/virtual/2023/workshop/66518&quot;>;机器学习和物理科学&lt;/a>;&lt;br />;演讲者：&lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://nips.cc/virtual/2023/workshop/66494&quot;>;UniReps：统一神经模型中的表示&lt;/a>;&lt;br />;组织者：&lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://nips.cc/virtual/2023/workshop/66517&quot;>;基础模型中零/少样本学习的鲁棒性&lt;/a>; (R0-FoMo)&lt;br />; 演讲者：&lt;strong>; Partha Talukdar&lt;/strong>;&lt;br />; 组织者：&lt;strong>;Ananth Balashankar&lt;/strong>;、&lt;strong>;姚勤&lt;/strong>;、&lt;strong>;Ahmad Beirami&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66539&quot;>;扩散模型研讨会&lt;/a>;&lt;br />;演讲者：&lt;strong>;Tali Dekel&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66502&quot;>;时间视角下的算法公平性&lt;/a>;&lt;br />;圆桌会议负责人：&lt;strong>;Stephen Pfohl&lt;/strong>;&lt; br />; 组织者：&lt;strong>;Golnoosh Farnadi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66550&quot;>;深度学习中的后门：好处、坏人和丑人&lt;/a>;&lt;br />;组织者：&lt;strong>;Eugene Bagdasaryan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/ Workshop/66549&quot;>;OPT 2023：机器学习优化&lt;/a>;&lt;br />; 组织者：&lt;strong>;Cristóbal Guzmán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc /virtual/2023/workshop/66545&quot;>;机器学习促进创造力和设计&lt;/a>;&lt;br />;演讲者：&lt;strong>;Aleksander Holynski&lt;/strong>;、&lt;strong>;Alexander Mordvintsev&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66511&quot;>;机器人学习研讨会：大规模模型的预训练、微调和泛化&lt;/a>;&lt;br />;演讲者： &lt;strong>;Matt Barnes&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66516&quot;>;音频机器学习&lt;/a>;&lt;br />;组织者：&lt;strong>;Shrikanth Narayanan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66531&quot;>;基础模型时代的联邦学习&lt;/ a>; (FL@FM-NeurIPS&#39;23)&lt;br />; 演讲嘉宾：&lt;strong>;谢卓瑞&lt;/strong>;、&lt;strong>;徐峥&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://nips.cc/virtual/2023/workshop/66526&quot;>;社会责任语言建模研究&lt;/a>; (SoLaR)&lt;br />;小组成员：&lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66506&quot;>;我不敢相信这不是更好（ICBINB）：基础模型时代的失败模式&lt;/a>;&lt;br / >; 顾问委员会：&lt;strong>;Javier Antorán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66501&quot;>;系统机器学习&lt;/a>; &lt;br />; 主办方：&lt;strong>;王亚文&lt;/strong>;&lt;br />; 竞赛组委会：&lt;strong>;Bryan Perozzi&lt;/strong>;、&lt;strong>;Sami Abu-el-haija&lt;/strong>;&lt;br />; 指导委员会：&lt;strong>;Milad Hashemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66514&quot;>;自我监督学习：理论与实践&lt;/ a>;&lt;br />; 组织者：&lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;竞赛&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/competition/66581&quot;>;NeurIPS 2023 机器遗忘竞赛&lt;/ a>;&lt;br />; 组织者：&lt;b>;Isabelle Guyon&lt;/b>;、&lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023 /competition/66593&quot;>;Lux AI 挑战赛第二季 NeurIPS 版&lt;/a>;&lt;br />; 组织者：&lt;strong>;Bovard Doerschuk-Tiberi&lt;/strong>;、&lt;strong>;Addison Howard&lt;/strong>; &lt;/p>; &lt;/ div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/tutorial/73949&quot;>;以数据为中心的人工智能，实现可靠和负责任的人工智能：从理论到实践&lt;/a>;&lt;br />; &lt;strong>;Isabelle Guyon&lt;/strong >;、Nabeel Seedat、Mihaela va der Schaar &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;创意 AI 轨道&lt;/h2>; &lt; div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 创意AI表演1&amp;amp; 2&lt;br />; 演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; 组织者：&lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; &lt;em>;&lt; a href=&quot;https://nips.cc/virtual/2023/session/74093&quot;>;性能 1&lt;/a>;：12 月 11 日星期一 |下午 6:30 - 晚上 8:30，大堂舞台&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74094&quot;>;表演 2&lt;/a>;： 12 月 14 日，星期四 | 7:00PM - 9:00PM，大堂舞台&lt;/em>; &lt;/p>; &lt;p>; 创意人工智能会议 1 – 3&lt;br />; 演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong>;Yonatan Bitton&lt; /strong>;&lt;br />; 组织者：&lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74090&quot;>;会议 1&lt; /a>;：12 月 12 日，星期二 |下午 3:05 - 3:40，D2 厅&lt;/em>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74091&quot;>;会议 2&lt;/a>;： 12 月 13 日，星期三 |上午 10:45 - 下午 2:15，D2 厅&lt;/em>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74092&quot;>;第 3 场会议&lt;/a>;： 12 月 14 日，星期四 |上午 10:45 - 下午 2:15，D2 厅&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/session/75889&quot;>;创意 AI 视频&lt;/a >;&lt;br />; 主办方：&lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;博览会讲座&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78252&quot;>;图学习会面人工智能&lt;/a>;&lt;br />;演讲者：&lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel /78243&quot;>;共鸣器：音乐空间&lt;/a>;&lt;br />;演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong>;Michal Todorovic&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://nips.cc/Expo/Conferences/2023/talk%20panel/78237&quot;>;ML 中的经验严谨性是一项大规模可并行化的挑战&lt;/a>;&lt;br />; 演讲者：&lt;strong>;Megan Risdal (Kaggle)&lt;/ strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;口头演讲&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sW8yGZ4uVJ&quot;>;基于有序条件的策略梯度方法全局收敛&lt;/a>;&lt;br />; 梅金成，博Dai，&lt;strong>;Alekh Agarwal&lt;/strong>;，&lt;strong>; &lt;/strong>;Mohammad Ghavamzadeh*，Csaba Szepesvari，Dale Schuurmans &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum? id=y8UAQQHVTX&quot;>;私人永恒预测&lt;/a>;&lt;br />; Moni Naor、Kobbi Nissim、&lt;strong>;Uri Stemmer&lt;/strong>;、Chao Yan &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=PITeSdYQkv&quot;>;用户级差异隐私，每个用户的示例很少&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;、&lt;strong>;Pritish Kamath&lt;/strong>;、&lt; strong>;Ravi Kumar&lt;/strong>;、&lt;strong>;Pasin Manurangsi&lt;/strong>;、Raghu Meka、&lt;strong>;张驰远&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /forum?id=dVaWCDMBof&quot;>;DataComp：寻找下一代多模态数据集&lt;/a>;&lt;br />; Samir Yitzhak Gadre、Gabriel Ilharco、Alex Fang、Jonathan Hayase、Georgios Smyrnis、Thao Nguyen、Ryan Marten、Mitchell Wortsman、Dhruba Ghosh、张洁宇、Eyal Orgad、Rahim Entezari、Giannis Daras、Sarah Pratt、Vivek Ramanujan、Yonatan Bitton、Kalyani Marathe、Stephen Mussmann、Richard Vencu、Mehdi Cherti、Ranjay Krishna、Pang Wei Koh&lt;/strong >;、Olga Saukh、Alexander Ratner、Shuran Song、Hannaneh Hajishirzi、Ali Farhadi、Romain Beaumont、Sewoong Oh、Alex Dimakis、Jenia Jitsev、Yair Carmon、Vaishaal Shankar、Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/forum?id=w116w62fxH&quot;>;可实现回归的最佳学习者：PAC 学习和在线学习&lt;/a>;&lt;br />; Idan Attias、Steve Hanneke、Alkis Kalavasis、&lt;strong>;Amin Karbasi&lt;/strong >;, Grigoris Velegkas &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jDIlzSU8wJ&quot;>;扩散模型对于光流和单目深度估计的惊人有效性&lt;/a>;&lt;br />; Saurabh Saxena、&lt;strong>;Charles Herrmann&lt;/strong>;、&lt;strong>;Junhwa Hur&lt;/strong>;、&lt;strong>;Abhishek Kar&lt;/strong>;、&lt;strong>; &lt;/strong>;Mohammad Norouzi*、&lt;strong>;德清太阳&lt;/strong>;，&lt;strong>; &lt;/strong>;大卫·J·弗利特 &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;期刊轨道&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://jmlr.org/papers/v24/20-998.html&quot;>;使用图进行图聚类神经网络&lt;/a>;&lt;br />; &lt;strong>;Anton Tsitsulin&lt;/strong>;、&lt;strong>;John Palowitch&lt;/strong>;、&lt;strong>;Bryan Perozzi&lt;/strong>;、Emmanuel Müller &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;聚焦论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href =&quot;https://openreview.net/forum?id=1p6teT6F73&quot;>;高效变压器的交替更新&lt;/a>;（请参阅博客文章）&lt;br />; &lt;strong>;Cenk Baykal&lt;/strong>;、&lt;strong>;Dylan Cutler &lt;/strong>;、&lt;strong>;Nishanth Dikkala&lt;/strong>;、Nikhil Ghosh*、&lt;strong>;Rina Panigrahy&lt;/strong>;、&lt;strong>;王鑫&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/forum?id=EldbUlZtbd&quot;>;本地化是否有助于编辑？语言模型中基于因果关系的本地化与知识编辑的惊人差异&lt;/a>;&lt;br />; &lt;strong>;Peter Hase&lt;/strong>;、Mohit Bansal、&lt;strong>;Been Kim&lt;/strong>;、&lt;strong>; Asma Ghandeharioun &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jR2FkqW6GB&quot;>;在游戏中学习对学习者有好处吗？&lt;/a>;&lt;br />; William Brown ,&lt;strong>; Jon Schneider&lt;/strong>;,&lt;strong>; Kiran Vodrahalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Bj1QSgiBPP&quot;>;参与式个性化分类&lt;/a>;&lt;br />; Hailey Joren、&lt;strong>;Chirag Nagpal&lt;/strong>;、&lt;strong>;Katherine Heller&lt;/strong>;、&lt;strong>; &lt;/strong>;Berk Ustun &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=c2eedxSlPJ&quot;>;可分离数据上梯度下降的严格风险界限&lt;/a>;&lt;br />; Matan Schliserman，&lt;strong>;Tomer Koren&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=67o9UQgTD0&quot;>;神经语言模型中的反事实记忆&lt;/a>;&lt;br />; &lt;strong>;张驰远&lt;/strong>;,&lt;strong >; &lt;/strong>;Daphne Ippolito、Katherine Lee、Matthew Jagielski、Florian Tramèr、Nicholas Carlini &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5NxJuc0T1P&quot;>;Debias Coarsely，示例有条件地：通过最优输运和概率扩散模型进行统计降尺度&lt;/a>;&lt;br />; &lt;strong>;万忠一&lt;/strong>;、Ricardo Baptista、&lt;strong>; Anudhyan Boral&lt;/strong>;、&lt;strong>;陈一凡&lt;/strong>;,&lt;strong>;约翰·安德森&lt;/strong>;,&lt;strong>;沙飞&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;莱昂纳多·泽佩达-努涅斯&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=vTug54Uunq&quot;>;通用优化方法的更快利润最大化&lt;/a>;&lt;br />;Guanghui Wang、Zihao Hu、Vidya Muthukumar、&lt;strong>;Jacob Abernethy &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3PjCt4kmRx&quot;>;从像素到 UI 操作：学习通过图形用户界面遵循说明&lt;/a>;&lt; br />; Peter Shaw、Mandar Joshi、&lt;strong>;James Cohan&lt;/strong>;、&lt;strong>; &lt;/strong>;Jonathan Berant、Panupong Pasupat、胡鹤翔、Urvashi Khandelwal、Kenton Lee、Kristina N Toutanova &lt;/p>; &lt;p >; &lt;a href=&quot;https://openreview.net/forum?id=5Gw9YkJkFF&quot;>;PAC 根据标签比例学习线性阈值&lt;/a>;&lt;br />; &lt;strong>;Anand Brahmbhatt&lt;/strong>;、&lt;strong>; Rishi Saket&lt;/strong>;,&lt;strong>; Aravindan Raghuveer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CXPUg86A1D&quot;>;SPAE：用于多模式生成的语义金字塔自动编码器与冻结法学硕士&lt;/a>;&lt;br />;于丽君*、&lt;strong>;程勇&lt;/strong>;、&lt;strong>; &lt;/strong>;王志若、&lt;strong>;Vivek Kumar&lt;/strong>;、&lt;strong>; Wolfgang Macherey &lt;/strong>;、&lt;strong>;黄艳萍&lt;/strong>;、&lt;strong>;大卫·罗斯&lt;/strong>;、&lt;strong>;伊尔凡·埃萨&lt;/strong>;、&lt;strong>; &lt;/strong>;Yonatan Bisk、&lt;strong>;明-宣扬&lt;/strong>;、&lt;strong>;凯文·墨菲&lt;/strong>;、&lt;strong>; &lt;/strong>;亚历山大·豪普特曼、&lt;strong>;陆江&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=QatZNssk7T&quot;>;平衡对抗模型中的自适应数据分析&lt;/a>;&lt;br />; Kobbi Nissim，&lt;strong>; Uri Stemmer&lt;/strong>;，&lt;strong>; &lt;/strong>;Eliad Tsfadia &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NiQTy0NW1L&quot;>;Lexinvariant 语言模型&lt;/a>;&lt;br/>;Qian Huang、Eric Zelikman、Sarah Chen、&lt;strong >; Yuhuai Wu&lt;/strong>;、Gregory Valiant、Percy Liang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=HF6bnhfSqH&quot;>;论量子反向传播、信息重用和作弊测量折叠&lt;/a>;&lt;br />; &lt;strong>;阿米拉·阿巴斯&lt;/strong>;、&lt;strong>; &lt;/strong>;罗比·金、黄心源、&lt;strong>;威廉·J·哈金斯&lt;/strong>;、&lt;strong>;雷米斯莫瓦萨格、&lt;strong>;达尔吉尔博亚&lt;/strong>;、&lt;strong>;&lt;/strong>;&lt;strong>;贾罗德·麦克林&lt;/strong>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/forum?id=Pya0kCEpDk&quot;>;随机块模型和混合模型的私有估计算法&lt;/a>;&lt;br />;陈宏杰，&lt;strong>; Vincent Cohen-Addad&lt;/strong>;，&lt;strong>; &lt;/strong>;Tommaso d&#39;Orsi、&lt;strong>; Alessandro Epasto&lt;/strong>;、Jacob Imola、David Steurer、Stefan Tiegel &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DBz9E5aZey&quot;>;可证明通过虚拟粒子随机逼近的 SVGD 快速有限粒子变体&lt;/a>;&lt;br />; &lt;strong>;Aniket Das&lt;/strong>;、&lt;strong>;Dheeraj Nagaraj&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/forum?id=e0pRF9tOtm&quot;>;重新审视私人（随机）非凸优化：二阶平稳点和过度风险&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/strong>; ,&lt;strong>; &lt;/strong>;刘道高*、Sewoong Oh、Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=bKqrWLCMrX&quot;>;揭示视频分布变化下的自监督学习&lt;/a>;&lt;br />; Pritam Sarkar，&lt;strong>;Ahmad Beirami&lt;/strong>;，&lt;strong>;Ali Etemad&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/forum?id=ikkdTD3hQJ&quot;>;AIMS：无所不包的多层次细分&lt;/a>;&lt;br />; Lu Qi, Jason Kuen, Weidong Gui, Jiuxiang Gu, Zhe Lin, Bo杜宇旭、&lt;strong>;杨明轩&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rheCTpRrxI&quot;>;DreamHuman：来自文本的可动画 3D 头像&lt;/a>;&lt;br />; &lt;strong>;尼科斯·科洛图罗斯&lt;/strong>;、&lt;strong>;蒂莫·阿尔迪克&lt;/strong>;、&lt;strong>;安德烈·赞菲尔&lt;/strong>;、&lt;strong>;爱德华·加布里埃尔·巴扎万&lt;/strong>;、&lt; strong>;Mihai Fieraru&lt;/strong>;、&lt;strong>;Cristian Sminchisescu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=oaCDiKoJ2w&quot;>;后续行动也很重要: 通过任职后情境改善情境强盗&lt;/a>;&lt;br />;王超奇、叶子宇、&lt;strong>;冯哲&lt;/strong>;、&lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;、徐海峰&lt;/p>; &lt; p>; &lt;a href=&quot;https://openreview.net/forum?id=m21rQusNgb&quot;>;学习用于排名的列表级域不变表示&lt;/a>;&lt;br />; 西安睿成*，&lt;strong>;庄红雷&lt; /strong>;、&lt;strong>;秦珍&lt;/strong>;、Hamed Zamani*、&lt;strong>;路静&lt;/strong>;、&lt;strong>;吉马&lt;/strong>;、&lt;strong>;凯辉&lt;/strong>;、韩昭, &lt;strong>;Xuanhui Wang&lt;/strong>;, &lt;strong>;Michael Bendersky&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=hCdqDkA25J&quot;>;最优保证凸优化中的算法再现性和梯度复杂性&lt;/a>;&lt;br />; 张亮，杨俊驰，&lt;strong>;Amin Karbasi&lt;/strong>;，鸟何&lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=hJzEoQHfCe&quot;>;统一嵌入：网络规模机器学习系统经过实战检验的特征表示&lt;/a>;&lt;br />; Benjamin Coleman、Wang-Cheng Kang、&lt;strong>;Matthew Fahrbach&lt;/strong>; ,&lt;strong>; &lt;/strong>;Ruoxi Wang、Lichan Hong、Ed Chi、Derek Cheng &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xOJUmwwlJc&quot;>;接近通知校准深度神经网络&lt;/a>;&lt;br />; 熊苗、邓爱琳、&lt;strong>;Pang Wei Koh&lt;/strong>;、Jiaying Wu、Shen Li、Jianqing Xu、Bryan Hooi &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https ://openreview.net/forum?id=WfsWy59bX2&quot;>;通过相似聚类进行匿名学习：模型泛化的精确分析&lt;/a>;&lt;br />; &lt;strong>;Adel Javanmard&lt;/strong>;,&lt;strong>; Vahab Mirrokni&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EUiIbwV379&quot;>;通过更好的私有特征选择实现更好的私有线性回归&lt;/a>;&lt;br />; &lt;特拉维斯·迪克，詹妮弗·吉伦沃特*，马修·约瑟夫&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=XAyPlfmWpu&quot;>;二值化神经机器翻译&lt;/a>;&lt;br />; 张一驰、Ankush Garg、曹元、&lt;strong>;Łukasz Lew&lt;/strong>;、Behrooz Ghorbani*、张志如、Orhan Firat &lt;/p>; &lt;p>; &lt;a href= &quot;https://nips.cc/virtual/2023/poster/73665&quot;>;BoardgameQA：具有矛盾信息的自然语言推理数据集&lt;/a>;&lt;br />; &lt;strong>;Mehran Kazemi&lt;/strong>;，&lt;strong>;袁泉&lt;/strong>;、&lt;strong>;Deepti Bhatia&lt;/strong>;、&lt;strong>;Najoung Kim&lt;/strong>;、&lt;strong>;徐鑫&lt;/strong>;、&lt;strong>;Vaiva Imbrasaite&lt;/strong>;、&lt;strong>; Deepak Ramachandran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1vvsIJtnnr&quot;>;通过缓和的指数测量进行提升&lt;/a>;&lt;br />; &lt;strong>;Richard诺克，&lt;strong>; &lt;/strong>;埃桑·阿米德，&lt;strong>;曼弗雷德·沃穆斯&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SGlrCuwdsB &quot;>;（基于分数的）文本控制生成模型的概念代数&lt;/a>;&lt;br />; Zihao Wang，Lin Gui，Jeffrey Negrea，&lt;strong>;Victor Veitch&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q8mH2d6uw2&quot;>;通过不连续网络进行深度合约设计&lt;/a>;&lt;br />; Tonghan Wang、&lt;strong>;Paul Dütting&lt;/strong>;、Dmitry Ivanov、Inbal Talgam -Cohen，David C. Parkes &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=YoghyvSG0H&quot;>;Diffusion-SS3D：半监督 3D 物体检测的扩散模型&lt;/a >;&lt;br />; 何正如、戴振轩、林彦雨、&lt;strong>;杨明轩&lt;/strong>;、&lt;strong>;蔡艺轩&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PYASzxr2OP&quot;>;通过比较反馈获取用户偏好以进行个性化多目标决策&lt;/a>;&lt;br />; Han Shao、Lee Cohen、Avrim Blum、 &lt;strong>;Yshay Mansour&lt;/strong>;、Aadirupa Saha、Matthew Walter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qCglMj6A4z&quot;>;线性相关噪声的梯度下降：理论差异隐私及其应用&lt;/a>;&lt;br />; Anastasia Koloskova*、&lt;strong>;Ryan McKenna&lt;/strong>;、&lt;strong>;Zachary Charles&lt;/strong>;、&lt;strong>;J Keith Rush&lt;/strong>;、&lt;strong >;Hugh Brendan McMahan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCwToX315b&quot;>;Entrywise 变换矩阵乘积的低秩近似的难度&lt;/a>;&lt; br />; &lt;strong>;Tamas Sarlos&lt;/strong>;、&lt;strong>; &lt;/strong>;宋星友、David P. Woodruff、张秋仪 &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/forum?id=JhQP33aMx2&quot;>;多模态基础模型的逐模块自适应蒸馏&lt;/a>;&lt;br />;&lt;br />; 陈亮，&lt;strong>;于家辉&lt;/strong>;，&lt;strong>;明轩杨&lt;/strong>;、&lt;strong>;Matthew Brown&lt;/strong>;、崔寅、赵拓、&lt;strong>;宫伯清&lt;/strong>;、周天一&lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=zGRWp7yRqd&quot;>;多重交换 k-Means++&lt;/a>;&lt;br />; Lorenzo Beretta、&lt;strong>;Vincent Cohen-Addad&lt;/strong>;、&lt;strong>;Silvio Lattanzi&lt;/strong>;、 &lt;strong>; Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8vuDHCxrmy&quot;>;OpenMask3D：开放词汇 3D 实例分割&lt;/a>;&lt;br />; Ayça Takmaz、Elisabetta Fedele、Robert Sumner、Marc Pollefeys、&lt;strong>;Federico Tombari&lt;/strong>;、&lt;strong>;Francis Engelmann&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/forum?id=7RMGI4slcb&quot;>;多语言学习中数据集不平衡的情况下的顺序问题&lt;/a>;&lt;br />; Dami Choi*, &lt;strong>;Derrick Xin&lt;/strong>;, &lt;strong>;Hamid Dadkhahi&lt;/a>;&lt;br />;强>;、Justin Gilmer、Ankush Garg、Orhan Firat、Chih-Kuan Yeh、Andrew M. Dai、Behrooz Ghorbani &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=yEf8NSqTPu&quot; >;PopSign ASL v1.0：通过智能手机收集的独立美国手语数据集&lt;/a>;&lt;br />; Thad Starner、Sean Forbes、Matthew So、David Martin、Rohit Sridhar、Gururaj Deshpande、&lt;strong>;Sam Sepah&lt;/strong >;、Sahir Shahryar、Khushi Bhardwaj、Tyler Kwok、Daksh Sehgal、Saad Hassan、Bill Neubauer、Sofia Vempala、Alec Tan、Jocelyn Heath、Unnathi Kumar、Priyanka Mosur、Tavenner Hall、Rajandeep Singh、Christopher Cui、&lt;strong>;Glenn Cameron&lt; /strong>;、&lt;strong>;索希尔·戴恩&lt;/strong>;、&lt;strong>;加勒特·坦泽&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gaktiSjatl&quot;>;半隐式去噪扩散模型（SIDDMs）&lt;/a>;&lt;br />; 徐彦武*、龚明明、谢少安、&lt;strong>;魏伟&lt;/strong>;、&lt;strong>; Matthias Grundmann&lt;/strong>;、&lt;strong>; &lt;/strong>;Kayhan Batmanghelich，&lt;strong>;侯廷波&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xGz0wAIJrS&quot;>;State2Explanation：基于概念的解释有益于代理学习和用户理解&lt;/a>;&lt;br />; Devleena Das、Sonia Chernova、&lt;strong>;Been Kim&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum ?id=AwhpBEqmyo&quot;>;StoryBench：连续故事可视化的多方面基准&lt;/a>;&lt;br />; Emanuele Bugliarello*、Hernan Moraldo、Ruben Villegas、Mohammad Babaeizadeh、Mohammad Taghi Saffar、Han Zhang、Dumitru Erhan、&lt;strong>; Vittorio Ferrari&lt;/strong>;、Pieter-Jan Kindermans、&lt;strong>;Paul Voigtlaender&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=wv3bHyQbX7&quot;>;主题驱动通过学徒学习实现文本到图像生成&lt;/a>;&lt;br />;陈文虎、胡鹤翔、&lt;strong>;李彦东&lt;/strong>;、&lt;strong>;Nataniel Ruiz&lt;/strong>;、&lt;strong>;贾旭辉&lt;/strong>; strong>;、Ming-Wei Chang、William W. Cohen &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=plAix1NxhU&quot;>;TpuGraphs：大张量计算图上的性能预测数据集&lt;/a>;&lt;br />; &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;、&lt;strong>;Sami Abu-El-Haija&lt;/strong>;、Kaidi Cao*、&lt;strong>;Bahare Fatemi&lt;/strong>;、&lt;strong>; Mike Burrows&lt;/strong>;、Charith Mendis*、&lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=a147pIS2Co&quot;>;培训链-通过潜在变量推理进行思想&lt;/a>;&lt;br />; &lt;strong>;Du Phan&lt;/strong>;、&lt;strong>;Matthew D. Hoffman&lt;/strong>;、David Dohan*、Sholto Douglas、&lt;strong>; Tuan Anh Le&lt;/strong>;、Aaron Parisi、&lt;strong>;Pavel Sountsov&lt;/strong>;、Charles Sutton、Sharad Vikram、&lt;strong>; Rif A. Saurous&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=1ZzG6td0el&quot;>;信息约束下交互式高维估计的统一下界&lt;/a>;&lt;br />; Jayadev Acharya, Clement L. Canonne, &lt;strong>;孙子腾&lt;/strong>; , Himanshu Tyagi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=j5AoleAIru&quot;>;所见即所读？改进文本-图像对齐评估&lt;/a>;&lt;br />; &lt;strong>;Michal Yarom&lt;/strong>;、&lt;strong>;Yonatan Bitton&lt;/strong>;、&lt;strong>;Soravit Changpinyo&lt;/strong>;、&lt;strong>;Roee Aharoni&lt; /strong>;、&lt;strong>;乔纳森·赫齐格&lt;/strong>;、&lt;strong>;奥兰·朗&lt;/strong>;、&lt;strong>;&lt;/strong>;&lt;strong>;埃兰·奥菲克&lt;/strong>;、&lt;strong>;伊丹·斯佩克托&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=4kzhzjspyu&quot;>;什么时候基于信心的级联cascade延期足够？&lt;/a>; &lt;br />; &lt;br />; &lt;br />; /strong>;，&lt;strong>; neha gupta &lt;/strong>;，&lt;strong>; aditya krishna menon &lt;/strong>;，&lt;strong>; harikrishna narasimhan &lt;/strong>;，&lt;strong>; ankit singh singh rawat &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; sanjiv kumar &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=A18pgvSugf&quot;>;通过知识蒸馏加速分子图神经网络&lt;/a>; &lt;br />; Kelvinius，Dimitar Georgiev，Artur Petrov Toshev，&lt;strong>; Johannes Gasteiger &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum？与大型语言模型代理寻求&lt;/a>; &lt;br />; Ziniu Hu*，&lt;strong>; Ahmet Iscen &lt;/strong>;，&lt;strong>; Chen Sun &lt;/strong>;，Kai-Wei Chang，Yizhou Sun，&lt;strong>; David Ross &lt;/strong>;，&lt;strong>; Cordelia Schmid &lt;/strong>;，&lt;strong>; alireza fathi &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net.net/forum?id=9mjxdcr17vcr17v “>;超越不变性：测试时间标签转移适应用于解决“伪造”相关性&lt;/a>; &lt;br />; &lt;br />; Qingyao Sun，Kevin Patrick Murphy，&lt;strong>; Sayna Ebrahimi &lt;/strong>;，Alexander d&#39;Amour &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0tejorcgfd&quot;>;协作得分蒸馏量始终Jeong，&lt;strong>; kihyuk sohn &lt;/strong>;，jinwoo shin &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1sf2tiopyj&quot;>; Commonscenes：图形&lt;/a>; &lt;br />;广播Zhai，EvinPınarörnek，Shun-Cheng Wu，Yan di，&lt;strong>; Federico Tombari &lt;/strong>;，Nassir Navab，Benjamin Busam &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href = a href = “ https://openreview.net/forum?id=65Adexihih&quot;>;学习神经网络的计算复杂性：平滑度和退化&lt;/a>; &lt;br />; &lt;br />; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=bopg5dhh7l&quot;>;一种计算高效的在线牛顿方法&lt;/a>; &lt;br />; fnu devvrit*，sai surya devvrit* strong>; &lt;/strong>; rohan anil，&lt;strong>;葡萄etgupta &lt;/strong>;，&lt;strong>; cho-jui hsieh &lt;/strong>;，&lt;strong>; inderjit s dhillon &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=6edhfvhicp”>; ddf-ho：通过有条件的有向距离领域进行手持式对象重建&lt;/a>; &lt;br/>; &lt;br/>; chenyangguang Zhang，yan di Zhai，&lt;strong>; Fabian Manhardt &lt;/strong>;，&lt;strong>; Federico Tombari &lt;/strong>;，Xiangyang ji &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net.net/forum？ >;双面匪徒反馈的双重拍卖&lt;/a>; &lt;br />; &lt;br />; &lt;strong>; soumya basu &lt;/strong>;，Abishek Sankararaman &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://arxiv.org/abs /2305.19234&quot;>; grammar提示具有大型语言模型的特定域语言生成&lt;/a>; &lt;br />; &lt;br />; Bailin Wang，Zi Wang，Xuezhi Wang，&lt;strong>; Yuan Cao &lt;/strong>;，Rif A. Saurous，Yoon Kim Kim Kim Kim Kim Kim &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5Siz31Ogfv&quot;>;深神经网络培训的不一致，不稳定和概括差距Tong Zhang* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3ydukx2cpr&quot;>;通过图段培训大图属性预测&lt;/a>; &lt;br />; kaidi cao*，&lt;br />; &lt;strong>; Phitchaya Mangpo phothilimthana &lt;/strong>;，&lt;strong>; sami abu-el-haija &lt;/strong>;，&lt;strong>; dustin zelle &lt;/strong>;，&lt;strong>; yanqi zhou &lt;/strong>;，&lt;strong>; Strong>; Charith Mendis*，Jure Leskovec，&lt;strong>; Bryan Perozzi &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1gxkkvprbwm &quot;>;在计算配对统计范围内与配对统计数据本地差异隐私&lt;/a>; &lt;br />; &lt;strong>; badih ghazi &lt;/strong>;，&lt;strong>; pritish kamath &lt;/strong>;，&lt;strong>; ravi kumar &lt;/strong>;，&lt;strong>; pasin manurangsi &lt;/strong>; ，&lt;strong>; adam sealfon &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7udvprMpif&quot;>;关于蒸馏的学生 - 教师偏差：它付费以付费支付？&lt;/a>; &lt;br />; &lt;strong>; vaishnavh nagarajan &lt;/strong>;，&lt;strong>; aditya krishna menon &lt;/strong>;，&lt;strong>; srinadh bhojanapalli &lt;/strong>;，&lt;strong>; &lt;strong>; sanjiv kumar &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=czkozkozkwpma&quot;>;最佳的上下文划线具有未知上下文分布&lt;/a >; &lt;br />; &lt;strong>; Jon Schneider &lt;/strong>;，&lt;strong>; Julian Zimmert &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=8xrmbnap6z” >;滑动窗口模型中的近乎最佳的K群集&lt;/a>; &lt;br />; David Woodruff，&lt;strong>; Peilin Zhong &lt;/strong>;，Samson Zhou &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https：//https：/https：/ /openreview.net/forum?id=3H37xciuev&quot;>; post hoc的语言模型可以改善语言模型&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; Sameer Singh，Himabindu lakkaraju &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=bj0fquu32w&quot;>;推荐系统具有生成性检索&lt;/a>; >;，&lt;/strong>; Nikhil Mehta，Anima Singh，&lt;strong>; Raghunandan Hulikal Keshavan &lt;/strong>;，&lt;strong>; Trung Vu，Lukasz Heldt &lt;/strong>;，&lt;strong>; &lt;/strong>; &lt;/strong>; lichan hong，是的>;，Vinh Q. Tran &lt;/strong>;，&lt;strong>; Jonah Samost &lt;/strong>;，Maciej Kula，Ed H. Chi，Maheswaran sathiamoorthy &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https:///openreview.net.net /论坛？ >; moonkyung ryu &lt;/strong>;，&lt;strong>; craig boutilier &lt;/strong>;，pieter abbeel，Mohammad Ghavamzadeh*，Kangwook Lee，Kimin Lee* &lt;/p>; &lt;p>; &lt;p>; &lt;a href = /forum？id = 5vqfavuhcd“>;可复制的聚类&lt;/a>; &lt;br />; ，Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5cppz5hrjy6&quot;>;在增强学习中的可复制性&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; >;，Grigoris Velegkas，Lin Yang，Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=szfqlnrxes&quot;>; riemannian投射免费在线学习>; Zihao Hu，Guanghui Wang，&lt;strong>; Jacob Abernethy &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=29wbrapk8u>; -Lank功能&lt;/a>; &lt;br />; maksym andriushchenko，&lt;strong>; dara bahri &lt;/strong>;，&lt;strong>; hossein bobahi &lt;/strong>;，nicolas flammarion &lt;/p>; &lt;p>; &lt;p>; &lt;a href = a href =“ https：” https： //openreview.net/forum？id = 2HQ7MBQAPP&quot;>;平面正则化的感应偏置是什么？深矩阵分解模型的研究&lt;/a>; &lt;br />; Khashayar Gatmiry，Zhiyuan Li，Ching-Yao Chuang，&lt;strong>; Sashank Reddi &lt;/strong>;，Tengyu Ma，Stefanie Jegelka href =“ https://openreview.net/forum？id = jzqlgqbm8d”>; block low-rank preponditioner具有随机优化的共享基础&lt;/a>; &lt;br />; &lt;br />; &lt;br />; s dhillon &lt;/strong>;，&lt;strong>; cho-jui hsieh &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id= = ntd6x7uwyf&quot;>;封锁的协作匪徒：在线使用每个项目预算约束的协作过滤&lt;/a>; &lt;br />; &lt;strong>; soumyabrata pal &lt;/strong>;，&lt;strong>; arun sai suggala &lt;/strong>;，&lt;strong>; karthikeyan shanmugam &lt;/strong>;，&lt;strong>; &lt;strong>; prateek Jain &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nirtsciiz7&quot;>;边界指导的带有扩散模型的边界指导性学习 -  fime-delight-firee-frive-nodant >; Ye Zhu，Yu Wu，&lt;strong>; Zhiwei Deng &lt;/strong>;，Olga Russakovsky，Yan yan yan yan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum？适配器：快速推理的参数效率转移学习&lt;/a>; &lt;br />; tao lei，&lt;strong>; junwen bai &lt;/strong>;，siddhartha brahma，&lt;strong>; joshua ainslie &lt;/strong>;， Nan du*，&lt;strong>; Vincent Y. &lt;a href=&quot;https://openreview.net/forum?id=ktrwpwcmsc&quot;>;与现代Hopfield Networks的时间序列&lt;/a>; &lt;br />; Andreas Auer，&lt;strong>; Martin Gauch &lt;/strong>;，丹尼尔·克洛兹（Daniel Klotz），sepp hochreiter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum？ >; &lt;strong>; Chen Sun &lt;/strong>;，Calvin Luo，&lt;strong>; Xingyi Zhou &lt;/strong>;，&lt;strong>; Anurag Arnab &lt;/strong>;，&lt;strong>; Cordelia Schmid &lt;/strong>; &lt;a href=&quot;https://openreview.net/forum?id=payxfiukwy&quot;>;具有不同训练数据的模型的自然分配变化有效鲁棒性&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; /strong>;，&lt;strong>; ananth balashankar &lt;/strong>;，路德维格·施密特（Ludwig Schmidt），&lt;strong>; cho-jui hsieh &lt;/strong>;，alex beutel*，&lt;strong>; yao Qin a href =“ https://openreview.net/forum?id=nh5dp6uuvx”>;使用人类相似性判断改善神经网络表示&lt;/a>; &lt;br />; &lt;br />; &lt;br />; lukas uttenthaler*，Lorenz Linhardt，Linhardt，Jonas Dippel，Jonas Dippel，Robert A. Vandermeeulen，，Robert A. Vandermeulen，，，Robert A. Vandermeulen，，，，lorenz linhardt Katherine Hermann，Andrew K. Lampinen，Simon Kornblith &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum？ /a>; &lt;br />; xiyang liu，&lt;strong>; prateek jain &lt;/strong>;，&lt;strong>; weihao kong &lt;/strong>;，&lt;strong>; sewoong oh oh &lt;/strong>;，&lt;strong>; arun sai suggala &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=fdfyga5i0a&quot;>; mnemosyne：学习用变形金刚培训变形金刚&lt;/a>; &lt;br />; &lt;br />; Deepali Jain，deepala Strong>; Avinava Dubey &lt;/strong>;，Sumeet Singh，Vikas Sindhwani，Tingnan Zhang，Jie Tan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum？对于线性土匪&lt;/a>; &lt;br />; Ayush Sawarni，&lt;strong>; soumyabrata pal &lt;/strong>;，siddharth Barman &lt;/p>; &lt;p>; &lt;a href =“ >;倒角距离的接近线性时间算法&lt;/a>; &lt;br />; Ainesh Bakshi，Piotr Indyk，&lt;strong>; Rajesh Jayaram &lt;/strong>;，Sandeep Silwal，Erik Waingarten。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=fvif8vuz5b&quot;>;从高斯和产品分发的差异私人抽样&lt;/a>; &lt;br />; &lt;br />; &lt;br />; /strong>;，xiao hu*，&lt;strong>; ravi kumar &lt;/strong>;，&lt;strong>; pasin manurangsi &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net/forum？id =马尔可夫决策过程中静态风险度量的动态编程分解&lt;/a>; &lt;br />; &lt;br />; ：//openreview.net/forum？id = hfqfaynucq“>; resmem：了解您可以记住什么并记住其余的&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;strong>; Michal Lukasik &lt;/strong>; &lt;/strong>;，&lt;strong>; Zonglin li &lt;/strong>;，&lt;strong>; ankit Singh Rawat &lt;/strong>;，&lt;strong>; Manzil Zaheer，&lt;/strong>; &lt;strong>; Aditya Krishna Menon &lt;/strong>;，&lt;strong>; sanjiv kumar &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pcnpl9q39p&quot;>;负责AI（RAI）Games and Ensembles &lt;/a>; Gupta，Runtian Zhai，&lt;strong>; Arun Suggala &lt;/strong>;，Pradeep Ravikumar &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=dvlawv2rsi&quot;>; roboclip &quot;>; roboclip：一个示范足以学习机器人政策&lt;/a>; &lt;br />; >; &lt;a href=&quot;https://openreview.net/forum?id=i6aojhpcnq&quot;>;通过kernelized rate-distortht通过kernelized rate-distortion的最大化&lt;/a>; &lt;br />; &lt;br />; Avinava Dubey &lt;/strong>;，&lt;strong>; AMR AHMED &lt;/strong &lt;/strong>;，Snigdha Chaturvedi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=fmzvre0ge通过对抗正规化学习的强化学习：理论基础和稳定算法&lt;/a>; &lt;br />; Alexander Bukharin，Yan Li，Yue Yu，Yue Yu，Qingru Zhang，&lt;strong>; Zhehui Chen &lt;/strong>; tuo zhao &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ptvxckk0qde&quot;>; 1- hidden层神经网络中的简单偏见&lt;/a>; &lt;br />; &lt;br />; &lt;br />; Jatin Batra，&lt;strong>; Prateek Jain &lt;/strong>;，&lt;strong>; praneeth netrapalli &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2302.03806&quot;>; SLAM：学生 - 与未标记的示例蒸馏的标签混合&lt;/a>; &lt;br />; &lt;br />; vasilis kontonis，&lt;strong>; fotis iliopoulos &lt;/strong>;，&lt;strong>; khoa trinh &lt;/strong>;，&lt;strong>; cenk baykal &lt;/strong>; strong>; gaurav menghani &lt;/strong>;，&lt;strong>; erik vee &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=lchmp68gtj&quot;>;视觉定位和语义理解的神经图&lt;/a>; &lt;br />; Paul-Edouard Sarlin*，&lt;strong>; Eduard Trulls &lt;/strong>;，Marc Pollefeys，&lt;strong>; Jan Hosang &lt;/strong>;，&lt;strong>; Simon Lynen &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qvivwmaqdx&quot;>; soar soar：改进索引索引，以获取大约最近的邻居搜索&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;strong &lt;strong >; Philip Sun &lt;/strong>;，&lt;strong>; David Simcha &lt;/strong>;，&lt;strong>; Dave Dopson &lt;/strong>;，&lt;strong>; ruiqi guo &lt;/strong>;，&lt;strong>; sanjiv kumar &lt;/strong>; &lt;/strong>; &lt;/p &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=koafh16uoc&quot;>; StysledRop：任何样式的文本对图像合成&lt;/a>; &lt;br />; &lt;br />; &lt;br />; strong>;，&lt;strong>; lu jiang &lt;/strong>;，&lt;strong>; Jarred Barber &lt;/strong>;，Kimin Lee*，&lt;strong>; nataniel ruiz &lt;/strong>;，&lt;strong>; dialip krishnan &lt;/strong>;，huiwen chang* ，&lt;strong>; yuanzhen li &lt;/strong>;，&lt;strong>; irfan essa &lt;/strong>;，&lt;strong>;迈克尔·鲁宾斯坦&lt;/strong>;，&lt;strong>; yuan hao &lt;/strong>; ，&lt;strong>; Irina Blok &lt;/strong>;，&lt;strong>; Daniel Castro Chin &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=lsyqb4cwd3&quot;>;三座：通过预审前的图像模型&lt;/a>; &lt;br />; Jannik Kossen*，&lt;strong>; Mark Collier &lt;/strong>;，Basil Mustafa，Xiao Wang，Xiaohua Zhai，Lucas Beyer，Andreas Steiner，&lt;strong>; Jesse Berent，&lt;strong>; Jesse Berent，在>;两个阶段学习与多位专家推迟&lt;/a>; &lt;br />; Anqi Mao，Christopher Mohri，&lt;strong>; Mehryar Mohri &lt;/strong>;，Yutao Zhong Zhong &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href =“ href =” https：https：https：https： ///openreview.net/forum?id=zbzywp2gpl&quot;>; adanns：自适应语义搜索的框架&lt;/a>; &lt;br />; Aniket Rege，&lt;strong>; aditya kusupati &lt;/strong>;，Sharan Ranjit S，Alan Fan，Alan Fan，Qingqing cao，sham kakade，&lt;strong>; prateek jain &lt;/strong>;，ali farhadi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=srt1hhqgqa&quot;>;带有小得分手的多任务LM &lt;/a>; &lt;br />; bowen tan*，&lt;strong>; yun Zhu &lt;/strong>;，&lt;strong>; lijuan liuu &lt;/strong &lt;/strong &lt;/strong>;，eric xing，zh​​iting hu，&lt;strong>; jindong Chen &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=djz3mvdw86&quot;>;因果关系驱动的文本驱动的文本增强&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; Strong>; Amir Feder &lt;/strong>;，Yoav Wald，Claudia Shi，Susti Saria，David Blei &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=s0xrbmfihs&quot;>;随机特征：高斯内核的正面估计量&lt;/a>; &lt;br />; &lt;br />; valerii likosherstov，krzysztof choromanski，&lt;strong>; avinava dubey &lt;/strong>;，&lt;strong>; frederick liu &lt;/strong>; /strong>;，Adrian Weller &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=vm1zeyqwdc&quot;>; dizfusion hyperfeatures：在时间和空间中寻找语义通信&lt;/a>; &lt;br &lt;/a>; &lt;br />; Grace Luo，Lisa Dunlap，Dong Huk Park，&lt;strong>; Aleksander Holynski &lt;/strong>;，Trevor Darrell &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum？ >;可控图像生成的扩散自我诱导&lt;/a>; &lt;br />; &lt;strong>; dave Epstein &lt;/strong>;，Allan Jabri，&lt;strong>; Ben Poole &lt;/strong>;，Alexei a efros，&lt;strong>; Aleksander holynski &lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dngepkmnzo&quot;>;完全动态k-c群集在（k）更新时间&lt;/a>; &lt;br />; &lt;br />; Sayan Bhattacharya，Martin Nicolas Costa，&lt;strong>; Silvio Lattanzi &lt;/strong>;，&lt;strong>; Nikos parostidis &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ svjdiivysh“>;通过语言重写改进剪辑培训&lt;/a>; &lt;br />; &lt;strong>; lijie fan &lt;/strong>;，&lt;strong>; dilip krishnan &lt;/strong>;，Phillip Isela，Dina Katabi，&lt;strong>; strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uzuhikacms&quot;>; k-means聚类与基于远距离的隐私&lt;/a>; &lt;br>; &lt;br>; &lt;br>; &lt;br>; &lt;brong>; &lt;brong>; Alessandro Epasto &lt;/strong>;，&lt;strong>; vahab mirrokni &lt;/strong>;，Shyam Narayanan，&lt;strong>; Peilin Zhong &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net/forum？id = XU8AG5Q8M3“>; layoutgpt：大型语言模型的组成视觉计划和发电&lt;/a>; &lt;br />; Weixi Feng，Wanrong Zhu，Tsu-Jui Fu，&lt;strong>; varun Jampani &lt;/strong>;，&lt;strong>; varun Jampani &lt;/strong>; &lt;/strong>;，xuehai he，&lt;strong>; sugato basu &lt;/strong>;，Xin Eric Wang，William Yang Wang &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=sxxn3kntsv “>;专业化对话管理的离线加强学习&lt;/a>; &lt;br />; &lt;br />; dhawal gupta*，&lt;strong>; yinlam chow &lt;/strong>;，&lt;strong>; azamat tulepbergenov &lt;/strong>;，穆罕默德·加瓦姆扎德（Mohammad Ghavamzadeh）*，&lt;，&lt; strong>; craig boutilier &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ag6xop9qy7&quot;>;最佳无偏随机化因素用于带有标签差异隐私的回归&lt;/a>; &lt;br &lt;br &lt;br &lt;br &lt;br &lt;br />; &lt;strong>; Ashwinkumar badanidiyuru &lt;/strong>;，&lt;strong>; badih ghazi &lt;/strong>;，&lt;strong>; pritish kamath &lt;/strong>;，&lt;strong>; ravi kumar &lt;/strong>;，&lt;strong>; eThand>; Ethan Jacob Leeman &lt;/strong>; strong>;，&lt;strong>; &lt;/strong>; &lt;strong>; pasin manurangsi &lt;/strong>;，&lt;strong>; avinash v varadarajan &lt;/strong>;，&lt;strong>; chiyuan zhang zhang &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=wbfhfvjjkj”>;释义远程逃避AI生成的文本的检测器，但检索是有效的防御&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; Yixiao Song，Marzena Karpinska，John Wieting，Mohit Iyyer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=samrn9tnxe&quot;>; remax：remax：在有效的综合季节&lt;/remax上进行更好的培训&lt;/ A>; &lt;br />; Shuyang Sun*，&lt;strong>; Weijun Wang &lt;/strong>;，Qihang Yu*，&lt;strong>; Andrew Howard &lt;/strong>;，Philip Torr，Liang-Chieh Chen*&lt;/p>; &lt;p>; &lt;p>; &lt;p &lt;p>; &lt; a href =“ https://openreview.net/forum?id= sourowc5un”>;稳健而积极的无服务器协作学习&lt;/a>; &lt;br />; &lt;br />; nicholas Franzese，Adam dziedzic，&lt;strong>; Christopher A. Choceets-Choquette-Choote-Choo &lt;stronge /strong>;，马克·R·托马斯（Mark R. OpenReview.net/forum?id=sdyhltcc5j&quot;>; Spectr：通过最佳运输快速投机解码&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;strong>; Ziteng Sun &lt;/strong>;，&lt;strong>; Ananda Theertha Suresh &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;strong &lt;strong &lt;strong &lt;strong >; jae hun ro &lt;/strong>;，&lt;strong>; ahmad beirami &lt;/strong>;，&lt;strong>; himanshu Jain &lt;/strong>;，&lt;strong>; felix yu &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href =&#39; https://openreview.net/forum?id=yz7ip645ra&quot;>;结构性预测具有更强的一致性保证&lt;/a>; &lt;br/>; &lt;br />; &lt;br/>; Anqi Mao，&lt;strong>; Mehryar Mohri &lt;/strong>; >; &lt;a href=&quot;https://openreview.net/forum?id=qgig7wzohz&quot;>; affinity-waw graph网络&lt;/a>; &lt;br />; &lt;/strong>;，IraKtena，PetarVeličković，&lt;strong>; sreenivas gollapudi &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rjc5lsn5lsn5qu&quot;>; artic3d：嘈杂的Web图像收藏中的稳健表达的3D形状&lt;/a>; &lt;br />; Chun-Han Yao*，&lt;strong>; Amit raj &lt;/strong>;，wei-chih Hung，&lt;strong>; yuanzhen li &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;strong >;迈克尔·鲁宾斯坦（Michael Rubinstein）&lt;/strong>;，&lt;strong>; ming-hsuan yang &lt;/strong>;，&lt;strong>; &lt;/strong>; &lt;strong>; ///openreview.net/forum?id= eodnah3pfb&quot;>; black-box差异差异ML &lt;/a>; &lt;br />; &lt;br />; &lt;brter />; &lt;strong>; haim kaplan &lt;/strong>;，&lt;strong>; yishay mansour &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt; Strong>; Shay Moran &lt;/strong &lt;/strong>;，Kobbi Nissim，&lt;strong>; uri stemmer &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=orh4e0ao9r&quot;>;绕过模拟器：近乎最佳的对抗线性上下文强盗&lt;/a>; &lt;br />; haolin liu，chen-yu wei，&lt;strong>; julian julian Zimmert &lt;/strong>; &lt;/pring>; &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href = a href =&#39;href =&#39; OpenReview.net/forum?id=kxoxrvnwbB&quot;>; dataseg：驯服通用多数据套件多任务分段模型&lt;/a>; &lt;br />; &lt;br />; &lt;strong>; xiuye gu &lt;/strong>;，yin cui*，yin cui* Huang &lt;/strong>;，&lt;strong>; Abdullah Rashwan &lt;/strong>;，&lt;strong>; Xuan Yang &lt;/strong>;，&lt;strong>; Xingyi Zhou &lt;/strong>;，&lt;strong>; Golnaz Ghiasi &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; weicheng kuo &lt;/strong>;，&lt;strong>; Huizhong Chen &lt;/strong>;，Liang-Chieh Chen*，&lt;strong>; David Ross &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net /论坛？ Claudio Gentile &lt;/strong>;，&lt;strong>; Andres Munoz Medina &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q3fcwoc9l0&quot;>;有效的数据子集选择跨模型的培训：转导和感应网络&lt;/a>; &lt;br />; &lt;br />; Eeshaan Jain，Tushar Nandy，&lt;strong>; Gaurav aggarwal &lt;/strong>;，&lt;strong>; Ashish tendulkar &lt;/strong>;，Rishabh Iyer，Abir de &lt;/p &lt;/p &lt;/p &lt;/p &lt;/p &lt;/p &lt;/p &lt;/p &lt;/p &lt;/p &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=h2lkx9sqcd&quot;>;通过二阶方法更快地差异化优化&lt;/a>; &lt;br />; &lt;br />; &lt;br />; >;，Mahdi Haghifam*，Thomas Steinke，Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=gdvcfovxt3&quot;>; >; &lt;br />; Lee Cohen，&lt;strong>; Yishay Mansour &lt;/strong>;，Michal Moshkovitz &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=s1fjxzjxzj0jy&quot;>; focused fosised upissed upissed upissed upissed uped pocused transferer：上下文缩放的对比培训&lt;/a>; &lt;br />; Szymon Tworkowski，Konrad Staniszewski，MikołajPacek，Yuhuai Wu*，Henryk Michalewski，PiotrMiłośmiLoLoś /论坛？ a href =“ https://openreview.net/forum?id= = ni7emxq2pl”>; h-consistenciency bounds：特征和扩展&lt;/a>; &lt;br />; &lt;br />; &lt;br />; anqi mao，&lt;strong>; mehryar mohri mohri &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kjmghto8cs&quot;>;逆动力学预处理学习多任务模仿的良好表示&lt;/a>; &lt;br />; ofir nachum &lt;/strong>;，琼·布鲁纳（Joan Bruna）&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pvpujuvjqd&quot;>;大多数神经网络几乎是可学习的&lt;/a>; &lt;/a>; &lt;br />; &lt;strong>; Amit Daniely &lt;/strong>;，Nathan Srebro，gal Vardi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nq84yy9iut&quot;>;多类的增强：简单而直觉的弱学习：标准&lt;/a>; &lt;br />; Nataly Brukhim，&lt;strong>; Amit Daniely &lt;/strong>;，&lt;strong>; yishay Mansour &lt;/strong>;，&lt;strong>; shay Moran &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;a Href =“ https://openreview.net/forum？ Guibas,&lt;strong>; Ke Li&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=izNfcaHJk0&quot;>;Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-分布式平均值估计中的交流权衡&lt;/a>; &lt;br />; we-ning Chen，Dan Song，Ayfer Ozgur，&lt;strong>; Peter Kairouz &lt;/strong>; &lt;/pr>; &lt;/p>; &lt;p>; &lt;a href =“ https：https：https： ///openreview.net/forum?id=rzdboh1tbh&quot;>; private联合频率估计：适应实例的硬度&lt;/a>; &lt;br />; Jingfeng Wu*，&lt;strong>; Wennan Zhu &lt;/strong>;，&lt;strong>; Peter Kairouz &lt;/strong>;，vladimir braverman &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pvlc0remkq&quot;>; retvec：retvec：弹性和有效的文本矢量>; &lt;strong>; Elie Bursztein &lt;/strong>;，&lt;strong>; Marina Zhang &lt;/strong>;，&lt;strong>; Owen Skipper Vallis &lt;/strong>;，&lt;strong>; Xinyu Jia &lt;/strong>;，&lt;strong>; Alexey Kurakin &lt;/strong >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ne6zeqlfcz&quot;>;符号发现优化算法的符号发现&lt;/a>; &lt;br />; &lt;br />; xiangning chen* &lt;/strong>;，&lt;strong>; da huang &lt;/strong>;，&lt;strong>; esteban Real &lt;/strong>;，&lt;strong>; kaiyuan wang &lt;/strong>;，&lt;strong>; hieu pham &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; xuanyi dong &lt;/strong>;，&lt;strong>; href =“ https://openreview.net/forum？id=lds9d17hrd”>;一个有两个特征的故事：稳定的扩散补充Dino为零 - 零的语义通信&lt;/a>; &lt;br />; &lt;br />; Junyi Zhang，&lt;br />; &lt;/strong>;，&lt;strong>; junhwa hur &lt;/strong>;，&lt;strong>; Luisa F. Polania &lt;/strong>;，&lt;strong>; Varun Jampani &lt;/strong>;，&lt;strong>; Deqing Sun &lt;/strong>;，&lt;strong>; ming-hsuan yang &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=iSd = isd = isd = isd = isd = isd = isd = isd = isd = iisd = &lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; Hanneke，&lt;strong>; Shay Moran &lt;/strong>;，Jonathan Shafer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=i2h2seiq2t&quot;>;一个统一的DP--快速逐渐剪切框架SGD &lt;/a>; &lt;br />; &lt;strong>; William Kong &lt;/strong>;，&lt;strong>; Andres Munoz Medina &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net/forum ？ /strong>;，&lt;strong>; h。 Brendan McMahan &lt;/strong>;，&lt;strong>; alina oprea &lt;/strong>;，&lt;strong>; sewoong oh oh &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =“ https:///openreview.net/forum?id= ZEM6HF97PZ“>;（放大）带矩阵分解：一种统一的私人训练方法&lt;/a>; &lt;br/>; Christopher A Choquette-Choo，&lt;strong>; Arun Ganesh &lt;/strong>;，&lt;strong>; Ryan McKenna &lt;/strong>;， &lt;strong>; H Brendan McMahan &lt;/strong>;，&lt;strong>; Keith Rush &lt;/strong>;，&lt;strong>; &lt;/strong>; abhradeep guha thakurta，&lt;strong>; Zheng Xu Xu &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=xcghx9fdxm”>;通过弃权的顺序预测中的对抗性弹性&lt;/a>; &lt;br />; &lt;br />; shetty &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=utlkuam68h&quot;>;交替的梯度下降和混合物的综合多模式感知&lt;/a>; &lt;br />; &lt;br />; &lt;br />; strong>; hassan akbari &lt;/strong>;，&lt;strong>; dan kondratyuk &lt;/strong>;，&lt;strong>; yin cui &lt;/strong>;，&lt;strong>; rachel hornung &lt;/strong>;，&lt;strong>; huisheng wang &lt;/strong>;，&lt; Strong>; Hartwig Adam &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf/0083ab45a583fe1992b3c4f92222081c50b922081c50b9d30c30d30c3.pdf&quot;>; android in the Field：a glod-scale dateforce &lt; /a>; &lt;br />; &lt;strong>; Christopher Rawles &lt;/strong>;，&lt;strong>; Alice Li &lt;/strong>;，&lt;strong>; Daniel Rodriguez &lt;/strong>;，&lt;strong>; Oriana Riva &lt;/strong>;，Timothy Lillicrap &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=cirhwarbp0&quot;>;基于对抗性图像obfusccations的鲁棒性&lt;/a>; &lt;br />; &lt;br />; &lt;br />; strong>;，&lt;strong>; chun-ta lu &lt;/strong>;，&lt;strong>; hussein hazimeh &lt;/strong>;，&lt;strong>; otilia stretcu &lt;/strong>;，&lt;strong>; wei qiao &lt;/strong &lt;/strong>;，&lt;strong>; yintao liu &lt;/strong>;，&lt;strong>; merve kaya &lt;/strong>;，&lt;strong>; Cyrus rashtchian &lt;/strong>;，&lt;strong>; ariel fuxman &lt;/strong>;，&lt;strong>; mehmet tek &lt;/strong>;，sven gowal &lt;/p &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uijjdc8k6&quot;>;通过社区参与建立社会culturally-cultury-culturally-Culturomentimentiment-Cultife Inderipements Imcorials &lt;/a>; &lt;br />; &lt;br />; &lt;br />; >;，Jaya Goyal，&lt;strong>; Dinesh Tewari &lt;/strong>;，&lt;strong>; Shachi Dave &lt;/strong>;，&lt;strong>; vinodkumar prabhakaran &lt;/strong>; &lt;/prabhakaran &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href = a href = a href =“ https：// https：// https：// OpenReview.net/forum?id=l9i9fhhfs3&quot;>; consensus和Ml Fairness的肤色注释的主观性&lt;/a>; &lt;br />; &lt;br />; &lt;brand>; Candice Schumann &lt;/strong>; &lt;strong>; Auriel Wright &lt;/strong>;，Ellis Monk Jr*，&lt;strong>; Courtney Heldreth &lt;/strong>;，&lt;strong>; &lt;/strong>; &lt;strong>; susanna ricco &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=zdli6oxpwd”>;计算人级差异隐私下的不同元素>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gjnvvswsweld &quot;>; dices数据集：安全性AI安全性的多样性&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;strong>; &lt;strong>; >;，Alex S. Taylor，&lt;strong>; Mark Diaz &lt;/strong>;，&lt;strong>; Christopher M.Homan &lt;/strong>;，&lt;strong>; Alicia Parrish &lt;/strong>;，Greg Serapio-García，&lt;strong>; vinodkumar prabhakaran &lt; /strong>;，&lt;strong>; ding wang &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ss3ck3yx5z&quot;>;在Imagenet转移到现实世界数据集上确实会进展？&lt;/a>; &lt;br />; Alex Fang，&lt;strong>; Simon Kornblith &lt;/strong>;，Ludwig Schmidt &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=Aaa2uoo0hhmr” >;从2D注释中估算通用3D房间结构&lt;/a>; &lt;br />; &lt;br />; Denys Rozumnyi*，&lt;strong>; Stefan Popov &lt;/strong>;，&lt;strong>; kevis-kokitsi Maninis &lt;/strong>;，MatthiasNießnieous，&lt;strong>; vittorio法拉利&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6hzifay9gd&quot;>;大语言模型作为属性培训数据生成器：多样性和偏见的故事&lt;/a >; &lt;br />; Yue Yu，Yuchen Zhuang，Jieyu Zhang，Yu Meng，Alexander Ratner，Ranjay Krishna，&lt;strong>; Jiaming Shen &lt;/strong>;，&lt;strong>; &lt;/strong>; &lt;/strong>; &lt;/strong>; a href =“ https://openreview.net/forum?id=y45zcxslfx”>; madlad-400：多语言和文档级的大型审核数据集&lt;/a>; &lt;br />; Strong>;，Biao Zhang，Xavier Garcia，Derrick Xin，&lt;strong>; Aditya Kusupati &lt;/strong>;，Romi Stella，Ankur Bapna，Orhan Firat &lt;/p>; &lt;p>; &lt;p>; &lt;a href = &lt;a href =“ https://openreview.net/论坛？ //openreview.net/forum?id=8tmHs2pifg&quot;>; navi：具有高质量3D形状和姿势注释的类别-Agnostic图像集&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;strong>; Kevis-Kokitsi Maninis &lt;/strong>;，&lt;strong>; Andreas Engelhardt &lt;/strong>;，&lt;strong>; arjun karpur &lt;/strong>;，&lt;strong>; karen truong &lt;/strong>;，&lt;strong>; kyle sargent &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt; Strong>; Stefan Popov &lt;/strong>;，&lt;strong>; Andre Araujo &lt;/strong>;，&lt;strong>; Ricardo Martin Brualla &lt;/strong>;，&lt;strong>; kaushal Patel &lt;/strong>;，&lt;strong>; daniel vlasic &lt;/strong>;， &lt;strong>; Vittorio Ferrari &lt;/strong>;，&lt;strong>; Ameesh Makadia &lt;/strong>;，Ce Liu*，&lt;strong>; Yuanzhen li &lt;/strong>;，&lt;strong>; &lt;a href=&quot;https://openreview.net/forum?id=x6cocxrnxg&quot;>;神经理想大型涡流模拟：用神经随机微分方程建模湍流&lt;/a>; &lt;br />; &lt;br />; &lt;br />; ，&lt;strong>; Zhong yi Wan &lt;/strong>;，&lt;strong>; leonardo Zepeda-Nunez &lt;/strong>;，&lt;strong>; James Lottes &lt;/strong>;，&lt;strong>; Qing Wang wang &lt;/strong>;，&lt;strong>; yi-fan陈&lt;/strong>;，&lt;strong>;约翰·罗伯茨·安德森（John Roberts Anderson）&lt;/strong>;，&lt;strong>; fei sha &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&#39;https://openreview.net/forum?id= wfuemocyhz”>;重新开始采样以改善生成过程&lt;/a>; &lt;br />; yilun Xu，Mingyang Deng，Xiang Cheng，&lt;strong>; Yonglong Tian &lt;/strong>;，Ziming Liu，Tommi Jaakkola href =“ https://openreview.net/forum?id=xuybp16q5j”>;推荐系统中的重新思考激励措施：单调奖励总是有益的&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; fan yao，chuanhao li，chuanhao li，karthik abinav sankararararaman，yiming liaiao ，&lt;strong>; Yan Zhu &lt;/strong>;，Qifan Wang，Hongning Wang，Haifeng Xu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jgymuum？ Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union&lt;/a>;&lt;br />; Zifu Wang, &lt;strong>;Maxim Berman&lt;/strong>;, Amal Rannen-Triki, Philip Torr, Devis Tuia, Tinne Tuytelaars, Luc Van Gool, Jiaqian Yu, Matthew B. Blaschko &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0H5fRQcpQ7&quot;>;RoboHive: A Unified Framework for Robot Learning&lt;/a>;&lt;br />; &lt;strong>;Vikash Kumar&lt;/strong>;, Rutav Shah, Gaoyue Zhou, Vincent Moens, Vittorio Caggiano, Abhishek Gupta, &lt;strong>;Aravind Rajeswaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=Vn5qZGxGj3&quot;>;SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data&lt;/a>;&lt;br />; Mélisande Teng, Amna Elmustafa, Benjamin Akera, Yoshua Bengio, Hager Radi, &lt; strong>;Hugo Larochelle&lt;/strong>;, David Rolnick &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sqTcCXkG4P&quot;>;Sparsity-Preserving Differentially Private Training of Large Embedding Models&lt;/ a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, Yangsibo Huang*, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Amer Sinha&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id= xpjsOQtKqx&quot;>;StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners&lt;/a>;&lt;br />; &lt;strong>;Yonglong Tian&lt;/strong>;, &lt;strong>;Lijie Fan&lt;/strong>;, Phillip Isola, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EPz1DcdPVE&quot;>;Towards Federated Foundation Models : Scalable Dataset Pipelines for Group-Structured Learning&lt;/a>;&lt;br />; &lt;strong>;Zachary Charles&lt;/strong>;, &lt;strong>;Nicole Mitchell&lt;/strong>;, &lt;strong>;Krishna Pillutla&lt;/strong>;, &lt;strong>; Michael Reneer&lt;/strong>;, &lt;strong>;Zachary Garrett&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zWxKYyW9ik&quot;>;Universality and Limitations of Prompt Tuning&lt; /a>;&lt;br />; Yihan Wang, Jatin Chauhan, Wei Wang, &lt;strong>;Cho-Jui Hsieh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id =sovxUzPzLN&quot;>;Unsupervised Semantic Correspondence Using Stable Diffusion&lt;/a>;&lt;br />; Eric Hedlin, Gopal Sharma, Shweta Mahajan, &lt;strong>;Hossam Isack&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;, &lt;strong>; Andrea Tagliasacchi&lt;/strong>;,&lt;strong>; &lt;/strong>;Kwang Moo Yi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QEDjXv9OyY&quot;>;YouTube-ASL: A Large -Scale, Open-Domain American Sign Language-English Parallel Corpus&lt;/a>;&lt;br />; &lt;strong>;Dave Uthus&lt;/strong>;, &lt;strong>;Garrett Tanzer&lt;/strong>;, &lt;strong>;Manfred Georg&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=swNtr6vGqg&quot;>;The Noise Level in Linear Regression with Dependent Data&lt;/a>;&lt;br />; Ingvar Ziemann, &lt;strong>; Stephen Tu&lt;/strong>;, George J. Pappas, Nikolai Matni &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple- style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http:/ /blog.research.google/feeds/8223613466421639113/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2023/12/google-at-neurips-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/8223613466421639113&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/8223613466421639113&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-neurips-2023.html &quot; rel=&quot;alternate&quot; title=&quot;Google at NeurIPS 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https: //img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s72-c/google_research_sticker-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>; &lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6514019106482066697&lt;/id>;&lt;published>; 2023-12-08T10:34:00.000-08:00&lt;/published>;&lt;updated>;2023-12-08T14:25:05.067-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger. com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Sparsity-preserving differentially private training&lt;/stitle>;&lt;content type =&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yangsibo Huang, Research Intern, Google Research; Chiyuan Zhang, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s1600/Sparse%20DP-SGD.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;em>;Large embedding models&lt;/em>; have emerged as a fundamental tool for various applications in recommendation systems [&lt;a href=&quot;https://research.google/pubs/pub45530/&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.05158&quot;>;2&lt;/a>;] and natural language processing [&lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;>;3&lt;/a>;, &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;>;4&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;5&lt;/a>;]. Such models enable the integration of non-numerical data into deep learning models by mapping categorical or &lt;a href=&quot;https://en.wikipedia.org/wiki/String_(computer_science)&quot;>;string&lt;/a>;-valued input attributes with large vocabularies to fixed-length representation vectors using embedding layers. These models are widely deployed in personalized recommendation systems and achieve state-of-the-art performance in language tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;>;language modeling&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;sentiment analysis&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;question answering&lt;/a>;. In many such scenarios, privacy is an equally important feature when deploying those models. As a result, various techniques have been proposed to enable private data analysis. Among those, &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>; (DP) is a widely adopted definition that limits exposure of individual user information while still allowing for the analysis of population-level patterns. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; For training deep neural networks with DP guarantees, the most widely used algorithm is &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>; (DP &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>;). One key component of DP-SGD is adding &lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_noise&quot;>;Gaussian noise&lt;/a>; to every coordinate of the gradient vectors during training. However, this creates scalability challenges when applied to &lt;em>;large embedding models&lt;/em>;, because they rely on gradient sparsity for efficient training, but adding noise to all the coordinates destroys sparsity. &lt;/p>; &lt;p>; To mitigate this gradient sparsity problem, in “&lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;Sparsity-Preserving Differentially Private Training of Large Embedding Models&lt;/a>;” (to be presented at &lt;a href=&quot;https://nips.cc/Conferences/2023&quot;>;NeurIPS 2023&lt;/a>;), we propose a new algorithm called &lt;em>;adaptive filtering-enabled sparse training&lt;/em>; (DP-AdaFEST). At a high level, the algorithm maintains the sparsity of the gradient by selecting only a subset of feature rows to which noise is added at each iteration. The key is to make such selections differentially private so that a three-way balance is achieved among the privacy cost, the training efficiency, and the model utility. Our empirical evaluation shows that DP-AdaFEST achieves a substantially sparser gradient, with a reduction in gradient size of over 10&lt;sup>;5&lt;/sup>;X compared to the dense gradient produced by standard DP-SGD, while maintaining comparable levels of accuracy 。 This gradient size reduction could translate into 20X wall-clock time improvement. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overview&lt;/h2>; &lt;p>; To better understand the challenges and our solutions to the gradient sparsity problem, let us start with an overview of how DP-SGD works during training. As illustrated by the figure below, DP-SGD operates by clipping the gradient contribution from each example in the current random subset of samples (called a mini-batch), and adding coordinate-wise &lt;a href=&quot;https://en. wikipedia.org/wiki/Gaussian_noise&quot;>;Gaussian noise&lt;/a>; to the average gradient during each iteration of &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>; （新加坡元）。 DP-SGD has demonstrated its effectiveness in protecting user privacy while maintaining model utility in a variety of applications [&lt;a href=&quot;https://arxiv.org/pdf/2204.13650.pdf&quot;>;6&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2110.06500.pdf&quot;>;7&lt;/a>;]. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s1999/image5.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;An illustration of how DP-SGD works. During each training step, a mini-batch of examples is sampled, and used to compute the per-example gradients. Those gradients are processed through clipping, aggregation and summation of Gaussian noise to produce the final privatized gradients.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The challenges of applying DP-SGD to large embedding models mainly come from 1) the non-numerical feature fields like user/product IDs and categories, and 2) words and tokens that are transformed into dense vectors through an embedding layer. Due to the vocabulary sizes of those features, the process requires large embedding tables with a substantial number of parameters. In contrast to the number of parameters, the gradient updates are usually extremely sparse because each mini-batch of examples only activates a tiny fraction of embedding rows (the figure below visualizes the ratio of zero-valued coordinates, ie, the sparsity, of the gradients under various batch sizes). This sparsity is heavily leveraged for industrial applications that efficiently handle the training of large-scale embeddings. For example, &lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Google Cloud TPUs&lt;/a>;, custom-designed AI accelerators that are optimized for training and inference of large AI models, have &lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/building-large-scale-recommenders-using-cloud-tpus&quot;>;dedicated APIs&lt;/a>; to handle large embeddings with sparse updates. This leads to &lt;a href=&quot;https://eng.snap.com/training-models-with-tpus&quot;>;significantly improved training throughput&lt;/a>; compared to training on GPUs, which at this time did not have specialized optimization for sparse embedding lookups. On the other hand, DP-SGD completely destroys the gradient sparsity because it requires adding independent Gaussian noise to &lt;em>;all&lt;/em>; the coordinates. This creates a road block for private training of large embedding models as the training efficiency would be significantly reduced compared to non-private training. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s1814/image3.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1058&quot; data-original-width=&quot;1814&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Embedding gradient sparsity (the fraction of zero-value gradient coordinates) in the &lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge -dataset&quot;>;Criteo pCTR&lt;/a>; model (see below). The figure reports the gradient sparsity, averaged over 50 update steps, of the top five categorical features (out of a total of 26) with the highest number of buckets, as well as the sparsity of all categorical features. The sprasity decreases with the batch size as more examples hit more rows in the embedding table, creating non-zero gradients. However, the sparsity is above 0.97 even for very large batch sizes. This pattern is consistently observed for all the five features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Algorithm&lt;/h2>; &lt;p>; Our algorithm is built by extending standard DP-SGD with an extra mechanism at each iteration to privately select the “hot features”, which are the features that are activated by multiple training examples in the current mini-batch. As illustrated below, the mechanism works in a few steps: &lt;/p>; &lt;ol>; &lt;li>;Compute how many examples contributed to each feature bucket (we call each of the possible values of a categorical feature a “bucket”). &lt;/li>;&lt;li>;Restrict the total contribution from each example by clipping their counts. &lt;/li>;&lt;li>;Add Gaussian noise to the contribution count of each feature bucket. &lt;/li>;&lt;li>;Select only the features to be included in the gradient update that have a count above a given threshold (a sparsity-controlling parameter), thus maintaining sparsity. This mechanism is differentially private, and the privacy cost can be easily computed by composing it with the standard DP-SGD iterations. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s1600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;743&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the process of the algorithm on a synthetic categorical feature that has 20 buckets. We compute the number of examples contributing to each bucket, adjust the value based on per-example total contributions (including those to other features), add Gaussian noise, and retain only those buckets with a noisy contribution exceeding the threshold for (noisy) gradient update.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Theoretical motivation&lt;/h2>; &lt;p>; We provide the theoretical motivation that underlies DP-AdaFEST by viewing it as optimization using stochastic &lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_complexity_(optimization)&quot;>;gradient oracles&lt;/a>;. Standard analysis of stochastic gradient descent in a theoretical setting decomposes the test error of the model into &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&quot;>;“bias” and “variance” terms&lt;/a>;. The advantage of DP-AdaFEST can be viewed as reducing variance at the cost of slightly increasing the bias. This is because DP-AdaFEST adds noise to a smaller set of coordinates compared to DP-SGD, which adds noise to all the coordinates. On the other hand, DP-AdaFEST introduces some bias to the gradients since the gradient on the embedding features are dropped with some probability. We refer the interested reader to Section 3.4 of the &lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We evaluate the effectiveness of our algorithm with large embedding model applications, on public datasets, including one ad prediction dataset (&lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset&quot;>;Criteo-Kaggle&lt;/a>;) and one language understanding dataset (&lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;SST-2&lt;/a>;). We use &lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;DP-SGD with exponential selection&lt;/a>; as a baseline comparison. &lt;/p>; &lt;p>; The effectiveness of DP-AdaFEST is evident in the figure below, where it achieves significantly higher gradient size reduction (ie, gradient sparsity) than the baseline while maintaining the same level of utility (ie, only minimal performance降解）。 &lt;/p>; &lt;p>; Specifically, on the Criteo-Kaggle dataset, DP-AdaFEST reduces the gradient computation cost of regular DP-SGD by more than 5x10&lt;sup>;5&lt;/sup>; times while maintaining a comparable &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;>;AUC&lt;/a>; (which we define as a loss of less than 0.005). This reduction translates into a more efficient and cost-effective training process. In comparison, as shown by the green line below, the baseline method is not able to achieve reasonable cost reduction within such a small utility loss threshold. &lt;/p>; &lt;p>; In language tasks, there isn&#39;t as much potential for reducing the size of gradients, because the vocabulary used is often smaller and already quite compact (shown on the right below). However, the adoption of sparsity-preserving DP-SGD effectively obviates the dense gradient computation. Furthermore, in line with the bias-variance trade-off presented in the theoretical analysis, we note that DP-AdaFEST occasionally exhibits superior utility compared to DP-SGD when the reduction in gradient size is minimal. Conversely, when incorporating sparsity, the baseline algorithm faces challenges in maintaining utility. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s1860/image6.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1860&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A comparison of the best gradient size reduction (the ratio of the non-zero gradient value counts between regular DP-SGD and sparsity-preserving algorithms) achieved under ε =1.0 by DP -AdaFEST (our algorithm) and the baseline algorithm (&lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;DP-SGD with exponential selection&lt;/a>;) compared to DP-SGD at different thresholds for utility不同之处。 A higher curve indicates a better utility/efficiency trade-off.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In practice, most ad prediction models are being continuously trained and evaluated. To simulate this online learning setup, we also evaluate with time-series data, which are notoriously challenging due to being non-stationary. Our evaluation uses the &lt;a href=&quot;https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/&quot;>;Criteo-1TB&lt;/a>; dataset, which comprises real-world user-click data collected over 24 days. Consistently, DP-AdaFEST reduces the gradient computation cost of regular DP-SGD by more than 10&lt;sup>;4&lt;/sup>; times while maintaining a comparable AUC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s2500/image7 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;891&quot; data-original-width=&quot;2500&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A comparison of the best gradient size reduction achieved under ε =1.0 by DP-AdaFEST (our algorithm) and DP-SGD with exponential selection (a previous algorithm) compared to DP-SGD at different thresholds for utility difference. A higher curve indicates a better utility/efficiency trade-off. DP-AdaFEST consistently outperforms the previous method.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present a new algorithm, DP-AdaFEST, for preserving gradient sparsity in differentially private training — particularly in applications involving large embedding models, a fundamental tool for various applications in recommendation systems and natural language processing. Our algorithm achieves significant reductions in gradient size while maintaining accuracy on real-world benchmark datasets. Moreover, it offers flexible options for balancing utility and efficiency via sparsity-controlling parameters, while our proposals offer much better privacy-utility loss. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was a collaboration with Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi and Amer Sinha.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6514019106482066697/comments/default&quot; rel=&quot;replies &quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html#comment-form&quot; rel =&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;edit&quot; type =&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html&quot; rel=&quot;alternate&quot; title=&quot;Sparsity-preserving differentially private training&quot; type=&quot;text/html&quot; />;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot; >;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s72 -c/Sparse%20DP-SGD.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr :total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5912569868305742102&lt;/id>;&lt;published>;2023-12-07T09:51:00.000-08:00&lt;/ published>;&lt;updated>;2023-12-07T09:53:23.920-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt; /category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ ns#&quot; term=&quot;Virtual Reality&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VALID: A perceptually validated virtual avatar library for inclusion and diversity&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class= &quot;byline-author&quot;>;Posted by Mar Gonzalez-Franco, Research Scientist, Google AR &amp;amp; VR&lt;/span>;&lt;p>; 随着虚拟现实 (VR) 和增强现实 (AR) 技术的不断普及，虚拟化身正成为我们数字交互中越来越重要的一部分。特别是，虚拟化身是许多社交 VR 和 AR 交互的中心，因为它们是代表远程参与者和促进协作的关键。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在过去的十年中，跨学科科学家付出了大量的努力来更好地理解化身的使用，并做出了许多有趣的观察，包括用户&lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frvir.2020.575943/full&quot;>;体现他们的化身&lt;/a>;的能力（即，认为化身身体是他们自己的错觉） ）和&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9089510&quot;>;自我头像追随者效果&lt;/a>;，它在头像和用户的动作之间创建了足够强的绑定头像实际上可以影响用户的行为。 &lt;/p>; &lt;p>; 在实验中使用化身不仅是为了研究用户在 VR 空间中的互动和行为方式，也是为了发现人类感知和神经科学的局限性。事实上，一些 VR 社交实验通常依赖于重新创建在现实世界中无法轻松重现的场景，例如酒吧爬行到 &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0052766&quot; >;探索内群体与外群体效应&lt;/a>;，或欺骗实验，例如&lt;a href=&quot;https://doi.org/10.1371/journal.pone.0209704&quot;>;米尔格拉姆对虚拟现实中权威的服从&lt;/一个>;。其他研究试图探索深层神经科学现象，例如&lt;a href=&quot;https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2021.0453&quot;>;人类运动控制机制&lt;/a>;。这或许遵循大脑可塑性的&lt;a href=&quot;https://www.nature.com/articles/35784&quot;>;橡胶手错觉&lt;/a>;，一个人可以开始感觉自己拥有一块橡胶手，而他们真正的手隐藏在窗帘后面。使用个性化&lt;a href=&quot;https://doi.org/10.1186/s13063-022-06683-1&quot;>;化身&lt;/a>;进行精神治疗的可能疗法也越来越多。在这些情况下，VR 成为一种&lt;a href=&quot;https://en.wikipedia.org/wiki/Ecological_validity&quot;>;生态上有效的&lt;/a>;工具，使科学家能够探索或治疗人类行为和感知。 &lt;/p>; &lt;p>; 如果没有能够方便地进行实验的研究工具和库，这些实验和疗法都不可能存在。因此，近年来围绕头像创建和动画发布了多个系统和开源工具。然而，现有的头像库尚未在多样性谱上进行系统验证。与化身互动时，社会&lt;a href=&quot;https://doi.org/10.1016/j.concog.2013.04.016&quot;>;偏见和动态&lt;/a>;也会转移到 VR/AR，这可能会导致得出不完整的结论VR/AR 中人类行为的研究。 &lt;/p>; &lt;p>; 为了部分解决这个问题，我们与中佛罗里达大学合作创建并发布了开源&lt;a href=&quot;https://github.com/google/valid-avatar-library&quot; >;Virtual Avatar Library for Inclusion and Diversity&lt;/a>; (VALID). &lt;a href=&quot;https://doi.org/10.3389/frvir.2023.1248915&quot;>;我们最近的论文&lt;/a>;中进行了描述，发表于&lt;a href=&quot;https://www.frontiersin.org/journals/virtual -reality&quot;>;&lt;i>;虚拟现实前沿&lt;/i>;&lt;/a>;，该化身库可随时用于 VR/AR 实验，其中包括美国人口普查局认可的 7 个不同种族和民族的 210 个化身。这些化身经过感知验证，旨在促进虚拟化身研究的多样性和包容性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSB V0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s1717/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1413&quot; data-original-width=&quot;1717&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJk E8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;VALID 库中提供的所有 42 个基本头像的头像都是在与来自 &lt;a href=&quot;https://www 的 7 个民族和种族群体的成员的广泛互动中创建的.federalregister.gov/documents/2023/01/27/2023-01635/initial-proposals-for-updating-ombs-race-and-ethnicity-statistical-standards&quot;>;联邦登记册&lt;/a>;，其中包括（AIAN、亚洲人、黑人、西班牙裔、中东和北非地区、NHPI 和白人）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;库的创建和验证&lt;/h2>; &lt;p>; 我们的多样化头像库的种族和民族的初步选择遵循&lt;a href=&quot;https://www.npr.org/2023/01/26/1151608403/mena-race-categories-us-census的最新指南-middle-eastern-latino-hispanic&quot;>;美国人口普查局&lt;/a>;建议，截至 2023 年，使用代表美国社会大部分人口的 7 个民族和种族群体，这也可以推断到全球人口。这些群体包括西班牙裔或拉丁裔、美洲印第安人或阿拉斯加原住民 (AIAN)、亚洲人、黑人或非裔美国人、夏威夷原住民或其他太平洋岛民 (NHPI)、&lt;/em>; &lt;em>;白人、中东或北非&lt;/em>; (MENA)。我们预计图书馆将继续发展，通过未来添加的头像带来更多的多样性和代表性。 &lt;/p>; &lt;p>; 化身是手工建模和创建的，使用的过程将平均面部特征与每个种族群体的代表利益相关者的广泛合作相结合，他们的反馈用于艺术地修改化身的面部网格。然后我们对来自 33 个国家的参与者进行了一项在线研究，以确定图书馆中每个头像的种族和性别是否可识别。除了头像之外，我们还提供通过观察用户对所有 42 个基本头像的种族和性别进行统计验证的标签（见下文）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVd MSOG8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V /s1999/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;736&quot; data-original-width=&quot;1999&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanv DqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V/s16000/image3.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在库验证期间向参与者展示的黑人/非裔美国人头像的头像示例。&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 我们发现所有亚洲人、黑人和白人化身都被所有参与者普遍认为是他们的模仿种族，而我们的美洲印第安人或阿拉斯加原住民（AIAN） ）、西班牙裔、中东或北非 (MENA) 化身通常只能由同一种族的参与者识别。这也表明，参与者种族可以提高对同一种族的虚拟角色的识别度。图书馆发布的论文强调了在研究 VR 中的化身行为时如何考虑这种群体内的熟悉度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfL ljkA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s1999/image1.jpg&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1509&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1M vkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s16000/image1.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;由其他种族参与者和同种族参与者分开的 42 个基本化身的同意率的混淆矩阵热图。此矩阵中可见的一个有趣的方面是，参与者在识别自己种族的化身方面明显优于其他种族。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;数据集详细信息&lt;/h2>; &lt;p>; 我们的模型采用 &lt;a href=&quot;https://www.autodesk.com/products/fbx/overview&quot;>;FBX 格式&lt;/a>;，与以前的头像库兼容，例如常用的&lt;a href=&quot;https://github.com/microsoft/Microsoft-Rocketbox&quot;>;Rocketbox&lt;/a>;，并且可以轻松集成到大多数游戏引擎中，例如&lt;a href=&quot;https://unity. com/&quot;>;Unity&lt;/a>; 和 &lt;a href=&quot;https://www.unrealengine.com/&quot;>;Unreal&lt;/a>;。此外，头像还配有 69 块骨骼和 65 种面部混合形状，使研究人员和开发人员能够轻松创建和应用动态面部表情和动画。这些头像被故意制作成部分卡通化，以避免极端相似的场景，在这种情况下，一个人可能被模仿，但仍然具有足够的代表性，能够进行可靠的用户研究和社会实验。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteT gNWC12kdwkvcDLaUSuq1lod1RUh67-lu0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s1999/image4.jpg &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;986&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh 67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s16000/image4.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;有效头像中包含的骨架索具（允许动画的骨骼）和一些面部混合形状的图像。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;br />; &lt;p>; 头像可以进一步组合休闲装和五种职业装，包括医疗、军事、工人和商务。这是对先前库的有意改进，在某些情况下，在化身服装中再现了陈规定型的性别和种族偏见，并为某些专业化身提供了非常有限的多样性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6 t7PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s1537/image5.jpg&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1023&quot; data-original-width=&quot;1537&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNH Wdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;有效头像中包含一些示例服装的图像。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;开始使用 VALID&lt;/h2 >; &lt;p>; 我们相信，&lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;促进包容性和多样性的虚拟头像库&lt;/a>;（VALID）将成为宝贵的资源致力于 VR/AR 应用的研究人员和开发人员。我们希望它将有助于创造更加包容和公平的虚拟体验。为此，我们邀请您探索我们在开源 &lt;a href=&quot;https://en.wikipedia.org/wiki/MIT_License&quot;>;MIT 许可证&lt;/a>;下发布的头像库。您可以免费下载头像并在各种设置中使用它们。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这个化身库是与来自纽约大学的 Tiffany D. Do、Steve Zelenty 和 Ryan P McMahan 教授合作诞生的。佛罗里达州中部。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5912569868305742102/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html#comment-form&quot; rel=&quot;replies&quot; title= “0 条评论” type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;edit&quot; type=&quot;application/atom+ xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html&quot; rel=&quot;alternate&quot; title=&quot;VALID：一个经过感知验证的虚拟化身库，具有包容性和多样性&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84 i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s72 -c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8414954450937764241&lt;/id>;&lt;已发布>;2023-12-05T17:32:00.000-08:00&lt;/已发布>;&lt;已更新>;2023-12-06T19:51:14.038-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= &quot;EMNLP&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 在 EMNLP 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：项目经理 Malaya Jules，谷歌 &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80Z VGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s320/EMNLP%202023.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google is proud to be a &lt;a href=&quot;https://2023.emnlp.org/sponsors/&quot;>;Diamond Sponsor&lt;/a>; of &lt;a href=&quot;https://2023.emnlp.org/&quot;>;Empirical Methods in Natural Language Processing&lt;/a>; (EMNLP 2023), a premier annual conference, which is being held this week in Sentosa, Singapore. Google has a strong presence at this year&#39;s conference with over 65 accepted papers and active involvement in 11 workshops and tutorials. Google is also happy to be a &lt;a href=&quot;https://www.winlp.org/winlp-2023-workshop/winlp-2023-sponsors/&quot;>;Major Sponsor&lt;/a>; for the &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/winlp-2023-workshop/&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1701403043253017&amp;amp;usg=AOvVaw2oPPnFe0AEcdP1iSblNV91&quot;>;Widening NLP&lt;/a>; workshop (WiNLP), which aims to highlight global representations of people, perspectives, and cultures in AI and ML. We look forward to sharing some of our extensive NLP research and expanding our partnership with the broader research community. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 我们希望您能够参观 Google 展位，与积极追求 NLP 最新创新的研究人员交谈，并查看一些预定的展位活动（例如，下面列出的演示和问答环节）。 Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) and &lt;a href=&quot;https://www.linkedin.com/showcase/googleresearch/?viewAsMember=true&quot;>;LinkedIn&lt;/a>; accounts to find out more about the Google booth activities at EMNLP 2023. &lt;/p>; &lt;p>; Take a look below to learn more about the Google research being presented at EMNLP 2023 (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;板和线高度Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Sponsorship Chair: &lt;strong>;&lt;em>;Shyam Upadyay&lt;/em>;&lt;/strong>; &lt;br />; Industry Track Chair: &lt; strong>;&lt;em>;Imed Zitouni&lt;/em>;&lt;/strong>; &lt;br />; Senior Program Committee: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Annie Louis&lt;/ strong>;&lt;/em>;, &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brian Roark&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;p>; &lt;i>;This schedule is subject to change. Please visit the Google booth for more information.&lt;/i>;&lt;/p>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Developing and Utilizing Evaluation Metrics for Machine Translation &amp;amp;改进多语言 NLP &lt;br />; 演讲者：&lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dan Deutch&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jan -Thorsten Peter&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;David Vilar Torres&lt;/em>;&lt;/strong>; &lt;br />; 12 月 8 日星期五 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Differentiable Search Indexes &amp;amp;生成检索&lt;br />;演讲者：&lt;strong>;&lt;em>;Sanket Vaibhav Mehta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vinh Tran&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; Kai Hui&lt;/em>;&lt;/strong>;、&lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;br />; 12 月 8 日星期五 | 3:30PM -4:00PM SST &lt;/p>; &lt;p>; Retrieval and Generation in a single pass &lt;br />; Presenter: &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Amplifying Adversarial Attacks &lt;br />; Presenter:&lt;strong>; &lt;em>;Anu Sinha&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 12:30PM -1:45PM SST &lt;/p>; &lt;p>; 自动提示设计：通用自适应提示（请参阅&lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive -prompting-of-large.html&quot;>;blog post&lt;/a>;) &lt;br />; Presenter: &lt;strong>;&lt;em>;Xingchen Qian&lt;sup>;*&lt;/sup>;&lt;/em>;&lt;/strong>;, &lt;strong >;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 3:30PM -4:00PM SST &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;已接受论文&lt;/h2>; &lt;div style =&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03291.pdf&quot;>;SynJax：JAX 的结构化概率分布&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;米洛什·斯塔诺耶维奇&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;洛朗·萨特兰&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2311.11077.pdf&quot;>;适配器：用于参数高效和模块化迁移学习的统一库&lt;/a>; &lt;br />; &lt;em>;Clifton Poth&lt;/em>;、&lt;em>;Hannah Sterz&lt;/em>;、&lt; em>;Indraneil Paul&lt;/em>;、&lt;em>;Sukannya Purkayastha&lt;/em>;、&lt;em>;Leon Engländer&lt;/em>;、&lt;em>;Timo Imhof&lt;/em>;、&lt;em>;Ivan Vulić&lt;/em>;、&lt;强>;&lt;em>;塞巴斯蒂安·鲁德&lt;/em>;&lt;/strong>;、&lt;em>;伊琳娜·古列维奇&lt;/em>;、&lt;strong>;&lt;em>;乔纳斯·菲佛&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://arxiv.org/pdf/2306.08937.pdf&quot;>;DocumentNet：弥合文档预训练中的数据差距&lt;/a>; &lt;br />; &lt;em>;Lijun Yu&lt;/em>;，&lt;strong >;&lt;em>;苗金&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;孙晓宇&lt;/em>;&lt;/strong>;、&lt;em>;陈嘉仪&lt;/em>;、&lt;em>;Alexander Hauptmann&lt;/em>; >;, &lt;strong>;&lt;em>;戴汉俊&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;魏巍&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2311.08592.pdf&quot;>;AART：人工智能辅助红队为新的法学硕士支持的应用程序提供多样化的数据生成&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Bhaktipriya Radharapu&lt;/em>; &lt;/strong>;、&lt;strong>; &lt;em>;凯文·罗宾逊&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Preethi Lahoti&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.15239.pdf&quot;>;CRoW：现实世界任务中常识推理的基准测试&lt;/a>; &lt;br />; &lt;em>;Mete Ismayilzada&lt;/em>;、&lt;em>;Debjit Paul&lt;/em>;、&lt;em>;Syrielle Montariol&lt;/em>;、&lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;、&lt;em>; Antoine Bosselut&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11610.pdf&quot;>;大型语言模型可以自我改进&lt;/a>; &lt;br />; &lt;em >;黄嘉欣&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;顾世祥&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;侯乐&lt;/em>;&lt;/strong>; ,&lt;strong>; &lt;em>;吴跃新&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;王学智&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;于鸿坤&lt;/em>;&lt;/strong>; >;, &lt;em>;Jiawei Han&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14767.pdf&quot;>;剖析自回归语言模型中事实关联的回忆&lt; /a>; &lt;br />; &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Katja Filippova&lt; /em>;&lt;/strong>;，&lt;strong>;&lt;em>;阿米尔·格洛伯森&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10160.pdf&quot; >;停止以纯文本形式上传测试数据：通过评估基准减轻数据污染的实用策略&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/强>;，&lt;em>;奥马尔·戈德曼&lt;/em>;，&lt;em>;约夫·戈德堡&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.16391.pdf&quot;>;选择性标签：如何从根本上降低文档提取模型的数据标签成本&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;周一超&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;James Bradley Wendt&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;Navneet Potti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;谢静&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sandeep Tata&lt; /em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2112.12870.pdf&quot;>;测量自然语言生成模型中的归因&lt;/a>; &lt;br />; &lt;强>;&lt;em>;汉娜·拉什金&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;维塔利·尼古拉耶夫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马修·拉姆&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迈克尔·柯林斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dipanjan&lt;/em>; &lt;em>;Das&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;斯拉夫·彼得罗夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;高拉夫·辛格·托马尔&lt;/em>;&lt;/strong>;、&lt;em>; &lt;strong>;尤利亚图尔克&lt;/strong>;&lt;/em>;，&lt;strong>;&lt;em>;大卫·雷特&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.02011。 pdf&quot;>;逆缩放可以变成 U 形&lt;/a>; &lt;br />; &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/ strong>;、&lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;Quoc Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://arxiv.org/pdf/2305.14282.pdf&quot;>;INSTRUCTSCORE：通过自动反馈进行可解释文本生成评估&lt;/a>; &lt;br />; &lt;em>;Wenda Xu&lt;/em>;，&lt;em>;Danqing Wang&lt;/em>; >;、&lt;em>;潘亮明&lt;/em>;、&lt;em>;宋振桥&lt;/em>;、&lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;、&lt;em>;王威廉杨&lt;/em>;、 &lt;em>;李雷&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.14796.pdf&quot;>;论会话问答中对话历史表征的稳健性：综合研究和新的基于提示的方法&lt;/a>; &lt;br />; &lt;em>;Zorik Gekhman&lt;/em>;，&lt;em>;Nadav Oved&lt;/em>;，&lt;strong>; &lt;em>;Orgad Keller&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;伊丹·斯佩克托&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Roi Reicart&lt;/em>;&lt;/strong>;&lt;/p>;&lt;p>;&lt;a href=&quot;https: //arxiv.org/pdf/2208.04347.pdf&quot;>;研究有效扩展 Transformer 以实现长输入汇总&lt;/a>; &lt;br />; &lt;em>;Jason Phang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong >;&lt;em>;赵耀&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Peter J Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/2212.09744.pdf&quot;>;DSI++：使用新文档更新变压器内存&lt;/a>; &lt;br />; &lt;em>;Sanket Vaibhav Mehta&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>;&lt;em>; Jai Gupta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; >;Vinh Q. Tran&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;饶金峰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Marc Najork&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;艾玛·斯特鲁贝尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;唐纳德·梅茨勒&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2305.12029.pdf&quot;>;MultiTurnCleanup：多轮口语会话记录清理基准&lt;/a>; &lt;br />; &lt;em>;沉华&lt;sup>;*&lt;/sup>;&lt;/em>;，&lt;strong>;&lt; em>;Vicky Zayats&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;约翰 C 罗霍尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丹尼尔·大卫·沃克&lt;/em>;&lt;/strong>;、&lt;强>; &lt;em>;德克·帕德菲尔德&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14318.pdf&quot;>;q2d：将问题转化为对话进行教学模特如何搜索&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shlomi Cohen-Ganor&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>; Ido Hakimi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yoad Lewenberg&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt; strong>; &lt;em>;Enav Weinreb&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02171.pdf&quot;>;具体序列中抽象状态表示的出现造型&lt;/a>; &lt;br />; &lt;em>;田云&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>;曾子来&lt;/em>;、&lt;em>;Kunal Handa&lt;/em>;、&lt;strong>; &lt;em>;Ashish V Thapliyal&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;庞波&lt;/em>;&lt;/strong>;、&lt;em>;Ellie Pavlick&lt;/em>;、&lt;em>;陈孙&lt;/em>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14332.pdf&quot;>;跨语言问答的归因评估和建模&lt;/a>; &lt;br />; &lt;em>;Benjamin Muller&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安·鲁德&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;利维奥·巴尔迪尼·苏亚雷斯&lt;/em>;&lt;/strong>; >;、&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;王欣怡&lt;/em>;&lt;/强>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2305.14281.pdf&amp;sa=D&amp;amp;source=docs&amp;amp; ust=1701762357021319&amp;amp;usg=AOvVaw2Q_4he8TIMmr8URRV1w4uX&quot;>;多模态预训练中视觉关系的弱监督学习&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Emanuele Bugliarello&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Aida Nematzadeh&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;丽莎·安妮·亨德里克斯&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/ pdf/2305.13286.pdf&quot;>;语言如何相互影响？ Studying Cross-Lingual Data Sharing During LM Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni&lt;/em>;, &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>;, &lt;em>;Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14214.pdf&quot;>;CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models&lt;/a>; &lt;br>; &lt; em>;Benjamin Minixhofer&lt;/em>;, &lt;strong>;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>;, &lt;em>;Ivan Vulić&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2302.01328.pdf&quot;>;IC3: Image Captioning by Committee Consensus&lt;/a>; &lt;br />; &lt;em>;David Chan&lt;/em>;, &lt;strong>;&lt;em>;Austin Myers&lt;/em>;&lt; /strong>;,&lt;strong>;&lt;em>;Sudheendra Vijayanarasimhan&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;David A Ross&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;John Canny&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11877.pdf&quot;>;The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over -Confident Large Language Models&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;, &lt;em>;Omer Goldman&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Ido Dagan&lt;/em>;, &lt;em>;Shauli Ravfogel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14542.pdf&quot;>;Evaluating Large Language Models on Controlled Generation Tasks&lt;/a>; &lt;br />; &lt;em>;Jiao Sun&lt;/em>;, &lt;em>;Yufei Tian&lt;/em>;, &lt;em>;Wangchunshu Zhou&lt;/em>;, &lt;em>;Nan Xu&lt;/em >;, &lt;em>;Qian Hu&lt;/em>;, &lt;em>;Rahul Gupta&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;em>;Nanyun Peng&lt;/em>;, &lt; em>;Xuezhe Ma&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14324.pdf&quot;>;Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration&lt; /a>; &lt;br />; &lt;strong>;&lt;em>;Daniel Deutsch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;George Foster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Markus Freitag&lt; /em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11399.pdf&quot;>;以 0.1% 的额外计算超越缩放定律&lt;/a>; &lt;br />; &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; Hyung Won Chung&lt;sup>;*&lt;/sup>; &lt;/em>;, &lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;, &lt;em>;David R. So&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>; Siamak Shakeri&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Huaixiu Steven Zheng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Jinfeng Rao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Donald&lt;/em>; &lt;em>;Metzler&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Slav Petrov,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Neil Houlsby&lt;/em>; &lt;/strong>;, &lt;strong>;&lt;em>;Quoc V. Le&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2311.09006.pdf&quot;>;Data Similarity is Not Enough to Explain Language Model Performance&lt;/a>; &lt;br />; &lt;em>;Gregory Yauney&lt;sup>;*&lt;/sup>;&lt;/ em>;, &lt;strong>;&lt;em>;Emily Reif&lt;/em>;&lt;/strong>;, &lt;em>;David Mimno&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf /2311.00913.pdf&quot;>;Self-Influence Guided Data Reweighting for Language Model Pre-training&lt;/a>; &lt;br />; &lt;em>;Megh Thakkar&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>; Tolga Bolukbasi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sriram Ganapathy&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shikhar Vashishth&lt;/em>;&lt;/strong>;、&lt;em>; Sarath Chandar &lt;/em>;, &lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11826.pdf&quot;>;ReTAG: Reasoning Aware Table to Analytic Text Generation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Deepanway Ghosal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Preksha Nema&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;Aravindan Raghuveer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15265v1.pdf&quot;>;GATITOS: Using a New Multilingual Lexicon for Low-Resource Machine Translation&lt;/a>; &lt;br />; &lt;em>;Alex Jones&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;Ishank Saxena&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.20201v1.pdf&quot;>;Video-Helpful Multimodal Machine Translation&lt; /a>; &lt;br />; &lt;em>;Yihang Li, Shuichiro Shimizu&lt;/em>;, &lt;em>;Chenhui Chu&lt;/em>;, &lt;em>;Sadao Kurohashi&lt;/em>;, &lt;strong>;&lt;em>;Wei Li&lt;/ em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.08298.pdf&quot;>;Symbol Tuning Improves In-Context Learning in Language Models&lt;/a>; &lt;br / >; &lt;em>;Jerry Wei&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Andrew Kyle Lampinen&lt;/strong>;&lt; /em>;, &lt;em>;Xiangning Chen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Da Huang&lt;/em>;&lt;/strong>;, &lt;em>;Yi Tay&lt;sup>;*&lt;/ super>;&lt;/em>;、&lt;strong>;&lt;em>;新云&lt;/em>; &lt;em>;陈&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;卢一峰&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;Tengyu Ma&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Quoc V Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14755.pdf&quot;>;&quot;Don&#39;t Take This Out of Context!&quot; On the Need for Contextual Models and Evaluations for Stylistic Rewriting&lt;/a>; &lt;br />; &lt;em>;Akhila Yerukola&lt;/em>;, &lt;em>;Xuhui Zhou&lt;/em>;, &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em >;&lt;/strong>;, &lt;em>;Maarten Sap&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08264.pdf&quot;>;QAmeleon: Multilingual QA with Only 5 Examples &lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Priyanka Agrawal&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;克里斯·阿尔贝蒂&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Fantine Huot &lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Joshua Maynez&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ji Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kuzman Ganchev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Mirella Lapata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03540.pdf&quot;>;Speak, Read and Prompt: High-Fidelity Text-to-在最低限度监督下的演讲&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;尤金·哈里托诺夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;达米安·文森特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Zalán Borsos&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Raphaël Marinier&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sertan Girgin&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马特·沙里菲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Marco Tagliasacchi&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;Neil Zeghidour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09939.pdf&quot;>;AnyTOD：面向任务的可编程对话系统&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;赵杰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;曹源&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;拉加夫·古普塔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Harrison Lee&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;明秋王&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;哈根索尔陶&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;伊扎克·沙夫兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;吴永辉&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14613.pdf&quot;>;有选择地回答模棱两可的问题&lt;/a>; &lt;br />; &lt;强>;&lt;em>;杰里米·R.科尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;张俊泉&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;丹尼尔·吉利克&lt;/em>;&lt;/ &lt;strong>;、&lt;strong>;&lt;em>;朱利安·马丁·艾森施洛斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Bhuwan Dhingra&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;雅各布·爱森斯坦&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08954.pdf&quot;>;PRESTO：用于解析现实的面向任务的对话框的多语言数据集&lt;/a>;（参见&lt; a href=&quot;https://blog.research.google/2023/03/presto-multilingual-dataset-for-parsing.html&quot;>;博客文章&lt;/a>;）&lt;br />; &lt;strong>;&lt;em>;Rahul Goel &lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;瓦利德·阿马尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿迪亚·古普塔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Siddharth Vashishtha&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;佐野元树&lt;/em>;&lt;/strong>;、&lt;em>; Faiz Surani&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em >;Max Chang&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;HyunJeong Choe&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;大卫·格林&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Chuan He&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rattima Nitisaroj&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Anna&lt;/em>; &lt;em>;Trukhina&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shachi Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pararth Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rushin Shah&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;周瑜&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13281.pdf&quot;>;LM vs LM：通过交叉检查检测事实错误&lt;/a>; &lt;br />; &lt;em>;Roi Cohen&lt;/em>;、&lt;em>;May Hamri&lt;/em>;、&lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.03668.pdf&quot;>;生成套件多级多模式网页理解任务&lt;/a>; &lt;br />; &lt;em>;Andrea Burns&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Krishna Srinivasan&lt;/em>;&lt;/strong>; ,&lt;strong>;&lt;em>;约书亚·安斯利&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杰夫·布朗&lt;/em>;&lt;/strong>;、&lt;em>;布莱恩 A. 普卢默&lt;/em>;、&lt;em>; Kate&lt;/em>; &lt;em>;Saenko&lt;/em>;、&lt;strong>;&lt;em>;倪建模&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;郭曼迪&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2302.08956.pdf&amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701763216883702&amp;amp;usg =AOvVaw1QisabqC-Gy-U4ou1jbHAY&quot;>;AfriSenti：非洲语言的 Twitter 情绪分析基准&lt;/a>; &lt;br />; &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;、&lt;em>;Idris Abdulmumin&lt;/em>;、&lt;em>; Abinew Ali Ayele&lt;/em>;、&lt;em>;Nedjma Ousidhoum&lt;/em>;、&lt;em>;David Ifeoluwa Adelani&lt;/em>;、&lt;em>;Seid Muhie Yimam&lt;/em>;、&lt;em>;Ibrahim Said Ahmad&lt;/em>; , &lt;em>;Meriem Beloucif&lt;/em>;, &lt;em>;Saif M.&lt;/em>; &lt;em>;Mohammad&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>; Oumaima Hourrane&lt;/em>;、&lt;em>;Alipio Jorge&lt;/em>;、&lt;em>;Pavel Brazdil&lt;/em>;、&lt;em>;Felermino D. M&lt;/em>;。 &lt;em>;A. Ali&lt;/em>;、&lt;em>;Davis David&lt;/em>;、&lt;em>;Salomey Osei&lt;/em>;、&lt;em>;Bello Shehu-Bello&lt;/em>;、&lt;em>;Falalu Ibrahim Lawan&lt;/em>;、&lt; em>;Tajuddeen&lt;/em>; &lt;em>;Gwadabe&lt;/em>;、&lt;em>;Samuel Rutunda&lt;/em>;、&lt;em>;Tadesse Destaw Belay&lt;/em>;、&lt;em>;Wendimu Baye Messelle&lt;/em>;、&lt;em>; >; Hailu Beshada&lt;/em>; &lt;em>;Balcha&lt;/em>;、&lt;em>;Sisay Adugna Chala&lt;/em>;、&lt;em>;Hagos Tesfahun Gebrmichael&lt;/em>;、&lt;em>; Bernard Opoku&lt;/em>;、&lt;em>; >;Stephen Arthur&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13682.pdf&quot;>;通过令牌消除优化检索增强阅读器模型&lt;/a>; &lt;br / >; &lt;em>;Moshe Berchansky&lt;/em>;、&lt;em>;Peter Izsak&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、&lt;em>;Ido Dagan&lt;/em>;、&lt;em>; >;Moshe Wasserblat&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13194.pdf&quot;>;SEAHORSE：用于摘要评估的多语言、多方面数据集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;伊丽莎白·克拉克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安·格尔曼&lt;/em>;&lt;/ &lt;strong>;、&lt;strong>; &lt;em>;约书亚·梅内斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伊·阿哈罗尼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;维塔利·尼古拉耶夫&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Thibault Sellam&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Aditya Siddhan&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Dipanjan Das&lt;/em>; &lt;/strong>;，&lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13245.pdf&quot;>;GQA ：从多头检查点训练广义多查询变压器模型&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;James Lee-Thorp&lt;/a>; &lt;em>; em>;&lt;/strong>;、&lt;em>;米契尔·德容&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;尤里·泽姆良斯基&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;费德里科勒布朗&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;苏米特桑海&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09752。 pdf&quot;>;CoLT5：具有条件计算的更快的远程变压器&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;陶雷&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;米歇尔·德容&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;圣地亚哥·翁塔农&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;悉达多·梵天&lt;/em>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;尤里·泽姆良斯基&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;David Uthus&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;郭曼迪&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;詹姆斯·李-索普&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;宋云轩&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf /2310.16523.pdf&quot;>;通过集体批评和自我投票提高大型语言模型中人口表征的多样性&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>;,&lt;strong >; &lt;em>;尼古拉斯·布鲁姆&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;小马&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Raghavendra Kotikalapudi&lt;/em>;&lt;/strong>;、&lt;强>;&lt;em>;Sahitya Potluri&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Qijun Tan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Hansa Srinivasan&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;本·帕克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;艾哈迈德·贝拉米&lt;/em>;&lt;/strong>;、&lt;em>;亚历克斯·博伊特尔&lt;/em>;、&lt;strong>;&lt;em>; Jilin Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14926.pdf&quot;>;通用自适应提示&lt;/a>;（参见&lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;博客文章&lt;/a>;) &lt;br />; &lt;em>;万星辰&lt;sup >;*&lt;/sup>;&lt;/em>;、&lt;em>; &lt;strong>;孙若曦、Hootan Nakhost&lt;/strong>;&lt;/em>;、&lt;strong>;&lt;em>;戴汉俊&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;朱利安·马丁·艾森施洛斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sercan O. Arik&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;托马斯·普菲斯特&lt;/em>;&lt;/strong>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11171.pdf&quot;>;TrueTeacher：使用大型语言模型学习事实一致性评估&lt;/a>; &lt;br />; &lt;strong>;&lt; em>;佐里克·格赫曼&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·赫齐格&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伊·阿哈罗尼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Chen Elkind&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/ pdf/2310.07871.pdf&quot;>;多模态电子健康记录分层预训练&lt;/a>; &lt;br />; &lt;em>;Xiaochen Wang&lt;/em>;，&lt;em>;Junyu Luo&lt;/em>;，&lt;em>;Jiaqi Wang&lt; /em>;、&lt;em>;尹子仪&lt;/em>;、&lt;em>;崔素涵&lt;/em>;、&lt;em>;袁忠&lt;/em>;、&lt;strong>;&lt;em>;王亚庆&lt;/em>;&lt;/strong>; , &lt;em>;马凤龙&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14499.pdf&quot;>;NAIL：具有高效非自回归解码器的词汇检索索引&lt;/ a>; &lt;br />; &lt;strong>;&lt;em>;利维奥·巴尔迪尼·苏亚雷斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;丹尼尔·吉利克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杰里米·R.科尔&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;汤姆·科维亚特科斯基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11841。 pdf&quot;>;生成检索如何扩展到数百万个段落？&lt;/a>; &lt;br />; &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Kai Hui&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;庄红雷&lt;/em>;&lt;/strong>;、&lt;em>;林志颖&lt;/em>;、&lt;strong>;&lt;em>;唐纳德·梅茨勒&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.13959.pdf&quot;>;让每个例子都有意义：论自我影响力从嘈杂的 NLP 中学习的稳定性和实用性数据集&lt;/a>; &lt;br />; &lt;em>;Irina Bejan&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;Artem Sokolov&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; Katja Filippova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EMNLP 的发现&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11689.pdf&quot;>;适应自我评估以提高法学硕士的选择性预测&lt;/a >; &lt;br />; &lt;em>;陈杰峰&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;尹金成&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sayna Ebrahimi&lt;/em>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sercan O Arik&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Somesh Jha &lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.10062.pdf&quot;>;工具辅助生成策略的综合评估&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;strong>; &lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mor Geva&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16568.pdf&quot;>;1-PAGER：一次性答案生成和证据检索&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05401.pdf&quot;>;MaXM：迈向多语言视觉问答&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;薛琳婷&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ashish V. Thapliyal&lt;/em>;&lt;/strong>;、&lt; strong>;&lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Julien&lt;/em>;&lt;em>;Amelot&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陈曦&lt;/em>; em>;&lt;/strong>;,&lt;strong>; &lt;em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.18431v1.pdf&quot; >;SDOH-NLI：从临床记录推断健康社会决定因素的数据集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;、&lt;em>;Eric Loreaux&lt;sup >;*&lt;/sup>;&lt;/em>;、&lt;strong>;&lt;em>;塔尔舒斯特&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陈明君&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alvin Rajkomar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14815.pdf&quot;>;使用基于案例的推理进行机器阅读理解&lt;/ a>; &lt;br />; &lt;em>;Dung Ngoc Thai&lt;/em>;、&lt;em>;Dhruv Agarwal&lt;/em>;、&lt;em>;Mudit Chaudhary&lt;/em>;、&lt;em>;赵文龙&lt;/em>;、&lt;em>; Rajarshi Das&lt;/em>;、&lt;em>; Jay-Yoon Lee&lt;/em>;、&lt;em>;Hannaneh Hajishirzi&lt;/em>;、&lt;strong>;&lt;em>;Manzil Zaheer&lt;/em>;&lt;/strong>;、&lt;em>;安德鲁McCallum&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.06897.pdf&quot;>;非洲语言跨语言开放检索问答&lt;/a>; &lt;br / >; &lt;em>;Odunayo Ogundepo&lt;/em>;、&lt;em>;Tajuddeen Gwadabe&lt;/em>;、&lt;strong>;&lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jonathan H. Clark &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;、&lt;em>;David Ifeoluwa Adelani&lt;/em>;、&lt;em>;Bonaventure FP Dossou&lt;/em>;、&lt;em>; >;Abdou Aziz DIOP&lt;/em>;、&lt;em>;Claytone Sikasote&lt;/em>;、&lt;em>;Gilles HACHEME&lt;/em>;、&lt;em>;快乐 Buzaaba&lt;/em>;、&lt;em>;Ignatius Ezeani&lt;/em>;、&lt; Rooweither Mabuya&lt;/em>;、Salomey Osei&lt;/em>;、Chris&lt;/em>;、Chinenye Emezue&lt;/em>;、Albert Kahira&lt;/em>;、&lt;em>; Shamsuddeen Hassan Muhammad&lt;/em>;、&lt;em>;Akintunde Oladipo&lt;/em>;、&lt;em>;Abraham Toluwase Owodunni&lt;/em>;、&lt;em>;Atnafu Lambebo Tonja&lt;/em>;、&lt;em>;Iyanuoluwa Shode&lt;/em>;、 &lt;em>;Akari Asai&lt;/em>;、&lt;em>;Anuoluwapo Aremu&lt;/em>;、&lt;em>;Ayodele Awokoya&lt;/em>;、&lt;em>;Bernard Opoku&lt;/em>;、&lt;em>;Chiamaka Ijeoma Chukwuneke&lt;/em>; 、&lt;em>;Christine Mwase&lt;/em>;、&lt;em>;Clemencia Siro&lt;/em>;、&lt;em>;Stephen Arthur&lt;/em>;、&lt;em>;Tunde Oluwaseyi Ajayi&lt;/em>;、&lt;em>;Verrah Akinyi Otiende&lt;/em>; em>;、&lt;em>;Andre Niyongabo Rubungo&lt;/em>;、&lt;em>;Boyd Sinkala&lt;/em>;、&lt;em>;Daniel Ajisafe&lt;/em>;、&lt;em>;Emeka Felix Onwuegbuzia&lt;/em>;、&lt;em>;Falalu Ibrahim Lawan&lt;/em>;、&lt;em>;Ibrahim Said Ahmad&lt;/em>;、&lt;em>;Jesujoba Oluwadara Alabi&lt;/em>;、&lt;em>;CHINEDU EMMANUEL&lt;/em>; &lt;em>;MBONU&lt;/em>;、&lt;em>;Mofetoluwa Adeyemi&lt;/em>;、&lt;em>;Mofya Phiri&lt;/em>;、&lt;em>;Orevaoghene Ahia&lt;/em>;、&lt;em>;Ruqayya Nasir Iro&lt;/em>;、&lt;em>;Sonia Adhiambo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.08653.pdf&quot;>;概率神经总结中的不确定性校准和选择性生成：基准研究&lt;/a>; &lt;br />; &lt;strong>;&lt;em >;Polina Zablotskaia&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;杜潘&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;约书亚·梅内斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt; em>;Shashi Narayan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;任杰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;刘哲刘&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.09860.pdf&quot;>;Epsilon 采样摇滚：研究机器翻译最小贝叶斯风险解码的采样策略&lt;/a>; &lt;br />; &lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;、&lt;em>;Behrooz Ghorbani&lt;sup>;*&lt;/sup>;&lt;/em>;、&lt;em>;Patrick Fernandes&lt;sup>;*&lt;/sup>;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14552.pdf&quot;>;大型语言模型对推理任务的幻觉来源&lt;/a>; &lt;br />; &lt;em>;Nick McKenna&lt; /em>;、&lt;em>;李天一&lt;/em>;、&lt;em>;梁程&lt;/em>;、&lt;strong>;&lt;em>;Mohammad Javad Hosseini&lt;/em>;&lt;/strong>;、&lt;em>;马克·约翰逊&lt;/em>; >;, &lt;em>;Mark Steedman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.09017v2.pdf&quot;>;不要添加，不要错过：有效保留从预选文本跨度生成的内容&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;、&lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;、&lt;em>;Eran Hirsch&lt; /em>;, &lt;em>;Ido Dagan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07686.pdf&quot;>;什么使思想链提示有效？ A Counterfactual Study&lt;/a>; &lt;br />; &lt;em>;Aman Madaan&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; &lt;strong>;Katherine Hermann&lt;/strong>;&lt;/em>;, &lt;strong>;&lt; em>;Amir Yazdanbakhsh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03945.pdf&quot;>;Understanding HTML with Large Language Models&lt;/a>; &lt; br />; &lt;strong>;&lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Ofir Nachum&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yingjie Miao&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Mustafa Safdari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Austin Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha&lt;/em>; &lt; em>;Chowdhery&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Noah Fiedel&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Aleksandra Faust&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09928.pdf&quot;>;通过检测和删除输入来提高摘要模型的鲁棒性Noise&lt;/a>; &lt;br />; &lt;em>;Kundan Krishna&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yao Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jiaming Luo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mohammad&lt;/em>; &lt;em>;Saleh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter J. Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://arxiv.org/pdf/2310.15916.pdf&quot;>;情境学习创建任务向量&lt;/a>; &lt;br />; &lt;em>;Roee Hendel&lt;/em>;，&lt;strong>;&lt;em>;Mor Geva&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10544.pdf&quot;>;Pre -training Without Attention&lt;/a>; &lt;br />; &lt;em>;Junxiong Wang&lt;/em>;, &lt;em>;Jing Nathan Yan&lt;/em>;, &lt;strong>;&lt;em>;Albert Gu&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Alexander M Rush&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12441.pdf&quot;>;MUX-PLMs: Data Multiplexing for High-Throughput Language Models&lt;/a>; &lt;br />; &lt;em>;Vishvak Murahari&lt;/em>;, &lt;em>;Ameet Deshpande&lt;/em>;, &lt;em>;Carlos E Jimenez&lt;/em>;,&lt;em>; &lt;strong >;Izhak Shafran&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Mingqiu Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yuan&lt;/em>; &lt;em>;Cao&lt;/em>;&lt;/ strong>;, &lt;em>;Karthik R Narasimhan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14408.pdf&quot;>;PaRaDe: Passage Ranking Using Demonstrations with LLMs&lt;/ a>; &lt;br />; &lt;em>;Andrew Drozdov&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Honglei Zhuang&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Zhuyun Dai&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhen Qin&lt;/em>;&lt;/strong>;, &lt;em>;Razieh Rahimi&lt;/em>;, &lt;strong>;&lt;em>;Xuanhui Wang&lt;/em>;&lt;/strong >;,&lt;strong>; &lt;em>;Dana Alon&lt;/em>;&lt;/strong>;, &lt;em>;Mohit Iyyer&lt;/em>;, &lt;em>;Andrew McCallum&lt;/em>;, &lt;em>;Donald Metzler&lt;sup>;*&lt;/ sup>;&lt;/em>;,&lt;strong>; &lt;em>;Kai Hui&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13678.pdf&quot;>; Long-Form Speech Translation Through Segmentation with Finite-State Decoding Constraints on Large Language Models&lt;/a>; &lt;br />; &lt;em>;Arya D. McCarthy&lt;/em>;, &lt;strong>;&lt;em>;Hao Zhang&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shankar Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Felix Stahlberg&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ke Wu&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07496.pdf&quot;>;Unsupervised Opinion Summarization Using Approximate Geodesics&lt;/a>; &lt;br />; &lt;em>;Somnath Basu Roy Chowdhury&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Nicholas Monath&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kumar Avinava Dubey&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Amr Ahmed&lt;/em>;&lt;/strong>;, &lt;em>;Snigdha Chaturvedi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02883. pdf&quot;>;SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sercan O . Arik&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rajarishi Sinha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hootan Nakhost&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em >;戴汉军&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;殷鹏程&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://zi-lin.com/pdf/EMNLP_2023_retrieval.pdf&quot;>;Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty&lt;/a>; &lt;br />; &lt;em>;Zi Lin&lt; /em>;, &lt;strong>;&lt;em>;Quan Yuan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Panupong Pasupat&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeremiah Zhe Liu&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Jingbo Shang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.08740.pdf&quot;>;A结构化反射计算机控制的零样本语言智能体&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;李涛&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;李刚&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Zhiwei Deng&lt;/em>;&lt;/strong>;, &lt;em>;Bryan Wang&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Yang Li&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08371.pdf&quot;>;Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches&lt;/a>; &lt;br / >; &lt;em>;Daniel Fried&lt;/em>;, &lt;em>;Nicholas Tomlin&lt;/em>;, &lt;em>;Jennifer Hu&lt;/em>;,&lt;strong>; &lt;em>;Roma Patel&lt;/em>;&lt;/strong>;, &lt;strong >;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13535.pdf&quot;>;通过主动生成成对反事实来提高分类器的鲁棒性&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ananth Balashankar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuezhi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yao Qin &lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nithum Thain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jilin Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ed H.&lt;/em>; &lt;em>;Chi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Beutel&lt;/em>;&lt;/ strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14224.pdf&quot;>;mmT5: Modular Multilingual Pre-training Solves Source Language Hallucinations&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;乔纳斯·菲佛&lt;/em>;&lt;/strong>;、&lt;em>; &lt;strong>;弗朗西斯科·皮奇诺&lt;/strong>;&lt;/em>;、&lt;strong>;&lt;em>;马西莫·尼科西亚&lt;/em>;&lt;/strong>;、&lt; strong>;&lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Machel Reid&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian&lt;/em>; &lt;em>;Ruder&lt;/ em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.10551.pdf&quot;>;Scaling Laws vs Model Architectures: How Does Inductive Bias Influence Scaling?&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yi Tay&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Samira Abnar&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Hyung Won Chung&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; William Fedus&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jinfeng Rao&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dani Yogatama&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Donald Metzler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00142. pdf&quot;>;TaTA：非洲语言的多语言表到文本数据集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sebastian Ruder&lt; /em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vitaly Nikolaev,&lt;/em>; &lt;em>;Jan A. Botha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael Chavinda&lt;/em>;&lt; /strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2305.11938.pdf&quot;>;XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Ruder ,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alexander Gutkin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mihir Kale&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Min Ma&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Massimo Nicosia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Parker Riley&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jean Michel Amath Sarr&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;John Frederick&lt;/em>; &lt;em>;Wieting&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nitish Gupta&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;安娜·卡塔诺娃&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;克里斯托·基洛夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dana L Dickinson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Roark&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bidisha Samanta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;陶康妮&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;David Ifeoluwa Adelani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vera Axelrod&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Isaac Rayburn&lt;/em>; &lt;em>;Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dan Garrette&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Reeve Ingle&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Melvin Johnson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dmitry Panteleev&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://www .winlp.org/&amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701752203060961&amp;amp;usg=AOvVaw3sRozMNVYuwvyXeLluOgKI&quot;>;第七届拓宽NLP研讨会&lt;/a>;（WiNLP）&lt;br />; 主要赞助商&lt;br />; 主办单位：&lt;strong >;&lt;em>;Sunipa Dev&lt;/em>;&lt;/strong>; &lt;br />; 小组成员：&lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //sites.google.com/corp/view/crac2023/?pli=1&quot;>;第六届指称、照应和共指计算模型研讨会&lt;/a>; (CRAC)&lt;br />;特邀演讲嘉宾：&lt;strong>;&lt; em>;Bernd Bohnet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nlposs.github.io/2023/index.html&quot;>;第三届自然语言处理开源研讨会软件&lt;/a>;（NLP-OSS）&lt;br />;组织者：&lt;strong>;&lt;em>;Geeticka Chauhan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://splu- robonlp-2023.github.io/&quot;>;机器人空间语言理解与接地通信联合研讨会&lt;/a>; (SpLU-RoboNLP) &lt;br />;特邀演讲者：&lt;strong>;&lt;em>;Andy Zeng&lt;/​​em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gem-benchmark.com/workshop&quot;>;自然语言生成、评估和度量&lt;/a>;（GEM）&lt;br />; 组织者：&lt;强>;&lt;em>;伊丽莎白·克拉克&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arabicnlp2023.sigarab.org/&quot;>;第一届阿拉伯语自然语言处理会议&lt;/a>; （ArabicNLP）&lt;br />;主办方：&lt;strong>;&lt;em>;Imed Zitouni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.bigpictureworkshop.com/&quot;>;The大局：制定研究叙述&lt;/a>;（大局）&lt;br />;组织者：&lt;strong>;&lt;em>;诺拉·卡斯纳&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;塞巴斯蒂安·鲁德&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://blackboxnlp.github.io/&quot;>;BlackboxNLP 2023：第六届 NLP 分析和解释神经网络研讨会&lt;/a>; &lt;br />; 主办方: &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br />; 小组成员：&lt;strong>;&lt;em>;Neel Nanda&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= &quot;https://www.conll.org/2023&quot;>;计算自然语言学习 SIGNLL 会议&lt;/a>; (CoNLL) &lt;br />; 联合主席：&lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt; /strong>; &lt;br />; 领域和 AC：&lt;strong>;&lt;em>;Kyle Gorman&lt;/em>;&lt;/strong>;（言语和音系）、&lt;strong>;&lt;em>;刘飞&lt;/em>;&lt;/strong>;(自然语言生成）&lt;/p>; &lt;p>; &lt;a href=&quot;https://sigtyp.github.io/ws2023-mrl.html&quot;>;第三届多语言表示学习研讨会&lt;/a>; (MRL) &lt; br />; 主办方：&lt;strong>;&lt;em>;Omer Goldman&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;br />; 特邀演讲嘉宾：&lt;strong>;&lt;em>; >;Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt; div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://emnlp2023-creative-nlg.github.io/&quot;>;创意自然语言生成&lt;/a>; &lt;br />; 组织者： &lt;em>;Tuhin Chakrabarty&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class= &quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot; http://blog.research.google/feeds/8414954450937764241/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog .research.google/2023/12/google-at-emnlp-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http: //www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds /8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp- 2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at EMNLP 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dn l4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s72-c/EMNLP%202023.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com /mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3665865722098768988&lt;/ id>;&lt;发布>;2023-12-04T14:41:00.000-08:00&lt;/发布>;&lt;更新>;2023-12-04T14:46:06.688-08:00&lt;/更新>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt; /category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;QuantumComputing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;一种新的经典力学量子算法指数级加速&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院量子 AI 团队研究科学家 Robin Kothari 和 Rolando Somma&lt;/span>; &lt;img src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf 06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s1100/glued-trees.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 量子计算机承诺解决一些问题的速度比传统计算机快得多，但只有少数例子具有如此显着的加速，例如 &lt;a href=&quot;https://en.wikipedia.org/wiki /Shor%27s_algorithm&quot;>;Shor 因式分解算法&lt;/a>;和&lt;a href=&quot;https://blog.research.google/2023/10/developing-industrial-use-cases-for.html&quot;>;量子模拟&lt;/一个>;。在这几个例子中，大多数都涉及模拟本质上是量子力学的物理系统——这是量子计算机的自然应用。但是模拟本质上不是量子的系统又如何呢？量子计算机能否为此提供指数级优势？ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://link.aps.org/doi/10.1103/PhysRevX.13.041041&quot;>;模拟中的指数量子加速耦合经典振荡器&lt;/a>;”，发表于 &lt;a href=&quot;https://journals.aps.org/prx/&quot;>;Physical Review X&lt;/a>; (PRX) 并在&lt;a href=&quot;https ://focs.computer.org/2023/&quot;>;计算机科学基础研讨会&lt;/a>; (FOCS 2023)，我们报告了一种新量子算法的发现，该算法为模拟耦合提供了指数级优势&lt;a href= “https://en.wikipedia.org/wiki/Harmonic_oscillator&quot;>;经典谐振子&lt;/a>;。这些是自然界中一些最基本、普遍存在的系统，可以描述无数自然系统的物理学，从电路到分子振动再到桥梁力学。我们与麦考瑞大学的 Dominic Berry 和多伦多大学的 Nathan Wiebe 合作，发现了一种映射，可以将任何涉及耦合振荡器的系统转化为描述量子系统时间演化的问题。在某些限制下，使用量子计算机解决这个问题的速度比使用经典计算机快得多。此外，我们使用这种映射来证明，任何可以通过量子算法有效解决的问题都可以重新转换为涉及耦合振荡器网络的问题，尽管耦合振荡器的数量呈指数级增长。除了解锁量子计算机以前未知的应用之外，这一结果还提供了一种通过纯粹推理经典系统来设计新量子算法的新方法。 &lt;/p>; &lt;br />; &lt;h2>;模拟耦合振荡器&lt;/h2>; &lt;p>;我们考虑的系统由经典谐振子组成。单谐振子的一个例子是连接到弹簧上的质量（例如球）。如果将质量块从其静止位置移开，则弹簧将产生恢复力，沿相反方向推动或拉动质量块。该恢复力导致质量块来回振荡。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvM cWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/s359/oscillator.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;116&quot; data-original-width=&quot;359&quot; height=&quot;129&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-m DXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/w400-h129/oscillator.gif&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;谐振子的一个简单示例是通过弹簧连接到墙壁的质量。 [图片来源：&lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Simple_harmonic_oscillator.gif&quot;>;维基媒体&lt;/a>;]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 现在考虑&lt;em>;耦合&lt;/em>;谐波振荡器，其中&lt;em>;多个&lt;/em>;质量通过弹簧相互连接。移动一个质量，就会引起一波振荡穿过系统。正如人们所预料的那样，在经典计算机上模拟大量质量的振荡变得越来越困难。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IK fNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkuKE-mFxJ3e9sdM-ct3lU81Gs1 /s1129/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;832&quot; data-original-width=&quot;1129&quot; 高度=“295”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm 8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkuKE-mFxJ3e9sdM-ct3lU81Gs1/w400-h295/image4.png“宽度=”400“ />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过弹簧连接的质量示例系统，可以使用以下命令进行模拟量子算法。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;为了能够模拟大量耦合谐振子，我们提出了一个映射，对所有谐振子的位置和速度进行编码质量和弹簧进入量子位系统的量子波函数。由于描述量子位系统波函数的参数数量随着量子位数量呈指数增长，因此我们可以将 &lt;em>;N&lt;/em>; 个球的信息编码到仅约 log(&lt;em>;N &lt;/em>;）量子位。只要对系统有一个紧凑的描述（即质量和弹簧的属性），我们就可以演化波函数来稍后学习球和弹簧的坐标，并且所用的资源比我们使用时要少得多模拟球和弹簧的朴素经典方法。 &lt;/p>; &lt;p>; 我们证明了一类耦合经典振荡器系统可以在量子计算机上有效地模拟。但仅此一点并不能排除存在某种尚不为人知的巧妙经典算法的可能性，该算法在资源使用方面同样高效。为了证明我们的量子算法比任何可能的经典算法实现指数加速，我们提供了两个额外的证据。 &lt;/p>; &lt;br />; &lt;h2>;粘合树问题和量子预言&lt;/h2>; &lt;p>; 对于第一个证据，我们使用我们的映射来表明量子算法可以有效地解决一个著名的问题关于已知难以经典解决的图，称为&lt;a href=&quot;https://arxiv.org/abs/quant-ph/0209131&quot;>;粘合树问题&lt;/a>;。该问题需要两个分支树（一个图，其节点每个分支到另外两个节点，类似于树的分支路径）并通过一组随机边将它们的分支粘合在一起，如下图所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4t b6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s930/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;930&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw 0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;粘合树问题的直观表示。在这里，我们从标记为 ENTRANCE 的节点开始，并允许本地探索该图，该图是通过将两个二叉树随机粘合在一起而获得的。目标是找到标记为 EXIT 的节点。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 粘合树问题的目标是找到退出节点 - 的“根”第二棵树——尽可能高效。但粘合树的节点和边缘的确切配置最初对我们来说是隐藏的。要了解系统，我们必须查询&lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_machine&quot;>;oracle&lt;/a>;，它可以回答有关设置的具体问题。这个神谕允许我们探索树木，但仅限于本地。几十年前，&lt;a href=&quot;https://doi.org/10.1145/780542.780552&quot;>;研究表明&lt;/a>;在经典计算机上找到退出节点所需的查询数量与多项式因子成正比&lt;em>;N&lt;/em>;，节点总数。 &lt;/p>; &lt;p>; 但是将其重新转换为球和弹簧的问题，我们可以将每个节点想象为一个球，将两个节点之间的每个连接想象为一个弹簧。拨动入口节点（第一棵树的根），振荡将穿过树木。只需花费与树的&lt;em>;深度&lt;/em>;成比例的时间（比&lt;em>;N&lt;/em>;指数小）即可到达出口节点。因此，通过将胶合树球弹簧系统映射到量子系统并对其进行演化，我们可以检测出口节点的振动，并比使用经典计算机更快地确定它。 &lt;/p>; &lt;br />; &lt;h2>;BQP 完整性&lt;/h2>; &lt;p>; 通过检查一组问题，我们揭示了我们的算法比任何可能的经典算法效率指数级更高的第二个也是最有力的证据：量子计算机可以高效求解（即可以在&lt;a href=&quot;https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time&quot;>;多项式时间&lt;/a>;内求解），简称为&lt;a href=&quot;https: //en.wikipedia.org/wiki/BQP&quot;>;有界误差量子多项式时间&lt;/a>;或BQP。 BQP 中最难的问题称为“BQP-complete”。 &lt;/p>; &lt;p>; 虽然人们普遍认为存在一些量子算法可以有效解决而经典算法无法解决的问题，但这一点尚未得到证实。所以，我们能提供的最好证据是我们的问题是 BQP 完备的，也就是说，它是 BQP 中最难的问题之一。如果有人找到一种有效的经典算法来解决我们的问题，那么量子计算机有效解决的每个问题都将是经典可解决的！甚至连构成&lt;a href= “https://en.wikipedia.org/wiki/RSA_(cryptosystem)&quot;>;现代加密&lt;/a>;，并由 Shor 算法著名地解决，预计将是 BQP 完整的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1 B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBET4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s574/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;563&quot; data-original-width=&quot;574&quot; height=&quot;314&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmu XiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s320/image1.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;显示 BPP 和 BQP 类的可信关系的图表，它们是可以有效解决的问题集分别在经典计算机和量子计算机上解决。 BQP 完备问题是 BQP 中最难的问题。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 为了证明我们模拟球和弹簧的问题确实是 BQP 完备的，我们从模拟通用量子电路的标准 BQP 完全问题，并表明每个量子电路都可以表示为许多与弹簧耦合的球的系统。因此，我们的问题也是 BQP 完备的。 &lt;/p>; &lt;br />; &lt;h2>;影响和未来的工作&lt;/h2>; &lt;p>; 这项工作还揭示了 2002 年的工作，当时理论计算机科学家 Lov K. Grover 和他的同事 Anirvan M. Sengupta 使用&lt;a href=&quot;https://journals.aps.org/pra/abstract/10.1103/PhysRevA.65.032319&quot;>;耦合摆的类比&lt;/a>;来说明 Grover 著名的量子&lt;a href=&quot;https:// en.wikipedia.org/wiki/Grover%27s_algorithm&quot;>;搜索算法&lt;/a>;可以比传统方法更快地在未排序的数据库中找到正确的元素。通过正确的设置和初始条件，在系统进化一段时间后，就有可能判断一个&lt;em>;N&lt;/em>;摆是否与其他摆不同——类似于在数据库中找到正确的元素那只是~√(&lt;em>;N)&lt;/em>;。虽然这暗示了某些经典振荡系统和量子算法之间的联系，但它无法解释为什么格罗弗的量子算法实现了量子优势。 &lt;/p>; &lt;p>; 我们的结果使这种联系更加精确。我们证明，任何经典谐振子系统的动力学确实可以等价地理解为相应的指数较小的量子系统的动力学。这样我们就可以在 log(&lt;em>;N&lt;/em>;) 个量子位的量子计算机上模拟 Grover 和 Sengupta 的钟摆系统，并找到一种不同的量子算法，可以及时找到正确的元素 ~√(&lt;em>; N&lt;/em>;)。我们在经典系统和量子系统之间发现的类比可用于构建其他提供指数加速的量子算法，其中加速的原因现在从经典波的传播方式中更加明显。 &lt;/p>; &lt;p>; 我们的工作还表明，每种量子算法都可以等效地理解为经典波在耦合振荡器系统中的传播。这意味着，例如，我们原则上可以构建一个经典系统，在它的演化时间比任何已知的解决因式分解的经典算法的运行时间小得多的时间之后，它可以解决因式分解问题。这可能看起来像是一种高效的经典因式分解算法，但问题是振荡器的数量呈指数级增长，这使得它成为解决因式分解的不切实际的方法。耦合谐振子在自然界中无处不在，描述了从电路到分子链再到桥梁等结构的广泛系统。虽然我们在这里的工作重点是这类广泛问题的基本复杂性，但我们希望它将指导我们寻找谐振子问题的现实示例，在这些示例中量子计算机可以提供指数优势。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢我们的量子计算科学传播者 Katie McCormick 帮助撰写这篇博文。&lt;/em>; &lt;/em>; &lt;/p>; p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3665865722098768988/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/ >; &lt;link href =“ http://blog.research.google/2023/2023/a-new-quantum-quantum-algorithm-for-classical.html#comment-form” =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/36665865722098768768768988链接href =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/36665865722098768988” 。 />;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot; >;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s72 -c/glued-trees.jpg“ width =” 72“ xmlns：媒体=” http：//search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr>; &lt;thr>; &lt;thr：thr>; thr>; 0 &lt;/thr：thr：thr：总计>; &lt;/entry>; &lt;enter>; &lt;id>;标签：blogger.com，1999：Blog-8474926331452026626.POST-68038970217457115 &lt;/id>; &lt;/id>; &lt;/id>; &lt;出版>; 2023-12-04t10：00.000-08：00.000-08：00.00：00.00：00.00：00.00：00.00：00.00 &lt;/00 &lt;/00 &lt;/出版>; &lt;更新>; 2023-12-04T11：29：40.224-08：00 &lt;/updated>; &lt;类别方案=“ http://www.blogger.com/atom/atom/ns#” &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ optimization”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“安全与隐私”>; &lt;/category>; &lt;title type =“ text”>;摘要报告优化在“隐私沙框归因报告”报告中api &lt;/stitle &lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author &quot;>;Posted by Hidayet Aksu, Software Engineer, and Adam Sealfon, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN-hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s320 /Hero.jpg“ style =” display：none;” />; &lt;p>;近年来，&lt;a href=&quot;https://privacysandbox.com/&quot;>;隐私沙盒&lt;/a>;启动了倡议，以探索广告商通过瞄准广告商来衡量其运动有效性的负责任方法到&lt;a href=&quot;https://blog.chromium.org/2020/01/building-more-private-web-path-path-towards.html&quot;>;贬低第三方饼干&lt;/a>; =“ https://www.gov.uk/cma-cases/investigation-into-googles-privacy-sandbox-browser-changes-”>;解决与英国竞争和市场管理局的任何竞争问题&lt;/a>;）。 &lt;a href=&quot;https://en.wikipedia.org/wiki/http_cookie#third-party_cookie&quot;>; cookies &lt;/a>;是包含网站在用户设备上存储的用户偏好的小片段；它们可用于提供更好的浏览体验（例如，允许用户自动登录）并提供相关内容或广告。隐私沙箱试图通过提供隐私保护替代方案来解决有关使用cookie在网络上跟踪数据的问题。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;p>;许多浏览器使用&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/differential_privacy&quot;>; distinial私密&lt;/a>;（DP ）提供隐私提供的API，例如&lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/attribution-reporting&quot;>;属性报告API &lt;/a>;（ARA） t依靠cookie进行广告转换测量。 ARA对单个用户操作进行加密并在汇总&lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/&quot;>;摘要报告&lt;/a>;中，估算诸如测量目标之类的诸如测量目标之类转换的数量和价值（网站上有用的操作，例如进行购买或注册邮件列表）归因于广告系列。 &lt;/p>; &lt;p>;配置API参数的任务，例如，跨不同转换分配贡献预算，对于最大化摘要报告的实用性很重要。在“ &lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;摘要报告优化的“隐私沙箱归因报告API &lt;/a”中””，我们引入了一个正式的数学框架，用于建模摘要报告。然后，我们制定了最大化摘要报告的实用性作为优化问题的问题，以获得最佳的ARA参数。最后，我们使用实际和合成数据集评估了该方法，与基线非优化摘要报告相比，证明了实用性的明显改善。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; ara摘要报告&lt;/h2>; &lt;p>;我们使用以下示例来说明我们的符号。想象一个虚构的礼品店，称为&lt;em>; du＆amp; Penc &lt;/em>;使用数字广告来吸引客户。下表捕获了他们的假期销售，每个记录都包含（i）印象ID，（ii）广告系列以及（iii）展示广告的城市以及（i）的转换功能，购买的物品数量以及（ii）这些物品的总价值。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqrhtaPxDlj8o5sbubBQLdCq_RRnT2ZPmhxuvEIC_9SvLfhKkXh1t_elS4eRCQyHv2pYX4YkAX1UkEX3c3BdbB5PejqLF0uq_MAXuXKVA1YPKVnZ5tqertr02eNDFjJ0nnoeD_rGdDFWxLrTsCNGXOE9LrGSsuPhrtZ6SgqRomWRjpun1JdrwWDfnfJF0n/s1035/image4.png&quot; style=&quot;margin-左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-foriginal-height =“ 397” data-eriginal-width =“ 1035” height =“ 245” src =“ 。 5TQERTR02ENDFJJ0NNOED_RGDDFWXLRTSCNGXOE9LRGSSUPHRTZ6SGQROMWRJPUN1JDRWDFNFNFNFNFNFNFNFNFNFNFN/W640-H245/image 4.png width =“ width =” 640/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/字幕“ style =” text-align：center;“>;印象和转换功能日志du＆amp; penc。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h3>;数学模型&lt; /h3>; &lt;p>; ARA摘要报告可以通过四种算法进行建模：（1）贡献向量，（2）&lt;a href =“ https://github.com/wicg/wicg/wicg/attribution-reporting-reporting-reporting-porting-api/blob/blob/main/-main/- gentregate.md＃贡献bounding-and-budgeting“>;贡献边界&lt;/a>;，（3）&lt;a href =” https://github.com/wicg/wicg/attribution-reporting-reporting-reporting-reporting-api/blob/blob/main/main/main/main/aggregation_service_service_tee。 MD？”>;摘要报告&lt;/a>;和（4）重建值。贡献边界和摘要报告由ARA执行，而贡献向量和重建值由ADTECH提供商（使企业可以买卖数字广告的工具和系统）执行。这项工作的目的是协助ADTECHS优化摘要报告算法。 &lt;/p>; &lt;p>;贡献向量算法将测量值转换为已离散和缩放的ARA格式。缩放需要考虑到每印象的总体贡献限制。在这里，我们提出了一种剪辑并执行随机舍入的方法。该算法的结果是聚集键和值的直方图。 &lt;/p>; &lt;p>;接下来，边界算法在客户端设备上运行的贡献限制，并强制对属性报告的贡献，其中任何进一步的贡献都超过了限制。输出是归因转换的直方图。 &lt;/p>; &lt;p>;摘要报告算法在服务器端运行&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/trusted_execution_environment&quot;>;可信赖的执行环境满足DP。噪声是从离散&lt;a href=&quot;https://en.wikipedia.org/wiki/laplace_distribution&quot;>; laplace Distribution &quot;>; laplace Distribution &lt;/a>;中采样的，为了执行隐私预算，可能只查询一次报告。 &lt;/p>; &lt;p>;最后，重建值算法将测量值转换回原始比例。重建值和贡献向量算法是由ADTECH设计的，并且都影响了从摘要报告中收到的实用程序。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s1999/image1.png&quot; style=&quot;边距左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Forminal-height =“ 1014” data-Original-width =“ 1999” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>; ARA摘要报告的说明用法，其中包括贡献向量（算法A），贡献边界（算法C），摘要报告（算法S）和重建值（算法R）。算法C和S固定在API中。 ADTECH设计A和R。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;/div>; &lt; H2>;误差指标&lt;/h2>; &lt;p>;选择用于评估近似质量的误差度量时，有几个因素需要考虑。为了选择特定的度量，我们考虑了误差度量的理想属性，可以进一步用作目标函数。考虑到所需的属性，我们选择了&lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/design-design-decisisions/#rmsre&quot;>; 𝜏- thunciped to a>;（RMSRE &lt;sub>; 𝜏 &lt;/sub>;）作为其属性的错误度量。有关与其他可能的指标进行详细的讨论和比较，请参见&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>; paper &lt;/a>;。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; sub>;，我们为每个切片选择一个上限参数，&lt;em>; c &lt;/em>;和隐私预算。两者的组合决定了在ADTECH侧编码的实际测量方法（例如两个转换为$ 3），然后传递给ARA以进行贡献边界算法处理。 RMSRE &lt;sub>; 𝜏 &lt;/sub>;可以准确地计算出来，因为它可以根据剪辑和噪声分布的差异表示。遵循这些步骤，我们发现RMSRE &lt;sub>; 𝜏 &lt;/sub>;对于固定的隐私预算，𝛼，&lt;sub>; &lt;/sub>;或Capping参数c，capping参数为&lt;a href =“ https：// en。 wikipedia.org/wiki/convex_optimization&quot;>; convex &lt;/a>;（因此，可以有效地获得其他参数的误差 - 限量化值），而对于关节变量（c，𝛼），它可能不会变成非convex（因此我们可能不会始终能够选择最佳参数）。无论如何，任何现成的优化器都可用于选择隐私预算和封盖参数。在我们的实验中，我们使用&lt;a href=&quot;https://docs.scipy.org/doc/doc/scipy/reference/optimize.minimize-slslsqp.html&quot;>; slsqp &lt;/a>;来自&lt;a href =“ https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html&quot;>; scipy.optimize &lt;/a>;库。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>;转换数据集。但是，由于隐私问题，可以限制或放缓此类数据，或者根本无法使用。解决这些局限性的一种方法是使用复制真实数据特征的合成数据。 &lt;/p>; &lt;p>;我们提供了一种通过现实转换数据集的统计建模来负责任地生成综合数据的方法。我们首先对实际转换数据集进行经验分析，以发现ARA的相关特征。然后，我们设计了一个使用此分布知识来创建可以通过输入参数自定义的现实合成数据集的管道。 &lt;/p>; &lt;p>;管道首先会产生从&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/power_law&quot;>; powerlaw分布&lt;/a>;（步骤1）的印象它会产生从&lt;a href=&quot;https://en.wikipedia.org/wiki/poisson_distribution&quot;>; poisson Distribution&quot;>; poisson distribution &quot;>;（步骤2）的每种印象从&lt;a href=&quot;https://en.wikipedia.org/wiki/log-normal_distribution&quot;>; log-normal分布&lt;/a>;（步骤3）。借助数据集依赖性参数，我们发现这些分布与AD-DATASET特性紧密匹配。因此，可以从历史或公共数据集中学习参数，并生成合成数据集进行实验。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxsehj7tcj7tcj7tcj0k-h4e2zsedifg6ftkppppppppev0_z_z_vl4smolguoxkquudnbmnbmnbmnbmnbmnbmnbmnbmnbndlmnbmndlmnbndlmtndlmtndltndltndltt 9TWL5VUD4FIQ_ACXDIHJX-EK3QY-D8WJI_AQTVHGNZXFQWLYXXBOSS-VIHIWHIKWA-7_KNYRQHLKWXMRLLLLLLLLLBLBLBLBLBLBLBLBLBLBLBEYDX9SONX159DJWHBP41WWBP41WJNBLMEJ/S8BLMEJ/S841 /image2.png“ style =”边距 - 左：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 841” data-Original-width =“ 840” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>;整体数据集生成步骤具有插图的功能。&lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;实验评估&lt;/h2>; &lt;p>;我们在三个真实世界数据集中评估了我们的算法（&lt;a href =” https://ailab.criteo.com/criteo-ponsored-search-conversion-log-dataset/&quot;>; criteo &lt;/a>;，AdTech Real Estate和AdTech Travel）和三个合成数据集。 Criteo由1500万的点击组成，房地产由100K转换组成，旅行由30K转换组成。每个数据集都将训练集和一个测试集分配为。培训集用于选择贡献预算，剪辑阈值参数和转换计数限制（实际数据集每次点击只有一个转换），并且在测试集中评估了错误。每个数据集都使用印象功能将切片划分为切片。对于实际数据集，我们考虑每个切片的三个查询；对于合成数据集，我们考虑每个切片的两个查询。 &lt;/p>; &lt;p>;对于每个查询，我们选择RMSRE &lt;sub>; &lt;/sub>; 𝜏值是培训数据集中查询中位数的五倍。这样可以确保将误差指标的不变性到数据重新缩放，并允许我们通过使用每个功能使用𝝉来结合不同尺度的特征的错误。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s1971/image5.png&quot; style=&quot;边距 - 左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Forminal-height =“ 1028” data-Original-width =“ 1971” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>;实际数据集的散点图，说明了观察转换值的概率。拟合曲线代表了最佳的对数正态分布模型，可有效捕获数据中的基本模式。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;br />; &lt;div style =“ line-height：40 ％;“>; &lt;br />; &lt;/div>; &lt;h3>;结果&lt;/h3>; &lt;p>;我们将基于优化的算法与简单基线方法进行比较。对于每个查询，基线都使用均等的贡献预算和固定的培训数据来选择剪接阈值。在现实世界和合成数据集上，我们的算法比基线产生的误差大大低。我们基于优化的方法适应隐私预算和数据。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s1999/image3 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr caption”样式=“ text-align：center;”>; rmsre &lt;ub>;τ&lt;/sub>;用于隐私预算{1、2、2、4、8、8、16、32、64}，用于我们的算法和基线三个现实世界和三个合成数据集。我们基于优化的方法始终达到的误差要比使用固定分位数作为剪辑阈值的基线较低，并在查询中平均将贡献预算平均分配。 &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;我们研究了ARA中摘要报告的优化，该报告目前已在数百个上部署数百万个镀铬浏览器。我们为ARA提供了一种严格的预算优化问题的表述，目的是为研究人员提供鲁棒的抽象，以促进实际改进。 &lt;/p>; &lt;p>;我们的食谱利用历史数据来绑定和扩展在差异隐私下的未来数据的贡献，非常普遍，适用于广告以外的设置。一种基于这项工作的方法是使用过去的数据来了解数据分布的参数，然后将其从此分布中得出的合成数据应用于对未来数据的查询的隐私预算。请参阅&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>; paper &lt;/a>;和&lt;a href =“ https://github.com/google-research/google-research/google-research/google-research/tree/tree/主/ara_optimization“>;随附的代码&lt;/a>;用于详细的算法和证明。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;这项工作是与Badih Ghazi合作完成的Pritish Kamath，Ravi Kumar，Pasin Manurangsi和Avinash Varadarajan。我们感谢Akash Nadan的帮助。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/68038970217457115/comments/comments/comments/comments/default/default” “发表注释” type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2023/12/summary-report-eptimization-in-privacy.ht-privacy.html#comment-form” rel =“回复” title =“ 0注释” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/5474926331452026626/posts/posts/posts/default/68038970217457115 type =“ application/application/atom+xml”/>; &lt;link href =“ http://www.blogger.com/feeds/847492633145202626/posts/posts/default/68038038970217457115 />; &lt;link href =“ http://blog.research.google/2023/12/summary-report-timization-in-privacy.html” rel =“替代” title =“摘要”摘要报告摘要报告优化， API“ type =” text/html“/>; &lt;under>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147775266161 &lt;/uri>; &lt;emage>; noreply@blogger。 com &lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN- HLJ1TP-SHWMIMXYFONL9CXBW11ABP89RIL2O6LDJCT8MCJQ6HWU13VMN6CQNCNCNCNNWSNWSPH9D4LGO_CISNXVQKCYSYT_SIWN_SIWN/ rss/“>; &lt;/媒体：缩略图>; &lt;thr：总计>; 0 &lt;/ thr：thr>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626. /publubly>; &lt;updated>; 2023-12-05T09:38:21.187-08:00&lt;/updated>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“ specy”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http://www.blogger.com/atom /ns＃“ term =“ translate”>; &lt;/category>; &lt;title type =“ text”>;来自单语数据&lt;/stitle>; &lt;content type>; &lt;content type =“ html”>; &lt;span class =“ byline -author&quot;>;Posted by Eliya Nachmani, Research Scientist, and Michelle Tadmor Ramanovich, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1 -3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s1600/T3.png&quot; style=&quot;display: none;&quot; />; &lt;p>;语音到讲话翻译（S2ST）是一种&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/machine_translation&quot;>;机器翻译&lt;/a>;语言到另一个。这项技术有可能打破语言障碍，并促进来自不同文化和背景的人们之间的沟通。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;之前，我们介绍了&lt;a href =“ https://ai.googleblog.com/2019/05/introducing-translatotron-end-end-end-end-end-end-end-end-end-end-eend-to-to- end.html“>; translatotron 1 &lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2021/09/high-quality-robust--and-robust-and-responsible.html&quot;>; translatototron 2 &lt;/a>; ，有史以来第一个能够直接翻译两种语言之间的语音的模型。但是，他们在有监督的设置中使用并行语音数据进行了培训。平行语音数据的稀缺性是该领域的主要挑战，因此大多数公共数据集都是从文本中半半特定或完全合成的。这为学习文本中未表示的语音属性的翻译和重建增加了额外的障碍，因此没有反映在合成的培训数据中。 &lt;/p>; &lt;p>;在这里，我们提出&lt;a href=&quot;https://arxiv.org/abs/2305.17547&quot;>; translatotron 3 &lt;/a>;，一部小说无人监督的语音对语音转换体系结构。在Translatotron 3中，我们表明可以单独从单语数据中学习语音到语音翻译任务。这种方法不仅打开了更多语言对之间的翻译，还可以转化非文本语音属性，例如暂停，口语率和说话者的身份。我们的方法不包括对目标语言的任何直接监督，因此我们认为这是源自语言特征（例如，语气，情感）的正确方向，可以保存在翻译过程中。要启用语音到讲话翻译，我们使用&lt;a href=&quot;https://arxiv.org/abs/1511.06709&quot;>; Back-Translation &lt;/a>;，这是一种来自Unsupervised Machine Translion（umt）的技术源语言的综合翻译用于&lt;a href=&quot;https://arxiv.org/abs/1710.11041&quot;>;翻译文本而没有双语文本数据集&lt;/a>;。西班牙语和英语之间的语音翻译任务的实验结果表明，Translatotron 3优于基线级联系统。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; translatotron 3 &lt;/h2>; &lt;p>; translatotron 3解决了无监督的S2ST的问题，可以消除消除双语语音数据集的要求。为此，Translatotron 3的设计结合了三个关键方面：&lt;/p>; &lt;ol>; &lt;li>;将整个模型预先训练为&lt;a href =“ https://openaccess.thecvf.com/content/content/content/cvpr20222/html /HE_MASKED_AUTOENCODERS_ARE_SCALABL &lt;a href=&quot;https://en.wikipedia.org/wiki/mel-frequency_cepstrum&quot;>; mel spectogment &lt;/a>;的输入音频（而不是原始音频本身），并显示出有效提高的一般性功能the encoder. &lt;/li>; &lt;li>;基于&lt;a href=&quot;https://arxiv.org/abs/1710.04087&quot;>;多语言无人看管的嵌入式嵌入&lt;/a>;（MUSE），该嵌入&lt;a https://arxiv.org/1710.04087&quot;>; &lt;a href=&quot;https://arxiv.org/abs/1710.04087&quot;>;（MUSE），该&lt;a href=&quot;https://arxiv.org/abs/1710.04087&quot;>; &lt;/li>; &lt;li>;模型学习源和目标语言之间共享的嵌入式空间。 &lt;/li>; &lt;li>;基于背面翻译的重建损失，以完全无监督的方式训练编码器decter Direct S2ST模型。 &lt;/li>; &lt;/ol>; &lt;p>;该模型是使用无监督的缪斯嵌入损失，重建损失和S2S反向翻译损失的组合进行训练的。在推断期间，共享编码器被用来将输入编码为多语言嵌入空间，该空间随后由目标语言解码器解码。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h3>;架构&lt;/h3>; &lt;p>; translatotron 3使用共享的encoder来编码源和目标语言。解码器由语言解码器，声学合成器（负责翻译演讲的声学生成）和一个单一的注意模块组成，例如翻译2。但是，对于翻译3，有两个解码器，一个用于源语言，另一个用于源语言，另一种是对于目标语言。在培训期间，我们使用单语语音文本数据集（即，这些数据由语音文本对组成；它们是&lt;em>;不是&lt;/em>;翻译）。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h3>; encoder &lt;/h3>; &lt;p>;编码器与Translatototron中的语音编码器具有相同的体系结构2.编码器的输出分为两个部分：第一部分包含语义信息，而第二部分包含声学信息。通过使用缪斯损失，培训输出的前半部分是输入语音谱图文本的缪斯嵌入。后半部分将在没有缪斯损失的情况下进行更新。重要的是要注意，在源语言和目标语言之间共享相同的编码器。此外，缪斯嵌入本质上是多语言的。结果，编码器能够学习跨源和目标语言的多语言嵌入空间。这允许对输入进行更有效和有效的编码，因为编码器能够将两种语言的语音编码为通用的嵌入空间，而不是为每种语言维护单独的嵌入空间。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>; &lt;/div>; &lt;h3>;解码器&lt;/h3>; &lt;p>;喜欢translatotron 2，解码器由三个不同的组件组成，即语言解码器，声学合成器和注意模块。但是，为了有效处理源和目标语言的不同属性，Translatotron 3对于源和目标语言具有两个单独的解码器。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h3>;两个部分培训&lt;/h3>; &lt;p>;培训方法包括两个部分：（1）与重建和（2）背面翻译术语进行自动编码。在第一部分中，该网络经过培训，可以使用缪斯丢失和重建损失自动对多语言嵌入空间的输入进行自动编码。该阶段旨在确保网络生成有意义的多语言表示。在第二部分中，对网络进行了进一步的训练，可以利用反向翻译损失来翻译输入频谱图。减轻&lt;a href =的问题。”>;灾难性遗忘&lt;/a>;并在培训的第二部分中也应用了潜在空间多语言，缪斯丢失和重建损失也是应用的。为了确保编码器学习输入的有意义的属性，而不是简单地重建输入，我们将Specaugment应用于两个阶段的编码器输入。已证明它可以通过增强输入数据有效地提高编码器的概括能力。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h3>;训练目标&lt;/h3>; &lt;p>;在背面翻译训练阶段（在部分中进行了说明下面）训练网络以将输入频谱图转换为目标语言，然后返回到源语言。反向翻译的目的是实施潜在空间以多种语言。为了实现这一目标，应用以下损失：&lt;/p>; &lt;ul>; &lt;li>;缪斯群体损失：缪斯损失衡量输入光谱图的多语言嵌入与背面翻译频谱图的多语言嵌入之间的相似性。 &lt;/li>; &lt;li>;重建损失：重建损失衡量输入光谱图和反翻译光谱图之间的相似性。 &lt;/li>; &lt;/ul>; &lt;p>;除了这些损失外，在两个阶段的编码器输入中还应用了规格。在反向翻译训练阶段之前，对网络进行了培训，可以使用Muse损失和重建损失自动编码多语言嵌入空间的输入。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>;两个解码器，我们在培训期间都遭受缪斯损失。缪斯损失迫使编码器使用预先训练的缪斯嵌入来产生这种代表。在培训过程中，给定输入文本成绩单，我们从输入语言的嵌入中提取相应的缪斯嵌入。 The error between MUSE embeddings and the output vectors of the encoder is then minimized. Note that the encoder is indifferent to the language of the input during inference due to the multilingual nature of the embeddings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s1999 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;999&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The training and inference in Translatotron 3. Training includes the reconstruction loss via the auto-encoding path and employs the reconstruction loss via back-translation. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Audio samples&lt;/h2>; &lt;p>; Following are examples of direct speech-to-speech translation from Translatotron 3: &lt;/p>; &lt;h3>; Spanish-to-English (on Conversational dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left ： 汽车; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 Synthesized dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS reference (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance &lt;/h2>; &lt;p>; To empirically evaluate the performance of the proposed approach, we conducted experiments on English and Spanish using various datasets, including the &lt;a href=&quot;https://aclanthology.org/2020.lrec-1.520/&quot;>;Common Voice 11&lt;/a>; dataset, as well as two synthesized datasets derived from the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;Conversational&lt;/a>; and Common Voice 11 datasets. &lt;/p>; &lt;p>; The translation quality was measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; (higher is better) on ASR (automatic speech recognition) transcriptions from the translated speech, compared to the corresponding reference translation text. Whereas, the speech quality is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>; score (higher is better). Furthermore, the speaker similarity is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;average cosine similarity&lt;/a>; (higher is better). &lt;/p>; &lt;p>; Because Translatotron 3 is an &lt;em>;unsupervised&lt;/em>; method, as a baseline we used a cascaded S2ST system that is combined from ASR, unsupervised machine translation (UMT), and TTS (text-to -演讲）。 Specifically, we employ UMT that uses the nearest neighbor in the embedding space in order to create the translation. &lt;/p>; &lt;p>; Translatotron 3 outperforms the baseline by large margins in every aspect we measured: translation quality, speaker similarity, and speech quality. It particularly excelled on the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;conversational corpus&lt;/a>;. Moreover, Translatotron 3 achieves speech naturalness similar to that of the ground truth audio samples (measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>;, higher is better). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s1200/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Translation quality (measured by BLEU, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Speech similarity (measured by average cosine similarity between input speaker and output speaker, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Mean-opinion-score (measured by average MOS metric, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; As future work, we would like to extend the work to more languages and investigate whether zero-shot S2ST can be applied with the back-translation technique. We would also like to examine the use of back-translation with different types of speech data, such as noisy speech and low-resource languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The direct contributors to this work include Eliya Nachmani, Alon Levkovitch, Yifan Ding, Chulayutsh Asawaroengchai, Heiga Zhen, and Michelle Tadmor Ramanovich. We also thank Yu Zhang, Yuma Koizumi, Soroosh Mariooryad, RJ Skerry-Ryan, Neil Zeghidour, Christian Frank, Marco Tagliasacchi, Nadav Bar, Benny Schlesinger and Yonghui Wu.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8288223977991952319/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html&quot; rel=&quot;alternate&quot; title=&quot;Unsupervised speech-to-speech translation from monolingual data&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1-3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s72-c/T3.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1055595481112584523&lt;/id>;&lt;published>;2023-11-22T08:03:00.000-08:00&lt;/published>;&lt;updated>;2023-11-30T13:46:35.804-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;High-Performance Computing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Weather&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Improving simulations of clouds and their effects on climate&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Tapio Schneider, Visiting Researcher, and Yi-fan Chen, Engineering Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Today&#39;s climate models successfully capture broad global warming trends. However, because of uncertainties about processes that are &lt;a href=&quot;https://physicstoday.scitation.org/doi/abs/10.1063/PT.3.4772&quot;>;small in scale yet globally important&lt;/a>;, such as &lt;a href=&quot;http://rdcu.be/ohot&quot;>;clouds&lt;/a>; and &lt;a href=&quot;https://doi.org/10.3389/fmars.2019.00065&quot;>;ocean turbulence&lt;/a>;, these models&#39; predictions of upcoming climate changes are not very accurate in detail. For example, predictions of the time by which the global mean surface temperature of Earth will have warmed 2℃, relative to preindustrial times, &lt;a href=&quot;https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_TS.pdf&quot;>;vary by 40–50 years&lt;/a>; (a full human generation) among today&#39;s models. As a result, we do not have the &lt;a href=&quot;https://www.nature.com/articles/s41558-020-00984-6&quot;>;accurate and geographically granular predictions&lt;/a>; we need to plan resilient infrastructure, adapt supply chains to climate disruption, and assess the risks of climate-related hazards to vulnerable communities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In large part this is because clouds dominate errors and uncertainties in climate predictions for the coming decades [&lt;a href=&quot;https://doi.org/10.1029/2005GL023851&quot;>;1&lt;/a>;, &lt;a href=&quot;https://link.springer.com/article/10.1007/s00382-013-1725-9&quot;>;2&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1038/nclimate3402&quot;>;3&lt;/a>;]. Clouds reflect sunlight and exert a &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenhouse_effect&quot;>;greenhouse effect&lt;/a>;, making them crucial for regulating Earth&#39;s energy balance and mediating the response of the climate system to changes in greenhouse gas concentrations. However, they are too small in scale to be directly resolvable in today&#39;s climate models. Current climate models resolve motions at scales of tens to a hundred kilometers, with a &lt;a href=&quot;https://link.springer.com/article/10.1186/s40645-019-0304-z&quot;>;few pushing toward&lt;/a>; the &lt;a href=&quot;https://journals.ametsoc.org/view/journals/bams/101/5/bams-d-18-0167.1.xml&quot;>;kilometer-scale&lt;/a>;. However, the turbulent air motions that sustain, for example, the low clouds that cover large swaths of tropical oceans have scales of meters to tens of meters. Because of this wide difference in scale, climate models use empirical parameterizations of clouds, rather than simulating them directly, which result in large errors and uncertainties. &lt;/p>; &lt;p>; While clouds cannot be directly resolved in global climate models, their turbulent dynamics can be simulated in limited areas by using high-resolution &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_eddy_simulation&quot;>;large eddy simulations&lt;/a>; (LES). However, the high computational cost of simulating clouds with LES has inhibited broad and systematic numerical experimentation, and it has held back the generation of large datasets for training parameterization schemes to represent clouds in coarser-resolution global climate models. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;Accelerating Large-Eddy Simulations of Clouds with Tensor Processing Units&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/journal/19422466&quot;>;Journal of Advances in Modeling Earth Systems&lt;/a>;&lt;/em>; (JAMES), and in collaboration with a &lt;a href=&quot;https://clima.caltech.edu/&quot;>;Climate Modeling Alliance&lt;/a>; (CliMA) lead who is a visiting researcher at Google, we demonstrate that &lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Tensor Processing Units&lt;/a>; (TPUs) — application-specific integrated circuits that were originally developed for machine learning (ML) applications — can be effectively used to perform LES of clouds. We show that TPUs, in conjunction with tailored software implementations, can be used to simulate particularly computationally challenging &lt;a href=&quot;https://en.wikipedia.org/wiki/Marine_stratocumulus&quot;>;marine stratocumulus clouds&lt;/a>; in the conditions observed during the &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;Dynamics and Chemistry of Marine Stratocumulus&lt;/a>; (DYCOMS) field study. This successful TPU-based LES code reveals the utility of TPUs, with their large computational resources and tight interconnects, for cloud simulations. &lt;/p>; &lt;p>; Climate model accuracy for critical metrics, like precipitation or the energy balance at the top of the atmosphere, has improved roughly 10% per decade in the last 20 years. Our goal is for this research to enable a 50% reduction in climate model errors by improving their representation of clouds. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large-eddy simulations on TPUs&lt;/h2>; &lt;p>; In this work, we focus on stratocumulus clouds, which cover ~20% of the tropical oceans and are the most prevalent cloud type on earth. Current climate models are not yet able to reproduce stratocumulus cloud behavior correctly, which has been one of the largest sources of errors in these models. Our work will provide a much more accurate ground truth for large-scale climate models. &lt;/p>; &lt;p>; Our simulations of clouds on TPUs exhibit unprecedented computational throughput and scaling, making it possible, for example, to simulate stratocumulus clouds with 10× speedup over real-time evolution across areas up to about 35 × 54 km&lt;sup>;2&lt;/sup>;. Such domain sizes are close to the cross-sectional area of typical global climate model grid boxes. Our results open up new avenues for computational experiments, and for substantially enlarging the sample of LES available to train parameterizations of clouds for global climate models.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;8&quot; cellspacing=&quot;4&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s540/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s540/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of the cloud evolution from a simulation of a 285 x 285 x 2 km&lt;sup>;3&lt;/sup>; stratocumulus cloud sheet. This is the largest cloud sheet of its kind ever simulated. &lt;strong>;Left&lt;/strong>;: An oblique view of the cloud field with the camera cruising. &lt;strong>;Right&lt;/strong>;: Top view of the cloud field with the camera gradually pulled away.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The LES code is written in TensorFlow, an open-source software platform developed by Google for ML applications. The code takes advantage of TensorFlow&#39;s graph computation and &lt;a href=&quot;https://www.tensorflow.org/xla&quot;>;Accelerated Linear Algebra&lt;/a>; (XLA) optimizations, which enable the full exploitation of TPU hardware, including the high-speed, low-latency &lt;a href=&quot;https://patents.google.com/patent/US9372800&quot;>;inter-chip interconnects&lt;/a>; (ICI) that helped us achieve this unprecedented performance. At the same time, the TensorFlow code makes it easy to incorporate ML components directly within the physics-based fluid solver. &lt;/p>; &lt;p>; We validated the code by simulating canonical test cases for atmospheric flow solvers, such as a buoyant bubble that rises in neutral stratification, and a negatively buoyant bubble that sinks and impinges on the surface. These test cases show that the TPU-based code faithfully simulates the flows, with increasingly fine turbulent details emerging as the resolution increases. The validation tests culminate in simulations of the conditions during the DYCOMS field campaign. The TPU-based code reliably reproduces the cloud fields and turbulence characteristics observed by aircraft during a field campaign — a feat that is &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;notoriously difficult to achieve for LES&lt;/a>; because of the &lt;a href=&quot;https://doi.org/10.1029/2018MS001312&quot;>;rapid changes in temperature and other thermodynamic properties&lt;/a>; at the top of the stratocumulus decks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/s1023/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;815&quot; data-original-width=&quot;1023&quot; height=&quot;510&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/w640-h510/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;One of the test cases used to validate our TPU Cloud simulator. The fine structures from the density current generated by the negatively buoyant bubble impinging on the surface are much better resolved with a high resolution grid (10m, bottom row) compared to a low resolution grid (200 m, top row).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Outlook&lt;/h2>; &lt;p>; With this foundation established, our next goal is to substantially enlarge existing &lt;a href=&quot;https://doi.org/10.1029/2021MS002631&quot;>;databases&lt;/a>; of high-resolution cloud simulations that researchers building climate models can use to develop better cloud parameterizations — whether these are for physics-based models, ML models, or hybrids of the two. This requires additional physical processes beyond that described in the &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;paper&lt;/a>;; for example, the need to integrate radiative transfer processes into the code. Our goal is to generate data across a variety of cloud types, eg, thunderstorm clouds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s540/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;303&quot; data-original-width=&quot;540&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of a thunderstorm simulation using the same simulator as the stratocumulus simulation work. Rainfall can also be observed near the ground.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; This work illustrates how advances in hardware for ML can be surprisingly effective when repurposed in other research areas — in this case, climate modeling. These simulations provide detailed training data for processes such as in-cloud turbulence, which are not directly observable, yet are crucially important for climate modeling and prediction. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank the co-authors of the paper: Sheide Chammas, Qing Wang, Matthias Ihme, and John Anderson. We&#39;d also like to thank Carla Bromberg, Rob Carver, Fei Sha, and Tyler Russell for their insights and contributions to the work.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1055595481112584523/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and.html&quot; rel=&quot;alternate&quot; title=&quot;Improving simulations of clouds and their effects on climate&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3970135078050750650&lt;/id>;&lt;published>;2023-11-21T10:09:00.000-08:00&lt;/published>;&lt;updated>;2023-11-21T10:09:33.541-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Open sourcing Project Guideline: A platform for computer vision accessibility technology&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dave Hawkey, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Two years ago we &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;announced Project Guideline&lt;/a>;, a collaboration between Google Research and &lt;a href=&quot;https://www.guidingeyes.org/&quot;>;Guiding Eyes for the Blind&lt;/a>; that enabled people with visual impairments (eg, blindness and low-vision) to walk, jog, and run independently. Using only a Google Pixel phone and headphones, Project Guideline leverages on-device machine learning (ML) to navigate users along outdoor paths marked with a painted line. The technology has been &lt;a href=&quot;https://projectguidelinejp.withgoogle.com/intl/en/&quot;>;tested all over the world&lt;/a>; and even demonstrated during the &lt;a href=&quot;https://www.youtube.com/live/2cW1-plwqeQ?si=MTIX2uJkyWuLluht&amp;amp;t=7334&quot;>;opening ceremony at the Tokyo 2020 Paralympic Games&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Since the original announcement, we set out to improve Project Guideline by embedding new features, such as obstacle detection and advanced path planning, to safely and reliably navigate users through more complex scenarios (such as sharp turns and nearby pedestrians). The early version featured a simple frame-by-frame image segmentation that detected the position of the path line relative to the image frame. This was sufficient for orienting the user to the line, but provided limited information about the surrounding environment. Improving the navigation signals, such as alerts for obstacles and upcoming turns, required a much better understanding and mapping of the users&#39; environment. To solve these challenges, we built a platform that can be utilized for a variety of spatially-aware applications in the accessibility space and beyond. &lt;/p>; &lt;p>; Today, we announce the &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;open source release of Project Guideline&lt;/a>;, making it available for anyone to use to improve upon and build new accessibility experiences. The release includes &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;source code&lt;/a>; for the core platform, an &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/android&quot;>;Android application&lt;/a>;, pre-trained &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/vision/models&quot;>;ML models&lt;/a>;, and a &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/unreal&quot;>;3D simulation framework&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;System design&lt;/h2>; &lt;p>; The primary use-case is an Android application, however we wanted to be able to run, test, and debug the core logic in a variety of environments in a reproducible way. This led us to design and build the system using C++ for close integration with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; and other core libraries, while still being able to integrate with Android using the &lt;a href=&quot;https://developer.android.com/ndk&quot;>;Android NDK&lt;/a>;. &lt;/p>; &lt;p>; Under the hood, Project Guideline uses &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; to estimate the position and orientation of the user as they navigate the课程。 A segmentation model, built on the &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLabV3+&lt;/a>; framework, processes each camera frame to generate a binary mask of the guideline (see the &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;previous blog post&lt;/a>; for more details). Points on the segmented guideline are then projected from image-space coordinates onto a world-space ground plane using the camera pose and lens parameters (intrinsics) provided by ARCore. Since each frame contributes a different view of the line, the world-space points are aggregated over multiple frames to build a virtual mapping of the real-world guideline. The system performs piecewise curve approximation of the guideline world-space coordinates to build a spatio-temporally consistent trajectory. This allows refinement of the estimated line as the user progresses along the path. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s800/image1.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;355&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Project Guideline builds a 2D map of the guideline, aggregating detected points in each frame (&lt;strong>;red&lt;/strong>;) to build a stateful representation (&lt;strong>;blue&lt;/strong>;) as the runner progresses along the path.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A control system dynamically selects a target point on the line some distance ahead based on the user&#39;s current position, velocity, and direction. An audio feedback signal is then given to the user to adjust their heading to coincide with the upcoming line segment. By using the runner&#39;s velocity vector instead of camera orientation to compute the navigation signal, we eliminate noise caused by irregular camera movements common during running. We can even navigate the user back to the line while it&#39;s out of camera view, for example if the user overshot a turn. This is possible because ARCore continues to track the pose of the camera, which can be compared to the stateful line map inferred from previous camera images. &lt;/p>; &lt;p>; Project Guideline also includes obstacle detection and avoidance features. An ML model is used to estimate depth from single images. To train this monocular depth model, we used &lt;a href=&quot;https://blog.research.google/2023/10/sanpo-scene-understanding-accessibility.html&quot;>;SANPO&lt;/a>;, a large dataset of outdoor imagery from urban, park, and suburban environments that was curated in-house. The model is capable of detecting the depth of various obstacles, including people, vehicles, posts, and more. The depth maps are converted into 3D point clouds, similar to the line segmentation process, and used to detect the presence of obstacles along the user&#39;s path and then alert the user through an audio signal. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/s800/image2.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; height=&quot;480&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/w640-h480/image2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using a monocular depth ML model, Project Guideline constructs a 3D point cloud of the environment to detect and alert the user of potential obstacles along the path .&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A low-latency audio system based on the &lt;a href=&quot;https://developer.android.com/ndk/guides/audio/ aaudio/aaudio&quot;>;AAudio API&lt;/a>; was implemented to provide the navigational sounds and cues to the user. Several &lt;em>;sound packs&lt;/em>; are available in Project Guideline, including a spatial sound implementation using the &lt;a href=&quot;https://resonance-audio.github.io/resonance-audio/&quot;>;Resonance Audio API&lt; /a>;. The sound packs were developed by a team of sound researchers and engineers at Google who designed and tested many different sound models. The sounds use a combination of panning, pitch, and spatialization to guide the user along the line. For example, a user veering to the right may hear a beeping sound in the left ear to indicate the line is to the left, with increasing frequency for a larger course correction. If the user veers further, a high-pitched warning sound may be heard to indicate the edge of the path is approaching. In addition, a clear “stop” audio cue is always available in the event the user veers too far from the line, an anomaly is detected, or the system fails to provide a navigational signal. &lt;/p>; &lt;p>; Project Guideline has been built specifically for Google Pixel phones with the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Google Tensor&lt;/a>; chip. The Google Tensor chip enables the optimized ML models to run on-device with higher performance and lower power consumption. This is critical for providing real-time navigation instructions to the user with minimal delay. On a Pixel 8 there is a 28x latency improvement when running the depth model on the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Tensor Processing Unit&lt;/a>; (TPU) instead of CPU, and 9x improvement compared to GPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s1124/image3 .jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;748&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Testing and simulation&lt;/h2>; &lt;p>; Project Guideline includes a simulator that enables rapid testing and prototyping of the system in a virtual environment. Everything from the ML models to the audio feedback system runs natively within the simulator, giving the full Project Guideline experience without needing all the hardware and physical environment set up. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/s1200/image4.jpg&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1200&quot; height=&quot;480&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/w640-h480/image4.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Screenshot of Project Guideline simulator.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line- height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; To launch the technology forward, &lt;a href=&quot;https://www.wear.works&quot;>;WearWorks &lt;/a>; has become an early adopter and teamed up with Project Guideline to integrate their patented haptic navigation experience, utilizing haptic feedback in addition to sound to guide runners. WearWorks has been developing haptics for over 8 years, and previously empowered the first blind marathon runner to complete the NYC Marathon without sighted assistance. We hope that integrations like these will lead to new innovations and make the world a more accessible place.&lt;br />; &lt;/p>; &lt;p>; The Project Guideline team is also working towards removing the painted line completely, using the latest advancements in mobile ML technology, such as the &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;ARCore Scene Semantics API&lt;/a>;, which can identify sidewalks, buildings, and other objects in outdoor scenes. We invite the accessibility community to build upon and improve this technology while exploring new use cases in other fields. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of Project Guideline and the technologies behind it. We&#39;d like to thank Project Guideline team members: Dror Avalon, Phil Bayer, Ryan Burke, Lori Dooley, Song Chun Fan, Matt Hall, Amélie Jean-aimée, Dave Hawkey, Amit Pitaru, Alvin Shi, Mikhail Sirotenko, Sagar Waghmare, John Watkinson, Kimberly Wilber, Matthew Willson, Xuan Yang, Mark Zarich, Steven Clark, Jim Coursey, Josh Ellis, Tom Hoddes, Dick Lyon, Chris Mitchell, Satoru Arao, Yoojin Chung, Joe Fry, Kazuto Furuichi, Ikumi Kobayashi, Kathy Maruyama, Minh Nguyen, Alto Okamura, Yosuke Suzuki, and Bryan Tanaka. Thanks to ARCore contributors: Ryan DuToit, Abhishek Kar, and Eric Turner. Thanks to Alec Go, Jing Li, Liviu Panait, Stefano Pellegrini, Abdullah Rashwan, Lu Wang, Qifei Wang, and Fan Yang for providing ML platform support. We&#39;d also like to thank Hartwig Adam, Tomas Izo, Rahul Sukthankar, Blaise Aguera y Arcas, and Huisheng Wang for their leadership support. Special thanks to our partners Guiding Eyes for the Blind and Achilles International.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3970135078050750650/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html&quot; rel=&quot;alternate&quot; title=&quot;Open sourcing Project Guideline: A platform for computer vision accessibility technology&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6563276834599281497&lt;/id>;&lt;published>;2023-11-17T11:36:00.000-08:00&lt;/published>;&lt;updated>;2023-11-17T11:36:32.036-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research Awards&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;University Relations&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Emerging practices for Society-Centered AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Anoop Sinha, Research Director, Technology &amp;amp; Society, and Yossi Matias, Vice President, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The first of &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google&#39;s AI Principles&lt;/a>; is to “Be socially beneficial.” As AI practitioners, we&#39;re inspired by the transformative potential of AI technologies to benefit society and our shared environment at a scale and swiftness that wasn&#39;t possible before. From &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;helping address the climate crisis&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/how-were-using-ai-to-help-transform-healthcare/&quot;>;helping transform healthcare&lt;/a>;, to &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/global-accessibility-awareness-day-google-product-update/&quot;>;making the digital world more accessible&lt;/a>;, our goal is to apply AI responsibly to be helpful to more people around the globe. Achieving global scale requires researchers and communities to think ahead — and act — collectively across the AI ecosystem. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We call this approach Society-Centered AI. It is both an extension and an expansion of &lt;a href=&quot;https://hcil.umd.edu/human-centered-ai/&quot;>;Human-Centered AI, &lt;/a>;focusing on the aggregate needs of society that are still informed by the needs of individual users, specifically within the context of the larger, shared human experience. Recent AI advances offer unprecedented, societal-level capabilities, and we can now methodically address those needs — if we apply collective, multi-disciplinary AI research to society-level, shared challenges, from forecasting hunger to predicting diseases to improving productivity. &lt;/p>; &lt;p>; The &lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/AI_Opportunity_Agenda.pdf&quot;>;opportunity for AI&lt;/a>; to benefit society increases each天。 We took a look at our work in these areas and at the research projects we have supported. Recently, Google &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;announced that 70 professors were selected&lt;/a>; for the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;2023 Award for Inclusion Research Program&lt;/a>;, which supports academic research that addresses the needs of historically marginalized groups globally. Through evaluation of this work, we identified a few emerging practices for Society-Centered AI: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Understand society&#39;s needs&lt;/strong>; &lt;br />;Listening to communities and partners is crucial to understanding major issues deeply and identifying priority challenges to address. As an emerging general purpose technology, AI has the potential to address major global societal issues that can significantly impact people&#39;s lives (eg, educating workers, improving healthcare, and improving productivity). We have found the key to impact is to be centered on society&#39;s needs. For this, we focus our efforts on goals society has agreed should be prioritized, such as the United Nations&#39; &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/globalgoals&quot;>;17 Sustainable Development Goals&lt;/a>;, a set of interconnected goals jointly developed by more than 190 countries to address global challenges. &lt;/li>;&lt;li>;&lt;strong>;Collective efforts to address those needs &lt;br />;&lt;/strong>;Collective efforts bring stakeholders (eg, local and academic communities, NGOs, private-public collaborations) into a joint process of design, development, implementation, and evaluation of AI technologies as they are being developed and deployed to address societal needs. &lt;/li>;&lt;li>;&lt;strong>;Measuring success by how well the effort addresses society&#39;s needs &lt;br />;&lt;/strong>;It is important and challenging to measure how well AI solutions address society&#39;s needs. In each of our cases, we identified primary and secondary indicators of impact that we optimized through our collaborations with stakeholders. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Why is Society-Centered AI important?&lt;/h2>; &lt;p>; The case examples described below show how the Society-Centered AI approach has led to impact across topics, such as accessibility, health, and climate. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Understanding the needs of individuals with non-standard speech&lt;/h3>; &lt;p>; There are &lt;a href=&quot;https://www.nidcd.nih.gov/health/statistics/quick-statistics-voice-speech-language&quot;>;millions of people&lt;/a>; with non-standard speech (eg, impaired articulation, &lt;a href=&quot;https://en.wikipedia.org/wiki/Dysarthria&quot;>;dysarthria&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Spasmodic_dysphonia&quot;>;dysphonia&lt;/a>;) in the United States alone. In &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/impaired-speech-recognition/&quot;>;2019&lt;/a>;, Google Research launched &lt;a href=&quot;https://blog.research.google/2019/08/project-euphonias-personalized-speech.html&quot;>;Project Euphonia&lt;/a>;, a methodology that allows individual users with non-standard speech to train personalized speech recognition models. Our success began with the impact we had on each individual who is now able to use voice dictation on their mobile device. &lt;/p>; &lt;p>; Euphonia started with a Society-Centered AI approach, including collective efforts with the non-profit organizations &lt;a href=&quot;http://als.net/&quot;>;ALS Therapy Development Institute&lt;/a>; and &lt;a href=&quot;http://www.alsri.org/&quot;>;ALS Residence Initiative&lt;/a>; to understand the needs of individuals with &lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;amyotrophic lateral sclerosis&lt;/a>; (ALS) and their ability to use automatic speech recognition systems. Later, we developed &lt;a href=&quot;https://blog.research.google/2021/09/personalized-asr-models-from-large-and.html&quot;>;the world&#39;s largest corpus&lt;/a>; of non-standard speech recordings, which enabled us to train a &lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; to &lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;better recognize disordered speech by 37%&lt;/a>; on real conversation &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;word error rate&lt;/a>; (WER) measurement. This also led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/speech-accessibility-project/&quot;>;2022&lt;/a>; collaboration between the University of Illinois Urbana-Champaign, Alphabet, Apple, Meta, Microsoft, and Amazon to begin the &lt;a href=&quot;https://speechaccessibilityproject.beckman.illinois.edu/&quot;>;Speech Accessibility Project&lt;/a>;, an ongoing initiative to create a publicly available dataset of disordered speech samples to improve products and make speech recognition more inclusive of diverse speech patterns. Other technologies that use AI to help remove barriers of modality and languages, include &lt;a href=&quot;https://about.google/stories/making-conversation-more-accessible-with-live-transcribe/&quot;>;live transcribe&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/first-time-i-was-able-call-my-23-year-old-son/&quot;>;live caption&lt;/a>; and &lt;a href=&quot;https://blog.google/intl/en-in/products/explore-communicate/easier-access-to-web-pages-let/&quot;>;read aloud&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Focusing on society&#39;s health needs&lt;/h3>; &lt;p>; Access to timely maternal health information can save lives globally: &lt;a href=&quot;https://www.who.int/news/item/23-02-2023-a-woman-dies-every-two-minutes-due-to-pregnancy-or-childbirth--un-agencies&quot;>;every two minutes a woman dies during pregnancy or childbirth&lt;/a>; and &lt;a href=&quot;https://data.unicef.org/topic/child-survival/under-five-mortality/&quot;>;1 in 26 children die before reaching age five&lt;/a>;. In rural India, the education of expectant and new mothers around key health issues pertaining to pregnancy and infancy required scalable, low-cost technology solutions. Together with &lt;a href=&quot;https://armman.org/&quot;>;ARMMAN&lt;/a>;, Google Research supported &lt;a href=&quot;https://blog.research.google/2022/08/using-ml-to-boost-engagement-with.html&quot;>;a program&lt;/a>; that uses mobile messaging and machine learning (ML) algorithms to predict when women might benefit from receiving interventions (ie, targeted preventative care information) and encourages them to engage with the &lt;a href=&quot;https://armman.org/mmitra/&quot;>;mMitra&lt;/a>; free voice call program. Within a year, the mMitra program has shown a 17% increase in infants with tripled birth weight and a 36% increase in women understanding the importance of taking iron tablets during pregnancy. Over 175K mothers and growing have been reached through this automated solution, which public health workers use to improve the quality of information delivery. &lt;/p>; &lt;p>; These efforts have been successful in improving health due to the close collective partnership among the community and those building the AI technology. We have adopted this same approach via collaborations with caregivers to address a variety of medical needs. Some examples include: the use of the &lt;a href=&quot;https://health.google/caregivers/arda/&quot;>;Automated Retinal Disease Assessment&lt;/a>; (ARDA) to &lt;a href=&quot;https://blog.google/technology/health/5-myths-about-medical-ai-debunked/&quot;>;help screen for diabetic retinopathy&lt;/a>; in 250,000 patients in clinics around the world; our partnership with &lt;a href=&quot;https://www.icadmed.com/&quot;>;iCAD&lt;/a>; to bring our &lt;a href=&quot;https://blog.google/technology/ai/icad-partnership-breast-cancer-screening/&quot;>;mammography&lt;/a>; AI models to clinical settings to aid in breast cancer detection; and the development of &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>;, a medical large language model that is now being &lt;a href=&quot;https: //cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model&quot;>;tested with Cloud partners&lt;/a>; to help doctors provide better病人护理。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Compounding impact from sustained efforts for crisis response&lt;/h3>; &lt;p>; Google Research&#39;s flood prediction efforts began in 2018 with &lt;a href=&quot;https://www.blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;flood forecasting&lt;/a>; in India and &lt;a href=&quot;https://www.undp.org/bangladesh/blog/climate-change-google-and-bangladesh-floods&quot;>;expanded to Bangladesh&lt;/a>; to help combat the catastrophic damage from yearly floods. The initial efforts began with partnerships with &lt;a href=&quot;https://cwc.gov.in/&quot;>;India&#39;s Central Water Commission&lt;/a>;, local governments and communities. The implementation of these efforts used &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>; on Search and Maps, and, more recently, broadly expanded access via &lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>;. Continued &lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;collaborations&lt;/a>; and advancing an AI-based global flood forecasting model allowed us to &lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;expand this capability&lt;/a>; to &lt;a href=&quot;https://blog .google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;over 80 countries&lt;/a>; across Africa, the Asia-Pacific region, Europe, and South, Central, and North美国。 We also partnered with networks of community volunteers to further amplify flood alerts. By working with governments and communities to measure the impact of these efforts on society, we refined our approach and algorithms each year. &lt;/p>; &lt;p>; We were able to leverage those methodologies and some of the underlying technology, such as &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>;, from &lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;flood forecasting&lt;/a>; to similar societal needs, such as &lt;a href=&quot;https://blog.google/products/search/mapping-wildfires-with-satellite-data/&quot;>;wildfire forecasting&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;heat alerts&lt;/a>;. Our continued engagements with organizations led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/early-warning-system-wmo-google/&quot;>;support of additional efforts&lt;/a>;, such as the World Meteorological Organization&#39;s (WMO) &lt;a href=&quot;https://public.wmo.int/en/earlywarningsforall&quot;>;Early Warnings For All Initiative&lt;/a>;. The continued engagement with communities has allowed us to learn about our users&#39; needs on a societal level over time, expand our efforts, and compound the societal reach and impact of our efforts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Further supporting Society-Centered AI research&lt;/h2>; &lt;p>; &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;We recently funded&lt;/a>; 18 university research proposals exemplifying a Society-Centered AI approach, a new track within the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;Google Award for Inclusion Research Program&lt;/a>;. These researchers are taking the Society-Centered AI methodology and helping create beneficial applications across the world. Examples of some of the projects funded include: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;AI-Driven Monitoring of Attitude Polarization in Conflict-Affected Countries for Inclusive Peace Process and Women&#39;s Empowerment:&lt;/strong>; This project&#39;s goal is to create LLM-powered tools that can be used to monitor peace in online conversations in developing nations. The initial target communities are where peace is in flux and the effort will put a particular emphasis on mitigating polarization that impacts women and promoting harmony.&lt;/li>; &lt;li>;&lt;strong>;AI-Assisted Distributed Collaborative Indoor Pollution Meters: A Case Study , Requirement Analysis, and Low-Cost Healthy Home Solution for Indian Communities: &lt;/strong>;This project is looking at the usage of low-cost pollution monitors combined with AI-assisted methodology for identifying recommendations for communities to improve air quality and at home健康。 The initial target communities are highly impacted by pollution, and the joint work with them includes the goal of developing how to measure improvement in outcomes in the local community. &lt;/li>; &lt;li>;&lt;strong>;Collaborative Development of AI Solutions for Scaling Up Adolescent Access to Sexual and Reproductive Health Education and Services in Uganda: &lt;/strong>;This project&#39;s goal is to create LLM-powered tools to provide personalized coaching and learning for users&#39; needs on topics of sexual and reproductive health education in low-income settings in Sub-Saharan Africa. The local societal need is significant, with an estimated &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9652643/&quot;>;25% rate of teenage pregnancy&lt;/a>;, and the project aims to address the needs with a collective development process for the AI solution.&lt;strong>; &lt;/strong>; &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; Focusing on society&#39;s needs, working via multidisciplinary collective research, and measuring the impact on society helps lead to AI solutions that are relevant, long-lasting, empowering, and beneficial. See the &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/&quot;>;AI for the Global Goals&lt;/a>; to learn more about potential Society-Centered AI research problems. &lt;a href=&quot;https://blog.google/outreach-initiatives/google-org/httpsbloggoogleoutreach-initiativesgoogle-orgunited-nations-global-goals-google-ai-/&quot;>;Our efforts&lt;/a>; with non-profits in these areas is complementary to the research that we are doing and encouraging. We believe that further initiatives using Society-Centered AI will help the collective research community solve problems and positively impact society at large. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many thanks to the many individuals who have worked on these projects at Google including Shruti Sheth, Reena Jana, Amy Chung-Yu Chou, Elizabeth Adkison, Sophie Allweis, Dan Altman, Eve Andersson, Ayelet Benjamini&lt;/em>;, &lt;em>;Julie Cattiau, Yuval Carny, Richard Cave, Katherine Chou, Greg Corrado, Carlos De Segovia, Remi Denton, Dotan Emanuel, Ashley Gardner, Oren Gilon, Taylor Goddu, Brigitte Hoyer Gosselink, Jordan Green, Alon Harris&lt;/em>;, &lt;em>;Avinatan Hassidim, Rus Heywood, Sunny Jansen, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Ronit Levavi Morad, Bob MacDonald, Alicia Martin, Shakir Mohamed, Philip Nelson, Moriah Royz, Katie Seaver, Joel Shor, Milind Tambe, Aparna Taneja, Divy Thakkar, Jimmy Tobin, Katrin Tomanek, Blake Walsh, Gal Weiss, Kasumi Widner, Lihong Xi, and teams.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6563276834599281497/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot; rel=&quot;alternate&quot; title=&quot;Emerging practices for Society-Centered AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6928081948246813203&lt;/id>;&lt;published>;2023-11-16T13:11:00.000-08:00&lt;/published>;&lt;updated>;2023-12-07T08:16:11.190-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Adversarial testing for generative AI safety&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kathy Meier-Hellstern, Building Responsible AI &amp;amp; Data Systems, Director, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The&lt;em>; &lt;/em>;&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; (RAI-HCT) team within Google Research is committed to advancing the theory and practice of responsible human-centered AI through a lens of culturally-aware research, to meet the needs of billions of users today, and blaze the path forward for a better AI future. The BRAIDS (Building Responsible AI Data and Solutions) team within RAI-HCT aims to simplify the adoption of RAI practices through the utilization of scalable tools, high-quality data, streamlined processes, and novel research with a current emphasis on addressing the unique challenges posed by &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>; (GenAI). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; GenAI models have enabled unprecedented capabilities leading to a rapid surge of innovative applications. Google actively leverages &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;GenAI&lt;/a>; to &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview&quot;>;enhance&lt;/a>; its &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai-in-workspace-now-available&quot;>;products&#39; utility&lt;/a>; and to improve lives. While enormously beneficial, GenAI also presents risks for disinformation, bias, and security. In 2018, Google pioneered the &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>;, emphasizing beneficial use and prevention of harm. Since then, Google has focused on effectively implementing our principles in &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; through 1) a comprehensive risk assessment framework, 2) internal governance structures, 3) education, empowering Googlers to integrate AI Principles into their work, and 4) the development of processes and tools that identify, measure, and analyze ethical risks throughout the lifecycle of AI-powered products. The BRAIDS team focuses on the last area, creating tools and techniques for identification of ethical and safety risks in GenAI products that enable teams within Google to apply appropriate mitigations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;What makes GenAI challenging to build responsibly?&lt;/h2>; &lt;p>; The unprecedented capabilities of GenAI models have been accompanied by a new spectrum of potential failures, underscoring the urgency for a comprehensive and systematic RAI approach to understanding and mitigating potential safety concerns before the model is made broadly available. One key technique used to understand potential risks is &lt;em>;adversarial testing&lt;/em>;, which is testing performed to systematically evaluate the models to learn how they behave when provided with malicious or inadvertently harmful inputs across a range of scenarios. To that end, our research has focused on three directions: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Scaled adversarial data generation&lt;/em>;&lt;br />; Given the diverse user communities, use cases, and behaviors, it is difficult to comprehensively identify critical safety issues prior to launching a product or service. Scaled adversarial data generation with humans-in-the-loop addresses this need by creating test sets that contain a wide range of diverse and potentially unsafe model inputs that stress the model capabilities under adverse circumstances. Our unique focus in BRAIDS lies in identifying societal harms to the diverse user communities impacted by our models. &lt;/li>; &lt;li>;&lt;em>;Automated test set evaluation and community engagement&lt;/em>;&lt;br />; Scaling the testing process so that many thousands of model responses can be quickly evaluated to learn how the model responds across a wide range of potentially harmful scenarios is aided with automated test set evaluation. Beyond testing with adversarial test sets, community engagement is a key component of our approach to identify “unknown unknowns” and to seed the data generation process.&lt;/li>; &lt;li>;&lt;em>;Rater diversity&lt;/em>;&lt;br />; Safety evaluations rely on human judgment, which is shaped by community and culture and is not easily automated. To address this, we prioritize research on rater diversity.&lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scaled adversarial data generation&lt;/h2>; &lt;p>; High-quality, comprehensive data underpins many key programs across Google. Initially reliant on manual data generation, we&#39;ve made significant strides to automate the adversarial data generation process. A centralized data repository with use-case and policy-aligned prompts is available to jump-start the generation of new adversarial tests. We have also developed multiple synthetic data generation tools based on large language models (LLMs) that prioritize the generation of data sets that reflect diverse societal contexts and that integrate data quality metrics for improved dataset quality and diversity. &lt;/p>; &lt;p>; Our data quality metrics include: &lt;/p>; &lt;ul>; &lt;li>;Analysis of language styles, including query length, query similarity, and diversity of language styles.&lt;/li>; &lt;li>;Measurement across a wide range of societal and multicultural dimensions, leveraging datasets such as &lt;a href=&quot;https://github.com/google-research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>;, &lt;a href=&quot;https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;, the &lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;Societal Context Repository&lt;/a>;.&lt;/li>; &lt;li>;Measurement of alignment with Google&#39;s &lt;a href=&quot;https://policies.google.com/terms/generative-ai/use-policy&quot;>;generative AI policies&lt;/a>; and intended use cases.&lt;/li>; &lt;li>;Analysis of adversariality to ensure that we examine both explicit (the input is clearly designed to produce an unsafe output) and implicit (where the input is innocuous but the output is harmful) queries. &lt;/li>; &lt;/ul>; &lt;p>; One of our approaches to scaled data generation is exemplified in our paper on &lt;a href=&quot;https://arxiv.org/abs/2311.08592&quot;>;AI-Assisted Red Teaming&lt;/a>; (AART). AART generates evaluation datasets with high diversity (eg, sensitive and harmful concepts specific to a wide range of cultural and geographic regions), steered by AI-assisted recipes to define, scope and prioritize diversity within an application context. Compared to some state-of-the-art tools, AART shows promising results in terms of concept coverage and data quality. Separately, we are also working with MLCommons to contribute to &lt;a href=&quot;https://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot;>;public benchmarks for AI Safety&lt;/一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Adversarial testing and community insights &lt;/h2>; &lt;p>; Evaluating model output with adversarial test sets allows us to identify critical safety issues prior to deployment. Our initial evaluations relied exclusively on human ratings, which resulted in slow turnaround times and inconsistencies due to a lack of standardized safety definitions and policies. We have improved the quality of evaluations by introducing policy-aligned rater guidelines to improve human rater accuracy, and are researching additional improvements to better reflect the perspectives of diverse communities. Additionally, automated test set evaluation using LLM-based auto-raters enables efficiency and scaling, while allowing us to direct complex or ambiguous cases to humans for expert rating. &lt;/p>; &lt;p>; Beyond testing with adversarial test sets, gathering community insights is vital for continuously discovering “unknown unknowns”. To provide high quality human input that is required to seed the scaled processes, we partner with groups such as the &lt;a href=&quot;https://sites.google.com/corp/google.com/earr-external-research-group/home&quot;>;Equitable AI Research Round Table&lt;/a>; (EARR), and with our internal ethics and analysis teams to ensure that we are representing the diverse communities who use our models. The &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;Adversarial Nibbler Challenge&lt;/a>; engages external users to understand potential harms of &lt;a href=&quot;https://arxiv.org/abs/2305.14384&quot;>;unsafe, biased or violent outputs&lt;/a>; to end users at scale. Our continuous commitment to community engagement includes gathering feedback from diverse communities and collaborating with the research community, for example during &lt;a href=&quot;https://sites.google.com/view/art-of-safety&quot;>;The ART of Safety workshop&lt;/a>; at the &lt;a href=&quot;http://www.ijcnlp-aacl2023.org/&quot;>;Asia-Pacific Chapter of the Association for Computational Linguistics Conference&lt;/a>; (IJCNLP-AACL 2023) to address adversarial testing challenges for GenAI. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Rater diversity in safety evaluation&lt;/h2>; &lt;p>; Understanding and mitigating GenAI safety risks is both a technical and social challenge. Safety perceptions are intrinsically subjective and influenced by a wide range of intersecting factors. Our in-depth study on demographic influences on safety perceptions explored the &lt;a href=&quot;https://arxiv.org/abs/2306.11530&quot;>;intersectional effects of rater demographics&lt;/a>; (eg, race/ethnicity, gender, age) and content characteristics (eg, degree of harm) on safety assessments of GenAI outputs. Traditional approaches largely ignore inherent subjectivity and the systematic disagreements among raters, which can mask important cultural differences. Our &lt;a href=&quot;https://arxiv.org/abs/2311.05074&quot;>;disagreement analysis framework&lt;/a>; surfaced a variety of disagreement patterns between raters from diverse backgrounds including also with “ground truth” expert ratings. This paves the way to new approaches for assessing quality of human annotation and model evaluations beyond the simplistic use of gold labels. Our &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;NeurIPS 2023 publication&lt;/a>; introduces the &lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset&quot;>;DICES&lt;/a>; (Diversity In Conversational AI Evaluation for Safety) dataset that facilitates nuanced safety evaluation of LLMs and accounts for variance, ambiguity, and diversity in various cultural contexts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Summary&lt;/h2>; &lt;p>; GenAI has resulted in a technology transformation, opening possibilities for rapid development and customization even without coding. However, it also comes with a risk of generating harmful outputs. Our proactive adversarial testing program identifies and mitigates GenAI risks to ensure inclusive model behavior. Adversarial testing and red teaming are essential components of a Safety strategy, and conducting them in a comprehensive manner is essential. The rapid pace of innovation demands that we constantly challenge ourselves to find “unknown unknowns” in cooperation with our internal partners, diverse user communities, and other industry experts. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6928081948246813203/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Adversarial testing for generative AI safety&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1338549955376716163&lt;/id>;&lt;published>;2023-11-14T12:28:00.000-08:00&lt;/published>;&lt;updated>;2023-11-14T14:09:51.250-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Scaling multimodal understanding to long videos&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Isaac Noble, Software Engineer, Google Research, and Anelia Angelova, Research Scientist, Google DeepMind &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s1600/mirasol.png&quot; style=&quot;display: none;&quot; />; &lt;p>; When building machine learning models for real-life applications, we need to consider inputs from multiple modalities in order to capture various aspects of the world around us. For example, audio, video, and text all provide varied and complementary information about a visual input. However, building multimodal models is challenging due to the heterogeneity of the modalities. Some of the modalities might be well synchronized in time (eg, audio, video) but not aligned with text. Furthermore, the large volume of data in video and audio signals is much larger than that in text, so when combining them in multimodal models, video and audio often cannot be fully consumed and need to be disproportionately compressed. This problem is exacerbated for longer video inputs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2311.05698&quot;>;Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities&lt;/a>;”, we introduce a multimodal &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive&lt;/a>; model (Mirasol3B) for learning across audio, video, and text方式。 The main idea is to decouple the multimodal modeling into separate focused autoregressive models, processing the inputs according to the characteristics of the modalities. Our model consists of an autoregressive component for the time-synchronized modalities (audio and video) and a separate autoregressive component for modalities that are not necessarily time-aligned but are still sequential, eg, text inputs, such as a title or description. Additionally, the time-aligned modalities are partitioned in time where local features can be jointly learned. In this way, audio-video inputs are modeled in time and are allocated comparatively more parameters than prior works. With this approach, we can effortlessly handle much longer videos (eg, 128-512 frames) compared to other multimodal models. At 3B parameters, Mirasol3B is compact compared to prior &lt;a href=&quot;https://arxiv.org/abs/2204.14198&quot;>;Flamingo&lt;/a>; (80B) and &lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>;PaLI-X&lt;/a>; (55B) models. Finally, Mirasol3B outperforms the state-of-the-art approaches on &lt;a href=&quot;https://blog.research.google/2022/08/efficient-video-text-learning-with.html&quot;>;video question answering&lt;/a>; (video QA), long video QA, and audio-video-text benchmarks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s1240/image5.gif &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1240&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Mirasol3B architecture consists of an autoregressive model for the time-aligned modalities (audio and video), which are partitioned in chunks, and a separate autoregressive model for the unaligned context modalities (eg, text). Joint feature learning is conducted by the Combiner, which learns compact but sufficiently informative features, allowing the processing of long video/audio inputs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Coordinating time-aligned and contextual modalities&lt;/h2>; &lt;p>; Video, audio and text are diverse modalities with distinct characteristics. For example, video is a spatio-temporal visual signal with 30–100 frames per second, but due to the large volume of data, typically only 32–64 frames &lt;em>;per video&lt;/em>; are consumed by current models. Audio is a one-dimensional temporal signal obtained at much higher frequency than video (eg, at 16 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hertz&quot;>;Hz&lt;/a>;), whereas text inputs that apply to the whole video, are typically 200–300 word-sequence and serve as a context to the audio-video inputs. To that end, we propose a model consisting of an autoregressive component that fuses and jointly learns the time-aligned signals, which occur at high frequencies and are roughly synchronized, and another autoregressive component for processing non-aligned signals. Learning between the components for the time-aligned and contextual modalities is coordinated via cross-attention mechanisms that allow the two to exchange information while learning in a sequence without having to synchronize them in time. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Time-aligned autoregressive modeling of video and audio&lt;/h2>; &lt;p>; Long videos can convey rich information and activities happening in a sequence. However, present models approach video modeling by extracting all the information at once, without sufficient temporal information. To address this, we apply an autoregressive modeling strategy where we condition jointly learned video and audio representations for one time interval on feature representations from previous time intervals. This preserves temporal information. &lt;/p>; &lt;p>; The video is first partitioned into smaller video chunks. Each chunk itself can be 4–64 frames. The features corresponding to each chunk are then processed by a learning module, called the Combiner (described below), which generates a joint audio and video feature representation at the current step — this step extracts and compacts the most important information per chunk. Next, we process this joint feature representation with an autoregressive Transformer, which applies attention to the previous feature representation and generates the joint feature representation for the next step. Consequently, the model learns how to represent not only each individual chunk, but also how the chunks relate temporally. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s1259/image1.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;744&quot; data-original-width=&quot;1259&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use an autoregressive modeling of the audio and video inputs, partitioning them in time and learning joint feature representations, which are then autoregressively learned in sequence.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Modeling long videos with a modality combiner&lt;/h2>; &lt; p>; To combine the signals from the video and audio information in each video chunk, we propose a learning module called the Combiner. Video and audio signals are aligned by taking the audio inputs that correspond to a specific video timeframe. We then process video and audio inputs spatio-temporally, extracting information particularly relevant to &lt;em>;changes in the inputs&lt;/em>; (for videos we use &lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;sparse video tubes&lt;/a>;, and for audio we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;>;spectrogram&lt;/a>; representation, both of which are processed by a &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer)&lt;/a>;. We concatenate and input these features to the Combiner, which is designed to learn a new feature representation capturing both these inputs. To address the challenge of the large volume of data in video and audio signals, another goal of the Combiner is to reduce the dimensionality of the joint video/audio inputs, which is done by selecting a smaller number of output features to be produced. The Combiner can be implemented simply as a causal Transformer, which processes the inputs in the direction of time, ie, using only inputs of the prior steps or the current one. Alternatively, the Combiner can have a learnable memory, described below. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Combiner styles&lt;/h2>; &lt;p>; A simple version of the Combiner adapts a Transformer architecture. More specifically, all audio and video features from the current chunk (and optionally prior chunks) are input to a Transformer and projected to a lower dimensionality, ie, a smaller number of features are selected as the output “combined” features. While Transformers are not typically used in this context, we find it effective for reducing the dimensionality of the input features, by selecting the last &lt;em>;m&lt;/em>; outputs of the Transformer, if &lt;em>;m&lt;/em>; is the desired output dimension (shown below). Alternatively, the Combiner can have a memory component. For example, we use the &lt;a href=&quot;https://arxiv.org/abs/2211.09119&quot;>;Token Turing Machine&lt;/a>; (TTM), which supports a differentiable memory unit, accumulating and compressing features from all previous timesteps 。 Using a fixed memory allows the model to work with a more compact set of features at every step, rather than process all the features from previous steps, which reduces computation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s1798/image8.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;619&quot; data-original-width=&quot;1798&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a simple Transformer-based Combiner (&lt;b>;left&lt;/b>;) and a Memory Combiner (&lt;b>;right&lt;/b>;) , based on the Token Turing Machine (TTM), which uses memory to compress previous history of features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our approach on several benchmarks, &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp- content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1906.02467&quot;>;ActivityNet-QA &lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>;, for the video QA task, where a text-based question about a video is issued and the model needs to answer. This evaluates the ability of the model to understand both the text-based question and video content, and to form an answer, focusing on only relevant information. Of these benchmarks, the latter two target long video inputs and feature more complex questions. &lt;/p>; &lt;p>; We also evaluate our approach in the more challenging open-ended text generation setting, wherein the model generates the answers in an unconstrained fashion as free form text, requiring an exact match to the ground truth answer. While this stricter evaluation counts synonyms as incorrect, it may better reflect a model&#39;s ability to generalize. &lt;/p>; &lt;p>; 我们的结果表明，对于大多数基准测试（包括所有开放式生成评估），其性能均优于最先进的方法 - 值得注意的是，考虑到我们的模型只有 3B 参数，比以前的方法小得多，例如，火烈鸟80B。我们仅使用视频和文本输入来与其他工作进行比较。重要的是，我们的模型可以处理 512 帧，而无需增加模型参数，这对于处理更长的视频至关重要。最后，通过 TTM 组合器，我们看到了更好或相当的性能，同时减少了 18% 的计算量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhy phenFRP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s1200/image3.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZG dSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/ 上的结果06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>;（视频 QA）数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center “ cellpadding =“0”cellspacing =“ 0”类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6 IKiqsySkDDyvlBFiy3VkmcDRWJ- WRoZ-c7Tl-fozWXSEkznQSmJland23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s1200/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;860&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJland2 3SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;&lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>; 基准测试的结果，其中包含用于视频 QA 任务的长视频。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;音频视频基准测试结果&lt;/h2>; &lt;p>; 流行音频结果-视频数据集&lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG-Sound&lt;/a>;和&lt;a href=&quot;https://epic- kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; 如下所示。由于这些基准仅用于分类，因此我们将它们视为开放式文本生成设置，我们的模型在其中生成想要的班级；例如，对于与“打鼓”活动对应的类ID，我们期望模型生成文本“打鼓”。在某些情况下，即使我们的模型在生成开放式设置中输出结果，我们的方法仍大幅优于现有技术。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLA bdnYOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s1200/image6.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDu Qz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s16000/image6.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG 上的结果-声音&lt;/a>;（音频-视频QA）数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9- 4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4 v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s1200/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfP tEWi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>;（音频-视频 QA）数据集的结果。&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;自回归建模的好处&lt;/h2>; &lt;p>; 我们进行了消融研究将我们的方法与使用相同输入信息但使用标准方法（即没有自回归和组合器）的一组基线进行比较。我们还比较了预训练的效果。因为标准方法不适合处理更长的时间视频中，该实验仅针对 32 帧和 4 个块进行，在所有设置中进行公平比较。我们看到 Mirasol3B 的改进对于相对较短的视频仍然有效。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQI ZHuSeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s1200/image4.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEv PdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;比较我们模型的主要组成部分的消融实验。使用组合器、自回归建模和预训练都可以提高性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt; /div>; &lt;h2>;结论 &lt;/h2>; &lt;p>; 我们提出了一种多模态自回归模型，该模型通过协调时间对齐模态和时间未对齐模态之间的学习来解决与多模态数据异质性相关的挑战。使用组合器对时间对齐模态进行进一步的时间自回归处理，控制序列长度并产生强大的表示。我们证明了一个相对较小的模型可以成功地表示长视频并有效地与其他模式结合。我们在视频和音频视频问答方面优于最先进的方法（包括一些更大的模型）。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 AJ Piergiovanni、Isaac 共同撰写诺布尔、金大勋、迈克尔·柳、维克多·戈麦斯和阿内莉亚·安吉洛娃。我们感谢 Claire Cui、Tania Bedrax-Weiss、Abhijit Ogale、Yunhsuan Sung、Ching-Chung Chang、Marvin Ritter、Kristina Toutanova、Ming-Wei Chang、Ashish Thapliyal、Xiyang Luo、Wei Cheng Kuo、Aren Jansen、Bryan Seybold、Ibrahim Alabdulmohsin、 Jialin Wu、Luke Friedman、Trevor Walker、Keerthana Gopalakrishnan、Jason Baldridge、Radu Soricut、Mojtaba Seyedhosseini、Alexander D&#39;Amour、Oliver Wang、Paul Natsev、Tom Duerig、Younghui Wu、Slav Petrov、Zoubin Ghahramani 的帮助和支持。我们还感谢汤姆·斯莫尔准备动画。 &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1338549955376716163/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/11/scaling-multimodal-understanding-to.html&quot; rel=&quot;alternate&quot; title=&quot;将多模态理解扩展到长视频&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name >;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel= “http://schemas.google.com/g/2005#thumbnail” src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>; &lt;/作者>;&lt;media：thumbnail height =“72”url =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9 XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s72-c/mirasol.png&quot; width=&quot;72 &quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签： blogger.com，1999：blog-8474926331452026626.post-5546775652504697591&lt;/id>;&lt;已发布>;2023-11-10T10:05:00.000-08:00&lt;/已发布>;&lt;更新>;2023-11-10T10:05:13.301 - 08:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“可访问性”>;&lt;/类别>;&lt;类别方案=“http://www.blogger. com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category schema =&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;为研究界实现大规模健康研究&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究中心软件工程师 Chintan Ghate 和研究工程师 Diana Mincu&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfK ZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s1100/MSSignals-Hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 作为消费技术，例如 &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/google-cloud-fitbit-haga-collaborate-on-pilot-heart-研究&quot;>;健身追踪器&lt;/a>;和&lt;a href=&quot;https://blog.google/products/pixel/health-ai-better-sleep/&quot;>;手机&lt;/a>;在健康方面的应用越来越广泛-相关数据收集，利用这些数据途径来研究和增进我们对医疗状况的理解的机会也同样如此。我们&lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research.html&quot;>;之前&lt;/a>;谈到了我们的工作如何探索这项技术的使用在慢性疾病，特别是多发性硬化症（MS）的背景下。这项工作利用了 &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies 平台&lt;/a>;，这是一个用于创建临床研究应用程序的开源平台，让任何人都可以更轻松地完成这项工作以可信且安全的方式开展自己的研究并收集高质量的医疗保健数据。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 今天，我们介绍通过扩展 FDA MyStudies 平台开发的设置，并演示如何使用它来建立数字健康研究。我们还展示了通过这个名为 MS Signals 的平台创建的探索性研究，其中包括一款针对多发性硬化症患者的症状跟踪应用程序。该应用程序的目标有两个：1) 确保 FDA MyStudies 平台的增强带来更简化的研究创建体验； 2) 了解如何使用新的数据收集机制彻底改变患者的慢性病管理和跟踪。我们在 &lt;a href=&quot;https://www.fda-mystudies-flutter&quot;>;FDA MyStudies 平台上&lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;开源&lt;/a>;我们的扩展。 apache.org/licenses/LICENSE-2.0&quot;>;Apache 2.0 许可证&lt;/a>;为社区提供资源以建立自己的研究。 &lt;/p>; &lt;br />; &lt;h2>;扩展 FDA MyStudies 平台&lt;/h2>; &lt;p>; 最初的 FDA MyStudies 平台允许人们配置自己的研究应用程序、管理参与者以及创建单独的 iOS 和 Android 应用程序。为了简化研究创建过程并确保提高研究参与度，我们进行了一些可访问性更改。一些主要改进包括：通过使用 &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>; 生成跨平台（iOS 和 Android）应用程序，Flutter 是 Google 的一个开源框架，用于构建来自单一代码库的多平台应用程序；简化的设置，以便用户可以快速建立他们的研究原型（大多数情况下一天之内）；最重要的是，强调可及性，以便听到不同患者的声音。可访问性增强包括对平台基础功能的更改以及 MS Signals 研究应用程序的特定研究设计的更改。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;多平台支持，快速原型制作&lt;/h3>; &lt;p>; 我们决定使用 Flutter因为这将是一个可以一次性生成 iOS 和 Android 应用程序的单点，从而减少支持多个平台所需的工作。 Flutter 还提供了&lt;a href=&quot;https://docs.flutter.dev/tools/hot-reload&quot;>;热重载&lt;/a>;，它允许开发者构建和加载。快速预览功能。应用程序中的设计系统利用此功能提供一个中心点，从该中心点进行品牌推广和设计。该应用程序的主题可以更改以匹配新研究的基调并立即预览。应用程序中的演示环境也利用此功能，允许开发人员在其计算机上本地模拟和预览调查问卷。根据我们的经验，这在与临床医生现场进行用户体验以及问题的格式和措辞 A/B 测试时节省了大量时间。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;系统辅助功能增强&lt;/h3>; &lt;p>; 为了提高平台对更多用户的辅助功能，我们做了一些可用性增强：&lt;/p>; &lt;ol>; &lt;li>;Light &amp;amp;深色主题支持&lt;/li>; &lt;li>;粗体文本和粗体文本可变字体大小&lt;/li>; &lt;li>;高对比度模式&lt;/li>; &lt;li>;提高用户对辅助功能设置的认识&lt;/li>; &lt;/ol>; &lt;p>;长时间暴露在强光主题下可能会使眼睛疲劳，因此，支持深色主题功能是必要的，以便更轻松地频繁使用学习应用程序。一些小或轻的文本元素对于视力障碍的用户来说是难以辨认的，因此我们添加了 1) 粗体文本和对较大字体大小的支持以及 2) 高对比度颜色方案。为了确保易于找到辅助功能设置，我们放置了一个在应用程序首次启动时出现的介绍性一次性屏幕，该屏幕将直接将用户带到他们的系统辅助功能设置。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;研究辅助功能增强&lt;/h3>; &lt;p>;使研究本身更易于交互并减少认知超载，我们进行了以下更改：&lt;/p>; &lt;ol>; &lt;li>;澄清了入职流程&lt;/li>; &lt;li>;改进了问卷设计&lt;/li>; &lt;/ol>; &lt;p>; 首先，我们澄清了通过在用户首次打开应用程序时向用户提供所需步骤列表来进行入职流程，以减少混乱和参与者流失。 &lt;/p>; &lt;p>; 应用中最初的问卷设计以卡片的形式呈现每个问题，利用屏幕的一部分来实现卡片的阴影和深度效果。在许多情况下，这是一种令人愉悦的美感，但在优先考虑可访问性的应用程序中，这些视觉元素限制了屏幕上的可用空间。因此，当使用更易于访问、更大的字体大小时，会出现更频繁的断词，从而降低了可读性。我们简单地通过删除卡片设计元素并使用整个屏幕来解决此问题，从而通过更大的字体大小获得更好的视觉效果。 &lt;/p>; &lt;br />; &lt;h2>;MS Signals 原型研究&lt;/h2>; &lt;p>; 为了测试这些更改的可用性，我们使用重新设计的平台创建了一个名为 MS Signals 的原型研究应用程序，该应用程序使用调查来收集有关参与者多发性硬化症相关症状的信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJC tOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/s1760/MSSignals.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1728&quot; data-original-width=&quot;1760&quot; height=&quot;628&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7Yp Q2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/w640-h628/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals 应用程序屏幕截图。&lt;/em>;&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;!--&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left:auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG- dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsils5Kf Od48G7wSSjvL5AYWbN9oiB4d79k​​7/s1999/image2.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1971&quot; data-original-width=&quot;1999&quot; height=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2 NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k​​7/w640-h632/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals 应用程序屏幕截图。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt; /table>;-->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MS Studies 应用程序设计&lt;/h3>; &lt;p>; 作为第一步，在输入任何内容之前为了了解研究信息，参与者需要填写一份资格和研究理解调查问卷，以确保他们已经阅读了可能冗长的研究参与条款。这可能包括诸如“该研究在哪个国家/地区可用？”或“你可以退学吗？”这样的部分在大多数健康研究中都很常见，而且往往是参与者的第一个下车点。 &lt;/p>; &lt;p>; 为了最大限度地减少早期阶段的学习流失，我们保持资格测试简短，并将理解测试的正确答案反映给参与者。这有助于最大限度地减少用户可能需要完成初始资格调查问卷的次数，并确保他们清楚了解研究方案的重要方面。 &lt;/p>; &lt;p>; 成功注册后，参与者将进入主应用程序视图，其中包含三个页面：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;活动：&lt;/strong>; &lt;br />;页面列出了参与者可用的调查问卷，并且是他们花费大部分时间的地方。调查问卷的频率各不相同——有些是为收集病史而创建的一次性调查，而另一些则每天、每周或每月重复一次，具体取决于他们正在探索的症状或领域。对于一次性调查，我们在每个问题上方提供一个计数器，向用户表明他们已经走了多远以及还剩下多少问题，类似于资格和理解步骤中的调查问卷。 &lt;/li>; &lt;li>;&lt;strong>;仪表板：&lt;/strong>; &lt;br />; 为了确保参与者在研究期间输入的信息得到回报，仪表板区域以图表或形式显示他们的回答摘要。饼图形式。参与者可以向他们的护理人员展示这些数据，作为他们过去 6 个月的病情总结，这比许多人目前采用的传统笔和纸方法有所改进。 &lt;/li>; &lt;li>;&lt;strong>;资源：&lt;/strong>; &lt;br />; 一组与 MS 相关的有用链接、帮助文章和常见问题。 &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;问卷设计&lt;/h3>; &lt;p>;由于需要频繁输入数据会导致针对认知超载、参与者流失和数据质量差的问题，我们通过两种方式减轻了负担：&lt;/p>; &lt;ol>; &lt;li>;我们将大型调查问卷分解为较小的调查问卷，形成 6 项每日调查，其中包含 3-5 个调查问卷每个问题都是多项选择，并且与单一症状相关。通过这种方式，我们总共涵盖了 20 种主要症状，并以与临床医生在诊所中询问这些问题类似的方式呈现它们。&lt;/li>; &lt;li>;我们确保之前输入的信息可以在应用程序，以及输入时间。&lt;/li>; &lt;/ol>; &lt;p>; 在设计调查内容时，我们与经验丰富的临床医生和研究人员密切合作，最终确定了措辞和布局。虽然该领域的研究通常使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale&quot;>;李克特量表&lt;/a>;来收集症状信息，但我们定义了更直观的详细量表以提供更好的体验供参与者跟踪其疾病以及临床医生或研究人员查看疾病史。例如，在视力问题的情况下，我们不是要求参与者按照从 1 到 10 的等级对他们的症状进行评分，而是提出一个多项选择问题，详细说明他们可能遇到的常见视力问题。 &lt;/p>; &lt;p>; 这种详细的量表可以帮助患者更准确地跟踪他们的症状，包括帮助他们更清楚地定义症状的上下文。这种方法还允许研究人员回答超出症状相关性的问题。例如，对于视力问题，使用详细量表收集的数据将向研究人员揭示&lt;a href=&quot;https://www.webmd.com/eye-health/nystagmus&quot;>;眼球震颤&lt;/a>;在患者中是否更为突出与MS相比复视。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7Fqxb SB1d3tg​​TOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/s1780/MSSignals-2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1754&quot; data-original-width=&quot;1780&quot; height=&quot;630&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tg​​TOB7fA7o6boudBmPz fiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/w640-h630/MSSignals-2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;与左侧的李克特量表并排比较，以及右侧详细比例。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl 9PF6mE4QFdFbMEJTNxEO连字符连字符ERminiUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_MLSJTH_LVGU91JgDqGeCr6r/ s1999/image1.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1901&quot; data-original-width=&quot;1999&quot; height=&quot;608&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEO连字符ERminiUNLDaTH5BMNBOOv7kOTQpsNj YqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/w640-h608/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;并排比较左侧为李克特量表，右侧为详细量表。&lt;/em>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;h2>;关注可访问性&lt;/h2>; &lt;p>;基于移动设备的研究通常会给患有慢性病的参与者带来额外的挑战：文本可能难以阅读，颜色对比可能会让人很难看到某些信息，或者滚动浏览页面可能会很困难。这可能会导致参与者流失，反过来，如果经历更多信息的人可能会产生有偏见的数据集疾病的晚期形式无法提供数据。 &lt;/p>; &lt;p>; 为了防止此类问题，我们提供了以下辅助功能：&lt;/p>; &lt;ul>; &lt;li>;自始至终，我们都采用色盲可访问的配色方案。这包括提高关键文本和重要附加信息之间的对比度，否则这些信息可能会以较小的字体和褪色的文本颜色呈现。&lt;/li>; &lt;li>;我们通过放置所有按钮来减少访问关键控件所需的移动量靠近页面底部，并确保可以从屏幕底部控制弹出窗口。&lt;/li>; &lt;/ul>; &lt;p>; 为了测试 MS Signals 的可访问性，我们与 &lt;a href= “https://www.nationalmssociety.org/&quot;>;国家多发性硬化症协会&lt;/a>;招募用户体验研究参与者。为此，协会向其会员发出了参与号召，并要求 9 名受访者测试各种应用程序流程。大多数人表示，他们希望有一种比当前方法更好的方法来跟踪症状数据，他们认为 MS 信号是一种独特且有价值的工具，可以提高症状跟踪的准确性，并且他们希望分享与他们的医疗保健提供者的仪表板视图。 &lt;/p>; &lt;br />; &lt;h2>;后续步骤&lt;/h2>; &lt;p>; 我们希望鼓励每个人都使用&lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter &quot;>;开源&lt;/a>;平台开始建立和运行自己的研究。我们正在努力创建一套标准研究模板，其中将包含我们从上面学到的知识，我们希望尽快发布这些模板。如有任何问题、意见或疑问，请查看我们的&lt;a href=&quot;https://goo.gle/ms-signals&quot;>;资源页面&lt;/a>;。 &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5546775652504697591/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/11/enabling-large-scale-health-studies-for.html&quot; rel=&quot;alternate&quot; title=&quot;为研究界实现大规模健康研究&quot; type=&quot;text/html &quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5 y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtalfcfJ9sKX/s72- c/MSSignals-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-7987071997778787277&lt;/id>;&lt;已发布>;2023-11-09T14:23:00.000-08:00&lt;/已发布>;&lt;更新>;2023-11-09T14:23:38.431-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Africa&quot;>;&lt;/category>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“健康”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=&quot;RAI-HCT 亮点&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 研究院的负责任人工智能：人工智能研究背景 (CAIR)&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot; byline-author&quot;>;由 Google Research 研究科学家 Katherine Heller 代表 CAIR 团队发布&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Z bj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s1100 /CAIR-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 人工智能 (AI) 和相关的机器学习 (ML) 技术在我们周围的世界中的影响力越来越大，因此我们必须考虑我们所创造的技术的各个方面对社会和个人的潜在影响。为此，人工智能研究情境 (CAIR) 团队在整个人工智能管道的背景下开发新颖的人工智能方法：&lt;strong>;&lt;/strong>;从数据到最终用户反馈。构建人工智能系统的流程通常从&lt;em>;数据&lt;/em>;收集开始，然后设计&lt;em>;模型&lt;/em>;在该数据上运行，&lt;em>;部署&lt;/em>;模型现实世界，最后，编译和合并&lt;em>;人类反馈&lt;/em>;。 CAIR 团队的工作起源于健康领域，现已扩展到其他领域，影响着该管道的各个方面。在专注于模型构建的同时，我们特别注重构建具有责任感的系统，包括公平性、稳健性、透明度和包容性。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC -Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteC yeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/s1050/CAIR.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;383&quot; data-original-width=&quot;1050 “高度=“234”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC-Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5 gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/w640-h234/CAIR.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;数据&lt;/h2>; &lt;p>; CAIR 团队专注于理解构建机器学习系统的数据。提高机器学习数据集透明度的标准对我们的工作很有帮助。首先，我们采用文档框架来阐明数据集和模型特征，作为数据和模型文档技术开发的指导 - &lt;a href=&quot;https://arxiv.org/abs/1803.09010&quot;>;数据集数据表&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/1810.03993&quot;>;用于模型报告的模型卡&lt;/a>;。 &lt;/p>; &lt;p>; 例如，健康数据集高度敏感，但影响很大。为此，我们开发了&lt;a href=&quot;https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533239&quot;>;健康表&lt;/a>;，这是数据表的健康情境改编版。我们开发健康特定表的动机在于现有人工智能和健康监管框架的局限性。 &lt;a href=&quot;https://www.nejm.org/doi/10.1056/NEJMp1816373&quot;>;最近的研究&lt;/a>;表明数据隐私监管和标准（例如，&lt;a href=&quot;https://en.wikipedia .org/wiki/Health_Insurance_Portability_and_Accountability_Act&quot;>;HIPAA&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/General_Data_Protection_Regulation&quot;>;GDPR&lt;/a>;、&lt;a href=&quot;https://en .wikipedia.org/wiki/California_Consumer_Privacy_Act&quot;>;加州消费者隐私法&lt;/a>;）不确保数据收集、记录和使用符合道德规范。 Healthsheets 旨在填补道德数据集分析中的这一空白。 Healthsheets 的开发是与相关工作角色的许多利益相关者合作完成的，包括临床、法律和监管、生物伦理、隐私和产品。 &lt;/p>; &lt;p>; 此外，我们研究了数据表和健康表如何作为诊断工具来揭示数据集的局限性和优势。我们的目标是在社区中发起对话，并根据不断变化的医疗保健场景定制健康表。 &lt;/p>; &lt;p>; 为了促进这项工作，我们加入了 &lt;a href=&quot;http://www.datadiversity.org/&quot;>;STANDING Together&lt;/a>; 倡议，这是一个旨在开发&lt;a href= &quot;https://www.nature.com/articles/s41591-022-01987-w&quot;>;基于共识的国际标准，用于记录健康数据集中的多样性和代表性&lt;/a>;，并就如何降低风险提供指导偏见转化为伤害和健康不平等。作为这一跨越全球学术、临床、监管、政策、行业、患者和慈善组织的国际跨学科合作伙伴关系的一部分，我们能够参与有关国际医疗保健人工智能责任的对话。来自 32 个国家/地区的 250 多个利益相关者为完善标准做出了贡献&lt;strong>;。&lt;/strong>; &lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0 swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s1050/Healthsheets.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-高度=“731”数据原始宽度=“1050”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe 9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s16000/Healthsheets.png&quot;/>;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;健康表和站立共同：迈向健康数据文档和标准。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;模型&lt;/h2>; &lt;p>; 当机器学习系统部署在在现实世界中，他们可能无法按照预期的方式行事，在新的环境中做出糟糕的预测。发生此类故障的原因有多种，并可能带来负面后果，特别是在医疗保健领域。我们的工作旨在在意外模型行为成为重大问题之前识别可能会发现的情况，并减轻意外和不良后果。 &lt;/p>; &lt;p>; CAIR 团队的大部分建模工作侧重于识别和缓解模型&lt;a href=&quot;https://blog.research.google/2021/10/how-underspecation-presents.html&quot;>;未指定。&lt;/a>;。我们表明，在从训练域提取的保留数据上表现良好的模型在分布偏移下并不同样稳健或公平，因为这些模型依赖虚假相关性的程度各不相同。这给用户和从业者带来了风险，因为使用标准模型评估实践很难预测模型的不稳定性。 &lt;a href=&quot;https://www.jmlr.org/papers/v23/20-1335.html&quot;>;我们已经证明&lt;/a>;这种担忧出现在多个领域，包括计算机视觉、自然语言处理、医学成像和电子健康记录预测。 &lt;/p>; &lt;p>; 我们还展示了如何利用因果机制的知识来诊断和减轻新环境中的公平性和稳健性问题。了解因果结构使从业者能够预测真实分布变化下公平性属性的普遍性-世界医疗环境&lt;/a>;。此外，通过研究特定因果路径或“捷径”在机器学习系统中引入偏差的能力，我们演示了如何识别&lt;a href=&quot;https://www.nature.com/articles/s41467-023- 39902-7&quot;>;捷径学习&lt;/a>;导致机器学习系统中的预测无意中依赖于敏感属性（例如年龄、性别、种族）。我们已经展示了如何使用因果&lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_acycl_graph&quot;>;有向无环图&lt;/a>;来&lt;a href=&quot;https://proceedings.mlr.press/ v206/alabdulmohsin23a&quot;>;使机器学习系统适应复杂的分布变化形式下不断变化的环境&lt;/a>;。我们的团队目前正在研究如何对不同形式的偏差进行因果解释，包括&lt;a href=&quot;https://en.wikipedia.org/wiki/Selection_bias&quot;>;选择偏差&lt;/a>;、标签偏差和测量误差， &lt;a href=&quot;https://nips.cc/virtual/2022/58452&quot;>;激励技术设计，以减少模型开发和评估过程中的偏差&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPl IJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s727/image6.png&quot; style=&quot;左边距：1em；右边距：1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;442&quot; data-original-width=&quot;727&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjf cXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZV ebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s1078/image1.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;382&quot; data-original-width=&quot;1078&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-Won7NImE6drZjMi SchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;快捷学习：对于某些模型，年龄可能是使用医学图像时分类的快捷方式。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;br />; &lt;p>; CAIR团队专注于开发方法论来广泛构建更具包容性的模型。例如，我们还有&lt;a href=&quot;https://arxiv.org/abs/2302.03874&quot; >;致力于参与式系统的设计&lt;/a>;，该系统允许个人在机器学习系统进行预测时选择是否披露种族等敏感属性。我们希望我们的方法论研究能够积极影响社会对人工智能包容性的理解方法开发。 &lt;/p>; &lt;br />; &lt;h2>;部署&lt;/h2>; &lt;p>; CAIR 团队的目标是构建通过使用移动设备技术来改善所有人生活的技术。我们的目标是减少健康状况带来的痛苦，解决系统性不平等问题，并实现基于设备的透明数据收集。随着健身追踪器和手机等消费技术成为健康数据收集的核心，我们探索了这些技术在慢性疾病背景下的使用，特别是&lt;a href=&quot;https://en.wikipedia .org/wiki/Multiple_sclerosis&quot;>;多发性硬化症&lt;/a>;（MS）。我们开发了新的数据收集机制和预测，我们希望最终能够彻底改变患者的慢性病管理、临床试验、医疗逆转和药物开发。 &lt;/p>; &lt;p>; 首先，我们扩展了开源 &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies 平台&lt;/a>;，用于创建临床研究应用程序，使任何人都可以更轻松地以可信且安全的方式进行自己的研究并收集高质量的数据。我们的改进包括零配置设置，以便研究人员可以在一天内对他们的研究进行原型设计，通过使用 &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>; 生成跨平台应用程序，以及，最重要的是，强调可及性，以便所有患者的声音都能被听到。我们很高兴地宣布这项工作现已&lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;开源&lt;/a>;，作为原始 FDA-Mystudies 平台的扩展。您今天就可以开始建立自己的学习！ &lt;/p>; &lt;p>; 为了测试这个平台，我们构建了一个原型应用程序，我们称之为 MS Signals，它使用调查在新颖的消费者环境中与患者进行交互。我们与&lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;国家多发性硬化症协会&lt;/a>;合作，招募参与者参与该应用程序的用户体验研究，目标是降低退出率并改善平台更进一步。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_ oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/s915/MSSignals.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;915&quot; height=&quot;632&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8 MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/w640-h632/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals 应用程序屏幕截图。&lt;strong>;左：&lt;/strong>; 欢迎学习&lt;strong>;右：&lt;/strong>;&amp;nbsp;问卷。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 收集数据后，研究人员可能会用它来推动 MS 机器学习研究的前沿。在另一项研究中，我们与&lt;a href=&quot;https://neurology.duke.edu/&quot;>;杜克大学神经病学系&lt;/a>;建立了研究合作，并证明机器学习模型可以&lt;a href=&quot;https: //www.researchsquare.com/article/rs-2547289/v1&quot;>;使用从移动应用程序持续收集的数据，准确预测三个月内严重症状的发生率&lt;/a>;。结果表明，临床医生可以使用经过训练的模型来评估多发性硬化症参与者的症状轨迹，这可能为实施干预措施的决策提供信息。 &lt;/p>; &lt;p>; CAIR 团队参与了许多其他系统的部署，供内部和外部使用。例如，我们还与 &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 合作&lt;a href=&quot;https://ai.googleblog.com/2023/08/study -socially-aware-temporally-causal.html&quot;>;为患有学习障碍（例如阅读障碍）的儿童建立图书推荐系统&lt;/a>;。我们希望我们的工作对未来的产品开发产生积极影响。 &lt;/p>; &lt;br />; &lt;h2>;人类反馈&lt;/h2>; &lt;p>; 随着机器学习模型在发达国家变得无处不在，欠发达国家的声音可能很容易被忽视。 CAIR 团队的首要任务是弥合这一差距，与社区建立深厚的关系，并通过社区驱动的方法共同解决与 ML 相关的问题。 &lt;/p>; &lt;p>; 我们实现这一目标的方法之一是与 ML 基层组织合作，例如 &lt;a href=&quot;https://www.sisonkebiotik.africa/home&quot;>;Sisonkebiotik&lt;/a>;，一个由机器学习和医疗保健交叉领域的研究人员、从业者和爱好者组成的开放和包容的社区，共同努力建设能力并推动非洲的研究计划。我们与 Sisonkebiotik 社区合作，详细说明了历史上自上而下的全球健康方法的局限性，并提出了基于健康的补充方法，特别是草根参与社区 (GPC) 的方法。我们共同创建了&lt;a href=&quot;https://openreview.net/forum?id=jHY_G91R880&quot;>;机器学习和全球健康框架&lt;/a>;，为建立、发展和维护 GPC 制定了实用的路线图，基于各种 GPC 的共同价值观，例如 &lt;a href=&quot;https://www.masakhane.io/&quot;>;Masakhane&lt;/a>;、Sisonkebiotik 和 &lt;a href=&quot;https://ro-ya-cv4africa.github。 io/homepage/&quot;>;Ro&#39;ya&lt;/a>;。 &lt;/p>; &lt;p>; 我们正在参与开放倡议，通过人类反馈更好地了解人工智能在非西方国家健康方面的作用、看法和用例，最初的重点是非洲。我们与&lt;a href=&quot;https://ghananlp.org/&quot;>;加纳 NLP&lt;/a>; 一起努力详细说明&lt;a href=&quot;https://arxiv.org/abs/2304.02190&quot;>;的需求更好地理解非西方背景下的算法公平性和健康偏见&lt;/a>;。我们最近发起了一项研究，利用人类反馈来扩展这项工作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE 7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy /s927/MLPipelineBiases.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;927&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH 213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy/s16000/MLPipelineBiases.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;ML 流程中的偏差及其与非洲语境的关联差异轴。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; CAIR 团队致力于创造机会听取更多关于人工智能开发的观点。我们与 Sisonkebiotik 合作，共同举办&lt;a href=&quot;https://www.sisonkebiotik.africa/events/workshops/dl-indaba-2023&quot;>;健康数据科学研讨会&lt;/a>;，地点：&lt;a href=&quot; https://deeplearningindaba.com/2023/&quot;>;加纳深度学习 Indaba 2023&lt;/a>;。每个人的声音对于利用人工智能技术创造更美好的未来至关重要。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢 Negar Rostamzadeh、Stephen Pfohl、Subhrajit Roy、Diana Mincu、Chintan Ghate、Mercy Asiedu、Emily Salkey、Alexander D &#39;Amour, 杰西卡·施洛夫, 奇拉格·纳格帕尔, 埃尔塔耶布·艾哈迈德, 列夫·普罗列夫, 娜塔莉·哈里斯, 穆罕默德·哈瓦伊, 本·哈钦森, 安德鲁·斯马特, 阿瓦·迪昂, Mahima Pushkarna, Sanmi Koyejo, Kerrie Kauer, Do Hee Park, Lee Hartsell, Jennifer Graves, Berk Ustun、Hailey Joren、Timnit Gebru 和 Margaret Mitchell 的贡献和影响力，以及我们在 Learning Ally、国家多发性硬化症协会、杜克大学医院、STANDING Together、Sisonkebiotik 和 Masakhane 的许多朋友和合作者。&lt;/em>; &lt;/em>; &lt;/ p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7987071997778787277/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot; text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research .google/2023/11/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Google 研究中的负责任人工智能：人工智能研究背景 (CAIR)&quot; type=&quot;text/html&quot;/>; &lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height= “16”rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>;&lt; /gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O 5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s72-c/CAIR-hero.jpg&quot;宽度=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id >;标签：blogger.com，1999：blog-8474926331452026626.post-1303378857635363504&lt;/id>;&lt;已发布>;2023-11-09T11:20:00.002-08:00&lt;/已发布>;&lt;更新>;2023-11-09T11:59 ：02.393-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;量子计算&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;克服纠错量子处理器上的泄漏&lt;/stitle>;&lt;content type=&quot;html&quot;>; &lt;span class=&quot;byline-author&quot;>;发布者：量子 AI 团队研究科学家 Kevin Miao 和 Matt McEwen&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidYny_9v0IyOBy7QbYVgwS5FAHzgHhVUt5FQvjXhi6WWJHyIIal3awBfaXNu9l3 g47HElKNkFNf6XfpU7a_9ExGLlKFgOWvUOaT0PLhEoCfofJrazqT2IL1fBMtZMwfGJSnrpBrQZwQkYJOV6iMOmhWBWIH5OCPcsEPkgw -LuiPDBoLYToGGIN3Hjfpmwr/s320/Quantum%20leakage.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 组成&lt;a href=&quot;https://quantumai.google/hardware&quot;>;的&lt;a href=&quot;https://en.wikipedia.org/wiki/Qubit&quot;>;量子位&lt;/a>;谷歌量子设备&lt;/a>;非常脆弱且嘈杂，因此在构建有用的量子计算机的过程中，有必要采用纠错程序来识别和解释量子位错误。两种最常见的错误机制是&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_error_ Correction#Bit_flip_code&quot;>;位翻转错误&lt;/a>;（量子位的能量状态发生变化）和&lt; a href=&quot;https://en.wikipedia.org/wiki/Quantum_error_ Correction#Sign_flip_code&quot;>;相位翻转错误&lt;/a>;（编码的量子信息的相位发生变化）。 &lt;a href=&quot;https://blog.research.google/2021/08/demonstrating-fundamentals-of-quantum.html&quot;>;量子纠错&lt;/a>; (QEC) 有望解决和减轻这两个突出的错误。然而，还有各种各样的其他错误机制挑战了 QEC 的有效性。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 虽然我们希望量子位表现得理想&lt;a href=&quot;https://en.wikipedia.org/wiki/Two-state_quantum_system&quot;>;两个级系统&lt;/a>;没有损失机制，但现实情况并非如此。我们使用量子位的最低两个能级（构成&lt;em>;计算基础&lt;/em>;）来进行计算。这两个级别对应于量子位中激发的不存在（计算基态）或存在（计算激发态），并标记为 |0⟩ （“&lt;a href=&quot;https://en.wikipedia.org/wiki /Bra%E2%80%93ket_notation&quot;>;ket&lt;/a>; 零”）和 |1⟩（“ket 1”）。然而，我们的量子位还存在许多称为“泄漏状态”的更高级别，这些级别可能会被占用。按照通过指示量子位中有多少激发来标记能级的约定，我们将它们指定为 |2⟩、|3⟩、|4⟩ 等。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;克服量子纠错中的泄漏&lt;/a>;”中，发表于 &lt; em>;&lt;a href=&quot;https://www.nature.com/nphys/&quot;>;自然物理学&lt;/a>;&lt;/em>;，我们确定了我们的量子位何时以及如何将能量泄漏到更高的状态，并表明泄漏的能量状态可以通过我们的两个量子位门破坏附近的量子位。然后，我们确定并实施一种策略，可以消除泄漏并将其转换为 QEC 可以有效修复的错误。最后，我们表明这些操作可以显着提高 QEC 过程的性能和稳定性。最后一个结果尤其重要，因为额外的操作需要时间，通常会导致更多错误。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;使用不完美的量子位&lt;/h2>; &lt;p>;我们的量子处理器是由称为&lt;的超导量子位构建的em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Transmon&quot;>;transmons&lt;/a>;&lt;/em>;。与只有两个计算能级（计算基态和计算激发态）的理想量子位不同，跨量子位具有许多比计算激发态能量更高的附加状态。这些较高的泄漏态对于生成&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;>;纠缠&lt;/a>;（量子算法中的必要资源）的特定操作非常有用，并且还可以防止跨子变成过于非线性且难以操作。然而，Transmon 也可能通过各种过程无意中被激发到这些泄漏状态，包括我们用于执行操作的控制脉冲的缺陷或低温冰箱中残留的少量杂散热。这些过程统称为&lt;em>;泄漏&lt;/em>;，它描述了量子位从计算状态到泄漏状态的转变。 &lt;/p>; &lt;p>; 考虑在我们的 QEC 实验中广泛使用的特定双量子位运算：&lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates:~:text=% 5B1%5D-,受控%2DZ,-%2C%0A受控%20sign&quot;>;CZ 门&lt;/a>;。该门在两个量子位上运行，当两个量子位都处于 |1⟩ 水平时，相互作用会导致两个单独的激励在其中一个量子位中短暂“聚集”在一起，形成 |2⟩，而另一个量子位则变为 |0 ⟩，然后返回到每个量子位位于 |1⟩ 的原始配置。这种聚集是 CZ 门纠缠能力的基础。然而，门可能会遇到错误，并且激励不会返回到其原始配置，从而导致操作将量子位留在|2⟩（泄漏状态）中，这种可能性很小。当我们执行数百个或更多的 CZ 门时，这种小的泄漏错误概率就会累积。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwt MGSiPIWruSj-S6Nleklj7YF9Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/s559/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;559&quot; height=&quot;400&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwtMGSiPIWruSj-S6Nleklj7YF9 Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/w400-h400/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transmon 量子位支持许多超出计算基础 (|0⟩) 的泄漏状态 (|2⟩、|3⟩、|4⟩、...)和|1⟩）。虽然我们通常只使用计算基础来表示量子信息，但有时量子位会进入这些泄漏状态，并扰乱量子位的正常运行。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 单个泄漏事件对正常的量子位操作尤其有害，因为它会引起许多单独的错误。当一个量子位以泄漏状态开始时，CZ 门不再正确地纠缠量子位，从而阻止算法正确执行。不仅如此，应用于泄漏状态的一个量子位的 CZ 门也会导致另一量子位泄漏，从而将泄漏传播到整个设备。我们的工作包括广泛表征泄漏是如何引起的以及它如何与我们在量子处理器中使用的各种操作相互作用。 &lt;/p>; &lt;p>; 一旦量子位进入泄漏状态，它就可以在许多操作中保持该状态，然后再恢复到计算状态。这意味着单个泄漏事件会干扰该量子位上的许多操作，从而产生及时聚集在一起的操作错误（&lt;em>;时间相关&lt;/em>;错误）。泄漏通过 CZ 门在我们设备中的不同量子位之间传播的能力意味着我们还会同时看到相邻量子位上的大量错误（&lt;em>;空间相关&lt;/em>;错误）。事实上，泄漏会导致空间和时间相关的错误模式，因此从 QEC 算法的角度来看，诊断和纠正尤其困难。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;QEC 中泄漏的影响&lt;/h2>; &lt;p>; 我们的目标是通过实施来减轻量子位错误&lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;表面代码 QEC&lt;/a>;，一组应用于不完美集合的操作物理量子位形成逻辑量子位，其属性更接近理想量子位。简而言之，我们使用一组称为数据量子位的量子位来保存量子信息，而另一组测量量子位检查数据量子位，报告它们是否遭受了任何错误，但没有破坏数据量子位的微妙量子态。 QEC 的关键基本假设之一是每个操作都独立发生错误，但泄漏可能会持续存在于许多操作中并导致多个错误的相关模式。当泄漏导致违反这一假设时，我们的 QEC 策略的性能会受到显着限制。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2 -15jJ6uX2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g -WzBDS3jM-tuGSnxOHFb8kM4shNqKGPqGtKzs8FFRCPs-hH8mjsvkKLrNza/s1588/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-宽度=“1588”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2-15jJ6uX 2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g-WzBDS3jM-tuGSnxOHFb8kM4shNqKGPqGtKzs8FFRCPs-hH8mjsvkKLrNza/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;一旦泄漏出现在我们的表面代码 transmon 网格中，它就会持续存在相对于单个表面代码QEC周期来说较长的时间。更糟糕的是，一个量子位上的泄漏也会导致其邻居泄漏。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 我们的 &lt;a href=&quot;https:/ /blog.research.google/2021/08/demonstrating-fundamentals-of-quantum.html#:~:text=the%20Sycamore%20device.-,Leaky%20Qubits,-The%20goal%20of&quot;>;之前的工作&lt;/ a>; 表明，我们可以使用称为“多级重置”(MLR) 的操作消除测量量子位中的泄漏。这是可能的，因为一旦我们对测量量子位进行测量，它们就不再保存任何重要的量子信息。此时，我们可以将量子位与一个非常有损耗的频带进行交互，从而导致量子位所处的任何状态（包括泄漏状态）衰减到计算基态|0⟩。如果我们想象一个代表量子位中的激发的&lt;em>;Jenga&lt;/em>;塔，我们就会把整个堆栈翻倒。然而，仅拆除一块砖就更具挑战性。同样，MLR 不适用于数据量子位，因为它们&lt;em>;总是&lt;/em>;保存重要的量子信息，因此我们需要一种新的泄漏消除方法，以尽量减少对计算基础状态的干扰。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;轻轻消除泄漏&lt;/h2>; &lt;p>;我们引入了一种新的量子操作，称为&lt;em>;数据量子位泄漏消除（DQLR），其目标是数据量子位中的泄漏状态，并将其转换为数据量子位和相邻测量量子位中的计算状态。 DQLR 由两个量子位门组成（称为&lt;em>;Leakage iSWAP&lt;/em>; - 一个&lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates:~:text=1%5D% 5B6%5D-,Imaginary%20swap,-2&quot;>;iSWAP&lt;/a>; 具有泄漏状态的操作）受到 CZ 门的启发并与之类似，然后快速重置测量量子位以进一步消除错误。 Leakage iSWAP 门非常高效，并且极大地受益于我们在表面代码实验中对 CZ 门的广泛表征和校准。 &lt;/p>; &lt;p>; Recall that a CZ gate takes two single excitations on two different qubits and briefly brings them to one qubit, before returning them to their respective qubits. A Leakage iSWAP gate operates similarly, but almost in reverse, so that it takes a single qubit with two excitations (otherwise known as |2⟩) and splits them into |1⟩ on two qubits. The Leakage iSWAP gate (and for that matter, the CZ gate) is particularly effective because it does not operate on the qubits if there are fewer than two excitations present. We are precisely removing the |2⟩ &lt;em>;Jenga&lt;/em>; brick without toppling the entire tower. &lt;/p>; &lt;p>; By carefully measuring the population of leakage states on our transmon grid, we find that DQLR can reduce average leakage state populations over all qubits to about 0.1%, compared to nearly 1% without it. Importantly, we no longer observe a gradual rise in the amount of leakage on the data qubits, which was always present to some extent prior to using DQLR. &lt;/p>; &lt;p>; This outcome, however, is only half of the puzzle. As mentioned earlier, an operation such as MLR could be used to effectively remove leakage on the data qubits, but it would also completely erase the stored quantum state. We also need to demonstrate that DQLR is compatible with the preservation of a logical quantum state. &lt;/p>; &lt;p>; The second half of the puzzle comes from executing the QEC experiment with this operation interleaved at the end of each QEC cycle, and observing the logical performance. Here, we use a metric called &lt;em>;detection probability&lt;/em>; to gauge how well we are executing QEC. In the presence of leakage, time- and space-correlated errors will cause a gradual rise in detection probabilities as more and more qubits enter and stay in leakage states. This is most evident when we perform no reset at all, which rapidly leads to a transmon grid plagued by leakage, and it becomes inoperable for the purposes of QEC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s1532/image2.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1532&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;The prior state-of-the-art in our QEC experiments was to use MLR on the measure qubits to remove leakage. While this kept leakage population on the measure qubits (green circles) sufficiently low, data qubit leakage population (green squares) would grow and saturate to a few percent. With DQLR, leakage population on both the measure (blue circles) and data qubits (blue squares) remain acceptably low and stable.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; With MLR, the large reduction in leakage population on the measure qubits drastically decreases detection probabilities and mitigates a considerable degree of the gradual rise. This reduction in detection probability happens even though we spend more time dedicated to the MLR gate, when other errors can potentially occur. Put another way, the correlated errors that leakage causes on the grid can be much more damaging than the uncorrelated errors from the qubits waiting idle, and it is well worth it for us to trade the former for the latter. &lt;/p>; &lt;p>; When only using MLR, we observed a small but persistent residual rise in detection probabilities. We ascribed this residual increase in detection probability to leakage accumulating on the data qubits, and found that it disappeared when we implemented DQLR. And again, the observation that the detection probabilities end up lower compared to only using MLR indicates that our added operation has removed a damaging error mechanism while minimally introducing uncorrelated errors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s1205/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;1205&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Leakage manifests during surface code operation as increased errors (shown as error detection probabilities) over the number of cycles.使用DQLR，我们不再看到检测概率在更多的表面代码周期中显着上升。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;br />; &lt;div style =“ line-height：40％ ;“”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>; QEC缩放量表的前景&lt;/h2>; &lt;p>;鉴于这些有希望的结果，我们渴望在未来的QEC实验中实现DQLR，我们期望以外的错误机制以外的错误机制泄漏将大大改善，并在我们使用越来越大的Transmon网格时对泄漏的敏感性得到增强。特别是，我们的模拟表明，我们的表面代码的扩展几乎可以肯定需要大幅度降低泄漏生成率，或者在所有量子位（例如DQLR）上进行主动泄漏技术。 &lt;/p>; &lt;p>;通过了解泄漏的位置，捕获了泄漏动力，在泄漏的动态中，它在Transmon网格中呈现泄漏的动力，并表明我们在DQLR中具有有效的缓解策略关联的错误不再对在大型Transmon Qubits网格上执行表面代码QEC协议的前景构成生存威胁。由于挑战越来越少，以证明工作QEC的方式，通往有用的量子计算机的途径从未有任何希望。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;如果没有贡献，这项工作将是不可能的&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/1303378857635353504/comments/comments/comments/comments/default” “发表评论” type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2023/11/overcoming-leakage-leakage-on-on-error-corrceed.html#comment-form” rel =“回复” title =“ 0注释” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/5474926331452026626/posts/posts/defaults/default/130337857857857857857857857635363550 type =“ application/application/atom+xml”/>; &lt;link href =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/1303378578576357635363635535353504” />; &lt;link href =“ http://blog.research.google/2023/2023/11/overcoming-leakage-on-eror-corrceted.html” rel =“替代” title =“克服错误验证的量子处理器上的泄漏” type =“ text/html”/>; &lt;unam>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777775266161 &lt;/uri>; &lt;emage>; /email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/b16-rounded。 gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidYny_9v0IyOBy7QbYVgwS5FAHzgHhVUt5FQvjXhi6WWJHyIIal3awBfaXNu9l3g47HElKNkFNf6XfpU7a_9ExGLlKFgOWvUOaT0PLhEoCfofJrazqT2IL1fBMtZMwfGJSnrpBrQZwQkYJOV6iMOmhWBWIH5OCPcsEPkgw-LuiPDBoLYToGGIN3Hjfpmwr/ s72-c/Quantum％20leakage.jpg“ width =” 72“ xmlns：媒体=” http：//search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr>; &lt;thr：thr：thr>; &lt;thr：thr>; 0 &lt;/thr： Total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：Blogger.com，1999：Blog-8474926331452026626.POST-46661241136828203614 &lt;/id>; &lt;/id>; &lt;/id>; &lt;出版>; 2023-11-11-07T12：34：34：003--08：003-08：003-08：003-08：003-08：003-00 &lt;/ >; &lt;更新>; 2023-11-07T12：34：21.947-08：00 &lt;/Updated>; &lt;类别方案=“ http://www.blogger.com/atom/ns#”类别>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“ Machine Learning”>; &lt;/category>; &lt;/category>; &lt;title type =“ text”>;交替的更新用于有效的变形金刚&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由Xin Wang，软件工程师和Nishanth Dikkala发表，Google Research Science &lt;/span>; &lt;img src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk/s1600/altup.png&quot; style=&quot;display: none;&quot; />; &lt;p>;当代深度学习模型在许多领域都非常成功，从自然语言到计算机视觉。 &lt;a href=&quot;https://en.wikipedia.org/wiki/transformer_(machine_learning_model)&quot;>;变形金刚神经网络&lt;/a>;（变形金刚）是一种流行的深度学习体系结构，如今已构成自然语言基础的基础处理并开始扩展到其他域中的应用程序，例如&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;计算机视觉&lt;/a>;，&lt;a href =“ 。自动驾驶/“>;自动驾驶&lt;/a>;。此外，它们形成了所有当前最新的&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/language_model&quot;>;语言模型&lt;/a>;的骨干。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>;变压器网络中的规模提高导致性能提高，&lt;a href=&quot;https://arxiv.org/abs/abs/2206.07682&quot;>;出现行为&lt;/a>;不存在于较小的网络中。但是，规模的增加通常会导致计算成本和推理潜伏期的过高增加。一个自然的问题是，我们是否可以在不承担计算负担的情况下获得大型模型的好处。 &lt;/p>; &lt;p>;在“ &lt;a href=&quot;https://arxiv.org/abs/2301.13310&quot;>;有效变压器的交替更新&lt;/a>;”中，被视为pashiglight at &lt;a href =“ https：https：https：https： //neurips.cc/virtual/2023/poster/72994&quot;>; Neurips 2023 &lt;/a>;，我们介绍了Altup，一种方法，可以利用增加令牌表示而不增加计算成本。 Altup易于实现，广泛适用于任何变压器体系结构，并且需要最小的&lt;a href=&quot;https://en.wikipedia.org/wiki/hyperparameter_optimization&quot;>; HyperParameter Tuning &lt;/a>;。例如，在770m参数T5-LARGE模型上使用Altup的变体，添加〜100个参数会产生一个质量明显更高的模型。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>;工作。首先，它们将输入分为一系列令牌。然后将每个令牌映射到称为令牌嵌入的嵌入向量（通过嵌入表的均值）。我们将该向量的维度称为令牌表示维度。然后，通过使用其网络参数应用一系列计算模块（称为layers &lt;em>;）&lt;/em>;，将变压器按照令牌嵌入序列进行操作。每个变压器层中的参数数是该层的&lt;em>; width &lt;/em>;的函数，该函数由令牌表示尺寸确定。 &lt;/p>; &lt;p>;要在不产生计算负担的情况下实现比例的好处，例如稀疏的Experts（稀疏MOE）型号（例如，&lt;a href =” https://arxiv.org/abs /2101.03961&quot;>;开关变压器&lt;/a>;，&lt;a href=&quot;https://blog.research.google/2022/11/mixture-of-11/mixture-of-experts-with-experts-with-expert-choice.html？选择&lt;/a>;，&lt;a href=&quot;https://blog.research.google/2022/01/scaling-vision-vision-vision-with-sparse-mixuter-of.html?m = 1&quot;>; v-moe &lt;/a >;）主要集中于有效地扩大网络参数（在自我注意力中，在&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/feedforward_neural_network&quot;>; foreforward &quot;>; fefforward layers &lt;/a>;）基于输入的子集。这使我们可以扩大网络大小而不会显着增加每个输入的计算。但是，通过有条件地激活令牌表示向量的部分来扩大令牌表示维度本身的研究差距。 &lt;/p>; &lt;p>;最近的作品（例如，&lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;>;缩放法律&lt;/a>;和&lt;a href =“ https://arxiv.org.org /abs/2011.14522&quot;>; infinite-width网络）&lt;/a>;从经验和理论上确定，更广泛的令牌表示有助于学习更复杂的功能。这种现象在提高能力的现代体系结构中也很明显。例如，表示维度在&lt;a href =“ https：//arxiv.org/1910.106833中，表示尺寸从512（small）提高到768（基本）和1024（基本）和1024（分别为770m，3b和11b参数的型号） “>; T5型号&lt;/a>;，从4096（8B）到8192（64b）和18432（540b），in &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>; palm Models &lt;/a>; 。扩大的表示维度也显着提高了双重编码器检索模型的性能。但是，天真地扩大表示矢量需要相应地增加模型维度，而二次&lt;sup id =“ fnref1”>; &lt;a href=&quot;#fn1&quot; rel=&quot; rel=&quot;footnote&quot;>; 1 &lt;/a>; &lt;/a>; &lt;/sup>;增加馈电计算中的计算量。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;方法&lt;/h2>; &lt;p>; altup通过将加宽的表示向量划分为相等尺寸的块，仅在每一层处理一个块，并使用有效的预测校正机制来推断其他块的输出（如下所示）。这允许Altup同时保持模型维度，因此计算成本大致稳定，并利用增加令牌维度。增加的令牌维度使该模型可以将更多信息包装到每个令牌的嵌入中。通过保持每个变压器层的宽度恒定，Altup避免了计算成本的二次增加，否则该计算成本会随着表示形式的幼稚扩展而存在。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s1367/image2.png&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 823” data-Original-width =“ 1367” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “文本 - 单位：中心;”>;拓宽令牌表示没有（&lt;b>;左&lt;/b>;）和altup（&lt;b>;右&lt;/b>;）的例证。由于层宽度的增加，这种扩展会导致香草变压器的计算几乎二次增加。相比之下，交替的更新可以使图层宽度保持恒定且有效地通过在每层表示的子块上操作来计算输出。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;更具体地，每一层的输入是两个或多个块，其中一个被传递到1倍宽度变压器层中（请参见下图）。我们将此块称为“激活”块。该计算导致激活块的精确输出。同时，我们调用一个轻巧的预测指标，该预测指标计算所有输入块的加权组合。预测值以及激活块的计算值将传递给一个轻巧的校正器，该校正器根据观察到的值更新预测。这种校正机制使灭活块可以作为激活的块进行更新。预测步骤和校正步骤仅涉及有限数量的向量添加和乘法，因此比常规变压器层要快得多。我们注意到，该过程可以推广到任意数量的块。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s957/image3.png&quot; style=&quot;边距左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-froliginal-height =“ 504” data-Original-width =“ 957” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>;预测变量和纠正仪计算：预测变量将亚块与可训练的标量系数混合；校正器返回预测变量输出和变压器输出的加权平均值。与变压器相比，预测变量和校正器执行标量矢量乘法和可忽略不计的计算成本。预测器输出块与标量混合系数的线性混合P &lt;ub>; i，j &lt;/sub>;，并且校正器将预测器输出和变压器输出与权重G &lt;sub>; i &lt;/sub>;。&lt;/sub>;。 /tr>; &lt;/tbody>; &lt;/table>; &lt;p>;在较高级别上，altup类似于稀疏的MOE，因为它是一种以有条件访问的（外部）参数形式添加容量的方法。在稀疏的MOE中，其他参数采用Feed Forward Network（FFN）专家的形式，并且条件性相对于输入。在Altup中，外部参数来自宽扩大的嵌入式表，条件性采取表示表示向量的交替块激活的形式，如上图所示。因此，Altup的基础与稀疏MOE模型相同。 &lt;/p>; &lt;p>; altup比稀疏MOE的优点是，它不需要碎片，因为引入的附加参数的数量是因子&lt;sup ID =“ fnref2”>; &lt;a href =“ href =”＃fn2“ rel =”嵌入式桌子尺寸的脚注“>; 2 &lt;/a>; &lt;/sup>;，通常占整体型号大小的一小部分。此外，由于Altup着重于将更宽的令牌表示的部分有条件地激活部分，因此可以通过像MOE这样的正交技术协同应用，以获得互补的性能增长。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;评估&lt;/h2>; &lt;p>; altup在各种基准语言任务上的T5模型上进行了评估。用Altup增强的模型均匀地比外推密集模型以相同的精度而更快。例如，我们观察到，&lt;a href=&quot;https://gluebenchmark.com/&quot;>; Glue &lt;/a>;，使用Altup增强的T5大型型号会导致27％，39％，87％和29％的速度。 ，&lt;a href=&quot;https://super.gluebenchmark.com/&quot;>; superglue &lt;/a>;，&lt;a href=&quot;https://rajpurkar.github.io/squad-explorer/squad-explorer/&quot;>; squead &lt;/a>; squead &lt;/a>;和&lt;a href=&quot;https://nlp.cs.washington.edu/triviaqa/&quot;>; trivia-qa &lt;/a>;分别为基准。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s1386 /image5.png“ style =”边距 - 左左右：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 982” data-Original-width =“ 1386” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>;对各种尺寸和流行基准测试的T5模型上的altup评估。 Altup始终以相同的精度导致相对于基线的相对于基线的相当大的加速。潜伏期是在&lt;a href=&quot;https://cloud.google.com/tpu/docs/system-architecture-tpu-vm&quot;>; tpuv3 &lt;/a>;带有8个内核的。加速度定义为延迟的变化除以altup延迟（b = t5 base，l = t5大，xl = t5 xl型号）。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/t>; &lt;/tbody>; &lt;/table>; &lt;p>; &lt;p>;当我们将其应用于较大的模型时，Altup的相对性能会提高 - 将T5碱基 + Altup的相对加速与T5大 + Altup的相对加速进行比较。这证明了Altup的可扩展性及其在更大模型上的提高性能。总体而言，ALTUP始终导致具有更好的预测性能的模型，该模型比所有评估的模型尺寸和基准测试的相应基线模型具有相同的速度。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>;但是，层计算确实需要使用较宽的嵌入式表。在某些情况下，词汇大小（即，令牌仪可以产生的独特令牌的数量）非常大，这可能会导致初始嵌入查找和最终&lt;a href =“ ：//www.google.com/url？q = https：//develvevelers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax＆softmax＆sa = d＆amp; source = docs＆amp; ust = docs＆amp; ust = 1699300629405641＆amp; usg = aovvaw3tnz18-lvy0tpp_nclq_p-“>; linear + softmax操作&lt;/a>;。一个非常大的词汇也可能导致不良量的添加嵌入参数。为了解决这个问题，回收效果是Altup的扩展，可以通过保持嵌入式表的宽度相同，从而避免了这些计算和参数成本。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s989/image4.png&quot; style=&quot;边距左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-forminal-height =“ 515” data-eriginal-width =“ 989” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：center;“>;用&lt;em>; k &lt;/em>; = 2。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/tbody>; &lt;/table>; &lt;p>;在循环中的altup中，&lt;em>; k &lt;/em>; = 2的架构的插图，而是通过扩展初始令牌嵌入，我们复制嵌入&lt;em>; k &lt;/em>;次以形成更宽的令牌表示。因此，回收效果几乎没有相对于基线变压器添加其他参数，同时受益于更宽的令牌表示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRoANGe48-HncQDuNpxAVSUGf2GPp1qp6KpnvqMbQ5Mejlq4gWpq96vu9FxkDub8aO3RqYyDmTki2Xqkj9drOprwqrHDyoQvHI4oziYFguhDnkkYZO4GXdhAKgIlfffHJ5Po7mEVGCaAkOH7oobnt-8qKjWlerZp4EeoAjZg6IG_CEVO3hhsQvlPaV2GHU/s1999/image1.png&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 643” data-Original-width =“ 1999” src =“ https：// blogger。 googleusercontent.com/img/b/r29vz2xl/avvxsehroange48-hncqdunpxavsugf2gpp1qp6kp6kpnvqmbq4gwpq xk xkkdkdkdkdbdub dub x.xkyydbkiq quropr quroprnddry qufkhyyphyy qufwhyphyypheyy qufwhhddydhddydydndydndydydydydydydndydyy gxdhakgilffffhj5po7mevggaakoh7OOBNT-8QKJWLERZP4EEOAJZG6IG_CEVO3HHSQVLPAV2GHU/s16000/image1.png“/>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>;与基准相比，“文本合格：中心;”>; T5-B/L/XL上的回收效果。再生效果会导致严格改善预训练性能，而不会产生任何可感知的放缓。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/take>; &lt;/table>; &lt;p>;我们还评估了Altup的轻量级扩展，可再生的altup，Recy-altup，Recy-aLtup，Recyly-Altup，，，，，Altup，Recyly-Altup（使用&lt;em>; k &lt;/em>; = 2在T5底座上，大型和XL模型，并将其预训练的准确性和速度与基准的准确性和速度进行比较。由于可回收的ALTUP不需要在嵌入式表尺寸中扩展，因此与基线模型相同的可训练参数的增强模型几乎具有相同数量的可训练参数。与密集基线相比，我们再次观察到一致的改进。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; em>;有效地利用辅助参数到嵌入式表，并在整个层上保持较高的维度表示。我们认为，该计算中的关键要素在于Altup的预测机制，该机制执行了不同块的集合。这种加权组合可以使连续消息传递到整个矢量，尽管仅在每一层中激活其亚块。另一方面，回收的ALTUP不会在令牌嵌入中添加任何其他参数。但是，由于从一个变压器层移动到另一个变压器层时，保持较高的维表示向量，因此它仍然赋予模拟计算空间中模拟计算的好处。我们猜测，这通过通过网络增加信息流来有助于培训。一个有趣的研究方向是探索是否可以通过更有利的培训动态来解释再生效果的好处。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; denkentledgments &lt;/h2>; &lt;p>; &lt;em>;我们感谢我们的合作者Cenk Baykal，Dylan Cutler，Dylan Cutler，和Google Research的Rina Panigrahy和加利福尼亚大学伯克利分校的Nikhil Ghosh（在Google的研究实习期间完成的工作）。&lt;/em>; &lt;/p>; &lt;！ - 脚注 - >; &lt;hr width =“ 80％” />; &lt;p>; &lt;span class =“苹果式启动” style =“ font-size：x-small;”>; &lt;sup>; &lt;a name =“fn1&quot;>; &lt;b>; &lt;b>; 1 &lt;/b>; &lt;/b>; &lt;/b>; &lt;/ a>; &lt;/sup>;这是因为变压器的前进液层通常用模型维度进行四次缩放。 a>; &lt; /span>; &lt;br />; &lt;span class =“ Apple-style-Span” style =“ font-size：x-small;”>; &lt;sup>; &lt;a name =“fn2&quot;>; &lt;b>; 2 &lt; /b>; &lt;/a>; &lt;/sup>;此因子取决于用户指定的扩展因子，但通常为1，即，我们将嵌入式表尺寸加倍。 “>; &lt;sup>; &lt;/sup>; &lt;/a>; &lt;/span>; &lt;/span>; &lt;/p>; &lt;/content>; &lt;link href =” =“回复” title =“ post注释” type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2023/2023/11/alternating-updates-updates-for-forfility.html# comment-form“ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://wwwww.blogger.com/feeds/feeds/847492633145202626/ =&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4661241136828203614&quot; rel=&quot;self&quot; type=&quot;application/ atom+xml“/>; &lt;link href =” http://blog.research.google/2023/11/alternating-updates-updates-for-fordic.html“ rel =“替代” title =“交替” title =“有效变形金刚”类型的交替更新=“ text/html”/>; &lt;unam>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/10986265147775266161 &lt;/uri>; &lt;emager>; email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk /S72-c/altup.png“ width =” 72“ xmlns：媒体=” http：//search.yahoo.com/mrss/“>; &lt;/media：thumbnail>; &lt;thr>; &lt;thr>; &lt;thr：thr>; thr>; 0 &lt;/thr：thr：总计>; &lt;/entry>; &lt;enter>; &lt;id>;标签：blogger.com，1999：Blog-84749263331452026626.POST-8959024707515398632 &lt;/id>; &lt;/id>; &lt;出版>; &lt;出版>; 2023-11-03T11：23：23：00.001-07：001-07：001-07：001-00 &lt;/PARMEND>; &lt;更新>; 2023-11-03T11：27：07.156-07：00 &lt;/updated>; &lt;category scheme =“ http://www.blogger.com/atom/atom/ns#” term =“ term =” algorithms &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ iclr”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“大语言模型”>; &lt;/category>; &lt;title type =“ text”>;两个世界中最好的：在文本聚类中实现可扩展性和质量-author&quot;>;Posted by Sara Ahmadian and Mehran Kazemi, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR2mD5afdT_Qg9Ex4Lj94FaxB9KHiq20iLoDlOY8S8Rk5XsV6n_4dU9CFbCvSeBSONGQYqy-yMGY6-KU_y_RGICOpz76GNdzv7ecxor_rVnF31lZOd3STQF4MIE4F_EadrSI1DXY67MXnsCswY3w3X8vX8KgU_rRPs6eTZndYAbVcEezgwaUsdZEAHD59Z/s320/ hero.jpg“ style =” display：none;” />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/cluster_analysis&quot;>; clustering &lt;/a>;是数据挖掘中的基本，无处不在的问题，&lt;a href =“ 。聚类的标准形式是度量聚类和图形聚类。在度量聚类中，给定的度量空间定义了数据点之间的距离，这些距离是根据其&lt;em>;分离&lt;/em>;将其分组在一起的。在图形群集中，给定的图通过边缘连接相似的数据点，并且基于它们之间的&lt;em>; connections &lt;/em>;将群集过程分组在一起。两种聚类表格对于无法定义的类标签的大型语料库特别有用。此类语料库的示例是各种Internet平台的数字文本集合，其中包括组织和搜索文档，识别文本中的模式以及向用户推荐相关文档（请参阅以下帖子中的更多示例：&lt;a href = =” https://blog.research.google/2010/10/clustering-releated-ceries-come-base---on.html?m = 1&quot;>;基于用户意图的相关查询&lt;/a>; &lt;/a>; &lt;/a>;和&lt;a href =“ https：https：https：https： //blog.research.google/2021/10/practical-differentially-private.html&quot;>; practical dictionally私有聚类&lt;/a>;）。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;文本聚类方法的选择通常会带来困境。一种方法是使用嵌入模型，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/bert_(language_model)&quot;>; bert &lt;/a>; or &lt;/a>;或&lt;a href =“ https：// arxiv。 org/abs/1907.11692“>; Roberta &lt;/a>;，定义度量聚类问题。另一个是利用&lt;a href=&quot;https://medium.com/@geetkal67/astention-networks-aimple-way-way-way-way-way-to-cross-cross-cross-cross-crostention-3b3962666d82e&quot;>;交叉注意）型号，例如&lt;a href=&quot;https://blog.research.google/2022/04/pathways-language-model-model-palm-scaling-to.html&quot;>; palm &lt;/a>; or &lt;/a>;或&lt;a href =&#39; https://en.wikipedia.org/wiki/generative_pre-trained_trashing_transformer&quot;>; gpt &lt;/a>;，以定义图形群集问题。 CA模型可以提供高度准确的相似性分数，但是构建输入图可能需要对模型的二次推理调用。另一方面，可以通过嵌入模型产生的嵌入距离有效地定义度量空间。但是，与CA模型的相似性信号相比，这些相似性距离通常具有很大的低质量，因此产生的聚类可能要低得多。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxseirhbbz9egdnflc6zcmledtdplecmleodtdppzjjjegclhp8ble4nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn0 696irwyubayaqecpxftmp1wzjbo0mknzmnntjqrdsxkiwgzsyssssssssssssssss8xb-ovhyyxkjbwi0dlryvb_s204sbkkkptx8_fqtjxkk8_fqtjxk8f_wdioggqqnjtacacuxnewu/s835/fign “ style =”保证金左：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 641” data-Original-width =“ 835” src =“ https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRHbZ9egdnFlc6ZCmLeODTdPZoOXcjjEGCLHP8Ve3aihMExIjyLiX4n0Zy9l4nmkx-3k6rxYN5696irWyubaYAqecpxFTmP1wzjBo0MKnZMnnTjQrdSxKIWGzsySs8XB-OVHyyxKJBWI0dlrYvb_S204S6aSbkPTx8_FQTJXk8f_WDIoGgqNJTACuXNewu/s16000/figure%20top.png&quot; />;&lt;/a>;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt; a href =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxsejoy4juklu42gca4fdtnun4t5j8z-44k4rehyhyhyphenhhyphenhhyphenhhyphenhhyphenhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyhhyphendi hhyphendi hhyphendi afiuadrf_7arpof-xnmawggxs48a_3wng2trhn0ctkhgmva6zq7top7ie6fpifmbtf7yygpfgypfgyu3cmtyyybyifswlvvlvrxy6dueiy6dueiy6dueiy/s835/s835/figig figs fig.ppng;边缘权利：自动;“>; &lt;img border =“ 0” data-eriginal-height =“ 641” data-eriginal-width =“ 835” src =“ https://blogger.googleusercontent.com/img/img/b/b/ R29vZ2xl/AVvXsEjoY4juklU42gca4fdtnUN4t5j8Z-4rEEhyphenhyphenDi4kRfkV1wfqBs_wyldX9sU2pwtMHdVZPZf2vWnDweEPzwiLseYZenC3afIuaDRf_7arpoF-xnMAwgGXs48a_3wNG2trhN0CTkHgMVa6ZQ7ToP7IE6fpIFmbTF7YYGPFgYU3cMtyyBYIfSWVLVvRXY6Dueiy/s16000/figure%20bottom.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing =“ 0” class =“ tr-caption-container”样式=“边距 - 左：auto; margin-right: auto; text-align：中心;“>; &lt;tbody>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;基于嵌入式和跨注意的概述基于基于嵌入的相似性得分功能及其可伸缩性与质量困境。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;由此激励，在“ &lt;a href =” https://openreview.net/pdf?id=中p0jssa1auv“>; kwikbucks：与廉价和昂贵的信号的相关聚类”，在ICLR 2023 &lt;/a>;上介绍，我们描述了一种新颖的聚类算法，可以有效地结合嵌入模型中的可扩展性益处，并从CA模型中获得质量。 Graph clustering算法对CA模型和嵌入模型都具有查询访问权限，但是，我们将预算应用于对CA模型的查询数量。此算法使用CA模型来回答边缘查询，并从无限制访问中访问对相似性得分来自嵌入模型。我们描述了该提出的设置桥梁算法设计和实际考虑如何，并且可以应用于具有相似可用评分功能的其他聚类问题，例如图像和媒体上的聚类问题。我们演示了该算法如何产生高质量的簇，几乎对CA模型进行了线性查询调用。我们还拥有&lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks.zip&quot;>;开放式源&lt;/a>;我们实验中使用的数据。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;聚类算法&lt;/h2>; &lt;p>; kwikbucks算法是众所周知的扩展&lt;a href=&quot;https://cesa-bianchi.di.unimi.it/algo2/note/kwik.pdf&quot;>; kwikcluster算法（枢轴算法）&lt;/a>;。高级的想法是首先选择一组文档（即，中心），它们之间没有相似的边缘，然后在这些中心周围形成簇。为了从CA模型获得质量和嵌入模型的运行时效率，我们介绍了小说组合相似性Oracle &lt;/em>;机制。在这种方法中，我们利用嵌入模型来指导将要发送到CA模型的查询的选择。当给出一组中心文档和目标文档时，组合相似性Oracle机制从集合中输出一个类似于目标文档（如果存在）的中心。组合相似性Oracle使我们能够通过在选择中心和组成簇时限制对CA模型的查询调用数量来节省预算。它根据其嵌入与目标文档的相似性，然后查询对（即，目标文档和排名中心）的CA模型，从而通过第一个排名中心来执行此操作，如下所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s1601/process.png&quot; style=&quot;边距左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-original-height =“ 522” data-Original-width =“ 1601” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s16000/process.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：Center;“>;组合相似性Oracle对于一组文档和目标文档，请从集合中返回类似的文档，如果存在。&lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;然后，如果其中两个之间存在较强的连接，则在连接边缘的数量高于两个簇之间缺少边缘的数量时，我们执行后处理步骤以合并簇。此外，我们将以下步骤应用于对CA模型的查询进行进一步的计算节省，并在运行时提高性能：&lt;/p>; &lt;ol>; &lt;li>;我们利用&lt;a href =“ https://arxiv.org /pdf/2002.11557.pdf&quot;>; Query-效率相关聚类&lt;/a>;从一组随机选择的文档中形成一组中心，而不是从所有文档中选择这些中心（在下面的图中，中心节点为红色，是红色的。 ）。 &lt;/li>; &lt;li>;我们应用组合相似性Oracle机制来对所有非中心文档并行执行群集分配步骤，并留下与单例没有类似中心的文档。在下面的插图中，分配由蓝色箭头描绘，最初是两个（非中心）节点作为单例，因为没有分配。 &lt;/li>; &lt;li>;在后处理步骤中，为了确保可伸缩性，我们使用嵌入相似性分数来过滤潜在的合并（在下面的图中，绿色虚线边界显示了这些合并的群集）。 &lt;/li>; &lt;/ol>; &lt;div>; &lt;br />; &lt;table align =“中心” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; Margin-Right：auto;“>; &lt;tbody>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https://blogger.googleusercercontent.com/img/img/r29vz2xxl/r29vz2xl/avvz2xl/avvxsej1bj65zsg--zsg--zsg--zsg--zsg--zsg--zsg--zsg--zsg-- PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s1322/Kwikbux%20gif.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;704&quot; data -original-width=&quot;1322&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1bJ65zsG-PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s16000/Kwikbux%20gif.gif&quot; />;&lt;/a >; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;在给定的图形实例上群集算法进度的插图。 &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;结果&lt;p>;我们使用基于不同的基于嵌入的基于嵌入式的模型评估了具有不同属性的各种数据集上的新型聚类算法。我们将聚类算法的性能与两个表现最佳的基线进行比较（请参阅&lt;a href=&quot;https://openreview.net/pdf?id=p0jssa1auv&quot;>; paper &lt;/a>;有关更多详细信息）：&lt;/p>; &lt;/p>; &lt;/p>; &lt;/ ul>; &lt;li>;The &lt;a href=&quot;https://arxiv.org/pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; algorithm for budgeted clustering with access to CA only. &lt;/li>; &lt;li>; &lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/spectral_clustering&quot;>; spectral clustering &lt;/a>; &lt;em>; &lt;a href =“ https://en.wikipedia 。相似。 &lt;/li>; &lt;/ul>; &lt;p>;要评估聚类的质量，我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/precision_and_recall&quot;>; precision and Recell &lt;/a>;。精度用于计算所有共簇配对中类似对的百分比，回忆是所有类似对中共簇的类似对的百分比。为了测量从我们的实验中获得的解决方案的质量，我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/f-score&quot;>; f1-score &lt;/a>;是&lt;a href =“ https://en.wikipedia.org/wiki/wiki/harmonic_mean”>;谐波平均值&lt;/a>;精度和回忆，其中1.0是指示完美精度和回忆的最高值，而0是最低可能的值表示精度还是召回零是零的值。下表报告了Kwikbucks和各种基线的F1得分，如果我们仅允许对CA模型进行线性数量的查询。我们表明，与在所有数据集中平均最佳基线相比，Kwikbucks的性能可实现45％的相对提高。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s1574/results.jpg&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 582” data-Original-width =“ 1574” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s16000/results.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “ Text-Align：Center;”>;使用各种公共数据集比较群集算法与两个基线算法：（1）&lt;a href=&quot;https://arxiv.org/arxiv.org/pdf/pdf/2002.11557.pdf&quot;>;聚类&lt;/a>;算法仅适用于仅访问CA的预算聚类，以及（2）&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/wiki/spectral_clustering&quot;>; Spectral clustering&quot;>; Spectral Clustering &lt;/a>;在&lt;em>;上&lt;a href=&quot;https://en.wikipedia.org/wiki/nearest_neighbor_graph&quot;>; k-nearest邻居（knn）图&lt;/a>; &lt;/em>;是通过查询&lt;em>; k &lt;/>; k &lt;/>;而形成的em>;  - 基于嵌入的相似性的每个顶点的最近邻居。可以下载预处理的数据集&lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks/kwikbuckss.zip/szip&quot;>;在这里/table>; &lt;p>;下图使用不同的查询预算比较了群集算法的性能与基线。我们观察到，Kwikbucks在各种预算上始终优于其他基准。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3CboOkZ4lbp9dmH-5EkPGe1zo6JPlOQbXT1n67ke4qN0kXeZXMQKb4SUM-E6TXuyWF9UF7vvKTtl2lecg-Z5tV0mA6Hobh9NKbmy__dqGRIYcTEIzdfGjCCyih2eZW4xF33n7Y1pJGpwyDvQ75rwQsp0kZICGSPoyR8ahiBQwcZvT7ZLLeAA1BQkJtLwU/s1236/stackoverflow.png “样式=” Margin-Left：Auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 602” data-Original-width =“ 1236” src =“ https：// blogger.googleusercontent.com/img/b/r29vz2xl/avvxsej3cbookz4lbp9dmh-5ekpge1zo6jploqbxt1n67ke4qn0kbxt1kmqum um-b4 qkxxxezxxeb4sqkxezxeb4squb4squb4squbb4squbbybmebmybmybmybmy__dmmybmy_dmnybmy ymmy ymmy ymybmy ym ym y CTEIZDFGJCCYIH2EZW4XF33N7Y1PJGPWYDVQ75RWQSPSSP0KZICGSPOYR8AHIBQW​​CZVT7ZLLEAA1BQKJTLWU/s16000/stackoverflow.png标题“ style =”文本align：中心;“>;当允许不同的预算查询跨意见模型时，Kwikbucks与Top-2基准的比较。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>;>; &lt;/table>; &lt;br />; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt; />; &lt; />;嵌入模型是可扩展的，但缺乏质量，而跨注意模型则提供了质量，但可扩展性损坏。我们提出了一种聚集算法，该算法提供了两全其美的最佳：嵌入模型的可扩展性和跨注意模型的质量。 Kwikbucks也可以应用于具有不同精度水平的多个相似性甲骨文的其他聚类问题。通过具有不同属性的各种数据集上的一组实验来验证这一点。有关更多详细信息，请参见&lt;a href=&quot;https://openreview.net/pdf?id=p0jssa1auv&quot;>; paper &lt;/a>;有关更多详细信息。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;在Sandeep Silwal在Sandeep Silwal的暑期实习期间启动了这个项目Google在2022年。我们要感谢我们的合着者Andrew McCallum，Andrew Nystrom，Deepak Ramachandran和Sandeep Silwal对这项工作的宝贵贡献。我们还要感谢Ravi Kumar和John Guilyard在此博客文章上提供的帮助。&lt;/em>; &lt;/p>; &lt;/div>; &lt;/conteg>; &lt;link href =“ http://blog.research.google/feeds/895902470751515398632/注释/默认值“ rel =” reply =“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2023/11/best-of-best-of-both -worlds-achieving.html＃comment-form“ rel =” reply =“ reply” title =“ 0 comment” type =“ text/html”/>; &lt;link href =“ http:/ http://www.blogger.com/feeds/8474749262631452026262626/帖子/默认/8959024707515398632“ rel =“ edit” type =“ application/application/atom+xml”/>; &lt;link href =“ http:/ http://wwwww.blogger.com/feed.com/feed.com/feed.com/feed.com/8474926331452026262626262626249595959595959595959595959595959595959595959595959595959090909090902 self“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2023/11/best-best-both-both-worlds-achieving.html“ =“两个世界上最好的：在文本群集中实现可伸缩性和质量” type =“ text/html”/>; &lt;aunder>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://wwwww.blogger.com/profile /12098626514775266161&lt;/uri>;&lt; ：//img1.blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail height =“ 72” url =“ https：//blogdger 。 3STQF4MIE4F_EADRSI1DXY67MXNSCSWY3WX8VX8VX8KGU_RRPS6ETZNDYABVCEEZGWAUSDZEAHD59Z/s72-c/hire.jpg/hiper.jpg f. /media：thumbnail>; &lt;thr:total>; 0&lt;/thr:total>; &lt;/entry>; &lt;entry>; &lt;id>; tag：blogger.com，1999年：blog-8474926331452026626.post-post-48829629627453745374537131313901122ppard.pulped>; parked>; pp. -11-02T15：01：00.001-07：00 &lt;/publined>; &lt;更新>; 2023-11-02T15：06：20.846-07：00 &lt;/updateated>; &lt;categorated>; &lt;category schemion =“ http：///www.blogger.com /atom/ns＃“ term =“大语言模型”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“ Machine Learning”>; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“自然语言理解”>; &lt;/caterory>; &lt;title type =“ text”>;零摄像的自适应提示&lt;/ stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-aTHOR”>;由学生研究员Xingchen Wan和Ruoxi Sun发表，研究科学家，云AI团队&lt;/span>; &lt;img src =” /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqTosaeIs4AMYukkmgUBEii6iQcrr9_dNKM0cHnW6m9Wi8yX0V-QCduQfkqLyQPNpVbze3OFO-nPG5Wm9DLMM8pEAfhQGq-TEJroLJGQyqNr-hlJNkToBCmgJbphrCRvlv95gkDQH0ScT7VrXu2VCTKyiWy8yYM4G8voF0kD0K2oxwg2M2xBcz1yQnU0Zb/s600/USP.gif&quot; style=&quot;display: none;&quot; />; &lt;p>;大语言模型（LLMS）的最新进展非常有前途，这反映在其在&lt;em>;少数&lt;/em>;和&lt;em>; Zero-Zero-Shot setup中的一般问题解决的能力上&lt;/em>; &lt;/em>; ，即使没有这些任务的明确培训。这是令人印象深刻的，因为在少数弹奏设置中，LLM仅在给出测试问题之前只提供了几个提问的演示。更具挑战性的是零拍设置，其中LLM仅使用&lt;em>;测试问题直接提示&lt;/em>;。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;即使少数弹奏设置已大大减少了适应特定用例的模型所需的数据量，但仍然在某些情况下生成样品提示可能具有挑战性。例如，即使在通用模型涵盖的广泛任务中进行少量演示也可能很困难，也可能是不可能的任务。例如，对于诸如长篇文章的摘要或需要域知识的任务（例如，医疗问答），生成样本答案可能具有挑战性。在这种情况下，由于不需要手动及时生成，因此具有较高零弹性性能的模型非常有用。但是，零射击性能通常较弱，因为LLM没有指导，因此很容易出现虚假输出。 &lt;/p>; &lt;p>;在“ &lt;a href=&quot;https://aclanthology.org/2023.findings-acl.216/&quot;>;更好的零弹药推理中，有自适应提示&lt;/a>;” &lt;a href=&quot;https://2023.aclweb.org/&quot;>; ACL 2023 &lt;/a>;，我们建议基于一致性的自适应提示（COSP）&lt;/em>;解决这一困境。 COSP是一种用于推理问题的零射击自动提示方法，该方法仔细选择和构造了&lt;em>; pseudo  -  &lt;/em>; &lt;/em>; llms的演示，仅使用未标记的样本（通常易于获得）和模型自己的预测。使用COSP，我们在很大程度上缩小了零射击和少量射击之间的性能差距，同时保留了零射击提示的理想通用性。我们以“ &lt;a href=&quot;https://arxiv.org/abs/2305.14926&quot;>;通用自我适应性提示&lt;/a>;”（USP）接受“ &lt;a href=&quot;https://arxiv.org/arxiv.org/arxiv.org/arxiv.org/arxiv.org/arxiv.org/arxiv.org/arxiv.org”。 emnlp.org/&quot;>; emnlp 2023 &lt;/a>;，在其中，我们将这个想法扩展到了广泛的一般性自然语言理解（NLU）和自然语言生成（NLG）任务，并证明了其有效性。 &lt;/p>; &lt;br />; &lt;h2>;提示LLM具有自己的输出&lt;/h2>; &lt;p>;知道LLMS从演示中受益，并且至少具有&lt;em>; &lt;/em>; Zero-Sero-Shot能力，我们是否想知道是否想知道该模型的零击输出可以作为模型提示自身的演示。面临的挑战是，零摄影解决方案是不完美的，我们冒着向LLMS提供质量较差的示威的风险，这可能比根本没有示威情况更糟糕。确实，下图表明，在问题中添加正确的演示可以导致测试问题的正确解决方案（demo1带有问题），而添加错误的演示（演示2 +问题，demo 3带有问题）会导致不正确的答案。 。因此，我们需要选择可靠的自我生成的示范。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/r29vz2xl/avvxsei5qshw0e9ob5qshw0e9ob5qshw0e9ob5vatwlopzgcgcgcgcgcgcgcgcgcgcgccccie1nsnsnsnsnsnsnsnsnsnsnsnsnsnsnsnsnsnsnsnsnsnsnsnsl9m2qccim5pxim2qccim5pxnm2q。 RKE0SYSJ2FIIBSNR_QHCRO-urHQZPQ7FHHPVM6WTYKLSFF9L5ERSBOQTT_J1-A8PJOOA9HTDVXENL4-WRBP-KN885AIZ7GBP-KN85AIZ7G-BVWNFLCAGAG4LUPOMVLLU8 /S1051/image6.png“ style =” Margin-Left：自动; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 1051” data-original-width =“ 776”高度=“ 640” src =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxsei5qshw0e9odtzaodtza9ob5vatwlopzggizgcggizgcgcggcggf9oko-5pxin3m2qcizrihsys73fizrihs73njrihs73nwwrihs73wwrihsnwwrihsnwwrihsnwwrihnwwwoznwwrihnwworhwoznwwrihwoznwworpsnwwwoznwworp ddwozr qhcro-urhqzpq7fhhpvm6wtyklSysff9l5ersboqtt_j1-a8pjooa9htdvxenl4-wrbp-kn85aiz7g-bvwnflcag4lupomvllu8/w472-h640/w472-h640/image6.png />; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left; left;”>;示例输入＆amp;推理任务的输出，这说明了对内部文化演示进行精心设计的选择过程的需求（&lt;a href=&quot;https://arxiv.org/abs/1608.01413&quot;>; Multiarith &lt;/a>; dataset＆nbsp; amp; amp; amp; &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>; palm-62b &lt;/a>;型）： blog.research.google/2022/05/language-models-perform-reasoning-via.html“ style =” text-align：left;&#39;>; the theque &lt;/a>; ：left;“>;＆nbsp;没有演示：正确的逻辑但答案错误； （2）正确的演示（demo1）和正确的答案； （3）正确但重复的演示（Demo2）导致重复输出； （4）错误的演示（Demo3）导致错误的答案；但是（5）将demo3和demo1结合在一起，再次导致正确的答案。&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;b>; &lt;b>; cosp利用了llms的关键观察：自信和一致的预测更可能是正确的。当然，这一观察结果取决于LLM的不确定性估计值。幸运的是，在大型模型中，&lt;a href=&quot;https://arxiv.org/abs/2207.05221&quot;>;上一A>;表明不确定性估计是稳健的。由于测量置信度只需要模型预测而不需要标签，因此我们建议将其用作正确性的零镜头。然后将高信心输出及其输入用作&lt;em>; pseudo &lt;/em>;示例。 &lt;/p>; &lt;p>;以此为前提，我们根据其&lt;a href=&quot;https://arxiv.org/abs/2203.111171&quot;>; self-consisissency &lt;/a>;估计该模型对其输出的信心并使用此措施选择强大的自我生成的演示。我们向LLM多次询问同样的问题，零射门&lt;a href=&quot;https://blog.research.google/2022/05/language-models-perform-perform-reasoning-via.html&quot;>; /a>;（COT）提示。为了指导模型生成一系列可能的理由和最终答案，我们包括由“温度”超参数控制的随机性。在极端情况下，如果该模型100％确定，则每次都应输出相同的最终答案。然后，我们计算答案的熵以衡量不确定性 - 具有高自符合性的答案以及LLM更确定的答案可能是正确的，并且将被选中。 &lt;/p>; &lt;p>;假设wem>; e &lt;/em>;提出了一系列未标记的问题，COSP方法是：&lt;/p>; &lt;ol>; &lt;li>;输入每个未标记的问题通过多次采样模型来获得多个理由和答案。最常见的答案是突出显示的，其次是一个分数，该得分衡量了多个采样输出的答案的一致性（较高）。除了赞成更一致的答案外，我们还对回应（即重复的单词或短语）中的重复进行惩罚，并鼓励选定示威的多样性。我们以评分函数的形式将偏好编码为一致，不重复和多样的输出，该计数功能由三个分数的加权总和组成，以选择自我生成的伪示例。&lt;/li>; &lt;li>;我们相处伪示例示出了测试问题，将其馈送到LLM，并获得最终的预测答案。&lt;/li>; &lt;/ol>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr =” tr -caption-container“ style =”边距 - 左：自动; margin-right：auto;“>; &lt;tbody>; &lt;ttry>; &lt;tr>; &lt;td style =” text-align：center;“ center;”>; &lt;a href =“ https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidn_WHASrzLG7rsqvKr5XqTi0VE0hEonQJsCRy0XHnM_DsWP4u1izMPUvfakpLd9FJvu47XXspD3vgIXrnrEhbrGR5vxSkcRRtbrU7HwHh6zLpywepMg39GUAW6uSVYxW-JzdOl5IVt9KPqLDVVCvjz86e6vQgZK79FKAF5wGdpztohbWhe7tVtVwcNBgh/s949/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original- height=&quot;396&quot; data-original-width=&quot;949&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidn_WHASrzLG7rsqvKr5XqTi0VE0hEonQJsCRy0XHnM_DsWP4u1izMPUvfakpLd9FJvu47XXspD3vgIXrnrEhbrGR5vxSkcRRtbrU7HwHh6zLpywepMg39GUAW6uSVYxW-JzdOl5IVt9KPqLDVVCvjz86e6vQgZK79FKAF5wGdpztohbWhe7tVtVwcNBgh/s16000/image4.png&quot; />;&lt;/a>;&lt; /td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left;”>; cosp的插图：在阶段1中（&lt;strong>;左&lt;/strong>;），我们多次运行零弹床以生成一个演示库（每个都由问题，生成的理由和预测组成）并分配得分。在第2阶段（&lt;strong>;右&lt;/strong>;）中，我们用伪demos（蓝色框）增加了当前的测试问题，然后再次查询LLM。对两个阶段的输出的多数投票构成了最终预测。&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;b />; &lt;b>; cosp专注于带有cot提示的提问任务由于问题具有独特的正确答案，因此很容易衡量自遇到。 But this can be difficult for other tasks, such as open-ended question-answering or generative tasks that don&#39;t have unique answers (eg, text summarization). To address this limitation, we introduce USP in which we generalize our approach to other general NLP tasks: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Classification&lt;/em>; (CLS): Problems where we can compute the probability of each class using the neural network output logits of each class. In this way, we can measure the uncertainty without multiple sampling by computing the entropy of the logit distribution.&lt;/li>; &lt;li>;&lt;em>;Short-form generation&lt;/em>; (SFG): Problems like question answering where we can use the same procedure mentioned above for COSP, but, if necessary, without the rationale-generating step.&lt;/li>; &lt;li>;&lt;em>;Long-form generation&lt;/em>; (LFG):&lt;em>; &lt;/em>;Problems like summarization and translation, where the questions are often open-ended and the outputs are unlikely to be identical, even if the LLM is certain. In this case, we use an &lt;em>;overlap metric&lt;/em>; in which we compute the average of the &lt;em>;pairwise&lt;/em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE score&lt;/a>; between the different outputs to the same query.&lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjn2mgVNUmsKVPYKo3zrcQnq3nHT0xIzCk2rIOK0fSrFIOEkyCrx7MWNnTrOdwnFRlGbid1cj8OqV2xBCfOtgv5oiuUPoQjRY9CpMnjM79P0mQmoyQqluMPZsqFQUtS7AtPy5Uw-sf5UT_dV_bRbGWSRQiR5U2tDIYd2zxsk_lboJsKG4mcBZKxp5gEeT_T/s1360/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;640&quot; data-original-width=&quot;1360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjn2mgVNUmsKVPYKo3zrcQnq3nHT0xIzCk2rIOK0fSrFIOEkyCrx7MWNnTrOdwnFRlGbid1cj8OqV2xBCfOtgv5oiuUPoQjRY9CpMnjM79P0mQmoyQqluMPZsqFQUtS7AtPy5Uw-sf5UT_dV_bRbGWSRQiR5U2tDIYd2zxsk_lboJsKG4mcBZKxp5gEeT_T/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Illustration of USP in exemplary tasks (classification, QA and text summarization). Similar to COSP, the LLM first generates predictions on an unlabeled dataset whose outputs are scored with logit entropy, consistency or alignment, depending on the task type, and pseudo-demonstrations are selected from these input-output pairs. In Stage 2, the test instances are augmented with pseudo-demos for prediction.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We compute the relevant confidence scores depending on the type of task on the aforementioned set of unlabeled test samples. After scoring, similar to COSP, we pick the confident, diverse and less repetitive answers to form a model-generated pseudo-demonstration set. We finally query the LLM again in a few-shot format with these pseudo-demonstrations to obtain the final predictions on the entire test set. &lt;/p>; &lt;br />; &lt;h2>;Key Results&lt;/h2>; &lt;p>; For COSP, we focus on a set of six arithmetic and commonsense reasoning problems, and we compare against 0-shot-CoT (ie, “&lt;a href=&quot;https://arxiv.org/abs/2205.11916&quot;>;Let&#39;s think step by step&lt;/a>;“ only). We use self-consistency in all baselines so that they use roughly the same amount of computational resources as COSP. Compared across three LLMs, we see that zero-shot COSP significantly outperforms the standard zero-shot baseline. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsRevpmsBSIsIMti071v4FFnK-khtMXTbqvf92wvdvKQhyphenhyphenTCADcsIOQhAtr1kfxkmoaQ5MNKmcJiYol1JmxCchzYl9Kn_9h9eaGZAvJBrRxsvJorbngS4fLaChBk6e9wtHgE7JvPMHN1gajpnhgZaCwuYOjo-CTUR9x8PbXEEbRG3vp8elQzMib5Kv5Qh-/s1883/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1738&quot; data-original-width=&quot;1883&quot; height=&quot;369&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsRevpmsBSIsIMti071v4FFnK-khtMXTbqvf92wvdvKQhyphenhyphenTCADcsIOQhAtr1kfxkmoaQ5MNKmcJiYol1JmxCchzYl9Kn_9h9eaGZAvJBrRxsvJorbngS4fLaChBk6e9wtHgE7JvPMHN1gajpnhgZaCwuYOjo-CTUR9x8PbXEEbRG3vp8elQzMib5Kv5Qh-/w400-h369/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Key results of COSP in six arithmetic (&lt;a href=&quot;https://arxiv.org/abs/1608.01413&quot; style= &quot;text-align: left;&quot;>;MultiArith&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot; style=&quot;text-align: left;&quot;>;GSM-8K&lt;/a>; , &lt;a href=&quot;https://aclanthology.org/D14-1058/&quot; style=&quot;text-align: left;&quot;>;AddSub&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1162 /tacl_a_00160&quot; style=&quot;text-align: left;&quot;>;SingleEq&lt;/a>;) and commonsense (&lt;a href=&quot;https://doi.org/10.18653/v1/N19-1421&quot; style=&quot;text-align : left;&quot;>;CommonsenseQA&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1162/tacl_a_00370&quot; style=&quot;text-align: left;&quot;>;StrategyQA&lt;/a>;) reasoning tasks using &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM-62B, PaLM-540B&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;GPT-3 ( code-davinci-001)&lt;/a>; models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class =&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcbJaMBl3_Pc5S_mp9lDWz4v_SS6Mzd3QBxyn-tWdtnDNgJLX1YPRHkdPtJBanhADoEDTzJqOsk2x4uOFoV8e0h8Tml_Nt1kOd5eT7TyC-MIqWEVEiY_kZQZ0wkPU9T5HwIHXv2IHeLXdGz1MvWiQFxNNe7CaKDfUs0j9R6F8cgN4T0dgtgp86xsF9cK6l/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1999&quot; height=&quot;325&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcbJaMBl3_Pc5S_mp9lDWz4v_SS6Mzd3QBxyn-tWdtnDNgJLX1YPRHkdPtJBanhADoEDTzJqOsk2x4uOFoV8e0h8Tml_Nt1kOd5eT7TyC-MIqWEVEiY_kZQZ0wkPU9T5HwIHXv2IHeLXdGz1MvWiQFxNNe7CaKDfUs0j9R6F8cgN4T0dgtgp86xsF9cK6l/w640-h325/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;USP improves significantly on 0-shot performance. “CLS” is an average of 15 classification tasks; “SFG” is the average of five short-form generation tasks; “LFG” is the average of two summarization tasks. “SFG (BBH)” is an average of all BIG-Bench Hard tasks, where each question is in SFG format.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For USP, we expand our analysis to a much wider range of tasks, including more than 25 classifications, short-form generation, and long-form generation tasks. Using the state-of-the-art PaLM 2 models, we also test against the &lt;a href=&quot;https://arxiv.org/abs/2210.09261&quot;>;BIG-Bench Hard&lt;/a>; suite of tasks where LLMs have previously underperformed compared to people. We show that in all cases, USP again outperforms the baselines and is competitive to prompting with golden examples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsJh9EIxiF8m6jqOM7vYniG6LX8gFVa0W3V7_DEOwMCEZjbk_M4104Tg-3qMUAbYXaR3RqMTQsrZ8TN3Np5I9SXHBVJGjB6E6_1khoubPbbR-AjUnV37TYGBHu2DJpR50Ek34dwcFkjJB-OTqW4T4E-zjQhJqPX-f_PxSj7322nU2qH2VK0TBMO1QKUkfT/s901 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;833&quot; height=&quot; 640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsJh9EIxiF8m6jqOM7vYniG6LX8gFVa0W3V7_DEOwMCEZjbk_M4104Tg-3qMUAbYXaR3RqMTQsrZ8TN3Np5I9SXHBVJGjB6E6_1khoubPbbR-AjUnV37TYGBHu2DJpR50Ek34dwcFkjJB-OTqW4T4E-zjQhJqPX-f_PxSj7322nU2qH2VK0TBMO1QKUkfT/w592-h640/image5.png&quot; width=&quot;592&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Accuracy on BIG- Bench Hard tasks with PaLM 2-M (each line represents a task of the suite). The gain/loss of USP (green stars) over standard 0-shot (green triangles) is shown in percentages. “Human” refers to average human performance; “AutoCoT” and “Random demo” are baselines we compared against in the&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2305.14926&quot;>;paper&lt;/a>;; and “3-shot” is the few-shot performance for three handcrafted demos in CoT format.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also analyze the working mechanism of USP by validating the key observation above on the relation between confidence and correctness, and we found that in an overwhelming majority of the cases, USP picks confident predictions that are more likely better in all task types considered, as shown in the如下图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjahSci_hJqgqxS-my79ZlINe9Dpi0yaLrMTXeTu-HXjmmAmhVXtylXK7BUdJGIPMFhFvqv31Wd6ux2ti1hJLIi9Rr8j_PIjD9ElUTWx0ViaP1fLg63Q9lqvgd7U74Uqi45IOU3CsqHaqjO94iSLyPD2dZCJf5x5hsHfU_6yukbJHpG6wDzdSVp7nwBHNFx/s1474/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;507&quot; data-original-width=&quot;1474&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjahSci_hJqgqxS-my79ZlINe9Dpi0yaLrMTXeTu-HXjmmAmhVXtylXK7BUdJGIPMFhFvqv31Wd6ux2ti1hJLIi9Rr8j_PIjD9ElUTWx0ViaP1fLg63Q9lqvgd7U74Uqi45IOU3CsqHaqjO94iSLyPD2dZCJf5x5hsHfU_6yukbJHpG6wDzdSVp7nwBHNFx/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;USP picks confident predictions that are more likely better. Ground-truth performance metrics against USP confidence scores in selected tasks in various task types (blue: CLS, orange: SFG, green: LFG) with PaLM-540B.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Zero-shot inference is a highly sought-after capability of modern LLMs, yet the success in which poses unique challenges. We propose COSP and USP, a family of versatile, zero-shot automatic prompting techniques applicable to a wide range of tasks. We show large improvement over the state-of-the-art baselines over numerous task and model combinations. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was conducted by Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Martin Eisenschlos, Sercan Ö. Arık, and Tomas Pfister. We would like to thank Jinsung Yoon Xuezhi Wang for providing helpful reviews, and other colleagues at Google Cloud AI Research for their discussion and feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4882962745371313901/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4882962745371313901&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4882962745371313901&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot; rel=&quot;alternate&quot; title=&quot;Zero-shot adaptive prompting of large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqTosaeIs4AMYukkmgUBEii6iQcrr9_dNKM0cHnW6m9Wi8yX0V-QCduQfkqLyQPNpVbze3OFO-nPG5Wm9DLMM8pEAfhQGq-TEJroLJGQyqNr-hlJNkToBCmgJbphrCRvlv95gkDQH0ScT7VrXu2VCTKyiWy8yYM4G8voF0kD0K2oxwg2M2xBcz1yQnU0Zb/s72-c/USP.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4489259534129284932&lt;/id>;&lt;published>;2023-11-01T10:30:00.002-07:00&lt;/published>;&lt;updated>;2023-11-06T09:07:40.958-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MetNet-3: A state-of-the-art neural weather model available in Google products&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Samier Merchant, Google Research, and Nal Kalchbrenner, Google DeepMind&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdgnhML03N9vxEdGH1TkBATtxGpjyO5XYgZwJY5dY0-sPIAvrmCll4J8I9owyJTNOHZdq6MMZskWsYJDZivZA_zvj2atWhUsPoxWnNyifiFAm83GC2EsZ4xgre8bCk32Yzv3vlR4pGn12H7T5Vkbz5BaErZ22JRB-OqveQ7EDHsrCYjKN65Soc1FrZNwvu/s1600/metnethero1.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Forecasting weather variables such as precipitation, temperature, and wind is key to numerous aspects of society, from daily planning and transportation to energy production.随着我们不断看到更多的洪水、干旱和热浪等极端天气事件，准确的预报对于准备和减轻其影响至关重要。未来的前 24 小时尤其重要，因为它们具有高度可预测性和可操作性，可以帮助人们及时做出明智的决策并保证安全。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today we present a new weather model called &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;MetNet-3&lt;/a>;, developed by Google Research and Google DeepMind. Building on the earlier &lt;a href=&quot;https://blog.research.google/2020/03/a-neural-weather-model-for-eight-hour.html?m=1&quot;>;MetNet&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;MetNet-2&lt;/a>; models, MetNet-3 provides high resolution predictions up to 24 hours ahead for a larger set of core variables, including precipitation, surface temperature, wind speed and direction, and dew point. MetNet-3 创建时间上平滑且高度精细的预测，提前时间间隔为 2 分钟，空间分辨率为 1 至 4 公里。 MetNet-3 achieves strong performance compared to traditional methods, outperforming the best single- and multi-member physics-based &lt;a href=&quot;https://en.wikipedia.org/wiki/Numerical_weather_prediction&quot;>;numerical weather prediction&lt;/a>; (NWP) models — such as &lt;a href=&quot;https://rapidrefresh.noaa.gov/hrrr/&quot;>;High-Resolution Rapid Refresh&lt;/a>; (HRRR) and &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts#:~:text=ENS%20is%20a%20probabilistic%20forecast,high%20winds%20or%20heavy%20rain).&quot;>;ensemble forecast suite&lt;/a>; (ENS) — for multiple regions up to 24 hours ahead. &lt;/p>; &lt;p>; Finally, we&#39;ve integrated MetNet-3&#39;s capabilities across various Google &lt;a href=&quot;https://support.google.com/websearch/answer/13692898&quot;>;products and technologies&lt;/a>; where weather is relevant. MetNet-3 目前在美国本土和欧洲部分地区推出，重点是 12 小时降水预报，正在帮助为多个国家和语言的人们提供准确可靠的天气信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQcwPsDQPUe4Uon7vWWSewbqcWsAdfUIJ4yLLFiCvdQKu4ffT6E5qIMeiabtxK5wudSL-jjxa_fW5aOaBvDILq_dQzeT4RMSULORJZrjwkDscDxLnLflUybqHlPf1J8O7KB171g5I9kLVgRbGP0mr0HxbG0pY7J9ojoEZLl4JZHaMQH490XmUR_IUj_YMO/s904/image55.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQcwPsDQPUe4Uon7vWWSewbqcWsAdfUIJ4yLLFiCvdQKu4ffT6E5qIMeiabtxK5wudSL-jjxa_fW5aOaBvDILq_dQzeT4RMSULORJZrjwkDscDxLnLflUybqHlPf1J8O7KB171g5I9kLVgRbGP0mr0HxbG0pY7J9ojoEZLl4JZHaMQH490XmUR_IUj_YMO/s16000/image55.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixRY-kzsepNdP_arXnJbHPJFViN_N4CzjOYH_1YxfjIDI5Nben4u8BoJ-tcYrrw4a3Jp7HFBGmakeBMqKAINeVFssClJHNUjvBhYHY6vpy6nOdpEoFDhCulwIE8OM9e7fRRwXqW01AeWUJjqmnNDn32ScCeQ2S64aNvDgigDes5vWA1_RrT7oMxK8sttG7/s904/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEixRY-kzsepNdP_arXnJbHPJFViN_N4CzjOYH_1YxfjIDI5Nben4u8BoJ-tcYrrw4a3Jp7HFBGmakeBMqKAINeVFssClJHNUjvBhYHY6vpy6nOdpEoFDhCulwIE8OM9e7fRRwXqW01AeWUJjqmnNDn32ScCeQ2S64aNvDgigDes5vWA1_RrT7oMxK8sttG7/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing= “0”类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MetNet-3 precipitation output summarized into actionable forecasts in Google Search on mobile.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Densification of sparse observations&lt;/h2>; &lt;p>; Many recent machine learning weather models use the atmospheric state generated by traditional methods (eg, data assimilation from NWPs) as the primary starting point to build forecasts. In contrast, a defining feature of the MetNet models has been to use direct observations of the atmosphere for training and evaluation. The advantage of direct observations is that they often have higher fidelity and resolution. However, direct observations come from a large variety of sensors at different altitudes, including weather stations at the surface level and satellites in orbit, and can be of varying degrees of sparsity. For example, precipitation estimates derived from radar such as &lt;a href=&quot;https://mrms.nssl.noaa.gov/&quot;>;NOAA&#39;s Multi-Radar/Multi-Sensor System&lt;/a>; (MRMS) are relatively dense images, whereas weather stations located on the ground that provide measurements for variables such as temperature and wind are mere points spread over a region. &lt;/p>; &lt;p>; In addition to the data sources used in previous MetNet models, MetNet-3 includes point measurements from weather stations as both inputs and targets with the goal of making a forecast at all locations.为此，MetNet-3 的关键创新是一种称为致密化的技术，它将基于物理的模型中传统的数据同化和模拟两步过程合并到神经网络的单次传递中。致密化的主要组成部分如下所示。尽管致密化技术单独适用于特定的数据流，但由此产生的致密化预测受益于进入 MetNet-3 的所有其他输入流，包括地形、卫星、雷达和 NWP 分析功能。 MetNet-3 的默认输入中不包含 NWP 预报。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie1m1p0i-MWhS7Ih5RGzV-AQuDDPwgao4SpmnSUTdSsy7fcEwk4Soj5IJ8FqtGjhvi4ot2HKZdaQh3Hpu4CviRsx7FujT_4bbvpV8mu15Zt5bO5KbMGaaqIZoAGUp77ltVYH-zt2HTwVxbuGZHJt-0lbXZT-ukJH_KtB3pnHdRrRpZ2r5WgMSNGXnu-H8j /s1929/image22.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1929&quot; data-original-width=&quot;1600&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie1m1p0i-MWhS7Ih5RGzV-AQuDDPwgao4SpmnSUTdSsy7fcEwk4Soj5IJ8FqtGjhvi4ot2HKZdaQh3Hpu4CviRsx7FujT_4bbvpV8mu15Zt5bO5KbMGaaqIZoAGUp77ltVYH-zt2HTwVxbuGZHJt-0lbXZT-ukJH_KtB3pnHdRrRpZ2r5WgMSNGXnu-H8j/s16000/image22.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;A&lt;/b>;) During training, a fraction of the weather stations are masked out from the input while kept in the target. &lt;b>;B&lt;/b>;) To evaluate generalization to untrained locations, a set of weather stations represented by squares is never used for training and is only used for evaluation. &lt;b>;C&lt;/b>;) Data from these held out weather stations with sparse coverage is included during evaluation to determine prediction quality in these areas. &lt;b>;D&lt;/b>;) The final forecasts use the full set of training weather stations as input and produce fully dense forecasts aided by spatial parameter sharing.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;High resolution in space and time&lt;/h2>; &lt;p>; A central advantage of using direct observations is their high spatial and temporal解决。例如，气象站和地面雷达站分别以 1 公里的分辨率每隔几分钟在特定点提供测量结果； this is in stark contrast with the assimilation state from the state-of-the-art model &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts#:~:text=ENS%20is%20a%20probabilistic%20forecast,high%20winds%20or%20heavy%20rain).&quot;>;ENS&lt;/a>;, which is generated every 6 hours at a resolution of 9 km with hour-by-hour forecasts. To handle such a high resolution, MetNet-3 preserves another of the defining features of this series of models, &lt;em>;lead time conditioning&lt;/em>;.以分钟为单位的预测提前时间直接作为神经网络的输入给出。这使得 MetNet-3 能够有效地对短至 2 分钟的时间间隔内的观测的高时间频率进行建模。 Densification combined with lead time conditioning and high resolution direct observations produces a fully dense 24 hour forecast with a temporal resolution of 2 minutes, while learning from just 1,000 points from the &lt;a href=&quot;https://madis.ncep.noaa.gov/madis_OMO.shtml&quot;>;One Minute Observation&lt;/a>; (OMO) network of weather stations spread across the United States. &lt;/p>; &lt;p>; MetNet-3 predicts a marginal multinomial probability distribution for each output variable and each location that provides rich information beyond just the mean. This allows us to compare the probabilistic outputs of MetNet-3 with the outputs of advanced probabilistic ensemble NWP models, including the ensemble forecast ENS from the &lt;a href=&quot;https://www.ecmwf.int/&quot;>;European Centre for Medium-Range Weather Forecasts&lt;/a>; and the &lt;a href=&quot;https://www.spc.noaa.gov/exper/href/&quot;>;High Resolution Ensemble Forecast&lt;/a>; (HREF) from the &lt;a href=&quot;https://www.noaa.gov/&quot;>;National Oceanic and Atmospheric Administration of the US&lt;/a>;. Due to the probabilistic nature of the outputs of both models, we are able to compute scores such as the &lt;a href=&quot;https://confluence.ecmwf.int/display/FUG/Section+12.B+Statistical+Concepts+-+Probabilistic+Data#:~:text=The%20Continuous%20Ranked%20Probability%20Score,the%20forecast%20is%20wholly%20inaccurate.&quot;>;Continuous Ranked Probability Score&lt;/a>; (CRPS).下图突出显示了致密化结果，并说明 MetNet 的预测不仅具有更高的分辨率，而且在重叠交付周期进行评估时也更加准确。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJsf6Y6gV9VjK_rS_Bf_WLWdsJOq3sQbdaW26VSp2vX1Fq5j7VcWl4VDi3BeBFpEcH_YGrkU9ozJyuP5dh8tWWCU4yGzlmGBTfwM-kXGKZvdvI1DF17V4kSJSGGBIacqaCO4N1Oc8P4PymPWdglJbew_cjP9reFSJuHR3_ikZfZFuzN6aC8F17TAtiJPIg/s768/image44.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiJsf6Y6gV9VjK_rS_Bf_WLWdsJOq3sQbdaW26VSp2vX1Fq5j7VcWl4VDi3BeBFpEcH_YGrkU9ozJyuP5dh8tWWCU4yGzlmGBTfwM-kXGKZvdvI1DF17V4kSJSGGBIacqaCO4N1Oc8P4PymPWdglJbew_cjP9reFSJuHR3_ikZfZFuzN6aC8F17TAtiJPIg/s16000/image44.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;b>;Top&lt;/b>;: MetNet-3&#39;s forecast of wind speed for each 2 minutes over the future 24 hours with a spatial resolution of 4km. &lt;b>;Bottom&lt;/b>;: ENS&#39;s hourly forecast with a spatial resolution of 18 km. &lt;br />;The two distinct regimes in spatial structure are primarily driven by the presence of the Colorado mountain ranges.较暗对应于较高的风速。 More samples available here: &lt;a href=&quot;https://youtube.com/watch?v=iB1DzHNqH_o&quot;>;1&lt;/a>;, &lt;a href=&quot;https://youtube.com/watch?v=LlWB558jKJk&quot; >;2&lt;/a>;, &lt;a href=&quot;https://youtube.com/watch?v=74bFo3nkbe4&quot;>;3&lt;/a>;, &lt;a href=&quot;https://youtube.com/watch?v= MKUzYQZn9sQ&quot;>;4&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhBhlSum7x274E9KQGzLnjM9iXNEhifOJjKzt1Cwa5YyABCbaB68Mkr3gFvIVUhyphenhyphenaIGOqUE78MqGTK992NK8zrdKrqKxtFlYf1qeWYNkTa4PVzD3u_9lmQAjKnbLILHAkPhIOCvyAI6qBtfyf-z_xgUys3gXRJd_GSs3-qnyq0yFbjvmxdXAbVldV-xrIRJ/s1120/image11.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0 &quot; data-original-height=&quot;694&quot; data-original-width=&quot;1120&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBhlSum7x274E9KQGzLnjM9iXNEhifOJjKzt1Cwa5YyABCbaB68Mkr3gFvIVUhyphenhyphenaIGOqUE78MqGTK992NK8zrdKrqKxtFlYf1qeWYNkTa4PVzD3u_9lmQAjKnbLILHAkPhIOCvyAI6qBtfyf-z_xgUys3gXRJd_GSs3-qnyq0yFbjvmxdXAbVldV-xrIRJ/s16000/image11. png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance comparison between MetNet-3 and NWP baseline for wind speed based on CRPS (lower is better).在超本地设置中，测试气象站的值在评估期间作为网络的输入给出； the results improve further especially in the early lead times.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In contrast to weather station variables, precipitation estimates are more dense as they come from ground radar. MetNet-3 的降水建模与 MetNet-1 和 2 类似，但将 1 公里空间粒度的高分辨率降水预报扩展到与其他变量相同的 24 小时提前时间，如下面的动画所示。在整个 24 小时范围内，MetNet-3 在降水方面的性能实现了比 ENS 更好的 CRPS 值。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidyInWvqdPpdIndLrxzykAODCeJ_p69uqvYOpasjFcBQU5o8Mtr-DiLfZXZrkJel9TD9SxZEmyIb58r6TZjRw57D8aSjl9P2jxCOsK7XZeXY0J3B8UMIFnl6aqXqhd0wft_NQGBi9KqpSUHAgw2c4JoYMdt27sKp6xcvOyMfjASpaZZzlI9o8lesj3GsrL/s720/image14.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;683&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEidyInWvqdPpdIndLrxzykAODCeJ_p69uqvYOpasjFcBQU5o8Mtr-DiLfZXZrkJel9TD9SxZEmyIb58r6TZjRw57D8aSjl9P2jxCOsK7XZeXY0J3B8UMIFnl6aqXqhd0wft_NQGBi9KqpSUHAgw2c4JoYMdt27sKp6xcvOyMfjASpaZZzlI9o8lesj3GsrL/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Case study for Thu Jan 17 2019 00:00 UTC showing the probability of instantaneous precipitation rate being above 1 mm/h on CONUS.较暗对应于较高的概率值。 The maps also show the prediction threshold when optimized towards Critical Success Index &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;CSI&lt;/a>; (dark blue contours).这个具体的案例研究显示了美国中部新的大降水模式的形成；这不仅仅是对现有模式的预测。 &lt;br />;&lt;b>;Top:&lt;/b>; ENS&#39;s hourly forecast. &lt;b>;Center:&lt;/b>; Ground truth, source NOAA&#39;s MRMS. &lt;b>;Bottom:&lt;/b>; Probability map as predicted by MetNet-3. &lt;a href=&quot;https://www.youtube.com/watch?v=TXqR9lL4368&quot;>;Native resolution available here.&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_4M2Sz50c_PDZkyHqZGfc5p5aRGpAS04ztN9N3s3VBn4_AD8GN7Vv6Vw-2phokpqtamutHT_6nGSsXb7271cfijLu3vJT1IV8Mmo1wlq1jfYcUPNs7TL6z0Cls3qGD1jA4Z0uRpj_rNXYLpFSbHEIqNOAA_V8VE_ZhsO7o-D64nDdmRei_hPEY7YT8lcg/s1102/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;682&quot; data-original-width=&quot;1102&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_4M2Sz50c_PDZkyHqZGfc5p5aRGpAS04ztN9N3s3VBn4_AD8GN7Vv6Vw-2phokpqtamutHT_6nGSsXb7271cfijLu3vJT1IV8Mmo1wlq1jfYcUPNs7TL6z0Cls3qGD1jA4Z0uRpj_rNXYLpFSbHEIqNOAA_V8VE_ZhsO7o-D64nDdmRei_hPEY7YT8lcg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance comparison between MetNet-3 and NWP baseline for instantaneous precipitation rate on CRPS (lower is better).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Delivering realtime ML forecasts&lt;/h2>; &lt;p>; Training and evaluating a weather forecasting model like MetNet-3 on historical data is only a part of the process of delivering ML-powered forecasts to users.开发用于天气预报的实时机器学习系统时需要考虑很多因素，例如从多个不同来源获取实时输入数据、运行推理、实现输出的实时验证、从模型的丰富输出中构建洞察带来直观的用户体验，并以 Google 规模提供结果——所有这些都是连续循环的，每隔几分钟刷新一次。 &lt;/p>; &lt;p>; We developed such a real-time system that is capable of producing a precipitation forecast every few minutes for the entire contiguous United States and for 27 countries in Europe for a lead time of up to 12 hours. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg88AA6lzoFtJd9ZOXt6AiiT_gTtFcJwsZNzUJ63kuYtq7XYs0LHUSp3q37zOPolA-rR_WQPciuDZsg-4Y3J0qrLUmNxMi1iBqyR4ICy4MKwRFXHtQhfkWdwPREd4qm9FVlN6rpLEebDC7MfBg7hToXhQvdsFoGObtu-Lqty3ZQSALf1yjna37tJY4fAptE/s1600/image6.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;367&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg88AA6lzoFtJd9ZOXt6AiiT_gTtFcJwsZNzUJ63kuYtq7XYs0LHUSp3q37zOPolA-rR_WQPciuDZsg-4Y3J0qrLUmNxMi1iBqyR4ICy4MKwRFXHtQhfkWdwPREd4qm9FVlN6rpLEebDC7MfBg7hToXhQvdsFoGObtu-Lqty3ZQSALf1yjna37tJY4fAptE/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the process of generating precipitation forecasts using MetNet-3.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The system&#39;s uniqueness stems from its use of near-continuous inference, which allows the model to constantly create full forecasts based on incoming data streams.这种推理模式不同于传统的推理系统，由于输入数据的独特特征，这种推理模式是必要的。该模型采用各种数据源作为输入，例如雷达、卫星和数值天气预报同化。这些输入中的每一个都具有不同的刷新频率以及空间和时间分辨率。一些数据源（例如天气观测和雷达）具有类似于连续数据流的特征，而其他数据源（例如数值天气预报同化）则类似于批量数据。该系统能够在空间和时间上对齐所有这些数据源，从而使模型能够以非常高的节奏创建对未来 12 小时降水的最新了解。 &lt;/p>; &lt;p>; With the above process, the model is able to predict arbitrary discrete probability distributions.我们开发了新颖的技术，将这种密集的输出空间转换为用户友好的信息，从而在整个 Google 产品和技术中提供丰富的体验。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Weather features in Google products&lt;/h2>; &lt;p>; People around the world rely on Google every day to provide helpful, timely, and accurate information about the weather.这些信息可用于多种目的，例如规划户外活动、旅行打包以及在恶劣天气事件中保持安全。 &lt;/p>; &lt;p>; The state-of-the-art accuracy, high temporal and spatial resolution, and probabilistic nature of MetNet-3 makes it possible to create unique hyperlocal weather insights. For the contiguous United States and Europe, MetNet-3 is operational and produces real-time 12 hour precipitation forecasts that are now served across Google &lt;a href=&quot;https://support.google.com/websearch/answer/13692898&quot;>;products and technologies&lt;/a>; where weather is relevant, such as Search.模型的丰富输出被合成为可操作的信息，并立即提供给数百万用户。 &lt;/p>; &lt;p>; For example, a user who searches for weather information for a precise location from their mobile device will receive highly localized precipitation forecast data, including timeline graphs with granular minute breakdowns depending on the product. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4R591KKD1ZkmhTrjo28JovCeeo2bGjb0Tn5Ohr8KEooVqZqSNlgsrJrROaPWn5XXBzEohkhZMjaX2AV3M1RikyLgO7LfIgTFt54-uumb7xxPU6blnuFC8dN8W2SjK85tBKfZQ9Kn4oR-988YKXVUTbu-N5LWWX6JurqN6RRad7Bve59oEdZC-eMsn4HH9/s600/metnet3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;600&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4R591KKD1ZkmhTrjo28JovCeeo2bGjb0Tn5Ohr8KEooVqZqSNlgsrJrROaPWn5XXBzEohkhZMjaX2AV3M1RikyLgO7LfIgTFt54-uumb7xxPU6blnuFC8dN8W2SjK85tBKfZQ9Kn4oR-988YKXVUTbu-N5LWWX6JurqN6RRad7Bve59oEdZC-eMsn4HH9/s16000/metnet3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MetNet-3 precipitation output in weather on the Google app on Android (&lt;b>;left&lt;/b>;) and mobile web Search (&lt;b>;right&lt; /b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2 >; &lt;p>; MetNet-3 is a new deep learning model for weather forecasting that outperforms state-of-the-art physics-based models for 24-hour forecasts of a core set of weather variables.它有潜力为天气预报创造新的可能性，并提高许多活动的安全性和效率，例如交通、农业和能源生产。 MetNet-3 已投入运行，其预报可在多个与天气相关的 Google 产品中提供。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of this effort 。我们要特别感谢 Google DeepMind（Di Li、Jeremiah Harmsen、Lasse Espeholt、Marcin Andrychowicz、Zack Ontiveros）、Google Research（Aaron Bell、Akib Uddin、Alex Merose、Carla Bromberg、Fred Zyda、Isalo Montacute、Jared Sisk）的人员、杰森·希基、卢克·巴林顿、马克·杨、玛雅·托希迪、娜塔莉·威廉姆斯、普拉莫德·古普塔、施瑞亚·阿格拉瓦尔、托马斯·特恩布尔、汤姆·斯莫尔、泰勒·拉塞尔）和 Google 搜索（奥古斯丁·佩斯西亚洛、比尔·迈尔斯、丹尼·切雷斯尼克、乔纳森·卡什、利奥尔·科恩） , Maca Piombi, Maia Diamant, Max Kamenetsky, Maya Ekron, Mor Schlesinger, Neta Gefen-Doron, Nofar Peled Levi, Ofer Lehr, Or Hillel, Rotem Wertman, Tamar Shevach,Vinay Ruelius Shah, Yechie Labai).&lt;/em>;&lt; /p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4489259534129284932/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4489259534129284932&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4489259534129284932&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http: //blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html&quot; rel=&quot;alternate&quot; title=&quot;MetNet-3: A state-of-the-art neural weather model available in Google products&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhdgnhML03N9vxEdGH1TkBATtxGpjyO5XYgZwJY5dY0-sPIAvrmCll4J8I9owyJTNOHZdq6MMZskWsYJDZivZA_zvj2atWhUsPoxWnNyifiFAm83GC2EsZ4xgre8bCk32Yzv3vlR4pGn12H7T5Vkbz5BaErZ22JRB-OqveQ7EDHsrCYjKN65Soc1FrZNwvu/s72-c/metnethero1.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4973261706617691472&lt;/id>;&lt;published>;2023-10-27T13:22:00.001- 07:00&lt;/published>;&lt;updated>;2023-10-31T14:14:34.982-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot; Android Wear&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Audioplethysmography for cardiac monitoring with hearable devices&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Xiaoran &quot;Van&quot; Fan, Experimental Scientist, and Trausti Thormundsson, Director, Google &lt;/span>; &lt;img src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnCItsRGs939CGCBBAi5fA-xwk9JQi7b2CQ3voKK583p4uTkQkLIXUd1lU71SLynom0Yt78bRxDflolUzzfol5UbfkPmCaNn8bIUxwAbLDatqZTPxP7SYOT45g9qJODdM1kvT6NQUsixtFTiBYr_h_Hx-pFRsmbcBKdnW3WkbcD4Bcr_cZE590VL-cSu-a/s320/hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; The market for &lt;a href=&quot;https://www.telink-semi.com/introduction-true-wireless-stereo/&quot;>;true wireless stereo&lt;/a>; (TWS) &lt;a href=&quot; https://en.wikipedia.org/wiki/Active_noise_control&quot;>;active noise canceling&lt;/a>; (ANC) hearables (headphones and earbuds) has been soaring in recent years, and the global shipment volume will nearly &lt;a href=&quot; https://www.idc.com/promo/wearablevendor&quot;>;double&lt;/a>; that of smart wristbands and watches in 2023. The on-head time for hearables has extended significantly due to the recent advances in ANC, transparency mode,和人工智能。 Users frequently wear hearables not just for music listening, but also for exercising, focusing, or simply mood adjustment. However, hearable health is still mostly uncharted territory for the consumer market. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;APG: Audioplethysmography for Cardiac Monitoring in Hearables&lt;/a>;,” presented at &lt;a href=&quot;https://sigmobile.org/mobicom/2023/&quot;>;MobiCom 2023&lt;/a>;, we introduce a novel active in-ear health sensing modality. Audioplethysmography (APG) enables ANC hearables to monitor a user&#39;s physiological signals, such as heart rate and heart rate variability, without adding extra sensors or compromising battery life. APG exhibits high resilience to motion artifacts, adheres to &lt;a href=&quot;https://www.health.belgium.be/sites/default/files/uploads/fields/fpshealth_theme_file/19099349/Canadian%20guidelines%20for%20ultrasound.pdf&quot;>;safety regulations&lt;/a>; with an 80 dB margin below the limit, remains unaffected by seal conditions, and is inclusive of all skin tones. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3if_joqkSgp5nI0YvfWgUtgi3FoOtmeNcwcDdNNaMlPizqHpbuaeVFrp1MguLPQpT0hqrQaldH0ymqwjDzsD-u3qDR-r8Z0rnB7lsDuF9iab9CdR6GRPI7OPtqLFR29mRwfHz06kwqhppe7mvI9iVIBi8DtpIYh6E-UXs1RwUffTRxKVViTuXpSLXLAXI/s1600/image4.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3if_joqkSgp5nI0YvfWgUtgi3FoOtmeNcwcDdNNaMlPizqHpbuaeVFrp1MguLPQpT0hqrQaldH0ymqwjDzsD-u3qDR-r8Z0rnB7lsDuF9iab9CdR6GRPI7OPtqLFR29mRwfHz06kwqhppe7mvI9iVIBi8DtpIYh6E-UXs1RwUffTRxKVViTuXpSLXLAXI/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;APG sends a low intensity ultrasound transmitting wave (TX wave) using an ANC headphone&#39;s speakers and collects the receiving wave (RX wave) via the on-board feedback microphones. The APG signal is a pulse-like waveform that synchronizes with heartbeat and reveals rich cardiac information, such as &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/32403992/#:~:text=The%20dicrotic%20notch%20is%20a,of%20diastole%20in%20these%20arteries.&quot;>;dicrotic notches&lt;/a>;. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Health sensing in the ear canal&lt;/h2>; &lt;p>; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Ear_canal&quot;>;auditory canal&lt;/a>; receives its blood supply from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_auricular_artery&quot;>;arteria auricularis profunda&lt;/a>;, also known as the deep ear artery. This artery forms an intricate network of smaller vessels that extensively permeate the auditory canal. Slight variations in blood vessel shape caused by the heartbeat (and &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8834671&quot;>;blood pressure&lt;/a>;) can lead to subtle changes in the volume and pressure of the ear canals, making the ear canal an ideal location for health sensing. &lt;/p>; &lt;p>; Recent research has explored using hearables for health sensing by packaging together a plethora of sensors — eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram&quot;>;photoplethysmograms&lt;/a>; (PPG) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Electrocardiography&quot;>;electrocardiograms&lt;/a>; (ECG) — with a microcontroller to enable health applications, such as sleep monitoring, heart rate and blood pressure tracking. However, this sensor mounting paradigm inevitably adds cost, weight, power consumption, acoustic design complexity, and form factor challenges to hearables, constituting a strong barrier to its wide adoption. &lt;/p>; &lt;p>; Existing ANC hearables deploy feedback and feedforward microphones to navigate the ANC function. These microphones create &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3550314&quot;>;new opportunities&lt;/a>; for various sensing applications as they can detect or record many bio-signals inside and outside耳道。 For example, feedback microphones can be used to listen to heartbeats and feedforward microphones can hear respirations. &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3458864.3467680&quot;>;Academic research&lt;/a>; on this passive sensing paradigm has prompted many mobile applications, including heart rate monitoring, ear disease diagnosis, respiration monitoring, and body activity recognition. However, microphones in consumer-grade ANC headphones come with &lt;a href=&quot;https://research.google/pubs/pub52579/&quot;>;built-in high-pass filters&lt;/a>; to prevent saturation from body motions or strong wind噪音。 The signal quality of passive listening in the ear canal also heavily relies on the earbud seal conditions. As such, it is challenging to embed health features that rely on the passive listening of low frequency signals (≤ 50 Hz) on commercial ANC headphones. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Measuring tiny physiological signals&lt;/h2>; &lt;p>; APG bypasses the aforementioned ANC headphone hardware constraints by sending a low intensity ultrasound probing signal through an ANC headphone&#39;s speakers. This signal triggers echoes, which are received via on-board feedback microphones. We observe that the tiny ear canal skin displacement and heartbeat vibrations modulate these ultrasound echoes. &lt;/p>; &lt;p>; We build a &lt;a href=&quot;https://en.wikipedia.org/wiki/Acoustic_resonance&quot;>;cylindrical resonance&lt;/a>; model to understand APG&#39;s underlying physics. This phenomenon happens at an extremely small scale, which makes the raw pulse signal invisible in the raw received ultrasound. We adopt &lt;a href=&quot;https://www.rp-photonics.com/optical_heterodyne_detection.html&quot;>;coherent detection&lt;/a>; to retrieve this micro physiological modulation under the noise floor (we term this retrieved signal as mixed-down signal, see the &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;paper&lt;/a>; for more details). The final APG waveform looks strikingly similar to a PPG waveform, but provides an improved view of cardiac activities with more pronounced &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/32403992/#:~:text=The%20dicrotic%20notch%20is%20a,of%20diastole%20in%20these%20arteries.&quot;>;dicrotic notches&lt;/a>; (ie, pressure waveforms that provide rich insights about the central artery system, such as blood pressure). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEoN64wQBOUKAhV6SHbEMqIcAuVCk01zBki8pnV1xoEseaJCT1uNRprkjKyFLYRin4a-iSrPjg3Pr-eEgc_YrOYihU61CaKl8CN5K8ET_g8vOFxHhUKQaq7Fb9xMhGVVLMS_AouTZvfDfXYRVz0ExhCIV4je2Hxqd-CVfKmmpfGjLIGeKawPEff6Ei5gfT/s1600/image5.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEoN64wQBOUKAhV6SHbEMqIcAuVCk01zBki8pnV1xoEseaJCT1uNRprkjKyFLYRin4a-iSrPjg3Pr-eEgc_YrOYihU61CaKl8CN5K8ET_g8vOFxHhUKQaq7Fb9xMhGVVLMS_AouTZvfDfXYRVz0ExhCIV4je2Hxqd-CVfKmmpfGjLIGeKawPEff6Ei5gfT/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A cylindrical model with cardiac activities ℎ(𝑡) that modulates both the phase and amplitude of the &lt;a href=&quot;https://www.digikey.com/en/articles /the-basics-of-mixers&quot;>;mixed-down&lt;/a>; signal. Based on the simulation from our analytical model, the amplitude 𝑅(𝑡) and phase Φ(𝑡) of the mixed-down APG signals both reflect the cardiac activities ℎ(𝑡).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;APG sensing in practice&lt;/h2>; &lt;p>; During our initial experiments, we observed that APG works robustly with bad earbuds seals and with music playing. However, we noticed the APG signal can sometimes be very noisy and could be heavily disturbed by body motion. At that point, we determined that in order to make APG useful, we had to make it more robust to compete with &lt;a href=&quot;https://curiouscyborg.com/history-of-photoplethysmography/#:~:text=The%20Photoplethysmogram%20was%20first%20observed,the%20light%20interaction%20with%20tissue.&quot;>;more than 80 years&lt;/a>; of PPG development. &lt;/p>; &lt;p>; While PPGs are widely used and highly advanced, they do have some limitations. For example, PPGs sensors typically use two to four &lt;a href=&quot;https://en.wikipedia.org/wiki/Diode&quot;>;diodes&lt;/a>; to send and receive light frequencies for sensing. However, due to the ultra high-frequency nature (hundreds of &lt;a href=&quot;https://en.wikipedia.org/wiki/Terahertz_radiation&quot;>;Terahertz&lt;/a>;) of the light, it&#39;s difficult for a single diode to send multiple colors with different frequencies. On the other hand, we can easily design a low-cost and low-power system that generates and receives more than ten audio tones (frequencies). We leverage &lt;a href=&quot;https://en.wikipedia.org/wiki/Diversity_scheme&quot;>;channel diversity&lt;/a>;, a physical phenomenon that describes how wireless signals (eg, light and audio) at different frequencies have different characters (eg, different attenuation and reflection coefficients) when the signal propagates in a medium, to enable a higher quality APG signal and motion resilience. &lt;/p>; &lt;p>; Next, we experimentally demonstrate the effectiveness of using multiple frequencies in the APG signaling. We transmit three probing signals concurrently with their frequencies spanning evenly from 30 KHz to 32 KHz. A participant was asked to shake their head four times during the experiment to introduce interference. The figure below shows that different frequencies can be transmitted simultaneously to gather various information with &lt;a href=&quot;https://www.rp-photonics.com/optical_heterodyne_detection.html&quot;>;coherent detection&lt;/a>;, a unique advantage to APG 。 &lt;/p>; &lt;p>; The 30 kHz phase shows the four head movements and the magnitude (amplitude) of 31 kHz shows the pulse wave signal. This observation shows that some ultrasound frequencies might be sensitive to cardiac activities while others might be sensitive to motion. Therefore, we can use the multi-tone APG as a calibration signal to find the best frequency that measures heart rate, and use only the best frequency to get high-quality pulse waveform. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP_DLCbWWNJRvMPnIbdAbij_YKuIuYGbTiN-drwsJnEkrybDQT_yNHIqKYrqhxfq8CrWSEVZ6kvCvZ1tIUoXtH5GBqEfyQjkjChwVqL_Cxc0dO6i3vJi_sPo7hjElUuxH9f8d3E6nkDBNGNNgCRdFhfllW8OnhGXVS120-e4Yh67vECY7_lQpDOH2I42TU/s1761/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1380&quot; data-original-width=&quot;1761&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP_DLCbWWNJRvMPnIbdAbij_YKuIuYGbTiN-drwsJnEkrybDQT_yNHIqKYrqhxfq8CrWSEVZ6kvCvZ1tIUoXtH5GBqEfyQjkjChwVqL_Cxc0dO6i3vJi_sPo7hjElUuxH9f8d3E6nkDBNGNNgCRdFhfllW8OnhGXVS120-e4Yh67vECY7_lQpDOH2I42TU/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;The mixed-down amplitude (upper row) and phase (bottom row) for a customized multi-tone APG signal that spans from 30 kHz to 32 kHz. With channel diversity, the cardiac activities are captured in some frequencies (eg, magnitude of 31 kHz) and head movements are captured in other frequencies (eg, magnitude of 30 kHz, 30 kHz, and phase of 31 kHz). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; After choosing the best frequency to measure heart rate, the APG pulse waveform becomes more visible with pronounced dicrotic notches , and enables accurate &lt;a href=&quot;https://en.wikipedia.org/wiki/Heart_rate_variability&quot;>;heart rate variability&lt;/a>; measurement. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0mjyX5qTrzW9rN1qexcI3Nm0hgn03m-K_kGn_A8yfqlviJ3Rc3tod-srqQ-6vuoPEo6em7z3Qx7NpgkYTMRiNVtzUidLMc1lp48n6O8hsLMqFQc8z5CKI8mqSowmEbg5ojxdG4vaYreLbv0q2YwQAk3hKwdSd1cdwQMGVtFmgwn46nvCYRuPEi6hdpcVh/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0mjyX5qTrzW9rN1qexcI3Nm0hgn03m-K_kGn_A8yfqlviJ3Rc3tod-srqQ-6vuoPEo6em7z3Qx7NpgkYTMRiNVtzUidLMc1lp48n6O8hsLMqFQc8z5CKI8mqSowmEbg5ojxdG4vaYreLbv0q2YwQAk3hKwdSd1cdwQMGVtFmgwn46nvCYRuPEi6hdpcVh/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;The final APG signal used in the measurement phase (&lt;strong>;left&lt;/strong>;) and chest &lt;a href=&quot;https://en.wikipedia.org/wiki /Electrocardiography&quot;>;ECG&lt;/a>; signal (&lt;strong>;right&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Multi-tone translates to multiple simultaneous observations, which enable the development of &lt;a href=&quot;https://en.wikipedia.org/wiki/Array_processing&quot;>;array signal processing&lt;/a>; techniques. We demonstrate the spectrogram of a running session APG experiment before and after applying &lt;a href=&quot;https://en.wikipedia.org/wiki/Signal_separation&quot;>;blind source separation&lt;/a>; (see the &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;paper&lt;/a>; for more details). We also show the ground truth heart rate measurement in the same running experiment using a &lt;a href=&quot;https://www.polar.com/us-en/sensors/h10-heart-rate-sensor&quot;>;Polar ECG chest strap &lt;/a>;。 In the raw APG, we see the running cadence (around 3.3 Hz) as well as two dim lines (around 2 Hz and 4 Hz) that indicate the user&#39;s heart rate frequency and its harmonics. The heart rate frequencies are significantly enhanced in &lt;a href=&quot;https://en.wikipedia.org/wiki/Signal-to-noise_ratio&quot;>;signal to noise ratio&lt;/a>; (SNR) after the blind source separation, which align with the ground truth heart rate frequencies. We also show the calculated heart rate and running cadence from APG and ECG. We can see that APG tracks the growth of heart rate during the running session accurately. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbwRfWHxYHVk1ARp9gzcaRTMIUO-FjAgaTqOUtfYMzJ1lpAsWSxm1MczllG4aq-3Rbi-M6u7KXqnPOdsiLJS4le05Alnf_68701LYb0DQrpbbqAYGP7jzYivDhoeniPeTmsBU7Uhg4HRyOlYdQCYWxL-xabuULYFlGdH_rIA1b1Q8ZXH8MHIYlPQcaUpYW/s1888/image3 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1423&quot; data-original-width=&quot;1888&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbwRfWHxYHVk1ARp9gzcaRTMIUO-FjAgaTqOUtfYMzJ1lpAsWSxm1MczllG4aq-3Rbi-M6u7KXqnPOdsiLJS4le05Alnf_68701LYb0DQrpbbqAYGP7jzYivDhoeniPeTmsBU7Uhg4HRyOlYdQCYWxL-xabuULYFlGdH_rIA1b1Q8ZXH8MHIYlPQcaUpYW/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;APG tracks the heart rate accurately during the running session and also measures the running cadence.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Field study and closing thoughts&lt;/h2>; &lt;p>; We conducted two rounds of user experience (UX ) studies with 153 participants. Our results demonstrate that APG achieves consistently accurate heart rate (3.21% median error across participants in all activity scenarios) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Heart_rate_variability&quot;>;heart rate variability&lt;/a>; (2.70% median error in inter-beat interval) measurements. Unlike PPG, which exhibits variable performance across skin tones, our study shows that APG is resilient to variation in: skin tone, sub-optimal seal conditions, and ear canal size. More detailed evaluations can be found in the &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3570361.3613281&quot;>;paper&lt;/a>;. &lt;/p>; &lt;p>; APG transforms any TWS ANC headphones into smart sensing headphones with a simple software upgrade, and works robustly across various user activities. The sensing carrier signal is completely inaudible and not impacted by music playing. More importantly, APG represents new knowledge in biomedical and mobile research and unlocks new possibilities for low-cost health sensing. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>; APG is the result of collaboration across Google Health, product, UX and legal teams. We would like to thank David Pearl, Jesper Ramsgaard, Cody Wortham, Octavio Ponce, Patrick Amihood, Sam Sheng, Michael Pate, Leonardo Kusumo, Simon Tong, Tim Gladwin, Russ Mirov, Kason Walker, Govind Kannan, Jayvon Timmons, Dennis Rauschmayer, Chiong Lai, Shwetak Patel, Jake Garrison, Anran Wang, Shiva Rajagopal, Shelten Yuen, Seobin Jung, Yun Liu, John Hernandez, Issac Galatzer-Levy, Isaiah Fischer-Brown, Jamie Rogers, Pramod Rudrapatna, Andrew Barakat, Jason Guss, Ethan Grabau, Pol Peiffer, Bill Park, Helen O&#39;Connor, Mia Cheng, Keiichiro Yumiba, Felix Bors, Priyanka Jantre, Luzhou Xu, Jian Wang, Jaime Lien, Gerry Pallipuram, Nicholas Gillian, Michal Matuszak, Jakub Wojciechowski, Bryan Allen, Jane Hilario, and Phil Carmack for their invaluable insights and support. Thanks to external collaborators Longfei Shangguan and Rich Howard, Rutgers University and University of Pittsburgh.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4973261706617691472/comments/ default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/audioplethysmography-for-cardiac.html #comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4973261706617691472&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4973261706617691472&quot; rel=&quot;self&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/audioplethysmography-for-cardiac.html&quot; rel=&quot;alternate&quot; title=&quot;Audioplethysmography for cardiac monitoring with hearable devices&quot; type =“ text/html”/>; &lt;unam>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777775266161 &lt;/uri>; &lt;emage>; /email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/b16-rounded。 gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnCItsRGs939CGCBBAi5fA-xwk9JQi7b2CQ3voKK583p4uTkQkLIXUd1lU71SLynom0Yt78bRxDflolUzzfol5UbfkPmCaNn8bIUxwAbLDatqZTPxP7SYOT45g9qJODdM1kvT6NQUsixtFTiBYr_h_Hx- pFRsmbcBKdnW3WkbcD4Bcr_cZE590VL-cSu-a/s72-c/hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0 &lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6605352025607219737&lt;/id>;&lt;published>;2023-10-26T11:01:00.000-07: 00&lt;/published>;&lt;updated>;2023-10-26T11:01:04.276-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot; >;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Collaboration&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/ atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Supporting benchmarks for AI safety with MLCommons&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline- author&quot;>;Posted by Anoop Sinha, Technology and Society, and Marian Croak, Google Research, Responsible AI and Human Centered Technology team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl /AVvXsEg5URA9ivhxcgmfXwa0668O0HslgYQ_p_x9JJbg6nDgryyNc2QImQernPyzkLPcj1esCMOQTUnEsldIlb21E0DWPDIiE2m76qSruF0jA6jfl1sNl6mBW8JUPSWzOfE7IahHRtviFpxUTPcEZoADXHNMy3gZ2hg369y5QxhY01QRVj5kwJx4uwRKzdcBFp9v/s1600/GoogleResearch.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Standard benchmarks are agreed upon ways of measuring important product qualities, and they exist in many fields. Some standard benchmarks measure safety: for example, when a car manufacturer touts a “five-star overall safety rating,” they&#39;re citing a benchmark. Standard benchmarks already exist in machine learning (ML) and AI technologies: for instance, the &lt;a href=&quot;https://mlcommons.org/en/&quot;>;MLCommons&lt;/a>; Association operates the &lt;a href=&quot;https://mlcommons.org/en/news/mlperf-inference-storage-q323/&quot;>;MLPerf&lt;/a>; benchmarks that measure the speed of cutting edge AI hardware such as Google&#39;s TPUs. However, though there has been significant work done on &lt;a href=&quot;https://blog.google/technology/ai/our-responsible-approach-to-building-guardrails-for-generative-ai/&quot;>;AI safety&lt;/a>;, there are as yet no similar standard benchmarks for AI safety. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to support a new effort by the non-profit MLCommons Association to develop standard AI safety benchmarks. Developing benchmarks that are effective and trusted is going to require advancing AI safety testing technology and incorporating a broad range of perspectives. The MLCommons effort aims to bring together expert researchers across academia and industry to develop standard benchmarks for measuring the safety of AI systems into scores that everyone can understand. We encourage the whole community, from AI researchers to policy experts, to &lt;a href=&quot;https://mlcommons.org/ai-safety&quot;>;join us&lt;/a>; in contributing to the effort. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Why AI safety benchmarks?&lt;/h2>; &lt;p>; Like most advanced technologies, AI has the potential for tremendous benefits but could also lead to negative outcomes without appropriate care. For example, AI technology can boost human productivity in a wide range of activities (eg, &lt;a href=&quot;https://blog.google/technology/health/how-ai-can-improve-health-for-everyone-everywhere/&quot;>;improve health diagnostics&lt;/a>; and research into diseases, analyze &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-transportation-energy-emissions-reduction/&quot;>;energy usage&lt;/a>;, and more). However, without sufficient precautions, AI could also be used to support harmful or malicious activities and respond in biased or offensive ways. &lt;/p>; &lt;p>; By providing standard measures of safety across categories such as harmful use, out-of-scope responses, AI-control risks, etc., standard AI safety benchmarks could help society reap the benefits of AI while ensuring that sufficient precautions are being taken to mitigate these risks. Initially, nascent safety benchmarks could help drive AI safety research and inform responsible AI development. With time and maturity, they could help inform users and purchasers of AI systems. Eventually, they could be a valuable tool for policy makers. &lt;/p>; &lt;p>; In computer hardware, benchmarks (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Standard_Performance_Evaluation_Corporation&quot;>;SPEC&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Transaction_Processing_Performance_Council&quot;>;TPC&lt;/a>;) have shown an amazing ability to align research, engineering, and even marketing across an entire industry in pursuit of progress, and we believe standard AI safety benchmarks could help do the same in this vital area. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;What are standard AI safety benchmarks?&lt;/h2>; &lt;p>; Academic and corporate research efforts have experimented with a range of AI safety tests (eg, &lt;a href=&quot;https://arxiv.org/abs/2009.11462&quot;>;RealToxicityPrompts&lt;/a>;, &lt;a href=&quot;https://crfm.stanford.edu/2022/11/17/helm.html&quot;>;Stanford HELM&lt;/a>; fairness, bias, toxicity measurements, and &lt;a href=&quot;https://blog.google/technology/ai/our-responsible-approach-to-building-guardrails-for-generative-ai/&quot;>;Google&#39;s guardrails for generative AI&lt;/a>;). However, most of these tests focus on providing a prompt to an AI system and algorithmically scoring the output, which is a useful start but limited to the scope of the test prompts. Further, they usually use open datasets for the prompts and responses, which may already have been (often inadvertently) incorporated into training data. &lt;/p>; &lt;p>; MLCommons proposes a multi-stakeholder process for selecting tests and grouping them into subsets to measure safety for particular AI use-cases, and translating the highly technical results of those tests into scores that everyone can understand. MLCommons is proposing to create a platform that brings these existing tests together in one place and encourages the creation of more rigorous tests that move the state of the art forward. Users will be able to access these tests both through online testing where they can generate and review scores and offline testing with an engine for private testing. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;AI safety benchmarks should be a collective effort&lt;/h2>; &lt;p>; Responsible AI developers use a diverse range of safety measures, including automatic testing, manual testing, red teaming (in which human testers attempt to produce adversarial outcomes), software-imposed restrictions, data and model best-practices, and auditing. However, determining that sufficient precautions have been taken can be challenging, especially as the community of companies providing AI systems grows and diversifies. Standard AI benchmarks could provide a powerful tool for helping the community grow responsibly, both by helping vendors and users measure AI safety and by encouraging an ecosystem of resources and specialist providers focused on improving AI safety. &lt;/p>; &lt;p>; At the same time, development of mature AI safety benchmarks that are both effective and trusted is not possible without the involvement of the community. This effort will need researchers and engineers to come together and provide innovative yet practical improvements to safety testing technology that make testing both more rigorous and more efficient. Similarly, companies will need to come together and provide test data, engineering support, and financial support. Some aspects of AI safety can be subjective, and building trusted benchmarks supported by a broad consensus will require incorporating multiple perspectives, including those of public advocates, policy makers, academics, engineers, data workers, business leaders, and entrepreneurs. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Google&#39;s support for MLCommons&lt;/h2>; &lt;p>; Grounded in our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; that were &lt;a href=&quot;https://blog.google/technology/ai/ai-principles/&quot;>;announced&lt;/a>; in 2018, Google is committed to specific practices for the safe, secure, and trustworthy development and use of AI (see our &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2019-progress-update.pdf&quot;>;2019&lt;/a>;, &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2020-progress-update.pdf&quot;>;2020&lt;/a>;, &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2021-progress-update.pdf&quot;>;2021&lt;/a>;, &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2022-progress-update.pdf&quot;>;2022&lt;/a>; updates). We&#39;ve also made significant &lt;a href=&quot;https://static.googleusercontent.com/media/publicpolicy.google/en//resources/whcommitments.pdf&quot;>;progress&lt;/a>; on key commitments, which will help ensure AI is developed boldly and responsibly, for the benefit of everyone. &lt;/p>; &lt;p>; Google is supporting the MLCommons Association&#39;s efforts to develop AI safety benchmarks in a number of ways. &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Testing platform&lt;/em>;: We are joining with other companies in providing funding to support the development of a testing platform. &lt;li>;&lt;em>;Technical expertise and resources&lt;/em>;: We are providing technical expertise and resources, such as the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;Monk Skin Tone Examples Dataset&lt;/a>;, to help ensure that the benchmarks are well-designed and effective. &lt;li>;&lt;em>;Datasets&lt;/em>;: We are contributing an internal dataset for multilingual representational bias, as well as already externalized tests for stereotyping harms, such as &lt;a href=&quot;https://github.com/google-research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>; and &lt;a href=&quot;https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;. Moreover, we are sharing our datasets that focus on collecting human annotations responsibly and inclusively, like &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;DICES&lt;/a>; and &lt;a href=&quot;https://www.kaggle.com/datasets/google/jigsaw-specialized-rater-pools-dataset&quot;>;SRP&lt;/a>;. &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; We believe that these benchmarks will be very useful for advancing research in AI safety and ensuring that AI systems are developed and deployed in a responsible manner. AI safety is &lt;a href=&quot;https://blog.google/technology/ai/a-shared-agenda-for-responsible-ai-progress/&quot;>;a collective-action problem&lt;/a>;. Groups like the &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/&quot;>;Frontier Model Forum&lt;/a>; and &lt;a href=&quot;https://partnershiponai.org/&quot;>;Partnership on AI&lt;/a>; are also leading important standardization initiatives. We&#39;re pleased to have been part of these groups and MLCommons since their beginning. We look forward to additional collective efforts to promote the responsible development of new generative AI tools. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many thanks to the Google team that contributed to this work : Peter Mattson, Lora Aroyo, Chris Welty, Kathy Meier-Hellstern, Parker Barnes, Tulsee Doshi, Manvinder Singh, Brian Goldman, Nitesh Goyal, Alice Friend, Nicole Delange, Kerry Barker, Madeleine Elish, Shruti Sheth, Dawn Bloxwich, William Isaac , Christina Butterfield.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6605352025607219737/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html#comment-form&quot; rel=&quot;replies &quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6605352025607219737&quot; rel=&quot;edit&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6605352025607219737&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot; rel=&quot;alternate&quot; title=&quot;Supporting benchmarks for AI safety with MLCommons&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5URA9ivhxcgmfXwa0668O0HslgYQ_p_x9JJbg6nDgryyNc2QImQernPyzkLPcj1esCMOQTUnEsldIlb21E0DWPDIiE2m76qSruF0jA6jfl1sNl6mBW8JUPSWzOfE7IahHRtviFpxUTPcEZoADXHNMy3gZ2hg369y5QxhY01QRVj5kwJx4uwRKzdcBFp9v/s72-c/GoogleResearch.png &quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7441213378686175839&lt;/id>;&lt;published>;2023-10-26T08:57:00.003-07:00&lt;/published>;&lt;updated>;2023-11-01T17 :00:03.449-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Spoken question answering and speech continuation using a spectrogram-powered LLM&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eliya Nachmani, Research Scientist, and Alon Levkovitch, Student Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgQEWecXit-a9UwHz781_B9s9ZqxsJIGt7ZXx2Lk0bcSQxXBkmLfjhNQxSiq3gqVjiUZ81_178hArCQ8nNL0OaVyAi8mKixeWN2PvXTLL4I08ht-eCVqtrTRo36dxGBSDNMdlathtEd4g_qdU3T4ZmPMdGNSXHwlDP689sxzbI4Wwosyu9wp-mjadKqL3MN/s1100/Spectron-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The goal of natural language processing (NLP) is to develop computational models that can understand and generate natural language. By capturing the statistical patterns and structures of text-based natural language, language models can predict and generate coherent and meaningful sequences of words. Enabled by the increasing use of the highly successful &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>; model architecture and with training on large amounts of text (with proportionate compute and model size), large language models (LLMs) have demonstrated remarkable success in NLP tasks. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, modeling spoken human language remains a challenging frontier. Spoken dialog systems have conventionally been built as a cascade of &lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_recognition&quot;>;automatic speech recognition&lt;/a>; (ASR), &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural-language_understanding&quot;>;natural language understanding&lt;/a>; (NLU), response generation, and &lt;a href=&quot;https://simple.wikipedia.org/wiki/Text_to_speech&quot;>;text-to-speech&lt;/a>; (TTS) systems. However, to date there have been few capable end-to-end systems for the modeling of spoken language: ie, single models that can take speech inputs and generate its continuation as speech outputs. &lt;/p>; &lt;p>;Today we present a new approach for spoken language modeling, called Spectron, published in “&lt;a href=&quot;https://arxiv.org/abs/2305.15255&quot;>;Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM&lt;/a>;.” Spectron is the first spoken language model that is trained end-to-end to directly process spectrograms as both input and output, instead of learning discrete speech representations. Using only a pre-trained text language model, it can be fine-tuned to generate high-quality, semantically accurate spoken language. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;question answering&lt;/a>; datasets.&lt;/p>; &lt;p>; We show that a pre-trained speech encoder and a language model decoder enable end-to-end training and state-of-the-art performance without sacrificing representational fidelity. Key to this is a novel end-to-end training objective that implicitly supervises speech recognition, text continuation, and conditional speech synthesis in a joint manner. A new spectrogram regression loss also supervises the model to match the higher-order derivatives of the spectrogram in the time and frequency domain. These derivatives express information aggregated from multiple frames at once. Thus, they express rich, longer-range information about the shape of the signal. Our overall scheme is summarized in the following figure: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4yanW_7FsK-zlJ5XKgusakSaIGAEvDPuRZpAZ5NGJl8mW6-4hd06ZWOsk4IIf0hl5XwWFcjkdu9gH4VwqcWCqLUfZQXDM80MuDml8Tt_P0RT3fDq5cxfoPCPNagOuU0SZbSz8Vn7x_bLkxNFXKpakBPwCkgXh8T6bB4lfHQevGBjs-U4c5WA6RgC8gukW/s1815/Spectron.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1815&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4yanW_7FsK-zlJ5XKgusakSaIGAEvDPuRZpAZ5NGJl8mW6-4hd06ZWOsk4IIf0hl5XwWFcjkdu9gH4VwqcWCqLUfZQXDM80MuDml8Tt_P0RT3fDq5cxfoPCPNagOuU0SZbSz8Vn7x_bLkxNFXKpakBPwCkgXh8T6bB4lfHQevGBjs-U4c5WA6RgC8gukW/s16000/Spectron.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;The Spectron model connects the encoder of a speech recognition model with a pre-trained Transformer-based decoder language model. At training, speech utterances split into a prompt and its continuation. Then the full transcript (prompt and continuation) is reconstructed along with the continuation&#39;s speech features. At inference, only a prompt is provided; the prompt&#39;s transcription, text continuation, and speech continuations are all generated by the model.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Spectron architecture&lt;/h2>; &lt;p>; The architecture is initialized with a pre-trained speech encoder and a pre-trained decoder language model. The encoder is prompted with a speech utterance as input, which it encodes into continuous linguistic features. These features feed into the decoder as a prefix, and the whole encoder-decoder is optimized to jointly minimize a &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy&quot;>;cross-entropy&lt;/a>; loss (for speech recognition and transcript continuation) and a novel reconstruction loss (for speech continuation). During inference, one provides a spoken speech prompt, which is encoded and then decoded to give both text and speech continuations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Speech encoder&lt;/h3>; &lt;p>; The speech encoder is a 600M-parameter &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;conformer&lt;/a>; encoder pre-trained on &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;large-scale data&lt;/a>; (12M hours). It takes the spectrogram of the source speech as input, generating a hidden representation that incorporates both linguistic and acoustic information. The input spectrogram is first subsampled using a convolutional layer and then processed by a series of conformer blocks. Each conformer block consists of a feed-forward layer, a self-attention layer, a convolution layer, and a second feed-forward layer. The outputs are passed through a projection layer to match the hidden representations to the embedding dimension of the language model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Language model&lt;/h3>; &lt;p>; We use a 350M or 1B parameter decoder language model (for the continuation and question-answering tasks, respectively) trained in the manner of &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;PaLM 2&lt;/a>;. The model receives the encoded features of the prompt as a prefix. Note that this is the only connection between the speech encoder and the LM decoder; ie, there is no cross-attention between the encoder and the decoder. Unlike most spoken language models, during training, the decoder is teacher-forced to predict the text transcription, text continuation, and speech embeddings. To convert the speech embeddings to and from spectrograms, we introduce lightweight modules pre- and post-network. &lt;/p>; &lt;p>; By having the same architecture decode the intermediate text and the spectrograms, we gain two benefits. First, the pre-training of the LM in the text domain allows continuation of the prompt in the text domain before synthesizing the speech. Secondly, the predicted text serves as intermediate reasoning, enhancing the quality of the synthesized speech, analogous to improvements in text-based language models when using intermediate &lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpads&lt;/a>; or &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; (CoT) reasoning. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Acoustic projection layers&lt;/h3>; &lt;p>; To enable the language model decoder to model spectrogram frames, we employ a multi-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Perceptron&quot;>;perceptron&lt;/a>; “pre-net” to project the ground truth spectrogram speech continuations to the language model dimension. This pre-net compresses the spectrogram input into a lower dimension, creating a bottleneck that aids the decoding process. This bottleneck mechanism prevents the model from repetitively generating the same prediction in the decoding process. To project the LM output from the language model dimension to the spectrogram dimension, the model employs a “post-net”, which is also a multi-layer perceptron. Both pre- and post-networks are two-layer multi-layer perceptrons. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Training objective&lt;/h3>; &lt;p>; The training methodology of Spectron uses two distinct loss functions: (i) &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy&quot;>;cross-entropy loss&lt;/a>;, employed for both speech recognition and transcript continuation, and (ii) &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;regression loss&lt;/a>;, employed for speech continuation. During training, all parameters are updated (speech encoder, projection layer, LM, pre-net, and post-net). &lt;/p>; &lt;br />; &lt;h2>;Audio samples&lt;/h2>; &lt;p>; Following are examples of speech continuation and question answering from Spectron: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;&lt;h3>;Speech Continuation&lt;/h3>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/1/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/2/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/2/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/3/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/3/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Prompt:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/4/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Continuation:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/4/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td colspan=&quot;2&quot; style=&quot;text-align: left;&quot;>;&lt;h3>;Question Answering&lt;/h3>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Question:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/1/Q.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Answer:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/1/S.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Question:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/2/Q.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;Answer:&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/2/S.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Performance&lt;/h2>; &lt;p>; To empirically evaluate the performance of the proposed approach, we conducted experiments on the&lt;a href=&quot;https://arxiv.org/abs/1912.07875&quot;>; Libri-Light dataset&lt;/a>;. Libri-Light is a 60k hour English dataset consisting of unlabelled speech readings from LibriVox audiobooks. We utilized a frozen neural vocoder called &lt;a href=&quot;https://research.google/pubs/pub51736/&quot;>;WaveFit&lt;/a>; to convert the predicted spectrograms into raw audio. We experiment with two tasks, speech continuation and spoken question answering (QA). Speech continuation quality is tested on the LibriSpeech test set. Spoken QA is tested on the Spoken WebQuestions datasets and a new test set named LLama questions, which we created. For all experiments, we use a 3 second audio prompt as input. We compare our method against existing spoken language models: &lt;a href=&quot;https://arxiv.org/abs/2209.03143&quot;>;AudioLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2102.01192&quot;>;GSLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2305.13009&quot;>;TWIST&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2305.11000&quot;>;SpeechGPT&lt;/a>;. For the speech continuation task, we use the 350M parameter version of LM and the 1B version for the spoken QA task. &lt;/p>; &lt;p>; For the speech continuation task, we evaluate our method using three metrics. The first is &lt;a href=&quot;https://en.wikipedia.org/wiki/Perplexity&quot;>;log-perplexity&lt;/a>;, which uses an LM to evaluate the cohesion and semantic quality of the generated speech. The second is &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;mean opinion score&lt;/a>; (MOS), which measures how natural the speech sounds to human evaluators. The third, speaker similarity, uses a speaker encoder to measure how similar the speaker in the output is to the speaker in the input. Performance in all 3 metrics can be seen in the following graphs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_o_DUlZ-28VmNk20zvWBWC5FNl3GMVeigztDAtB4TR5voCdX3zzuCF-6W6DDrMVGZ2szVhyphenhyphendPz1DHvjBrALQXCMkVzuM81oBck4Wb3N1VH1ndDReAPsfbQv5x7msOD3wMiHeZM-_AcN2ekgSI_G9sF5_u0lrLKc-xxSWc2d9qi6Ubt5cutgQfpIt8uKGr/s1200/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_o_DUlZ-28VmNk20zvWBWC5FNl3GMVeigztDAtB4TR5voCdX3zzuCF-6W6DDrMVGZ2szVhyphenhyphendPz1DHvjBrALQXCMkVzuM81oBck4Wb3N1VH1ndDReAPsfbQv5x7msOD3wMiHeZM-_AcN2ekgSI_G9sF5_u0lrLKc-xxSWc2d9qi6Ubt5cutgQfpIt8uKGr/w640-h396/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Log-perplexity for completions of LibriSpeech utterances given a 3-second prompt. Lower is better.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEgjUncRH61ZdMzRhsx9NEXoY0P0jIMkaXhycc9AjB2I0pr8bEWxGNaY0KEF4UG0dm6ZVjRT3_aXlNKny3mmjoVgPveK4rskOgU5sApzqzIXixabv7kMeZNMxUpxs_RDqSsnij-HILgUzVZT39hoBDimw9Ecnd2lMq2ylyRCVtcU_owo8jx9ndVrLUHbGhNT/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjUncRH61ZdMzRhsx9NEXoY0P0jIMkaXhycc9AjB2I0pr8bEWxGNaY0KEF4UG0dm6ZVjRT3_aXlNKny3mmjoVgPveK4rskOgU5sApzqzIXixabv7kMeZNMxUpxs_RDqSsnij-HILgUzVZT39hoBDimw9Ecnd2lMq2ylyRCVtcU_owo8jx9ndVrLUHbGhNT/w640-h396/image4.png&quot; width=&quot;640&quot; />; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left; left;”>; Speaker similarity between the prompt speech and the generated speech using the speaker encoder. Higher is better.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBdPk77at7a-tSVl1cgPTlMsj4tnV391evuyYPzMXxeviG48kljCNy828jCQ5eVdOcB7ML4FHduIHCuDweZwJiv2USbNqi1EXinzyiwQYM3Sxio3g-61xTBvtb9rVt_Y4Cvns-vl3V-4Gvn1RTpBquX3a6W_NNu6u-1DrK_GT5uGXOvP0WivyO7FF2-WTf/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBdPk77at7a-tSVl1cgPTlMsj4tnV391evuyYPzMXxeviG48kljCNy828jCQ5eVdOcB7ML4FHduIHCuDweZwJiv2USbNqi1EXinzyiwQYM3Sxio3g-61xTBvtb9rVt_Y4Cvns-vl3V-4Gvn1RTpBquX3a6W_NNu6u-1DrK_GT5uGXOvP0WivyO7FF2-WTf/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MOS given by human users on speech naturalness. Raters rate 5-scale subjective mean opinion score (MOS) ranging between 0 - 5 in naturalness given a speech utterance. Higher is better.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As can be seen in the first graph, our method significantly outperforms GSLM and TWIST on the log-perplexity metric, and does slightly better than state-of-the-art methods AudioLM and SpeechGPT. In terms of MOS, Spectron exceeds the performance of all the other methods except for AudioLM. In terms of speaker similarity, our method outperforms all other methods. &lt;/p>; &lt;p>; To evaluate the ability of the models to perform question answering, we use two spoken question answering datasets. The first is the LLama Questions dataset, which uses general knowledge questions in different domains generated using the LLama2 70B LLM. The second dataset is the &lt;a href=&quot;https://huggingface.co/datasets/web_questions&quot;>;WebQuestions&lt;/a>; dataset which is a general question answering dataset. For evaluation we use only questions that fit into the 3 second prompt length. To compute accuracy, answers are transcribed and compared to the ground truth answers in text form. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi60RnurzgglmP14m9BIScZH4BmupewxydVJ5GZMKclwDgyGc3-zZdj8CK7WXmGR_-pwno1Aql0N_u0ocmftoAlTCJY0H-1cYU8YTpxZu5sdIdmG-wZAc-hIwlAbCEVDPgGz4J4a1VMcXWctbjcfCTxsDSZPD69eGfBeGyAxvyYtUX0fE2dhdT5zb71gsJ0/s1200 /image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot; 396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi60RnurzgglmP14m9BIScZH4BmupewxydVJ5GZMKclwDgyGc3-zZdj8CK7WXmGR_-pwno1Aql0N_u0ocmftoAlTCJY0H-1cYU8YTpxZu5sdIdmG-wZAc-hIwlAbCEVDPgGz4J4a1VMcXWctbjcfCTxsDSZPD69eGfBeGyAxvyYtUX0fE2dhdT5zb71gsJ0/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Accuracy for Question Answering on the LLama Questions and Spoken WebQuestions datasets. Accuracy is computed using the ASR transcripts of spoken answers.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; First, we observe that all methods have more difficulty answering questions from the Spoken WebQuestions dataset than from the LLama questions dataset. Second, we observe that methods centered around spoken language modeling such as GSLM, AudioLM and TWIST have a completion-centric behavior rather than direct question answering which hindered their ability to perform QA. On the LLama questions dataset our method outperforms all other methods, while SpeechGPT is very close in performance. On the Spoken WebQuestions dataset, our method outperforms all other methods except for SpeechGPT, which does marginally better. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;This project was conceived and initiated by Michelle Tadmor Ramanovich with additional significant contributions from&lt;/i>;&lt;em>;&amp;nbsp;Eliya Nachmani, Alon Levkovitch, Roy Hirsch (&lt;a href=&quot;https://verily.com/&quot;>;Verily&lt;/a>;), Julian Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin (&lt;a href=&quot;https://verily.com/&quot;>;Verily&lt;/a>;) and RJ Skerry-Ryan. We also thank Heiga Zhen, Yifan Ding, Yu Zhang, Yuma Koizumi, Neil Zeghidour, Christian Frank, Marco Tagliasacchi, Nadav Bar, Benny Schlesinger and Blaise Aguera-Arcas. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7441213378686175839/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/spoken-question-answering-and-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7441213378686175839&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7441213378686175839&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/10/spoken-question-answering-and-speech.html&quot; rel=&quot;alternate&quot; title=&quot;Spoken question answering and speech continuation using a spectrogram-powered LLM&quot; type=&quot;text /html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQEWecXit-a9UwHz781_B9s9ZqxsJIGt7ZXx2Lk0bcSQxXBkmLfjhNQxSiq3gqVjiUZ81_178hArCQ8nNL0OaVyAi8mKixeWN2PvXTLL4I08ht-eCVqtrTRo36dxGBSDNMdlathtEd4g_qdU3T4ZmPMdGNSXHwlDP689sxzbI4Wwosyu9wp-mjadKqL3MN/ s72-c/Spectron-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr: total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7317378303167958176&lt;/id>;&lt;published>;2023-10-25T15:10:00.002-07:00&lt;/published >;&lt;updated>;2023-10-26T14:03:20.633-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>; &lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Climate&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Looking back at wildfire research in 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author &quot;>;Posted by Yi-Fan Chen, Software Engineer, and Carla Bromberg, Program Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCepSXB1QxIgipeUbH1YGd8N3TkVoT1tAgxG0fC0PPREhbsygTQ4i58OVw6-Dt_lagwgswT0jtxUorsVmtyO9UgPEDezJHz_QiKvbJlgYF8Db8W- 68KFdyZsq_uvM7YuZo9BRo__NC4BNxD6nHZpzsimeNOaV3X8dh9aliqbAbk8ycXb25s5NfLTfNe42_/s600/WildfireModeling.gif&quot; style=&quot;display: none;&quot; />; &lt;p>;Wildfires are becoming larger and affecting more and more communities around the world, often resulting in large-scale devastation. Just this year, communities have experienced catastrophic wildfires in &lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Greece_wildfires&quot;>;Greece&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Hawaii_wildfires#Maui&quot;>;Maui&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Canadian_wildfires&quot;>;Canada&lt;/a>; to name a few. While the underlying causes leading to such an increase are complex — including changing climate patterns, forest management practices, land use development policies and many more — it is clear that the advancement of technologies can help to address the new challenges.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;At Google Research, we&#39;ve been investing in a number of &lt;a href=&quot;https://research.google/teams/climate-and-sustainability/&quot;>;climate adaptation&lt;/a>; efforts, including the application of machine learning (ML) to aid in wildfire prevention and provide information to people during these events. For example, to help map fire boundaries, our wildfire boundary &lt;a href=&quot;https://blog.research.google/2023/02/real-time-tracking-of-wildfire.html&quot;>;tracker&lt;/a>; uses ML models and satellite imagery to map large fires in near real-time with updates every 15 minutes. To advance our various research efforts, we are partnering with wildfire experts and government agencies around the world.&lt;/p>; &lt;p>;Today we are excited to share more about our ongoing collaboration with the &lt;a href=&quot;https://www.fs.usda.gov/&quot;>;US Forest Service&lt;/a>; (USFS) to advance fire modeling tools and fire spread prediction algorithms. Starting from the newly developed USFS wildfire behavior model, we use ML to significantly reduce computation times, thus enabling the model to be employed in near real time. This new model is also capable of incorporating localized fuel characteristics, such as fuel type and distribution, in its predictions. Finally, we describe an early version of our new high-fidelity 3D fire spread model.&lt;/p>; &lt;br />; &lt;h2>;Current state of the art in wildfire modeling&lt;/h2>; &lt;p>;Today&#39;s most widely used state-of-the-art fire behavior models for fire operation and training are based on the &lt;a href=&quot;https://www.fs.usda.gov/research/treesearch/55928&quot;>;Rothermel fire model&lt;/a>; developed at the US Forest Service Fire Lab, by Rothermel et al., in the 1970s. This model considers many key factors that affect fire spread, such as the influence of wind, the slope of the terrain, the moisture level, the fuel load (eg, the density of the combustible materials in the forest), etc., and provided a good balance between computational feasibility and accuracy at the time. The Rothermel model has gained widespread use throughout the fire management community across the world.&lt;/p>; &lt;p>;Various operational tools that employ the Rothermel model, such as &lt;a href=&quot;https://www.frames.gov/behaveplus/home&quot;>;BEHAVE&lt;/a>;, &lt;a href=&quot;https://www.firelab.org/project/farsite&quot;>;FARSITE&lt;/a>;, &lt;a href=&quot;https://wfdss.usgs.gov/wfdss/pdfs/FSPro.pdf&quot;>;FSPro&lt;/a>;, and &lt;a href=&quot;https://firelab.org/project/flammap&quot;>;FlamMap&lt;/a>;, have been developed and improved over the years. These tools and the underlying model are used mainly in three important ways: (1) for training firefighters and fire managers to develop their insights and intuitions on fire behavior, (2) for fire behavior analysts to predict the development of a fire during a fire operation and to generate guidance for situation awareness and resource allocation planning, and (3) for analyzing forest management options intended to mitigate fire hazards across large landscapes.&amp;nbsp; These models are the foundation of fire operation safety and efficiency today.&lt;/p>; &lt;p>; However, there are limitations on these state-of-the art models, mostly associated with the simplification of the underlying physical processes (which was necessary when these models were created). By simplifying the physics to produce steady state predictions, the required inputs for fuel sources and weather became practical but also more abstract compared to measurable quantities.&amp;nbsp; As a result, these models are typically “adjusted” and “tweaked” by experienced fire behavior analysts so they work more accurately in certain situations and to compensate for uncertainties and unknowable environmental characteristics. Yet these expert adjustments mean that many of the calculations are not repeatable. &lt;/p>; &lt;p>;To overcome these limitations, USFS researchers have been working on a new model to drastically improve the physical fidelity of fire behavior prediction. This effort represents the first major shift in fire modeling in the past 50 years. While the new model &lt;a href=&quot;https://ebooks.publish.csiro.au/content/wildland-fire-behaviour&quot;>;continues to improve in capturing fire behavior&lt;/a>;, the computational cost and inference time makes it impractical to be deployed in the field or for applications with near real-time requirements. In a realistic scenario, to make this model useful and practical in training and operations, a speed up of at least 1000x would be needed.&lt;/p>; &lt;br />; &lt;h2>;Machine learning acceleration&lt;/h2>; &lt;p>;In partnership with the USFS, we have undertaken a program to apply ML to decrease computation times for complex fire models. Researchers knew that many complex inputs and features could be characterized using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;>;deep neural network&lt;/a>;, and if successful, the trained model would lower the computational cost and latency of evaluating new scenarios. Deep learning is a branch of machine learning that uses neural networks with multiple hidden layers of nodes that do not directly correspond to actual observations. The model&#39;s hidden layers allow a rich representation of extremely complex systems — an ideal technique for modeling wildfire spread.&lt;/p>; &lt;p>; We used the USFS physics-based, numerical prediction models to generate many simulations of wildfire behavior and then used these simulated examples to train the deep learning model on the inputs and features to best capture the system behavior accurately. We found that the deep learning model can perform at a much lower computational cost compared to the original and is able to address behaviors resulting from fine-scale processes. In some cases, computation time for capturing the fine-scale features described above and providing a fire spread estimate was 100,000 times faster than running the physics-based numerical models. &lt;/p>; &lt;p>;This project has continued to make great progress since the first report at &lt;a href=&quot;https://www.adai.pt/newevent/event/home/index.php?target=home&amp;amp;event =4&amp;amp;defLang=2&quot;>;ICFFR&lt;/a>; in December 2022. The joint Google–USFS &lt;a href=&quot;http://books.uc.pt/chapter?chapter=978989262298921&quot;>;presentation at ICFFR 2022&lt;/ a>; and the &lt;a href=&quot;https://www.firelab.org/project/deep-learning-high-resolution-wildfire-modeling&quot;>;USFS Fire Lab&#39;s project page&lt;/a>; provides a glimpse into the ongoing work朝这个方向。 Our team has expanded the dataset used for training by an order of magnitude, from 40M up to 550M training examples. Additionally, we have delivered a prototype ML model that our USFS Fire Lab partner is integrating into a training app that is currently being developed for release in 2024.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdwdwG4NlOjrJlRs25p5yGRUdNDErx7I9aZYLVgoPhrlyiOOx6x8toan0DKoBm2-SG2JfWkZsOluT7g_BYJpDYCTqMRu3cUHqQtyeE-Hgli5j9J3ap2Sg9SrjEfOMXj_CYhbi66iIFwSJjzkii0kJv3VV5V01jbIYFQP-DWjBc9RJKKEoJZPpIZKKYO2TJ/s1999/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdwdwG4NlOjrJlRs25p5yGRUdNDErx7I9aZYLVgoPhrlyiOOx6x8toan0DKoBm2-SG2JfWkZsOluT7g_BYJpDYCTqMRu3cUHqQtyeE-Hgli5j9J3ap2Sg9SrjEfOMXj_CYhbi66iIFwSJjzkii0kJv3VV5V01jbIYFQP-DWjBc9RJKKEoJZPpIZKKYO2TJ/w640-h360/image2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Google researchers visiting the USFS Fire Lab in Missoula, MT, stopping by&amp;nbsp;&lt;/span>;&lt;a href=&quot;https://inciweb.nwcg.gov/incident-information/mtfha-big-knife&quot; style=&quot;text-align: left;&quot;>;Big Knife Fire&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;&amp;nbsp;Operation Command Center.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Fine-grained fuel representation&lt;/h2>; &lt;p>;Besides training, another key use-case of the new model is for operational fire prediction. To fully leverage the advantages of the new model&#39;s capability to capture the detailed fire behavior changes from small-scale differences in fuel structures, high resolution fuel mapping and representation are needed. To this end, we are currently working on the integration of high resolution satellite imagery and geo information into ML models to allow fuel specific mapping at-scale. Some of the preliminary results will be presented at the upcoming &lt;a href=&quot;https://afefirecongress.org/schedule/&quot;>;10th International Fire Ecology and Management Congress&lt;/a>; in November 2023.&lt;/p>; &lt;br />; &lt;h2>;Future work&lt;/h2>; &lt;p>; Beyond the collaboration on the new fire spread model, there are many important and challenging problems that can help fire management and safety. Many such problems require even more accurate fire models that fully consider 3D flow interactions and fluid dynamics, thermodynamics and combustion physics. Such detailed calculations usually require high-performance computers (HPCs) or supercomputers. &lt;/p>; &lt;p>; These models can be used for research and longer-term planning purposes to develop insights on extreme fire development scenarios, build ML classification models, or establish a meaningful “danger index” using the simulated results. These high-fidelity simulations can also be used to supplement physical experiments that are used in expanding the operational models mentioned above. &lt;/p>; &lt;p>;In this direction, Google research has also &lt;a href=&quot;https://www.publish.csiro.au/WF/WF22225&quot;>;developed a high-fidelity large-scale 3D fire simulator&lt;/a>; that can be run on Google TPUs. In the near future, there is a plan to further leverage this new capability to augment the experiments, and to generate data to build insights on the development of extreme fires and use the data to design a fire-danger classifier and fire-danger index protocol.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4SgYHTgu7Uv6WNEJsEJCuFWKolnL13g9BxK2cDuMq7-SRM4dIUK9LbNxy3v0VyY4dU87T_NnehBz4iXCfHCWwoOU6V8-15P55Oi6FvT5BvcLgR_vzYB1xMPWUtj3AiwSsoPZY-9K5i9IwIADDAIjG7hzZZZZ00n7N2jVnjlp65ePQDsJdNIuX6avw8kD_/s480/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4SgYHTgu7Uv6WNEJsEJCuFWKolnL13g9BxK2cDuMq7-SRM4dIUK9LbNxy3v0VyY4dU87T_NnehBz4iXCfHCWwoOU6V8-15P55Oi6FvT5BvcLgR_vzYB1xMPWUtj3AiwSsoPZY-9K5i9IwIADDAIjG7hzZZZZ00n7N2jVnjlp65ePQDsJdNIuX6avw8kD_/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;An example of 3D high-fidelity simulation. This is a controlled burn field experiment (&lt;/span>;&lt;a href=&quot;https://www.fireweather.org/fireflux2&quot; style=&quot;text-align: left;&quot;>;FireFlux II&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;) simulated using&amp;nbsp;&lt;/span>;&lt;a href=&quot;https://www.publish.csiro.au/WF/WF22225&quot;>;Google&#39;s high fidelity fire simulator&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;We thank Mark Finney, Jason Forthofer, William Chatham and Issac Grenfell from US Forest Service Missoula Fire Science Laboratory and our colleagues John Burge, Lily Hu, Qing Wang, Cenk Gazen, Matthias Ihme,&amp;nbsp;Vivian Yang, Fei Sha and John Anderson for core contributions and useful discussions. We also thank Tyler Russell for his assistance with program management and coordination.&lt;/i>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7317378303167958176/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7317378303167958176&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7317378303167958176&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html&quot; rel=&quot;alternate&quot; title=&quot;Looking back at wildfire research in 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCepSXB1QxIgipeUbH1YGd8N3TkVoT1tAgxG0fC0PPREhbsygTQ4i58OVw6-Dt_lagwgswT0jtxUorsVmtyO9UgPEDezJHz_QiKvbJlgYF8Db8W-68KFdyZsq_uvM7YuZo9BRo__NC4BNxD6nHZpzsimeNOaV3X8dh9aliqbAbk8ycXb25s5NfLTfNe42_/s72-c/WildfireModeling.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-420799196308935505&lt;/id>;&lt;published>;2023-10-25T10:45:00.000-07:00&lt;/published>;&lt;updated>;2023-10-25T10:45:37.205-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;EMNLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Search&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Grammar checking at Google Search scale&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eric Malmi, Senior Research Scientist, and Jakub Adamek, Senior Software Engineer, Google, Bard Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW7pWsW1-E7U1LH1bxmjBL_2W5fS6uN2iRb2cNPBks57ubvFNJj-SjE2Zk1lFqndKWBVAeO3g3MLeJ96QUIUNJDyLTcWjeO835NT6HfJMYQBEdGqL-X_sxQ1yxMy16a0OcOLb6gSz2lqM6VofToVtN_s_F_wGB41AZwlj146y7ZXQ4PFsdywX10QO54MJ4/s320/HeroGC.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Many people with questions about grammar turn to Google Search for guidance. While existing features, such as “Did you mean”, already handle simple typo corrections, more complex grammatical error correction (GEC) is beyond their scope. What makes the development of new Google Search features challenging is that they must have high precision and recall while outputting results &lt;a href=&quot;https://ai.googleblog.com/2009/06/speed-matters.html&quot;>;quickly&lt; /a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The conventional approach to GEC is to treat it as a &lt;a href=&quot;https://workspace.google.com/blog/ai-and-machine-learning/using-neural-machine-translation-to-correct-grammatical-in-google-docs&quot;>;translation problem&lt;/a>; and use autoregressive &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; models to decode the response token-by-token, conditioning on the previously generated tokens. However, although Transformer models have proven to be effective at GEC, they aren&#39;t particularly efficient because the generation cannot be parallelized due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive&lt;/a>; decoding. Often, only a few modifications are needed to make the input text grammatically correct, so another possible solution is to treat GEC as a &lt;a href=&quot;https://arxiv.org/abs/2206.07043&quot;>;text editing&lt;/a>;问题。 If we could run the autoregressive decoder only to generate the modifications, that would substantially decrease the latency of the GEC model. &lt;/p>; &lt;p>; To this end, in “&lt;a href=&quot;https://aclanthology.org/2022.findings-emnlp.156/&quot;>;EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start&lt;/a>;”, published at Findings of &lt;a href=&quot;https://2022.emnlp.org/&quot; target=&quot;_blank&quot;>;EMNLP 2022&lt;/a>;, we describe a novel text-editing model that is based on the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>; Transformer encoder-decoder architecture. EdiT5 powers the new Google Search &lt;a href=&quot;https://support.google.com/websearch/answer/13420782&quot;>;grammar check feature&lt;/a>; that allows you to check if a phrase or sentence is grammatically correct and provides corrections when needed. Grammar check shows up when the phrase &quot;grammar check&quot; is included in a search query, and if the underlying model is confident about the correction. Additionally, it shows up for some queries that don&#39;t contain the “grammar check” phrase when Search understands that is the likely intent. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-BAXZ3wH-FcJmFoKvppChwvfZay7xH5bhiYmKQocV47sCb3IMfwEec1OpBWY3b1fd2rnA4Vhy4EqOCZiAR4Xpv6xPok9yC-ZqLogbER2lcmaje1NfBCoPAtdW7rEPG22PtNn4dVuYtYH61D9fnx7kudLZilxzUbi01ePhirqtTwMu6g9trhFFuLCDUCgP/s1412/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;1412&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-BAXZ3wH-FcJmFoKvppChwvfZay7xH5bhiYmKQocV47sCb3IMfwEec1OpBWY3b1fd2rnA4Vhy4EqOCZiAR4Xpv6xPok9yC-ZqLogbER2lcmaje1NfBCoPAtdW7rEPG22PtNn4dVuYtYH61D9fnx7kudLZilxzUbi01ePhirqtTwMu6g9trhFFuLCDUCgP/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;Model architecture&lt;/h2>; &lt;p>; For low-latency applications at Google, Transformer models are typically run on &lt;a href=&quot;https://cloud.google.com/tpu/docs/ intro-to-tpu&quot;>;TPUs&lt;/a>;. Due to their fast matrix multiplication units (MMUs), these devices are optimized for performing large matrix multiplications quickly, for example running a Transformer encoder on hundreds of tokens in only a few milliseconds. In contrast, Transformer decoding makes poor use of a TPU&#39;s capabilities, because it forces it to process only one token at a time. This makes autoregressive decoding the most time-consuming part of a translation-based GEC model. &lt;/p>; &lt;p>; In the EdiT5 approach, we reduce the number of decoding steps by treating GEC as a text editing problem. The EdiT5 text-editing model is based on the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>; Transformer encoder-decoder architecture with a few crucial modifications. Given an input with grammatical errors, the EdiT5 model uses an encoder to determine which input tokens to keep or delete. The kept input tokens form a draft output, which is optionally reordered using a non-autoregressive &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf&quot;>;pointer network &lt;/a>;。 Finally, a decoder outputs the tokens that are missing from the draft, and uses a pointing mechanism to indicate where each new token should be placed to generate a grammatically correct output. The decoder is only run to produce tokens that were missing in the draft, and as a result, runs for much fewer steps than would be needed in the translation approach to GEC. &lt;/p>; &lt;p>;To further decrease the decoder latency, we reduce the decoder down to a single layer, and we compensate by increasing the size of the encoder. Overall, this decreases latency significantly because the extra work in the encoder is efficiently parallelized.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2_MGEcdadlbXM1zMBQzROuRbYyqmcdhcytlnwdHeIJ_M0wrtioaRixoDOxMlE3acuMZRf4KB_T5TgzUCfBBmz3eeEMnnAKHZkvFmMzFnNu_2UAI1l3jhOp2dyQyOxElXwt7iEjun_gd77uk1TMiaBLlMfxEOxcFo8W48fdD9WqbYNvUZECUEd-LHTk1-R/s1878/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1332&quot; data-original-width=&quot;1878&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2_MGEcdadlbXM1zMBQzROuRbYyqmcdhcytlnwdHeIJ_M0wrtioaRixoDOxMlE3acuMZRf4KB_T5TgzUCfBBmz3eeEMnnAKHZkvFmMzFnNu_2UAI1l3jhOp2dyQyOxElXwt7iEjun_gd77uk1TMiaBLlMfxEOxcFo8W48fdD9WqbYNvUZECUEd-LHTk1-R/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an input with grammatical errors (“Guess when was I borned”), the EdiT5 model uses an encoder to determine which input tokens to keep (K) or delete (D), a pointer network (pointer) to reorder kept tokens, and a decoder to insert any new tokens that are needed to generate a grammatically correct output.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We applied the EdiT5 model to the &lt;a href=&quot;https://aclanthology.org/W19-4406/&quot;>;public BEA grammatical error correction benchmark&lt;/a>;, comparing different model sizes. The experimental results show that an EdiT5 large model with 391M parameters yields a higher &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F0.5 score&lt;/a>;, which measures the accuracy of the corrections, while delivering a 9x speedup compared to a T5 base model with 248M parameters. The mean latency of the EdiT5 model was merely 4.1 milliseconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQys5EavCOAEs0dy3M8r5Ar-9v7n5qtFkek1VSfeGBXH1pQHNv6Ebem3vDedO60BlNcp3HLGqYtsT4v4Evz2u2ksTgFfQcQIqd87NPTxVzYVyzvA85Vg_jO8a18KrMT2hOQQA5hg2frOJx2L7S5SM88VBCJzd13ClrSzHnl5xg29TMMpvqA4Yl-d98vTc1/s1987/image1.jpeg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1311&quot; data-original-width=&quot;1987&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQys5EavCOAEs0dy3M8r5Ar-9v7n5qtFkek1VSfeGBXH1pQHNv6Ebem3vDedO60BlNcp3HLGqYtsT4v4Evz2u2ksTgFfQcQIqd87NPTxVzYVyzvA85Vg_jO8a18KrMT2hOQQA5hg2frOJx2L7S5SM88VBCJzd13ClrSzHnl5xg29TMMpvqA4Yl-d98vTc1/s16000/image1.jpeg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Performance of the T5 and EdiT5 models of various sizes on the public BEA GEC benchmark plotted against mean latency. Compared to T5, EdiT5 offers a better latency-F0.5 trade-off. Note that the x axis is logarithmic.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Improved training data with large language models&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://arxiv.org/abs/2106.03830&quot;>;earlier research&lt;/a>;, as well as the results above, show that model size plays a crucial role in generating accurate grammatical corrections. To combine the advantages of large language models (LLMs) and the low latency of EdiT5, we leverage a technique called &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;hard distillation&lt;/a>;. First, we train a teacher LLM using similar datasets used for the &lt;a href=&quot;https://ai.googleblog.com/2021/10/grammar-correction-as-you-type-on-pixel.html&quot;>;Gboard grammar model&lt;/a>;. The teacher model is then used to generate training data for the student EdiT5 model. &lt;/p>; &lt;p>; Training sets for grammar models consist of &lt;em>;ungrammatical source / grammatical target&lt;/em>; sentence pairs. Some of the training sets have noisy targets that contain grammatical errors, unnecessary paraphrasing, or unwanted artifacts. Therefore, we generate new pseudo-targets with the teacher model to get cleaner and more consistent training data. Then, we re-train the teacher model with the pseudo-targets using a technique called &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/1909.13788&quot;>;self-training&lt;/a>;&lt;/em>; 。 Finally, we found that when the source sentence contains many errors, the teacher sometimes corrects only part of the errors. Thus, we can further improve the quality of the pseudo-targets by feeding them to the teacher LLM for a second time, a technique called &lt;em>;&lt;a href=&quot;https://aclanthology.org/D19-1435/&quot;>;iterative refinement&lt;/a>;&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJzgocbj86SS8moSqWrxIKz8E3CxZn712Di2CY7pwgXZ355Z4qUnDBxuEc91AU9jAdKwq4tvyaCzIqKfcJzThrjkka5q1LwXGWNa1xPi5AU7RpJDq-XmoNHs386B2bJqKDDnN8_l1mvLCSShlGjb66RWy25Z_1QXwMDpHDgsPON7r0pUqYdXZLPsx06hOC/s1089/image3.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1089&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgJzgocbj86SS8moSqWrxIKz8E3CxZn712Di2CY7pwgXZ355Z4qUnDBxuEc91AU9jAdKwq4tvyaCzIqKfcJzThrjkka5q1LwXGWNa1xPi5AU7RpJDq-XmoNHs386B2bJqKDDnN8_l1mvLCSShlGjb66RWy25Z_1QXwMDpHDgsPON7r0pUqYdXZLPsx06hOC/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Steps for training a large teacher model for grammatical error correction (GEC). Self-training and iterative refinement remove unnecessary paraphrasing, artifacts, and grammatical errors appearing in the original targets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Putting it all together&lt;/h2>; &lt;p>; Using the improved GEC data, we train two EdiT5-based models: a grammatical error correction model, and a grammaticality分类器。 When the grammar check feature is used, we run the query first through the correction model, and then we check if the output is indeed correct with the classifier model. Only then do we surface the correction to the user. &lt;/p>; &lt;p>; The reason to have a separate classifier model is to more easily trade off between precision and recall. Additionally, for ambiguous or nonsensical queries to the model where the best correction is unclear, the classifier reduces the risk of serving erroneous or confusing corrections. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have developed an efficient grammar correction model based on the state-of-the-art EdiT5 model architecture. This model allows users to check for the grammaticality of their queries in Google Search by including the “grammar check” phrase in the query. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We gratefully acknowledge the key contributions of the other team members, including Akash R, Aliaksei Severyn, Harsh Shah, Jonathan Mallinson, Mithun Kumar SR, Samer Hassan, Sebastian Krause, and Shikhar Thakur. We&#39;d also like to thank Felix Stahlberg, Shankar Kumar, and Simon Tong for helpful discussions and pointers.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/420799196308935505/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/grammar-checking-at-google-search-scale.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/420799196308935505&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/420799196308935505&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/grammar-checking-at-google-search-scale.html&quot; rel=&quot;alternate&quot; title=&quot;Grammar checking at Google Search scale&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW7pWsW1-E7U1LH1bxmjBL_2W5fS6uN2iRb2cNPBks57ubvFNJj-SjE2Zk1lFqndKWBVAeO3g3MLeJ96QUIUNJDyLTcWjeO835NT6HfJMYQBEdGqL-X_sxQ1yxMy16a0OcOLb6gSz2lqM6VofToVtN_s_F_wGB41AZwlj146y7ZXQ4PFsdywX10QO54MJ4/s72-c/HeroGC.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2376347423475108178&lt;/id>;&lt;published>;2023-10-20T10:07:00.000-07:00&lt;/published>;&lt;updated>;2023-11-27T16:21:27.895-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;distributed systems&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Publications&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Answering billions of reporting queries each day with low latency&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jagan Sankaranarayanan, Senior Staff Software Engineer, and Indrajit Roy, Head of Napa Product, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIZSvPVFwZPeTv8ivB-lHMGfoLkP-fDBuAy9GEklUVNBPPoqOXRfme5Psui7DbKssImVjFHtxoygKhFBvgpAG_C5852ocu9i7AOfWPeC1mSlaim8jfqsV55wZIULDUPk7WhxW1OISfL_CjZswN3CcZN7GJgVLBdepic8lfYAUCT0rAlXGGbnf-WuA6mRc6/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://ads.google.com/home/&quot;>;Google Ads&lt;/a>; infrastructure runs on an internal data warehouse called &lt;a href=&quot;https://research.google/pubs/pub50617/&quot;>;Napa&lt;/a>;. Billions of reporting queries, which power critical dashboards used by advertising clients to measure campaign performance, run on tables stored in Napa. These tables contain records of ads performance that are keyed using particular customers and the campaign identifiers with which they are associated. Keys are tokens that are used both to associate an ads record with a particular client and campaign (eg, &lt;code>;customer_id&lt;/code>;, &lt;code>;campaign_id&lt;/code>;) and for efficient retrieval. A record contains dozens of keys, so clients use reporting queries to specify keys needed to filter the data to understand ads performance (eg, by region, device and metrics such as clicks, etc.). What makes this problem challenging is that the data is &lt;em>;skewed&lt;/em>; since queries require varying levels of effort to be answered and have stringent latency expectations. Specifically, some queries require the use of millions of records while others are answered with just a few. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To this end, in “&lt;a href=&quot;https://research.google/pubs/pub52572/&quot;>;Progressive Partitioning for Parallelized Query Execution in Napa&lt;/a>;”, presented at &lt;a href=&quot;https://vldb.org/2023/&quot;>;VLDB 2023&lt;/a>;, we describe how the Napa data warehouse determines the amount of machine resources needed to answer reporting queries while meeting strict latency targets. We introduce a new progressive query partitioning algorithm that can &lt;a href=&quot;https://en.wikipedia.org/wiki/Parallel_computing&quot;>;parallelize&lt;/a>; query execution in the presence of complex data skews to perform consistently well in a matter of a few milliseconds. Finally, we demonstrate how Napa allows Google Ads infrastructure to serve billions of queries every day. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Query processing challenges&lt;/h2>; &lt;p>; When a client inputs a reporting query, the main challenge is to determine how to parallelize the query effectively. Napa&#39;s parallelization technique breaks up the query into even sections that are equally distributed across available machines, which then process these in parallel to significantly reduce query latency. This is done by estimating the number of records associated with a specified key, and assigning more or less equal amounts of work to machines. However, this estimation is not perfect since reviewing all records would require the same effort as answering the query. A machine that processes significantly more than others would result in run-time skews and poor performance. Each machine also needs to have sufficient work since needless parallelism leads to underutilized infrastructure. Finally, parallelization has to be a per query decision that must be executed near-perfectly billions of times, or the query may miss the stringent latency requirements. &lt;/p>; &lt;p>; The reporting query example below extracts the records denoted by keys (ie, &lt;code>;customer_id&lt;/code>; and &lt;code>;campaign_id&lt;/code>;) and then computes an aggregate (ie, &lt;code>;SUM(cost)&lt;/code>;) from an advertiser table. In this example the number of records is too large to process on a single machine, so Napa needs to use a subsequent key (eg, &lt;code>;adgroup_id&lt;/code>;) to further break up the collection of records so that equal distribution of work is achieved. It is important to note that at petabyte scale, the size of the data statistics needed for parallelization may be several terabytes. This means that the problem is not just about collecting enormous amounts of metadata, but also how it is managed. &lt;/p>; &lt;br />; &lt;span style=&quot;font-size: small;&quot;>; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>; SELECT customer_id, campaign_id, SUM(cost) FROM advertiser_table WHERE customer_id in (1, 7, ..., x ) AND campaign_id in (10, 20, ..., y) GROUP BY customer_id, campaign_id; &lt;/pre>;&lt;/span>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;This reporting query example extracts records denoted by keys (ie, &lt;code>;customer_id&lt;/code>; and &lt;code>;campaign_id&lt;/code>;) and then computes an aggregate (ie, &lt;code>;SUM(cost)&lt;/code>;) from an advertiser table. The query effort is determined by the keys&#39; included in the query. Keys belonging to clients with larger campaigns may touch millions of records since the data volume directly correlates with the size of the ads campaign. This disparity of matching records based on keys reflects the &lt;em>;skewness&lt;/em>; in data, which makes query processing a challenging problem.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; An effective solution minimizes the amount of metadata needed, focuses effort primarily on the skewed part of the key space to partition data efficiently, and works well within the allotted time. For example, if the query latency is a few hundred milliseconds, partitioning should take no longer than tens of milliseconds. Finally, a parallelization process should determine when it&#39;s reached the best possible partitioning that considers query latency expectations. To this end, we have developed a progressive partitioning algorithm that we describe later in this article. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Managing the data deluge&lt;/h2>; &lt;p>; Tables in Napa are constantly updated, so we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;>;log-structured merge forests&lt;/a>; (LSM tree) to organize the deluge of table updates. LSM is a forest of sorted data that is temporally organized with a &lt;a href=&quot;https://en.wikipedia.org/wiki/B-tree&quot;>;B-tree index&lt;/a>; to support efficient key lookup queries. B-trees store summary information of the sub-trees in a hierarchical manner. Each B-tree node records the number of entries present in each subtree, which aids in the parallelization of queries. LSM allows us to &lt;a href=&quot;https://research.google/pubs/pub50617/&quot;>;decouple&lt;/a>; the process of updating the tables from the mechanics of query serving in the sense that live queries go against a different version of the data, which is atomically updated once the next batch of ingest (called delta) has been fully prepared for querying. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrdgCgdHtjKomZM08fuF3qd6UWrgHHYvTgclc2iMV1UffAAMqVgsJVhGDSP4aPzPh4-kNb0hU7ZWkOAaf37k7PaTaH4k3m45fDzBaX9x0wxRSpvPADOXIDrtdTL62lFGJKzqEaDT6oz-fPxbkSxU3omqQAm8sWKmsA8SR4wHHKh6iAXbDGcU4a_WvEHsaL/s1999/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;667&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrdgCgdHtjKomZM08fuF3qd6UWrgHHYvTgclc2iMV1UffAAMqVgsJVhGDSP4aPzPh4-kNb0hU7ZWkOAaf37k7PaTaH4k3m45fDzBaX9x0wxRSpvPADOXIDrtdTL62lFGJKzqEaDT6oz-fPxbkSxU3omqQAm8sWKmsA8SR4wHHKh6iAXbDGcU4a_WvEHsaL/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div >; &lt;h2>;The partitioning problem&lt;/h2>; &lt;p>; The data partitioning problem in our context is that we have a massively large table that is represented as an LSM tree. In the figure below, Delta 1 and 2 each have their own B-tree, and together represent 70 records. Napa breaks the records into two pieces, and assigns each piece to a different machine. The problem becomes a partitioning problem of a forest of trees and requires a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tree_traversal&quot;>;tree-traversal algorithm&lt;/a>; that can quickly split the trees into two相等的部分。 &lt;/p>; &lt;p>; To avoid visiting all the nodes of the tree, we introduce the concept of “good enough” partitioning. As we begin cutting and partitioning the tree into two parts, we maintain an estimate of how bad our current answer would be if we terminated the partitioning process at that instant. This is the yardstick of how close we are to the answer and is represented below by a total error margin of 40 (at this point of execution, the two pieces are expected to be between 15 and 35 records in size, the uncertainty adds up to 40）。 Each subsequent traversal step reduces the error estimate, and if the two pieces are approximately equal, it stops the partitioning process. This process continues until the desired error margin is reached, at which time we are guaranteed that the two pieces are more or less equal. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP1MUE1iWt7K2hKPBmTmr4LfIgARhI3a-nKsMqrcmiOejHLJbxDyKIoV-9ZF8k9_X4w9EMLikeVOufzTTmBGMqBKdH_AyrWz_UihzrZFsaNPWFXB2dD8o2RIBbXCM0shFN8a6fUMW0g830qrJx0Q3mpOLfzyQwwuYB9nilFeKt3VfKQYde9Eq5VmN8D-2i/s473/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;327&quot; data-original-width=&quot;473&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP1MUE1iWt7K2hKPBmTmr4LfIgARhI3a-nKsMqrcmiOejHLJbxDyKIoV-9ZF8k9_X4w9EMLikeVOufzTTmBGMqBKdH_AyrWz_UihzrZFsaNPWFXB2dD8o2RIBbXCM0shFN8a6fUMW0g830qrJx0Q3mpOLfzyQwwuYB9nilFeKt3VfKQYde9Eq5VmN8D-2i/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;Progressive partitioning algorithm&lt;/h2>; &lt;p>; Progressive partitioning encapsulates the notion of “good enough” in that it makes a series of moves to reduce the error estimate. The input is a set of B-trees and the goal is to cut the trees into pieces of more or less equal size. The algorithm traverses one of the trees (“drill down&#39;&#39; in the figure) which results in a reduction of the error estimate. The algorithm is guided by statistics that are stored with each node of the tree so that it makes an informed set of moves at each step. The challenge here is to decide how to direct effort in the best possible way so that the error bound reduces quickly in the fewest possible steps. Progressive partitioning is conducive for our use-case since the longer the algorithm runs, the more equal the pieces become. It also means that if the algorithm is stopped at any point, one still gets good partitioning, where the quality corresponds to the time spent. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsnglKJOlArk8nIcelyOi8Yej5O2OUWCwWzitU9PsT5h-0zWDyvpVhxrdMsbsB0ECvC_q6Y91peJ6Q2r5HjXJWsH47LSEDODJw1lm-aOt8lpMhcTyMU2LOf7m2KcykAnyRLFrpx_I95spiJi5qNecVwlJ7x_aRhSDTCON6G_o09WeN8x4BwQeXbJOBOzFW/s390/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;281&quot; data-original-width=&quot;390&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsnglKJOlArk8nIcelyOi8Yej5O2OUWCwWzitU9PsT5h-0zWDyvpVhxrdMsbsB0ECvC_q6Y91peJ6Q2r5HjXJWsH47LSEDODJw1lm-aOt8lpMhcTyMU2LOf7m2KcykAnyRLFrpx_I95spiJi5qNecVwlJ7x_aRhSDTCON6G_o09WeN8x4BwQeXbJOBOzFW/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Prior work in this space uses a &lt;a href=&quot;https://research.google/pubs/ pub52572/&quot;>;sampled table to drive the partitioning process&lt;/a>;, while the Napa approach uses a B-tree. As mentioned earlier, even just a sample from a petabyte table can be massive. A tree-based partitioning method can achieve partitioning much more efficiently than a sample-based approach, which does not use a tree organization of the sampled records. We compare progressive partitioning with an alternative approach, where sampling of the table at various resolutions (eg, 1 record sample every 250 MB and so on) aids the partitioning of the query. Experimental results show the relative speedup from progressive partitioning for queries requiring varying numbers of machines. These results demonstrate that progressive partitioning is much faster than existing approaches and the speedup increases as the size of the query increases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3V3INsImX_TRgcEa2VFXMK2LtVwNyCjzqYco_QMSTQWMAIfb1D5rpF7iM4xFesoltnJI9RUQNVkuuZv7nMjDIfjzhIklmdcj1xl9B8uBMfDKLIhQ9UXgH1Vrgrf2xoNvHMrv0icZDI_PKQLo9ZA6bCzrlyvX3eayjxCH_IZjFxZ1Hy9atHm6oEUqFhZz7/s1452/image3.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1192&quot; data-original-width=&quot;1452&quot; height=&quot;525&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3V3INsImX_TRgcEa2VFXMK2LtVwNyCjzqYco_QMSTQWMAIfb1D5rpF7iM4xFesoltnJI9RUQNVkuuZv7nMjDIfjzhIklmdcj1xl9B8uBMfDKLIhQ9UXgH1Vrgrf2xoNvHMrv0icZDI_PKQLo9ZA6bCzrlyvX3eayjxCH_IZjFxZ1Hy9atHm6oEUqFhZz7/w640-h525/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Napa&#39;s progressive partitioning algorithm efficiently optimizes database queries, enabling Google Ads to serve client reporting queries billions of times each day. We note that tree traversal is a common technique that students in introductory computer science courses use, yet it also serves a critical use-case at Google. We hope that this article will inspire our readers, as it demonstrates how simple techniques and carefully designed data structures can be remarkably potent if used well. Check out the &lt;a href=&quot;https://research.google/pubs/pub52572/&quot;>;paper&lt;/a>; and a &lt;a href=&quot;https://www.youtube.com/watch?v=dtWwUWB5JyQ&quot;>;recent talk&lt;/a>; describing Napa to learn more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This blog post describes a collaborative effort between Junichi Tatemura , Tao Zou, Jagan Sankaranarayanan, Yanlai Huang, Jim Chen, Yupu Zhang, Kevin Lai, Hao Zhang, Gokul Nath Babu Manoharan, Goetz Graefe, Divyakant Agrawal, Brad Adelberg, Shilpa Kolhar and Indrajit Roy.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2376347423475108178/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt; link href=&quot;http://blog.research.google/2023/10/answering-billions-of-reporting-queries.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2376347423475108178&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/2376347423475108178&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google /2023/10/answering-billions-of-reporting-queries.html&quot; rel=&quot;alternate&quot; title=&quot;Answering billions of reporting queries each day with low latency&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name >;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel= “http://schemas.google.com/g/2005#thumbnail” src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>; &lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIZSvPVFwZPeTv8ivB-lHMGfoLkP-fDBuAy9GEklUVNBPPoqOXRfme5Psui7DbKssImVjFHtxoygKhFBvgpAG_C5852ocu9i7AOfWPeC1mSlaim8jfqsV55wZIULDUPk7WhxW1OISfL_CjZswN3CcZN7GJgVLBdepic8lfYAUCT0rAlXGGbnf-WuA6mRc6/s72-c/hero.jpg&quot; width= &quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;