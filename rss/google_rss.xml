<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2024-03-23T03:15:13.245-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="conference"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="datasets"></category><category term="Education"></category><category term="Neural Networks"></category><category term="Health"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="AI"></category><category term="Algorithms"></category><category term="CVPR"></category><category term="NLP"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Machine Intelligence"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="On-device Learning"></category><category term="AI for Social Good"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="Computer Science"></category><category term="Quantum AI"></category><category term="MOOC"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Self-Supervised Learning"></category><category term="accessibility"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="NeurIPS"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Responsible AI"></category><category term="ACL"></category><category term="Audio"></category><category term="ICML"></category><category term="ML"></category><category term="Physics"></category><category term="TPU"></category><category term="Android"></category><category term="EMNLP"></category><category term="ML Fairness"></category><category term="video"></category><category term="Awards"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Supervised Learning"></category><category term="User Experience"></category><category term="Collaboration"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="TTS"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Environment"></category><category term="Google Accelerated Science"></category><category term="Speech Recognition"></category><category term="DeepMind"></category><category term="Google Translate"></category><category term="Large Language Models"></category><category term="Video Analysis"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="RAI-HCT Highlights"></category><category term="UI"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Interspeech"></category><category term="Systems"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Differential Privacy"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="crowd-sourcing"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Climate"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Graphs"></category><category term="Kaggle"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gboard"></category><category term="Generative AI"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="Weather"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1350&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-7061041222399769838&lt;/id>;&lt;发布>;2024-03-20T13:54:00.000-07:00&lt;/发布>;&lt;更新>;2024-03- 20T13:54:06.249-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;用户体验&quot; >;&lt;/category>;&lt;title type=&quot;text&quot;>;肺癌筛查的计算机辅助诊断&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：软件工程师 Atilla Kiraly和 Rory Pilgrim，Google 研究部产品经理&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XB taXs2FsD7R8_tNqtH6D8X_KiMtZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s320/PULMA%20hero.jpg “样式=“显示：无；” />; &lt;p>; 肺癌是全球癌症相关死亡的主要原因 &lt;a href=&quot;https://www.who.int/news-room/fact-sheets/detail/cancer#:~:text= %20most%20common%20causes%20of，直肠%20（916%20000%20deaths）%3B&quot;>;2020 年报告的死亡人数为 180 万人&lt;/a>;。晚期诊断会大大降低生存机会。 &lt;a href=&quot;https://www.cdc.gov/cancer/lung/basic_info/screening.htm&quot;>;肺癌筛查&lt;/a>;通过&lt;a href=&quot;https://www.cancer.gov/about -cancer/diagnosis-staging/ct-scans-fact-sheet#:~:text=indicate%20real%20problems.-,Lung%20cancer,-Low%2Ddose%20CT&quot;>;计算机断层扫描&lt;/a>; (CT),它提供了详细的肺部 3D 图像，已被证明可以通过更早地检测潜在的癌症迹象，将高危人群的死亡率降低至少 20%。在美国，筛查涉及每年一次扫描，一些国家或病例建议或多或少地进行扫描。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://www.uspreventiveservicestaskforce.org/uspstf/recommendation/lung-cancer-screening&quot;>;美国预防服务工作组&lt;/a>;最近将肺癌筛查建议扩大了&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/34636916/&quot;>;大约80%&lt;/a>;，预计将增加筛查妇女以及少数种族和族裔群体的参与机会。然而，假阳性（即错误地报告无癌症患者的潜在癌症）可能会引起焦虑，并导致患者接受不必要的手术，同时增加医疗保健系统的成本。此外，根据医疗基础设施和放射科医生的可用性，筛查大量个体的效率可能具有挑战性。 &lt;/p>; &lt;p>; 在 Google，我们之前开发了&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;用于肺癌检测的机器学习 (ML) 模型&lt;/ a>;，并评估了它们自动检测和分类显示潜在癌症迹象的区域的能力。事实证明，在检测可能的癌症方面，其性能可与专家相媲美。虽然他们取得了很高的绩效，但要充分发挥他们的潜力，在现实环境中有效地传达发现是必要的。 &lt;/p>; &lt;p>; 为此，在“&lt;a href=&quot;https://pubs.rsna.org/doi/10.1148/ryai.230079&quot;>;肺癌筛查中的辅助人工智能：一项回顾性跨国研究美国和日本&lt;/a>;”，发表于&lt;em>;&lt;a href=&quot;https://pubs.rsna.org/journal/ai&quot;>;放射学人工智能&lt;/a>;&lt;/em>;，我们研究了机器学习如何建模可以有效地将研究结果传达给放射科医生。我们还引入了一个以用户为中心的通用界面，以帮助放射科医生利用此类模型进行肺癌筛查。该系统以 CT 成像作为输入，并使用四个类别（无怀疑、可能良性、可疑、高度可疑）以及相应的感兴趣区域输出癌症怀疑评级。我们通过美国和日本的随机读者研究，使用当地癌症评分系统 (&lt;a href=&quot;https://www.acr.org/-/media/ACR/Files/ RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf&quot;>;Lung-RADS V1.1&lt;/a>; 和 &lt;a href=&quot;https://www.jscts.org/pdf/guideline/gls3rdfig_english130621.pdf&quot;>;仙台分数&lt;/a>;）和模仿现实设置的图像查看器。我们发现，在两项读者研究中，读者特异性随着模型的帮助而增加。为了加快利用机器学习模型进行类似研究的进展，我们提供&lt;a href=&quot;https://github.com/Google-Health/google-health/tree/master/ct_dicom&quot;>;开源代码&lt;/a>;处理 CT 图像并生成与放射科医生使用的&lt;a href=&quot;https://en.wikipedia.org/wiki/Picture_archiving_and_communication_system&quot;>;图片存档和通信系统&lt;/a>; (PACS) 兼容的图像。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;开发一个接口来传达模型结果&lt;/h2>; &lt;p>; 将 ML 模型集成到放射科医生工作流程中涉及了解他们任务的细微差别和目标，以便为他们提供有意义的支持。在肺癌筛查方面，医院遵循定期更新的各个国家/地区特定指南。例如，在美国，Lung-RADs V1.1 指定 &lt;a href=&quot;https://www.acr.org/-/media/ACR/Files/RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf&quot; >;字母数字评分&lt;/a>;表示肺癌风险和后续建议&lt;em>;。 &lt;/em>;在评估患者时，放射科医生将 CT 加载到工作站中以读取病例、查找肺部结节或病变，并应用既定指南来确定后续决策。 &lt;/p>; &lt;p>; 我们的第一步是通过额外的训练来改进&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;之前开发的机器学习模型&lt;/a>;数据和架构改进，包括&lt;a href=&quot;https://research.google/pubs/attention-is-all-you-need/&quot;>;自我注意力&lt;/a>;。然后，我们没有针对特定的指南，而是尝试了一种独立于指南或其特定版本来传达人工智能结果的补充方式。具体来说，系统输出提供怀疑评级和定位（感兴趣区域），供用户结合自己的具体指南进行考虑。该界面可生成与 CT 研究直接相关的输出图像，无需更改用户的工作站。放射科医生只需要查看一小组附加图像。他们的系统或与系统的交互没有其他变化。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug- HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6FjTGzSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s857/image1.png&quot; style=&quot;margin&quot; -left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original -width=&quot;857&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug-HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6FjTG zSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s16000/image1.png&quot; />;&lt;/a >;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;辅助肺癌筛查系统输出示例。放射科医生的评估结果在发现可疑病变的 CT 体积位置上进行可视化。总体怀疑显示在 CT 图像的顶部。圆圈突出显示可疑病变，而方形则显示从不同角度（称为矢状视图）对同一病变的渲染。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 辅助肺癌筛查系统包括13 个模型，并具有类似于&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;之前的工作&lt;/a中使用的端到端系统的高级架构>;。这些模型相互协调，首先对肺部进行分割，获得总体评估，定位三个可疑区域，然后使用该信息为每个区域分配可疑评级。该系统使用 &lt;a href=&quot;https://cloud.google.com/kubernetes-engine&quot;>;Google Kubernetes Engine&lt;/a>; (GKE) 部署在 Google Cloud 上，该引擎提取映像、运行机器学习模型并提供了结果。这样可以实现可扩展性，并直接连接到&lt;a href=&quot;https://cloud.google.com/healthcare-api/docs/concepts/dicom&quot;>;DICOM 存储&lt;/a>;中存储图像的服务器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfkb9m GxEmL191sfpAclKD085x-u20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLEoWU/s646/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;394&quot; data-original-width=&quot;646&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfkb9mGxEmL191sfpAclKD085x-u 20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLeoWU/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;辅助肺癌筛查系统的 Google Cloud 部署概述以及提供图像和计算结果的各个组件的定向调用流程。使用 Google Cloud 服务将图像提供给查看者和系统。该系统在 Google Kubernetes Engine 上运行，该引擎提取图像、处理图像并将其写回到 DICOM 存储中。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style= &quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;读者研究&lt;/h2>; &lt;p>;为了评估系统在改善临床表现方面的效用，我们进行了两项读者研究（即设计的实验）使用预先存在的、去识别化的 CT 扫描来评估临床表现（比较有或没有技术帮助的专家表现）与 12 名放射科医生。我们向 6 位美国放射科医生和 6 位日本放射科医生介绍了 627 个具有挑战性的病例。在实验设置中，读者被分为两组，每个案例阅读两次，有或没有模型的帮助。读者被要求应用他们在临床实践中通常使用的评分指南，并报告他们对每个病例​​对癌症的总体怀疑。然后，我们比较了读者的反馈结果，以衡量模型对他们的工作流程和决策的影响。分数和怀疑水平是根据个人的实际癌症结果来判断的，以衡量敏感性、特异性和&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/roc-and -auc#:~:text=AUC%20stands%20for%20%22Area%20under,across%20all%20possible%20classification%20thresholds。&quot;>;ROC 曲线下面积&lt;/a>; (AUC) 值。这些是在有帮助和没有帮助的情况下进行比较的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1yLUWn Z1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFPmwXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s794/image3.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;297&quot; data-original-width=&quot;794&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1yLUWnZ1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFPm wXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >; 多案例多读者研究涉及每个读者对每个案例进行两次审查，一次有机器学习系统协助，一次没有机器学习系统协助。在此可视化中，读者首先在没有帮助的情况下查看 A 组（&lt;strong>;蓝色&lt;/strong>;），然后在冲洗期后在有帮助的情况下查看（&lt;strong>;橙色&lt;/strong>;）。第二个读者组遵循相反的路径，首先在帮助下阅读同一组案例 A 集。读者被随机分配到这些组中，以消除排序的影响。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;使用相同界面进行这些研究的能力突出了其对完全不同的癌症评分的普遍性系统，以及模型和辅助能力对不同患者群体的推广。我们的研究结果表明，当放射科医生在临床评估中使用该系统时，他们在没有可操作的肺癌发现的情况下正确识别肺部图像的能力（即&lt;em>;特异性&lt;/em>;）比之前提高了绝对 5-7%当他们不使用辅助系统时。这可能意味着每筛查 15-20 名患者，就有可能避免不必要的后续程序，从而减轻他们的焦虑和医疗保健系统的负担。反过来，这可以帮助提高肺癌筛查计划的可持续性，特别是随着&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/34636916/&quot;>;更多人有资格接受筛查&lt;/a >;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8Zh GHZmy52srebu49xIL4wYkuvyftssXzvohoSoBKt9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s1999/image2.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;824&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8ZhGHZmy52srebu49xIL4wYkuvyftssXzvohoSoBK t9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;在美国和日本的读者研究中，通过机器学习模型的帮助，读者的特异性得到了提高。特异性值是根据可操作的发现（发现可疑的东西）与没有可操作的发现的读者评分得出的，并与个体的真实癌症结果进行比较。在模型协助下，读者标记出较少的癌症阴性个体进行后续访问。对癌症阳性个体的敏感性保持不变。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;通过合作将其转化为现实世界的影响 &lt;/h2>; &lt;p>; 该系统结果表明，有可能减少随访次数、减少焦虑，并降低肺癌筛查的总体成本。为了将这项研究转化为现实世界的临床影响，我们正在与：&lt;a href=&quot;https://deephealth.com/&quot;>;DeepHealth&lt;/a>;，一家领先的人工智能健康信息学提供商；与印度领先的放射学服务提供商 &lt;a href=&quot;https://apolloradiologyintl.com/&quot;>;Apollo Radiology International&lt;/a>; 合作，探索将该系统纳入未来产品的途径。此外，我们还希望通过 &lt;a href=&quot;https://github.com/Google-Health/google-health/tree/master/ct_dicom&quot;>; 帮助其他研究人员研究如何最好地将 ML 模型结果集成到临床工作流程中开源代码&lt;/a>;用于读者研究并结合本博客中描述的见解。我们希望这将有助于加速医学影像研究人员对其人工智能模型进行读者研究，并促进该领域的转化研究。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;该项目的主要贡献者包括 Corbin Cunningham、Zaid Nabulsi、Ryan Najafi、Jie Yang、Charles Lau、Joseph R. Ledsam、叶文兴、Diego Ardila、Scott M. McKinney、Rory Pilgrim、Hiroaki Saito、Yasuteru Shimamura、Mozziyar Etemadi、Yun Liu、David Melnick、Sunny Jansen、Nadia Harhen 、David P. Nadich、Mikhail Fomitchev、Ziyad Helali、Shabir Adeel、Greg S. Corrado、Lily Peng、Daniel Tse、Shravya Shetty、Shruthi Prabhakara、Neeral Beladia 和 Krish Eswaran。感谢 Arnav Agharwal 和 Andrew Sellergren 的开源支持，以及 Vivek Natarajan 和 Michael D. Howell 的反馈。还要衷心感谢放射科医生在整个研究过程中通过图像解释和注释工作实现了这项工作，以及 Jonny Wong 和 Carli Sampson 协调读者研究。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt; /content>;&lt;link href=&quot;http://blog.research.google/feeds/7061041222399769838/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/computer-aided-diagnosis-for-lung.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7061041222399769838&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/7061041222399769838&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2024/03/computer-aided-diagnosis-for-lung.html&quot; rel=&quot;alternate&quot; title=&quot;肺癌筛查的计算机辅助诊断&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI &lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http: //schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author >;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XBtaXs2FsD7R8 _tNqtH6D8X_KiMtZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s72-c/PULMA%20hero.jpg&quot;宽度=&quot; 72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签:blogger.com,1999:blog-8474926331452026626.post-4615278636568583418&lt;/id>;&lt;published>;2024-03-20T09:06:00.000-07:00&lt;/published>;&lt;updated>;2024-03-20T09:06:06.753 -07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“环境”>;&lt;/类别>;&lt;类别方案=“http://www.blogger” .com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;利用人工智能扩大全球对可靠洪水预报的访问&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt; span class=&quot;byline-author&quot;>;发布者：Yossi Matias，工程副总裁研究和 Gray Nearing，研究科学家，Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX-dCHpcYtnS x96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s320/洪水%20预测% 20hero%20image.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 洪水是&lt;a href=&quot;https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content&quot;>;最常见的自然灾害&lt;/ a>;，并造成全球每年大约&lt;a href=&quot;https://www.swissre.com/risk-knowledge/mitigating-climate-risk/floods.html&quot;>;500亿美元&lt;/a>;的经济损失。 &lt;a href=&quot;https://library.wmo.int/records/item/57630-2021-state-of-climate-services-water?offset=1#:~:text=WMO%2DNo.,1278&amp;amp; text=More%20than%202%20billion%20people,for%20the%20past%2020%20years.&quot;>;自 2000 年以来，与洪水相关的灾害发生率增加了一倍多&lt;/a>;&lt;a href=&quot;https: //www.nature.com/articles/s41598-020-70816-2&quot;>;由于气候变化&lt;/a>;。近 &lt;a href=&quot;https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content&quot;>;15 亿人&lt;/a>;，占 19%世界人口面临着严重洪水事件的巨大风险。升级预警系统，让这些人群能够获得准确、及时的信息&lt;a href=&quot;https://elibrary.worldbank.org/doi/abs/10.1596/1813-9450-6058&quot;>;每年可以挽救数千人的生命&lt; /a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在可靠的洪水预报对全球人们生活的潜在影响的推动下，我们于 2017 年开始了洪水预报工作。通过此 &lt;a href=&quot;https ://blog.google/technology/ai/google-ai-global-flood-forecasting/&quot;>;多年历程&lt;/a>;，多年来，我们在推进研究的同时建立了实时运营系统洪水预报系统，&lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;在 Google 搜索、地图、Android 通知和通过&lt;a href=&quot;http://g.co/floodhub&quot;>;洪水中心&lt;/a>;。不过，为了&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;在全球范围内扩展&lt;/a>;，尤其是在某些地方在无法获得准确的当地数据的地方，需要取得更多的研究进展。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://www.nature.com/articles/s41586-024-07145-1&quot;>;未测量流域的极端洪水全球预测&lt;/a>;”中，发表在&lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;自然&lt;/a>;&lt;/em>;中，我们展示了机器学习 (ML) 技术如何显着改善全球范围&lt;a href= “https://sites.research.google/floodforecasting/&quot;>;洪水预报&lt;/a>;相对于洪水相关数据稀缺的国家当前最先进的技术。借助这些基于人工智能的技术，我们将当前可用的全球临近预报的可靠性平均从零天延长到五天，并将非洲和亚洲地区的预报改进为与欧洲当前可用的预报类似。模型的评估是与欧洲中期天气预报中心 (&lt;a href=&quot;https://www.ecmwf.int/&quot;>;ECMWF&lt;/a>;) 合作进行的。 &lt;/p>; &lt;p>; 这些技术还使&lt;a href=&quot;http://g.co/floodhub&quot;>;洪水中心&lt;/a>;能够提前 7 天提供实时河流预报，&lt;a href =&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;覆盖&lt;/a>; 80 多个国家的河流河段。人们、社区、政府和国际组织可以利用这些信息来采取预期行动，帮助保护弱势群体。 &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot;frameborder=&quot;0&quot; height=&quot;360 “ src=&quot;https://www.youtube.com/embed/ET04pDj-RvM?si=WJJXEtwJqtyMRuC_?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;[ET04pDj-RvM]&quot;>;&lt;/iframe>;&lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;Google 的洪水预报 &lt;/h2>; &lt;p>; 为 FloodHub 工具提供支持的机器学习模型是与多个合作伙伴（包括学术界、政府、国际组织和机构）合作多年研究的成果。非政府组织。 &lt;/p>; &lt;p>; 2018 年，我们&lt;a href=&quot;https://blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;启动了试点&lt; /a>; 印度恒河-雅鲁藏布江流域的早期预警系统，&lt;a href=&quot;https://arxiv.org/abs/1901.09583&quot;>;假设&lt;/a>;认为机器学习可以帮助解决具有挑战性的问题可靠的大规模洪水预报。第二年，该试点进一步&lt;a href=&quot;https://blog.google/technology/ai/tracking-our-progress-on-flood-forecasting/&quot;>;扩大&lt;/a>;&lt;a href=&quot;https: //ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html&quot;>;通过结合淹没模型、实时水位测量、创建高程图和水文建模。 &lt;/p>; &lt;p>; 在与学者的&lt;a href=&quot;https://ai.googleblog.com/2019/03/a-summary-of-google-flood-forecasting.html&quot;>;合作&lt;/a>;中，特别是，我们与&lt;a href=&quot;https://www.jku.at/en/institute-for-machine-learning/&quot;>;JKU 机器学习研究所&lt;/a>;一起探索了基于机器学习的水文模型，表明基于&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;>;LSTM&lt;/a>;的模型可以&lt;a href=&quot;https://hess. copernicus.org/articles/23/5089/2019/&quot;>;比传统的概念性和基于物理的&lt;a href=&quot;https://en.wikipedia.org/wiki/Hydrological_model&quot;>;水文学产生更准确的模拟&lt;/a>;模型&lt;/a>;。这项研究带来了&lt;a href=&quot;https://blog.research.google/2020/09/the-technology-behind-our-recent.html&quot;>;洪水预报改进&lt;/a>;，实现了&lt;a href= &quot;https://blog.google/technology/ai/flood-forecasts-india-bangladesh/&quot;>;扩大&lt;/a>;我们的预测覆盖范围，将印度和孟加拉国全部覆盖。我们还与耶鲁大学的研究人员合作，测试可增加&lt;a href=&quot;https://egc.yale.edu/about/perspectives/pande-and-coauthors-using-technology-save-lives-during-印度季风季节&quot;>;洪水警报的范围和影响&lt;/a>;。 &lt;/p>; &lt;p>; 我们的水文模型通过处理降水和物理流域信息等公开的天气数据来预测河流洪水。此类模型必须根据来自各个河流的&lt;a href=&quot;https://en.wikipedia.org/wiki/Stream_gauge&quot;>;水流测量站&lt;/a>;的长数据记录进行校准。全球河流流域（流域）拥有流量计的比例较低，流量计价格昂贵，但却是提供相关数据所必需的，而且水文模拟和预报提供流量计具有挑战性&lt;a href=&quot;https://www.tandfonline.com/doi /full/10.1080/02626667.2013.803183&quot;>;缺乏此基础设施的流域的预测&lt;/a>;。 &lt;a href=&quot;https://www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;国内生产总值&lt;/a>; (GDP) 下降与&lt;a href=&quot;https:// www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;洪水风险的脆弱性&lt;/a>;，并且一个国家的国民生产总值与公开数据量之间存在负相关关系。机器学习通过允许&lt;a href=&quot;https://www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;在所有可用河流数据上训练单个模型&lt;/a>;来帮助解决这个问题适用于&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091&quot;>;没有可用数据&lt;/a>;的未计量盆地。通过这种方式，模型可以在全球范围内进行训练，并且可以对任何河流位置进行预测。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZrbCK nROTNn_hmOXBDxWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s1051/Streamflow%20data%20from%20the %20Global%20Runoff%20Data%20Center.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width= “1051”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZrbCKnROTNn_hmOXBD xWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s16000/Streamflow%20data%20from%20the%20Global%20Runoff%20Data%20Center.jpg&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;公开可用的流量之间存在逆（对数）相关性一个国家的数据和国民生产总值。来自&lt;a href=&quot;https://www.bafg.de/GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;/a>;的水流数据。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 我们的学术合作促成了机器学习研究，该研究开发了&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091&quot;>;估计河流预报的不确定性&lt;/ a>; 并展示了 ML 河流预测模型&lt;a href=&quot;https://hess.copernicus.org/articles/25/2685/2021/hess-25-2685-2021-relations.html&quot;>;如何从多个数据中合成信息来源&lt;/a>;。他们证明，这些模型可以&lt;a href=&quot;https://hess.copernicus.org/articles/26/3377/2022/hess-26-3377-2022.html&quot;>;可靠地模拟极端事件&lt;/a>;，甚至当这些事件不是训练数据的一部分时。为了&lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;为开放科学做出贡献&lt;/a>;，我们将在 2023 年开放-在 &lt;em>;&lt;a href=&quot;https://www.nature.com/articles/s41597-023-01975-w&quot;>;Nature Scientific Data&lt;/a>;&lt;/ 中获取了社区驱动的大样本水文学数据集嗯>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;河流预报模型&lt;/h2>; &lt;p>; 国家和国际机构使用的大多数水文模型洪水预报和河流建模是状态空间模型，仅取决于日常输入（例如降水、温度等）和系统的当前状态（例如土壤湿度、积雪等）。 LSTM 是状态空间模型的一种变体，通过定义表示单个时间步长的神经网络来工作，其中处理输入数据（例如当前天气状况）以生成该时间步长的更新状态信息和输出值（流） 。 LSTM 按顺序应用来进行时间序列预测，从这个意义上说，其行为类似于科学家通常概念化水​​文系统的方式。根据经验，我们发现 &lt;a href=&quot;https://hess.copernicus.org/articles/23/5089/2019/&quot;>;LSTM 在河流预测任务上表现良好&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xxrB 87mupNTPpioKT0wRgSSc1FwYDmfCUWyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s960/image1.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xxrB87mupNTPpioKT0wRgSSc1FwYDmfCU WyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;LSTM 的示意图，LSTM 是一种按时间顺序运行的神经网络。可以在&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;>;此处&lt;/a>;找到易于理解的入门读物。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 我们的河流预报模型使用两个连续应用的 LSTM：（1）“后报”LSTM 摄取截至当前时间（或者更确切地说，预报发布时间）的历史天气数据（动态后报特征） ），（2）“预测”LSTM 从后播 LSTM 中提取状态以及预测的天气数据（动态预测特征）来做出未来的预测。将一年的历史天气数据输入到后报 LSTM 中，将 7 天的预报天气数据输入到预报 LSTM 中。静态特征包括流域的地理和地球物理特征，这些特征被输入到后报和预测 LSTM 中，并允许模型学习各种类型流域的不同水文行为和响应。 &lt;/p>; &lt;p>; 预测 LSTM 的输出被输入到使用 &lt;a href=&quot;https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf 的“头”层&quot;>;混合密度网络&lt;/a>;产生概率预测（即，水流概率分布的预测参数）。具体来说，该模型在每次预测时都会预测重尾概率密度函数的混合参数，称为“非对称拉普拉斯分布”时间步。结果是一个混合密度函数，称为&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2019/file/d80126524c1e9641333502c664fc6ca1-Paper.pdf&quot;>;非对称拉普拉斯的可数混合&lt;/a>;（ CMAL）分布，表示特定时间特定河流中体积流量的概率预测。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ3dm 8TRicy_wkTVtM4Xio_mhQPsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s960/LSTM-based%20river %20forecast%20model.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ3dm8TRicy_wkTVtM4Xio_mhQ PsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s16000/LSTM-based%20river%20forecast%20model.jpeg&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;基于 LSTM 的河流预报模型架构。按顺序应用两个 LSTM，一个摄取历史天气数据，一个摄取预测天气数据。模型输出是每个预测时间步长的水流概率分布参数。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot; >; &lt;br />; &lt;/div>; &lt;h2>;输入和训练数据&lt;/h2>; &lt;p>; 该模型使用三种类型的公开数据输入，大部分来自政府来源：&lt;/p>; &lt;ol>; &lt;li>;&lt; em>;代表地理和地球物理变量的静态流域属性：&lt;/em>;来自 &lt;a href=&quot;https://www.Hydrosheds.org/Hydroatlas&quot;>;HydroATLAS 项目&lt;/a>;，包括长期气候指数等数据（降水量、温度、积雪比例）、土地覆盖和人为属性（例如，作为人类发展指标的夜间灯光指数）。 &lt;/li>;&lt;li>;&lt;em>;历史气象时间序列数据&lt;/em>;：用于在预测发布之前一年启动模型。数据来自&lt;a href=&quot;https://gpm.nasa.gov/data/imerg&quot;>;NASA IMERG&lt;/a>;，&lt;a href=&quot;https://psl.noaa.gov/data/gridded/ data.cpc.globalprecip.html&quot;>;NOAA CPC 全球统一日降水量分析&lt;/a>;，以及 &lt;a href=&quot;https://cds.climate.copernicus.eu/cdsapp#!/dataset/ reanalysis-era5-land?tab=overview&quot;>;ECMWF ERA5-land 再分析&lt;/a>;。变量包括每日总降水量、气温、太阳和热辐射、降雪和表面压力。 &lt;/li>;&lt;li>;&lt;em>;7 天预测范围内的预测气象时间序列&lt;/em>;：用作预测 LSTM 的输入。这些数据与上面列出的气象变量相同，来自 &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/datasets/set-i&quot;>;ECMWF HRES 大气模型&lt;/a>;。 &lt;/li>; &lt;/ol>; &lt;p>; 训练数据是来自&lt;a href=&quot;https://www.bafg.de/GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;的每日流量值&lt; /a>; 1980 - 2023 年期间。使用来自 5,680 个不同流域水流测量仪（如下所示）的数据训练单个水流预测模型，以改进&lt;a href=&quot;https://eartharxiv.org/repository/view/6363 /&quot;>;准确度&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3​​YF6L3BweAC0hhMkET63 4isx6xzUswrYfDwp8oueoWJ7c3hf0os-RISANrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s1417/gauge_locations_map(1 ).jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;689&quot; data-original-width=&quot;1417&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3​​YF6L3BweAC0hhMkET634isx6xzUswrYfDwp8oueoWJ 7c3hf0os-RIsaNrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s16000/gauge_locations_map(1).jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;5,680 个流量计的位置，为 &lt;a href=&quot;https://www.bafg.de/ 的河流预报模型提供训练数据GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;/a>;。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;改进当前最先进的技术&lt;/h2>; &lt;p>;我们将我们的河流预测模型与&lt;a href=&quot;https://www .globalfloods.eu/&quot;>;GloFAS 版本 4&lt;/a>;，当前最先进的全球洪水预报系统。这些实验表明，机器学习可以更早地针对规模更大、影响力更大的事件提供准确的警告。 &lt;/p>; &lt;p>; 下图显示了预测河流位置不同严重程度事件时&lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1分数&lt;/a>;的分布世界各地，精确度为正负 1 天。 F1 分数是精确度和召回率的平均值，事件严重性通过 &lt;a href=&quot;https://en.wikipedia.org/wiki/Return_period#:~:text=A%20return%20period%2C%20also%20known 来衡量,river%20discharge%20flows%20to%20发生。&quot;>;返回期&lt;/a>;。例如，2 年重现期事件是预计平均每两年超过一次的水流量。我们的模型在长达 4 天或 5 天的交付时间内实现了可靠性评分，平均而言，与 GloFAS 即时预报（0 天交付时间）的可靠性相似或更好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjwzwV6QYl4yIlWs1xdHz2HRiNi2I8WUaTGBVlVvA4guppIGpJ3RMj8ypE7chWz8sV5KJuS4dPe9PUd6TqWe46W 8Yelga1Nq28Mts72zqJhLJXDgMjSa6VCHlb9ZH3eo8XETWSqj8lNraejCAezFpkGpfJrPIl4xMhRPHSdO1WX7bZmVSLDFMZOwMfarb5/s3908/分布%20of%20F1%20分数%20over %202-year%20.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1844&quot; data-original-width=&quot;3908 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjwzwV6QYl4yIlWs1xdHz2HRiNi2I8WUaTGBVlVvA4guppIGpJ3RMj8ypE7chWz8sV5KJuS4dPe9PUd6TqWe46W8Yelga1Nq28Mts72 zqJhLJXDgMjSa6VCHlb9ZH3eo8XETWSqj8lNraejCAezFpkGpfJrPIl4xMhRPHSdO1WX7bZmVSLDFMZOwMfarb5/s16000/Distributions%20of%20F1%20scores%20over%202-year%20.jpeg&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://en.wikipedia.org/wiki/F- Score&quot;>;F1 得分&lt;/a>; 2014-2023 年期间全球 2,092 个流域的 2 年重现期事件，由 GloFAS（&lt;strong>;蓝色&lt;/strong>;）和我们的模型（&lt;strong>;橙色&lt;/strong>; >;) 在不同的交货时间。平均而言，我们的模型在统计上与 2 年（显示）以及 1 年、5 年和 10 年事件（未显示）提前 5 天的 GloFAS 即时预报（0 天提前时间）一样准确.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 此外（未显示），我们的模型在更大和更罕见的极端事件中实现了准确性，在 5 年重现期事件中的精确度和召回率得分在 1 年重现期事件中，与 GloFAS 的精度相似或更好。有关更多信息，请参阅&lt;a href=&quot;https://www.nature.com/articles/s41586-024-07145-1&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;展望未来&lt;/h2>; &lt;p>;洪水预报计划是我们&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;适应和恢复能力工作&lt;/a>;并体现了 Google 的承诺&lt;a href=&quot;https:// /research.google/teams/climate-and-sustainability/&quot;>;应对气候变化&lt;/a>;，同时帮助全球社区增强抵御能力。我们相信人工智能和机器学习将继续在帮助推动气候行动科学研究方面发挥关键作用。 &lt;/p>; &lt;p>; 我们积极&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/4-flood-forecasting-collaboration-case-studies-show-how-ai-can-help-与有需要的社区/&quot;>;与多个国际援助组织（例如人道主义数据中心和红十字会）合作&lt;/a>;，提供可行的洪水预报。此外，与&lt;a href=&quot;https://wmo.int/&quot;>;世界气象组织&lt;/a>; (WMO) 持续合作&lt;a href=&quot;https://blog.google/outreach-initiatives /sustainability/early-warning-system-wmo-google/&quot;>;支持气候灾害预警系统&lt;/a>;，我们正在进行一项研究，以帮助了解人工智能如何帮助解决国家洪水预报机构面临的现实挑战。 &lt;/p>; &lt;p>; 虽然这里介绍的工作表明洪水预报向前迈出了重要一步，但未来的工作还需要进一步将洪水预报覆盖范围扩大到全球更多地点以及其他类型的洪水相关事件和灾害，包括山洪和灾害。城市洪水。我们期待与学术界和专家界、地方政府和业界的合作伙伴继续合作，以实现这些目标。 &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4615278636568583418/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4615278636568583418&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4615278636568583418&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html&quot; rel=&quot;alternate&quot; title=&quot;利用人工智能扩大全球获得可靠洪水预报的机会&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX- dCHpcYtnSx96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s72-c/洪水%20预测%20英雄%20 image.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-520087429457973735&lt;/id>;&lt;已发布>;2024-03-19T13:15:00.000- 07:00&lt;/发布>;&lt;更新>;2024-03-19T13:15:33.664-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= HCI&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;自我监督学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;UI&quot;>;&lt;/category >;&lt;title type=&quot;text&quot;>;ScreenAI：用于 UI 和视觉情境语言理解的视觉语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Srinivas Sunkara 和Gilles Baechler，Google 研究部软件工程师&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDka BFsFVx-miaDL6L0UhSz1Is8_L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s320/ScreenAI%20-%20hero。 jpeg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 屏幕用户界面 (UI) 和信息图表（例如图表、图表和表格）在人类交流和人机交互中发挥着重要作用，因为它们促进了丰富的交互式用户体验。 UI 和信息图表共享相似的设计原则和视觉语言（例如图标和布局），这提供了构建可以理解、推理并与这些界面交互的单一模型的机会。然而，由于其复杂性和不同的呈现格式，信息图表和 UI 提出了独特的建模挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为此，我们引入“&lt;a href=&quot;https://arxiv.org/abs/2402.04615&quot;>;ScreenAI：一种视觉语言UI 和信息图表理解模型&lt;/a>;”。 ScreenAI 通过来自 &lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>;PaLI 架构&lt;/a>;的灵活修补策略进行了改进”pix2结构&lt;/a>;。我们在独特的数据集和任务组合上训练 ScreenAI，其中包括一项新颖的屏幕注释任务，该任务要求模型识别屏幕上的 UI 元素信息（即类型、位置和描述）。这些文本注释为大型语言模型 (LLM) 提供了屏幕描述，使它们能够自动大规模生成问答 (QA)、UI 导航和摘要训练数据集。仅用 5B 参数，ScreenAI 就可以在基于 UI 和信息图表的任务上实现最先进的结果 (&lt;a href=&quot;https://x-lance.github.io/WebSRC/&quot;>;WebSRC&lt;/a>;和 &lt;a href=&quot;https://github.com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;），以及 &lt;a href=&quot;https://github.com/vis-nlp 上一流的性能/ChartQA&quot;>;图表 QA&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1&quot;>;DocVQA&lt;/a>; 和 &lt; a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>; 与相似大小的模型进行比较。我们还发布了三个新数据集：&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#screen-annotation-dataset-details&quot;>;屏幕注释&lt;/ a>; 评估模型的布局理解能力，以及&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa/tree/main?tab=readme-ov-file#short_answers- directory&quot;>;ScreenQA Short&lt;/a>; 和&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#complexqa&quot; target=&quot;_blank&quot;>;复杂 ScreenQA&lt; /a>; 对其 QA 能力进行更全面的评估。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ScreenAI&lt;/h2>; &lt;p>; ScreenAI 的架构基于 &lt;a href=&quot;https:// /arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>;，由多模态编码器块和自回归解码器组成。 PaLI 编码器使用创建图像嵌入的视觉转换器 (ViT) 和将图像和文本嵌入连接起来的多模态编码器作为输入。这种灵活的架构使 ScreenAI 能够解决可以重新转换为文本+图像到文本问题的视觉任务。 &lt;/p>; &lt;p>; 在 PaLI 架构之上，我们采用了 pix2struct 中引入的灵活修补策略。不使用固定网格图案，而是选择网格尺寸以保留输入图像的原始纵横比。这使得 ScreenAI 能够在各种长宽比的图像上正常工作。 &lt;/p>; &lt;p>; ScreenAI 模型分两个阶段进行训练：预训练阶段和微调阶段。首先，应用自监督学习自动生成数据标签，然后用于训练 ViT 和语言模型。 ViT 在微调阶段被冻结，其中使用的大多数数据都是由人类评估者手动标记的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls 1ZV-IE-d75jP5M02cRSZTYuU8FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s1600/image6.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;583&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls1ZV-IE-d75jP5M02cRSZTYuU8F JBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;ScreenAI模型架构。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;数据生成&lt;/h2>; &lt;p>; 为了创建 ScreenAI 的预训练数据集，我们首先编译来自各种设备（包括台式机、移动设备和平板电脑）的大量屏幕截图。这是通过使用&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot; target=&quot;_blank&quot;>;可公开访问的网页&lt;/a>;并遵循用于&lt;a href=&quot;的编程探索方法来实现的https://dl.acm.org/doi/10.1145/3126594.3126651&quot; target=&quot;_blank&quot;>;RICO 数据集&lt;/a>;，适用于移动应用。然后，我们应用基于 &lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot; target=&quot;_blank&quot;>;DETR&lt;/a>; 模型的布局注释器，该模型可识别并标记各种 UI 元素（例如，图像、象形图、按钮、文本）及其空间关系。使用能够区分 77 种不同图标类型的&lt;a href=&quot;https://arxiv.org/abs/2210.02663&quot; target=&quot;_blank&quot;>;图标分类器&lt;/a>;对象形图进行进一步分析。这种详细的分类对于解释通过图标传达的微妙信息至关重要。对于分类器未覆盖的图标以及信息图表和图像，我们使用 PaLI 图像字幕模型来生成提供上下文信息的描述性字幕。我们还应用&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot; target=&quot;_blank&quot;>;光学字符识别&lt;/a>; (OCR) 引擎来提取和注释屏幕上的文本内容。我们将 OCR 文本与之前的注释相结合，创建每个屏幕的详细描述。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1V pdNMsBr0ugBcBdL_r6dUwOwdfJfBMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s1747/image2.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1055&quot; data-original-width=&quot;1747&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1VpdNMsBr0ugBcBdL_r6dUwOwdfJfBMn 3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;带有生成注释的移动应用屏幕截图，其中包括 UI 元素及其描述，例如，&lt;code>;TEXT&lt;/code>; 元素还包含来自 OCR 的文本内容，&lt;code>;IMAGE&lt;/code>; 元素包含图像标题，&lt;code>;LIST_ITEMs&lt;/code>; 包含其所有子元素。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;基于LLM的数据生成&lt;/h3>; &lt;p>;我们使用&lt;a href=&quot;https://blog.google/technology/ai/google增强预训练数据的多样性-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>; 通过两步过程生成输入-输出对。首先，使用上述技术生成屏幕注释，然后我们围绕此模式制作提示，以便法学硕士创建合成数据。这个过程需要及时的工程和迭代细化才能找到有效的提示。我们通过针对质量阈值的人工验证来评估生成的数据的质量。 &lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px;white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;你只讲 JSON。不要编写非 JSON 的文本。您将看到以下移动屏幕截图，并用文字进行了描述。您能否针对屏幕截图的内容生成 5 个问题以及相应的简短答案？答案应尽可能简短，仅包含必要的信息。您的答案应结构如下：问题：[ {{问题：问题，答案：答案}}，...] {THE SCREEN SCHEMA} &lt;/font>;&lt;/pre>; &lt;br />; &lt;tablealign= &quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;QA 数据生成提示示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 通过结合法学硕士的自然语言能力通过结构化模式，我们模拟各种用户交互和场景，以生成合成的、真实的任务。特别是，我们生成三类任务：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;问答&lt;/strong>;：模型被要求回答有关屏幕截图内容的问题，例如，“什么时候餐厅开门了吗？” &lt;/li>;&lt;li>;&lt;strong>;屏幕导航&lt;/strong>;：要求模型将自然语言话语转换为屏幕上的可执行操作，例如“单击搜索按钮”。 &lt;/li>;&lt;li>;&lt;strong>;屏幕摘要&lt;/strong>;：要求模型用一两句话总结屏幕内容。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR44ceN wDpkvaSLa4R3v4C-hESnHDEC-JUUx31zZmDHDDwhWAMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7 /s1398/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1398&quot; data-original-width=&quot;1272&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR44ceNwDpkvaSLa4R3v4C-hEsnHdEc -JUUx31zZmDHDDwhWaMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用现有 ScreenAI 模型和 LLM 生成用于 QA、摘要和导航任务的数据的工作流程框图。每个任务都使用自定义提示来强调所需的方面，例如与计数相关的问题、涉及推理的问题等。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;img height=&quot;540&quot; src=&quot;https://lh7-us.googleusercontent.com/LmUtXBMXK-zy_rMShHQ_Hk4vQeXu2Kpx8zfzjhE3uAREczbkbGTEjZ7OMTbqtB37lD4rF31xJsoWdVXNAXLbbM1Uc_01WZWmOfBg9RwyAUETo Ppa1W38Pt117Zj5LrNfnxXqjXoAJDZd-zcAIgU4QSoBaAKsIrSi8_POI14F5hguN1NJL9a2RsrKg6WHz7w&quot; style=&quot;margin-left: auto; margin-right: auto; margin-top : 0 像素;&quot; width=&quot;705&quot; />;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLM 生成的数据。屏幕 QA、导航和摘要示例。对于导航，操作边界框在屏幕截图上显示为红色。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;实验与结果&lt;/h2>; &lt;p>; 如前所述，ScreenAI 的训练分两个阶段：预训练和微调。预训练数据标签是通过自我监督学习获得的，微调数据标签来自人类评估者。 &lt;/p>; &lt;p>; 我们使用公共 QA、摘要和导航数据集以及与 UI 相关的各种任务来微调 ScreenAI。对于 QA，我们使用多模式和文档理解领域的完善基准，例如 &lt;a href=&quot;https://github.com/vis-nlp/ChartQA&quot;>;ChartQA&lt;/a>;、&lt;a href=&quot;https ://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1&quot;>;DocVQA&lt;/a>;，&lt;a href=&quot;https://rrc.cvc.uab.es/?ch =17&amp;amp;com=tasks&quot;>;多页 DocVQA&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>;、&lt;a href=&quot;https://ocr -vqa.github.io/&quot;>;OCR VQA&lt;/a>;、&lt;a href=&quot;https://x-lance.github.io/WebSRC/&quot;>;Web SRC&lt;/a>; 和 &lt;a href=&quot;https ://github.com/google-research-datasets/screen_qa&quot;>;ScreenQA&lt;/a>;。对于导航，使用的数据集包括&lt;a href=&quot;https://github.com/google-research-datasets/uibert/tree/main&quot;>;引用表达式&lt;/a>;、&lt;a href=&quot;https://github.com/uibert/tree/main&quot;>;引用表达式&lt;/a>; com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2209.15099&quot;>;马克杯&lt;/a>;和&lt;a href=&quot;https://github.com /google-research/google-research/tree/master/android_in_the_wild&quot;>;野外 Android&lt;/a>;。最后，我们使用 &lt;a href=&quot;https://github.com/google-research-datasets/screen2words&quot;>;Screen2Words&lt;/a>; 进行屏幕摘要，并使用 &lt;a href=&quot;https://paperswithcode.com/paper/ widget-captioning-generate-natural-language/review/&quot;>;小部件标题&lt;/a>;，用于描述特定的 UI 元素。除了微调数据集之外，我们还使用三个新颖的基准来评估微调的 ScreenAI 模型： &lt;/p>; &lt;ol>; &lt;li>;屏幕注释：启用评估模型布局注释和空间理解功能。 &lt;/li>;&lt;li>;ScreenQA Short：ScreenQA 的变体，其真实答案已被缩短，仅包含与其他 QA 任务更好地结合的相关信息。 &lt;/li>;&lt;li>;复杂 ScreenQA：用更困难的问题（计数、算术、比较和不可回答的问题）补充 ScreenQA Short，并包含各种宽高比的屏幕。 &lt;/li>; &lt;/ol>; &lt;p>; 经过微调的 ScreenAI 模型在各种 UI 和基于信息图表的任务上实现了最先进的结果 (&lt;a href=&quot;https://x-lance.github. io/WebSRC/&quot;>;WebSRC&lt;/a>; 和 &lt;a href=&quot;https://github.com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;）以及 &lt;a href=&quot;https 上一流的性能://github.com/vis-nlp/ChartQA&quot;>;图表质量检查&lt;/a>;，&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1 &quot;>;DocVQA&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>; 与相似大小的模型进行比较。 ScreenAI 在 Screen2Words 和 OCR-VQA 上实现了具有竞争力的性能。此外，我们还报告了新基准数据集的结果，作为进一步研究的基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOswns-GWJRdSCj3UmyxytOZxfoM64 psBSKCjLYa-3zkXDt8mGvFbNpydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s1183/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1137&quot; data-original-width=&quot;1183&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOSwns-GWJRdSCj3UmyxytOZxfoM64psBSKCjLYa-3zkXDt8mGvFbN pydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;将 ScreenAI 的模型性能与相似尺寸的最先进 (SOTA) 模型进行比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 接下来，我们检查 ScreenAI 的扩展能力，并观察到在所有任务中，增加模型大小可以提高性能，并且在最大尺寸时改进尚未饱和。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNMvTyz1RhM0wqgn7eAGB9Lev3YUhKhHrcAmJt3SB1Gi6ozIaxHoPzAj-bm6II-_91viG2FXrfNZiiwSSI_YNQGwKGyO6YkAW05Cfl9oys869f7DMyJcthlj6c0CLwzMAGP8HM9AmxdCK92d4PL2Ujz-tI4CZsQOlzlecMLgElWBjl9FZtj-zWIWata2k/s1999/image1 。 //blogger.googleusercontent.com/img/b/r29vz2xl/avvxsejknmvtyz1rhm0wqgn7eagb9lev3yuhrcamjt3yuhkhhrcamjhhrcagbbbbbbbbbbb1gi6ozhopzhopzhopzhopzajjjjj bm6ii-_91vig2fggggggggegsrfsrfsrfsrys ynn yn yn yn yn.n yn.n yn.n y._9 y._9 yn.y y._9 y._9 kiyy yyn yyn yyn yyn yyn yy._9 y._9 y._9 y 69f7dmyjcthlj6c0clwzmagp8hm9amxdck92d4pl2ujz-ti4czsqolzlecmlgelwbjl9fztj-zwiwata2k/s16000/s16000/image1.png =“ TR-CAPTION”样式=“ Text-Align：Center;”>;模型性能随大小增加而增加，即使在5B参数的最大尺寸下，性能也没有饱和。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;我们与统一一起介绍了Screenai模型使我们能够开发从所有这些域中利用数据的自我监督的学习任务的表示。我们还说明了使用LLM的数据生成的影响，并通过修改训练混合物来研究改善模型性能在特定方面。我们应用所有这些技术来构建经过多任务训练的模型，以在许多公共基准上采用最先进的方法来竞争性能。但是，我们还注意到，我们的方法仍然落后于大型模型，需要进一步的研究来弥合这一差距。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; Wang，Fedir Zubach，Hassan Mansoor，Vincent Etter，Victor Carbune，Jason Lin，Jindong Chen和Abhanshu Sharma。我们感谢 Fangyu Liu、Xi Chen、Efi Kokiopoulou、Jesse Berent、Gabriel Barcik、Lukas Zilka、Oriana Riva、Gang Li、Yang Li、Radu Soricut 和 Tania Bedrax-Weiss 的富有洞察力的反馈和讨论，以及 Rahul Aralikatte、Hao Cheng和Daniel Kim在数据准备方面的支持。我们还感谢 Jay Yagnik、Blaise Aguera y Arcas、Ewa Dominowska、David Petrou 和 Matt Sharifi 的领导、远见和支持。我们非常感激能够帮助我们在这篇文章中创建动画。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/5200874294579797373735/comments/comments/comments/comments/default “ rel =”回复“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/screenai-viseal-visual-visual-language-model--model-- for-ui.html＃comment-form“ rel =” reply =“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http:/ http://www.blogger.com/feeds/847492626331452026262626262626262626/posts /默认/520087429457973735“ rel =“ edit” type =“ application/atom+xml”/>; &lt;link href =” “ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/screenai-visual-visual-visual-visual-language-model-model-for-ui.html“ title =“ screenai：UI和视觉上的语言理解的视觉语言模型” type =“ text/html”/>; &lt;aunder>; &lt;names>; google ai &lt;/name>; &lt;uri>; http：//www.blogger。 com/profile/120986265147775266161 &lt;/uri>; &lt;email>; noreply@blogger.com &lt;/email>; &lt;gd：image height =“ 16” =“ https://img1.blogblog.com/img/b16-rounded.gif” width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail height =“ 72” url =“ url =” https：https：https：https：https：https：https： //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s72-c/ScreenAI%20-%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search 。 -4328167517765145678 &lt;/id>; &lt;/id>; &lt;出版>; 2024-03-19T08：00：00.000-07：00 &lt;/00 &lt;/publined>; &lt;更新>; 2024-03-19T08：00. 00：00.150-07：00.150-07：00 &lt;/opect>; &lt;/00 &lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/ “ http://www.blogger.com/atom/ns#” term =“ crowd-sourcing”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term = “数据集”>; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“多样性”>; &lt;/category>; &lt;类别>; .com/atom/ns＃“ term =“ health”>; &lt;/category>; &lt;title type =“ text”>; scin：代表性皮肤病学图像的新资源&lt;/stitle>; &lt;content type =“ html”>; =&quot;byline-author&quot;>;Posted by Pooja Rao, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OYnZO3CdKpArGn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUbJ/s1600/ scinhero.png“ style =” display：none;” />; &lt;p>;健康数据集在研究和医学教育中起着至关重要的作用，但是创建代表现实世界的数据集可能会很具有挑战性。例如，皮肤病的外观和严重程度各不相同，并且不同肤色的表现也不同。然而，现有的皮肤病学图像数据集通常缺乏日常状况（例如皮疹，过敏和感染）的代表，并且偏向更轻的肤色。此外，种族和民族信息经常缺失，阻碍了我们评估差异或制定解决方案的能力。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>;要解决这些限制，我们正在发布&lt;a href=&quot;https://github.com/google-research-research-datasets/scin&quot;>;皮肤状况图像网络（SCIN）数据集&lt;/a>;与医生与&lt;a href=&quot;https://med.stanford.edu/&quot;>; Stanford Medicine合作。我们设计 SCIN 是为了反映人们在线搜索的广泛问题，补充临床数据集中常见的疾病类型。它包含各种肤色和身体部位的图像，有助于确保未来的人工智能工具对所有人有效工作。我们已经制作了&lt;a href=&quot;https://github.com/google-research-datasets/scin&quot;>; SCIN数据集&lt;/a>;作为研究人员，教育者和开发人员，以及以及采取仔细的步骤来保护贡献者的隐私。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO6uguDArYkvnLUz5glvPlNpI1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s1327/image1.png&quot; imageanchor =“ 1” style =“边距左：自动; margin-right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 1118” data-Original-width =“ 1327” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO6uguDArYkvnLUz5glvPlNpI 1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s16000/image1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：center;“>; scin数据集中的图像和元数据示例。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =” line-height： 40％;“>; &lt;br>; &lt;/div>; &lt;h2>;数据集组成&lt;/h2>; &lt;p>; SCIN数据集当前包含10,000多张皮肤，指甲或头发状况，这是由经历它们的个人直接贡献的。根据机构审查委员会批准的研究，在美国个人的知情同意下，自愿做出了所有贡献。为了为皮肤科医生的回顾性标签提供背景信息，贡献者被要求拍摄特写和稍远距离的图像。他们可以选择自我报告人口统计信息和&lt;a href=&quot;https://en.wikipedia.org/wiki/fitzpatrick_scale&quot;>;晒黑倾向&lt;/a>;（自我报告的fitzpatrick皮肤类型，并描述与它们的关注有关的质地，持续时间和症状。 &lt;/p>; &lt;p>;一到三个皮肤科医生标记了每个贡献，最多五个皮肤病学条件，以及每个标签的置信度评分。 SCIN 数据集包含这些单独的标签，以及从它们导出的聚合和加权的鉴别诊断，可用于模型测试或训练。这些标签是回顾性分配的，并不等同于临床诊断，但它们使我们能够将 SCIN 数据集中皮肤病状况的分布与现有数据集进行比较。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3A5D50ZpAr-93i3a69KUFOZy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s1851/image2.png&quot; imageanchor =“ 1” style =“边距左：自动; margin-right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 775” data-Original-width =“ 1851” src =“ src =” https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3A5D50ZpAr-93i3a69KUFOZy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：中心;“>; SCIN数据集在很大程度上包含过敏，炎症和传染性状况，而来自临床来源的数据集则集中于良性和恶性&lt;a href &lt;a href =“ https:///en.wikipedia.orgg/ wiki/delasm“>;肿瘤&lt;/a>;。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/take>; &lt;p>;，而许多现有的皮肤病学数据集则集中于恶性肿瘤和良性肿瘤，并旨在帮助皮肤癌诊断，SCIN数据集主要由常见的过敏，炎症和感染状况组成。 SCIN 数据集中的大多数图像都显示出早期阶段的问题——超过一半是在照片拍摄前不到一周出现的，30% 是在照片拍摄前不到一天出现的。此时间范围内的情况在卫生系统中很少见，因此在现有皮肤病学数据集中代表性不足。 &lt;/p>; &lt;p>;我们还获得了菲茨帕特里克皮肤类型（估计的FST或EFST）和外行标记估计&lt;a href=&quot;htttps:///en.wikipedia.org/wiki/wiki/monk_skin_skin_scale &quot;>; monk蒙克肤色&quot;>; monk_skin_scale&quot;>; &lt;/a>;（emst）图像。这使得可以将皮肤状况和皮肤类型分布与现有皮肤病学数据集中的皮肤状况和皮肤类型分布进行比较。尽管我们没有选择性地针对任何皮肤类型或肤色，但与临床来源的类似数据集相比，SCIN 数据集具有平衡的 Fitzpatrick 皮肤类型分布（更多类型 3、4、5 和 6）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-cOeINXc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s1851 /image3.png“ ImageAnchor =” 1“样式=” Margin-Left：auto; Margin-Right：auto;“>; &lt;img border =” 0“ data-Original-height =” 620“ data-original-width =” 1851&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-cOeINXc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt; /tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;自我报告和皮肤科医生估计的scin数据集中的菲茨帕特里克皮肤类型分布与现有的未增强的皮肤病数据集相比a href =“ https://github.com/mattgroh/fitzpatrick17k”>;（fitzpatrick17k17k &lt;/a>;，&lt;a href=&quot;https://wwwww.fc.up.ppt.ppt.pt.pt/addi/addi/addi/addi/ph2%20database.htmll>; ph²&lt;/a>;，&lt;a href=&quot;https://www.it.pt/automaticpage?id=3459&quot;>; skinl2 &lt;/a>;和&lt;a href =“ https：//www.ncbi.nlm。 nih.gov/pmc/articles/pmc7479321/“>; pad-ufes-20 &lt;/a>;）。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>; &lt;a href =“ href =” https：https： //en.wikipedia.org/wiki/fitzpatrick_scale&quot;>; fitzpatrick皮肤类型&lt;/a>;量表最初是作为照相量表开发的，用于测量皮肤类型对UV辐射的响应，并且在皮肤病学研究中广泛使用。 Monk 肤色等级是一种较新的 10 色度等级，用于测量肤色而不是皮肤光型，捕捉较深肤色之间更细微的差异。虽然这两种量表都不是为了使用图像进行回顾性估计，但包含这些标签的目的是为了使未来能够对皮肤病学中的皮肤类型和色调表示进行研究。例如，SCIN 数据集为美国人口中这些皮肤类型和色调的分布提供了初始基准。 &lt;/p>; &lt;p>; SCIN数据集对女性和年轻人的表现很高，可能反映了各种因素的组合。这些可能包括皮肤状况发生率的差异、在线寻求健康信息的倾向以及不同人群对研究做出贡献的意愿的差异。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;众包方法&lt;/h2>; &lt;p>;为了创建SCIN数据集，我们使用了一种新颖的众库方法，我们在随附的&lt;a href=&quot;https://arxiv.org/abs/2402.18545&quot;>;研究论文&lt;/a>;与调查人员合作，&lt;a href =“ https://med.stanford.edu /“>;斯坦福医学&lt;/a>;。这种方法使个人能够在医疗保健研究中发挥积极作用。它使我们能够在人们健康问题的早期阶段（可能在他们寻求正式护理之前）接触到他们。至关重要的是，这种方法使用网络搜索结果页面上的广告（许多人健康之旅的起点）来与参与者建立联系。 &lt;/p>; &lt;p>;我们的结果表明，众包可以产生低垃圾邮件速率的高质量数据集。超过 97.5% 的贡献是皮肤状况的真实图像。在执行进一步的过滤步骤以排除超出 SCIN 数据集范围的图像并删除重复项后，我们能够发布在 8 个月的研究期间收到的近 90% 的贡献。大多数图像都很清晰且曝光良好。大约一半的贡献包括自我报告的人口统计数据，80% 包含与皮肤状况相关的自我报告信息，例如质地、持续时间或其他症状。我们发现皮肤科医生回顾性鉴别诊断的能力更多地取决于自我报告信息的可用性，而不是图像质量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ54DXlxFtpClbIIZzZAb6fDufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s1999/image4.png&quot; imageanchor =“ 1” style =“边距左：auto; margin-right：auto;”>; &lt;img border =“ 0” data-eriginal-height =“ 1610” data-Original-width =“ 1999” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ54DXlxFtpClbIIZzZAb6fDufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：中心;“>;皮肤科医生对标签的信心（比例为1-5）取决于自我报告的人口统计和症状信息的可用性。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody >; &lt;/table>; &lt;p>;虽然无法保证完美的图像去识别，但保护贡献图像的个人的隐私是创建SCIN数据集时的重中之重。通过知情同意书，贡献者知道潜在的重新识别风险，并建议避免将图像上载具有识别功能。提交后的隐私保护措施包括手动编辑或裁剪以排除潜在的识别区域、反向图像搜索以排除公开可用的副本以及元数据删除或聚合。 SCIN &lt;a href=&quot;https://github.com/google-research-datasets/scin?tab=License-1-ov-file#readme&quot;>;数据使用许可&lt;/a>;禁止尝试重新识别贡献者。 &lt;/p>; &lt;p>;我们希望SCIN数据集将是那些致力于推进包容性皮肤病学研究，教育和AI工具开发的人的有用资源。通过展示传统数据集创建方法的替代方法，SCIN 为自我报告数据或回顾性标记可行的领域中更具代表性的数据集铺平了道路。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢所有合著者 Abbi Ward ，吉米·李（Jimmy Li），朱莉·王（Julie Wang），斯里拉姆·拉克斯米纳（Sriram Lakshminarasimhan），阿什利·卡里克（Ashley Carrick），比尔森·坎帕纳（Bilson Campana），杰伊·哈特福德（Jay Hartford），杰伊·哈特福德（Jay Hartford），普拉德普·库玛（Pradeep Kumar），蒂亚·蒂亚西里索（Tiya Tiyasirisokchai），桑尼·维尔曼尼（Sunny Virmani ），史蒂文·林（Steven Lin）（斯坦福医学），贾斯汀·科（Stanford Ko）（斯坦福医学），艾伦·卡尔西卡扬灵（Alan Karthikesalingam）和克里斯托弗·苏格尔（Christopher Spers）。我们还感谢 Yetunde Ibitoye、Sami Lachgar、Lisa Lehmann、Javier Perez、Margaret Ann Smith（斯坦福大学医学院）、Rachelle Sico、Amit Talreja、Annisah Um&#39;rani 和 Wayne Westerlind 对这项工作的重要贡献。最后，我们感谢Heather Cole-Lewis，Naama Hammel，Ivor Horn，Michael Howell，Yun Liu和Eric Teasley对研究设计和手稿的见解。 &lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/43281675177765145678/comments/comments/default” /atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/scin-new-resource-for-representative.html#comment-form“ 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/posts/default/43281675177777777765145145678 “/>; &lt;link href =” http://www.blogger.com/feeds/847492633145202626/ ：//blog.research.google/2024/03/scin-new-resource-for-merpresentative.html“ rel =“替代” title =“ scin：代表性皮肤病学图像的新资源”类型=“ text/html” />; &lt;ause>; &lt;name>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147775266161 &lt;/uri>; &lt;Email>; noreply@blogger.com &lt;/emaglogger.com &lt;/email>; height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/img/b16-round.gif.gif” >;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OYnZO3CdKpArGn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUbJ/s72-c/SCINHero 。条目>; &lt;id>;标签：blogger.com，1999：Blog-8474926331452026626.POST-757976001746578714 &lt;/id>; &lt;/id>; &lt;出版>; 2024-03-18T11：41：41：00.000-07：00.000-07：00：00：00：00：00：00 &lt;/00 &lt;/00 &lt;/ipland>; 20224-03 -18T12：01：42.865-07：00 &lt;/updated>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” http://www.blogger.com/atom/ns#“ term =“ Machine Learning”>; &lt;/category>; &lt;Title type type =“ Text”>; Melon：从具有未知姿势的图像中重建3D对象&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由高级软件工程师马克·马修斯（Mark Matthews）和研究科学家Dmitry Lagun发表，Google Research &lt;/span>; &lt;img src =“ https：//blogger.googleusercontent. 。 VAY-MTNOK9IVJDVT_CCZVR6IP_R8YV32MHTW2-FCKCTF4UOFRGMYQ3PCPCKZAZ-NYMCE/S320/MELON/MELON％20HERO.JPG“ style =” style =“ display：none; display：none; />; &lt;p>;一个人的先前经验和对世界的理解通常使他们能够轻松地推断对象的整体外观，即使只看了几张2D图片。但是，只有几个图像，计算机可以在3D中重建对象的形状的能力一直是一个困难的算法问题。这个基本的计算机视觉任务具有从创建电子商务3D模型到自动驾驶汽车导航的应用程序。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;问题的关键部分是如何确定拍摄图像的确切位置，称为&lt;em>; pose推理&lt;/em>;。如果已知相机姿势，则一系列成功的技术 - 例如&lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>; Neural Radiance Fields &lt;/a>;（nerf）或&lt;a href =“ https：https： //repo-sam.inria.fr/fungraph/3d-gaussian-splatting/&quot;>; 3d高斯分裂&lt;/a>;  - 可以在3D中重建一个对象。但是，如果这些姿势不可用，那么我们将面临一个困难的“鸡蛋”问题，如果我们知道3D对象，我们可以确定姿势，但是直到我们知道相机姿势之前，我们才能重建3D对象。伪符号使问题更难解决 - 即，从不同角度看时，许多物体看起来相似。例如，像椅子这样的方形物体倾向于每90°旋转时看起来相似。可以通过从各个角度在转盘上渲染并绘制其光度&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/wiki/selfsimurility&quot;>; self-sim-sim-sim-sim-sim-sim-sim-sim-sim-sim-sim-A >;地图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s1764/image5.png&quot; style=&quot;边距 - 左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-forminal-height =“ 923” data-eriginal-width =“ 1764” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>;玩具卡车型号的自相似图。 &lt;strong>;左：&lt;/strong>;该模型是从各种&lt;a href=&quot;https://en.wikipedia.org/wiki/azimuth&quot;>; azimuthal Angles &lt;/a>;θ中渲染的。 &lt;strong>;右：&lt;/strong>;平均&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/norm_(mathematics)#euclidean_norm&quot;>; l2 &lt;/a>; rgb与此相似的RGB相似之处θ*。伪相似性用虚线的红线表示。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;上面的图仅可视化旋转的一个维度。当引入更多的自由度时，它变得更加复杂（很难可视化）。伪对称性使问题变得不适定，简单的方法常常收敛于局部最小值。在实践中，这种方法可能会将后视图误认为是对象的前视图，因为它们具有相似的轮廓。以前的技术（例如 &lt;a href=&quot;https://chenhsuanlin.bitbucket.io/bundle- adjustment-NeRF/&quot;>;BARF&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/abs/2205.15768 “>; Samurai &lt;/a>;）通过依靠最初接近全球最小值的初始姿势估计来进行侧链。但如果这些都不可用，我们该如何解决这个问题呢？ &lt;/p>; &lt;p>; 方法，例如 &lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_GNeRF_GAN-Based_Neural_Radiance_Field_Without_Posed_Camera_ICCV_2021_paper.pdf&quot;>;GNeRF&lt;/a>; 和 &lt;a href=&quot; https://dl.acm.org/doi/10.1145/3503161.3548078&quot;>;VMRF&lt;/a>;利用&lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;>;生成对抗网络&lt;/a>; （甘斯）克服问题。这些技术能够人为地“放大”有限数量的训练视图，从而帮助重建。但是，GAN技术通常具有复杂的，有时不稳定的训练过程，使实践中难以实现稳健而可靠的融合。一系列其他成功的方法，例如 &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/html/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2023_paper.html&quot;>;SparsePose&lt;/a>; 或 &lt;a href=&quot;https: //rust-paper.github.io/&quot;>;RUST&lt;/a>;，可以从有限数量的视图中推断姿势，但需要对大量姿势图像数据集进行预训练，而这些图像并不总是可用的，并且可能会受到影响推断不同类型图像的姿势时的“域间隙”问题。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>;MELON: NeRF with Unposeed Images in SO(3)&lt;/a>;”中，重点关注&lt;a href= “https://3dvconf.github.io/2024/&quot;>;3DV 2024&lt;/a>;，我们提出了一种技术，可以在以 3D 方式重建对象的同时完全从头开始确定以对象为中心的相机姿势。 &lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON&lt;/a>;（NeRF 的模等效潜在优化）是第一个无需初始相机姿态估计和复杂训练即可实现此目的的技术方案或标记数据的预训练。 MELON 是一种相对简单的技术，可以轻松集成到现有的 NeRF 方法中。我们证明，MELON 可以从未摆出的图像中以最先进的精度重建 NeRF，同时只需要 4-6 个物体的图像。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MELON&lt;/h2>; &lt;p>; 我们利用两项关键技术来帮助这种不适定的收敛问题。第一个是一个非常轻量级的、动态训练的&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;卷积神经网络&lt;/a>; (CNN) 编码器，可以从训练图像中回归相机姿势。我们将缩小的训练图像传递给四层 CNN，以推断相机姿态。该 CNN 从噪声中初始化，无需预训练。它的容量非常小，以至于它迫使相似的图像呈现相似的姿势，提供隐式正则化，极大地帮助收敛。 &lt;/p>; &lt;p>; 第二种技术是模损失，同时考虑对象的伪对称性。我们从每个训练图像的一组固定视点渲染对象，仅通过最适合训练图像的视图反向传播损失。这有效地考虑了每个图像的多个视图的合理性。在实践中，我们发现大多数情况下只需要 &lt;em>;N&lt;/em>;=2 个视图（从另一侧查看对象），但有时使用 &lt;em>;N&lt;/em>;=4 可以获得更好的结果正方形对象。 &lt;/p>; &lt;p>; 这两种技术被集成到标准 NeRF 训练中，只不过不是固定相机姿势，而是由 CNN 推断姿势并通过模损失复制。光度梯度通过最合适的相机反向传播到 CNN。我们观察到相机通常会快速收敛到全局最佳姿势（参见下面的动画）。经过神经场训练后，MELON 可以使用标准 NeRF 渲染方法合成新颖的视图。 &lt;/p>; &lt;p>; 我们通过使用 &lt;a href=&quot;https://github.com/bmild/nerf&quot;>;NeRF-Synthetic&lt;/a>; 数据集来简化问题，该数据集是 NeRF 研究的流行基准，在姿势推断文献。这个合成数据集的摄像头距离精确固定，并且方向一致，要求我们仅推断出相机。这与位于地球中心的物体相同，相机始终指向它，并沿着表面移动。然后我们只需要纬度和经度（2 个自由度）来指定相机姿态。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gAyA_P5BCPwXuEcz7l ApC8WQbGfMvj_aAxShjgsmcklf_-4ekgbFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s1315/image4.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;395&quot; data-original-width=&quot;1315&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRApBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gayA_P5BCPwXuEcz7lApC8WQbGfMvj_aAxShjgsmcklf_-4ekg bFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;MELON 使用动态训练的轻量级 CNN 编码器来预测每个图像的姿势。预测的姿势通过模损失来复制，它仅惩罚距地面真实颜色的最小 L2 距离。在评估时，神经场可用于生成新颖的视图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们计算两个关键指标来评估 MELON 在 NeRF 合成数据集上的性能。地面真相和推断姿势之间的方向误差可以量化为单个角度误差，我们在所有训练图像中平均平均是姿势误差。然后，我们通过测量&lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;峰值信噪比&lt;/a>;来测试 MELON 从新视图渲染对象的准确性（PSNR）反对提出测试意见。我们看到，MELON 在训练的前 1,000 步内快速收敛到大多数相机的近似姿势，并在 50k 步后实现了 27.5 dB 的有竞争力的 PSNR。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02y b3CDIiqXbadI4lnvcZ_MI9sHUkz8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s960/image1 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02yb3CDIiqXbadI4lnvcZ_MI9sHUkz 8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;优化过程中玩具卡车模型上的 MELON 收敛。 &lt;strong>;左&lt;/strong>;：NeRF 的渲染。 &lt;strong>;右&lt;/strong>;：预测（蓝色&lt;em>;x&lt;/em>;）和地面实况（红点）摄像机的极坐标图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MELON 在 NeRF 合成数据集中的其他场景中取得了类似的结果。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7cK-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRK ToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqyWIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s1999/image2.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7ck-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRKToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqy WIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;100k 训练步骤后 NeRF-Synthetic 场景上的 ground-truth (GT) 和 MELON 的重建质量比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;嘈杂的图像&lt;/h3>; &lt;p>; MELON 在执行&lt;a href=&quot;https://en.wikipedia 时也能很好地工作。 org/wiki/View_synthesis&quot;>;新颖的视图合成&lt;/a>;来自极其嘈杂的、未摆姿势的图像。我们向训练图像中添加不同数量的&lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise&quot;>;高斯白噪声&lt;/a>;。例如，下面 &lt;em>;σ&lt;/em>;=1.0 中的物体是无法辨认的，但 MELON 可以确定该物体的姿势并生成该物体的新颖视图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTR VkA0R8fj2A7lOnIdFDIE3YkTh_hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s1182/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1182&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTRVkA0R8fj2A7lOnIdFDIE3YkTh _hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;从嘈杂的未摆姿势的 128×128 图像中合成新颖的视图。顶部：训练视图中存在的噪声级别示例。底部：从嘈杂的训练视图和平均角姿势错误的重建模型。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>; &lt;p>;这也许不足为奇，因为这些技术如&lt;a href = “https://bmild.github.io/rawnerf/&quot;>;RawNeRF&lt;/a>; 展示了 NeRF 在已知相机姿势的情况下出色的去噪能力。事实上，MELON 对于未知相机姿势的噪声图像如此有效，这是出乎意料的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了 MELON，一种可以确定以对象为中心的相机的技术姿势重建 3D 对象，无需近似姿势初始化、复杂的 GAN 训练方案或对标记数据进行预训练。 MELON 是一种相对简单的技术，可以轻松集成到现有的 NeRF 方法中。虽然我们只在合成图像上演示了 MELON，但我们正在调整我们的技术以适应现实世界的条件。请参阅&lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>;论文&lt;/a>;和&lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON 网站&lt;/a>; >; 了解更多。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢我们的论文共同作者Axel Levy、Matan Sela 和 Gordon Wetzstein，以及 Florian Schroff 和 Hartwig Adam 在构建这项技术方面不断提供帮助。我们还感谢 Matthew Brown、Ricardo Martin-Brualla 和 Frederic Poitevin 对论文草稿提供的有益反馈。我们还感谢使用 SLAC 共享科学数据设施 (SDF) 的计算资源。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/757976001746578714 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing- 3d-objects-from.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626 /posts/default/757976001746578714&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/757976001746578714&quot; rel= “self” type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html&quot; rel=&quot;alternate&quot; title=&quot;MELON：从姿势未知的图像重建 3D 对象&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https: //img1.blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/wurs>; &lt;媒体：thumbnail height =“ 72” url =“ https：//blognger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXPaz_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_ gVaY-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s72-c/MELON%20HERO.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search. yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post- 977556648557231190&lt;/id>;&lt;发布>;2024-03-15T11:22:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-15T11:22:13.760-07:00&lt;/更新>;&lt;类别方案= http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI &quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;HEAL：机器学习性能健康公平评估框架&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者迈克·谢克曼 (Mike Schaekermann)，谷歌研究院研究科学家；艾弗·霍恩 (Ivor Horn)，首席健康股权官Director, Google Core&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIxRz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s1600/HEAL-Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>;健康公平是全世界的主要社会问题，其差异具有许多原因。这些来源包括限制获得医疗保健，临床治疗的差异，甚至是诊断技术的根本差异。例如，在皮肤病学中，诸如少数群体，社会经济地位较低或医疗保健方面的个人等人口的皮肤癌预后较差。尽管机器学习（ML）和人工智能（AI）的最新进展有很大的希望，以帮助改善医疗保健，但这种从研究到床边的过渡必须伴随着对它们是否以及如何影响健康平等的仔细了解。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;em>;健康公平&lt;/em>;由公共卫生组织定义为所有人尽可能健康的机会。重要的是，权益可能与&lt;em>; equality &lt;/em>;不同。例如，具有更大障碍的人可以改善健康状况的人可能需要更多或不同的努力来体验这个公平的机会。同样，AI在医疗保健文献中所定义的公平不是公平&lt;em>;公平。 AI公平经常努力在不同的患者人群中为AI技术的同等表现而努力，但这并不是将优先级绩效置于预先存在的健康差异方面的目标。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s1999 /image2.jpg“ style =”边距左左右：自动;缘右：auto;“>; &lt;img border =“ 0” data-eriginal-height =“ 1999” data-eriginal-width =“ 1609” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s16000/image2.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>;健康权益考虑。干预措施（例如，基于ML的工具，以深蓝色表示），如果它有助于减少健康结果的现有差异（以较轻的蓝色表示）。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/tbody>; &lt;/table >; &lt;p>;在“ &lt;a href=&quot;https://www.thelancet.com/journals/eclinm/article/piis2589-5370(24)00058-0/fulltext&quot;>;机器学习绩效健康公平评估（HEAL） ：框架和皮肤病学AI模型案例研究&lt;/a>;” /a>;，我们提出了一种方法来定量评估基于ML的健康技术是否公平地发挥作用。换句话说，ML模型对于该模型旨在解决的状况最差的健康结果的人的表现是否很好？该目标基于以下原则，即健康公平应优先考虑并衡量模型绩效在不同的健康成果方面，这可能是由于包括结构性不平等的许多因素（例如，人口统计学，社会，文化，文化，政治，经济，环境，环境和环境，以及地理）。 &lt;/p>; &lt;br />; &lt;h2>;健康平等框架（HEAL）&lt;/h2>; &lt;p>; HEAL框架提出了一个四步过程，以估算基于ML的健康技术公平性能的可能性：&lt;/&lt;/&lt;/ p>; &lt;ol>; &lt;li>;确定与健康不平等相关的因素并定义工具性能指标，&lt;/li>; &lt;li>;识别并量化了先前存在的健康差异，&lt;/li>; &lt;li>;衡量工具的性能每个亚种群，&lt;/li>; &lt;li>;衡量该工具优先级相对于健康差异的绩效的可能性。 &lt;/li>; &lt;/ol>; &lt;p>;最后一步的输出被称为治疗度量，该指标量化了ML模型的性能与健康差异有关。换句话说，该模型在健康结果较差的人群中的表现是否更好？ &lt;/p>; &lt;p>;这个四步过程旨在为ML模型性能更加公平提供改进，并定期进行迭代和重新评估。例如，步骤（2）中健康结果数据的可用性可以告知步骤（1）中人口统计因素和括号的选择，并且可以通过新的数据集，模型和人群再次应用框架。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s1999/image1.jpg “样式=” Margin-Left：Auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 1352” data-Original-width =“ 1999” src =“ https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-字幕“ style =”文本align：中心;“>;健康公平框架的机器学习绩效评估框架（治愈）。＆nbsp;我们的指导原则是避免加剧健康不平等，这些步骤有助于我们确定差异并评估不公平的模型&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;的绩效，在这项工作中，我们朝着鼓励对AI技术的健康公平考虑以及鼓励在模型开发过程中优先考虑努力，以减少暴露于可能导致不同结果的结构不平等的亚种群的健康不平等。我们应该注意，当前的框架没有建模因果关系，因此无法量化新技术将对减少健康结果差异产生的实际影响。但是，治疗指标可能有助于确定改进的机会，而当前的绩效没有优先考虑先前存在的健康差异。 &lt;/p>; &lt;br />; &lt;h2>;皮肤病学模型案例研究&lt;/h2>; &lt;p>; 作为一个说明性案例研究，我们将该框架应用于皮肤病学模型，该模型利用类似于中描述的卷积神经网络&lt;a href=&quot;https://blog.research.google/2019/09/usise-deep-learning-to-inform.html&quot;>;先前的工作&lt;/a>;。此示例皮肤病学模型经过训练，可使用包含 29,000 个病例的开发数据集对 288 种皮肤状况进行分类。该模型的输入包括三张皮肤关注的照片以及人口统计信息和简短的结构化病史。该输出包括可能匹配的皮肤条件的排名列表。 &lt;/p>; &lt;p>;使用HEAL框架，我们通过评估该模型是否优先级相对于先前存在的健康结果来评估该模型。该模型旨在根据皮肤关注和患者元数据的照片来预测可能的皮肤病条件（来自数百个清单）。该模型的评估是使用TOP-3协议度量进行的，该公约量化了前3个输出条件与皮肤科医生面板最有可能的条件相匹配的频率。通过与健康结果排名的TOP-3一致性的反相关来计算愈合度量。 &lt;/p>; &lt;p>;我们使用了一个5,420个远程表现案例的数据集，富含年龄，性别和种族/种族/种族/种族的多样性，以回顾性地评估模型的治疗度量。该数据集由来自美国的20岁以上患者的“存储和前向”病例组成，来自美国的初级保健提供者和澳大利亚的皮肤癌诊所。根据对文献的综述，我们决定将种族/种族，性别和年龄作为不平等的潜在因素探索，并使用抽样技术来确保我们的评估数据集对所有种族/种族，性别和年龄段的所有种族/种族和年龄段都充分表示。为了量化每个子组的健康结果，我们依靠&lt;a href =“ https://www.who.int/data/gho/gho/gho/gho/data/themes/mortality--global-health-health-ealth-estimates/global-estimates/global-estimates/global-eastimates/global-eastimates/global-eastimates/ health-est-easting-leading-causes-of-dalys”>; public &lt;/a>; &lt;a href =“ https://www.thelancet.com/journals/journals/lancet/lancet/article/piis0140-6736 (20203030925-9/fulltext “>;数据库&lt;/a>;得到世界卫生组织的认可，例如&lt;a href=&quot;https://www.who.int/data/gho/gho/indicator-metadata-registry/imr-registry/imr-details/4427&quot;>;生命失去&lt;/a>;（ylls）和&lt;a href=&quot;https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158&quot;>; disability-disability-udjusted life &lt;/a &lt;/a >;（Dalys；损失的年代加上残疾人）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6rOqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s1511 /Table1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot; 1511&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6rOqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s16000/Table1.png&quot; />;&lt;/a>;&lt;/td>;&lt; /tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;治疗跨种族/种族亚群的所有皮肤病条件的治疗度量TOP-3协议）以及健康成果和工具性能的排名。&lt;br />;（*更高；测量模型相对于此表中的轴的可能性。 class =“ tr-caption-container” style =“边距 - 左：auto; margin-right：auto;“>; &lt;tbody>; &lt;ttroby>; &lt;tr>; &lt;td style =“ text-align：center; center;”>; &lt;a href =&#39;>; &lt;a href =“ https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza-QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s1518/Table2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;316&quot; data-original-width=&quot;1518&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza- QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s16000/Table2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;治愈性别的所有皮肤病学条件的公制度量，包括健康成果（每100,000个Dalys），模型绩效（前3个协议）以及健康结果和工具绩效的排名。 （如上所述。）&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table &lt;p>;我们的分析估计，该模型可能是80.5％，可以在种族/民族亚组中公正地表现，而92.1％的模型有可能在整个种族/民族子组中公平地表现性别。 &lt;/p>; &lt;p>;但是，尽管该模型可能在癌症状况方面可以平等地在各个年龄段中进行公正地表现，但我们发现它在非癌症条件下的年龄组之间有改善的余地。例如，这些70+的健康结果与非癌症的皮肤状况相关，但该模型并未优先考虑该亚组的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_NfzCKl0_vOvZA3PPnLkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s1508/Table3 。 src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_NfzCKl0_vOvZA3PPnLkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s16000/Table3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;跨年龄段的所有癌症和非癌症皮肤病的治疗指标-3协议），以及健康成果和工具性能的排名。 （如上所述。）&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h2>;将事物放在上下文&lt;/h2>; &lt;p>;进行整体评估中，无法使用治疗指标隔离中。取而代之的是，该指标应与许多其他因素以及从计算效率和数据隐私到伦理价值以及可能影响结果的各个方面（例如，选择偏见或跨人口统计组的代表性差异）以及各个方面都应进行上下文化。 &lt;/p>; &lt;p>; As an adversarial example, the HEAL metric can be artificially improved by deliberately reducing model performance for the most advantaged subpopulation until performance for that subpopulation is worse than all others.出于说明目的，给定亚群A和B，A和B的健康结果比B更差，请考虑两个模型之间的选择：模型1（M1）在亚群中的表现比亚群B级好5％。模型2（M2）执行5％ worse on subpopulation A than B. The HEAL metric would be higher for M1 because it prioritizes performance on a subpopulation with worse outcomes. However, M1 may have absolute performances of just 75% and 70% for subpopulations A and B respectively, while M2 has absolute performances of 75% and 80% for subpopulations A and B respectively. Choosing M1 over M2 would lead to worse overall performance for all subpopulations because some subpopulations are worse-off while no subpopulation is better-off. &lt;/p>; &lt;p>; Accordingly, the HEAL metric should be used alongside a &lt;a href=&quot;https://en.wikipedia.org/wiki/Pareto_efficiency&quot;>;Pareto condition&lt;/a>; (discussed further in the paper) ，这限制了模型的变化，使得每个亚群的结果与现状相比要么保持不变，要么有所改善，并且任何亚群的表现都不会恶化。 &lt;/p>; &lt;p>; The HEAL framework, in its current form, assesses the likelihood that an ML-based model prioritizes performance for subpopulations with respect to pre-existing health disparities for specific subpopulations. This differs from the goal of understanding whether ML will reduce disparities in outcomes across subpopulations in reality. Specifically, modeling improvements in outcomes requires a causal understanding of steps in the care journey that happen both before and after use of any given model. Future research is needed to address this gap. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The HEAL framework enables a quantitative assessment of the likelihood that health AI technologies prioritize performance with respect to health disparities. The case study demonstrates how to apply the framework in the dermatological domain, indicating a high likelihood that model performance is prioritized with respect to health disparities across sex and race/ethnicity, but also revealing the potential for improvements for non-cancer conditions across age. The case study also illustrates limitations in the ability to apply all recommended aspects of the framework (eg, mapping societal context, availability of data), thus highlighting the complexity of health equity considerations of ML-based tools. &lt;/p>; &lt;p>; This work is a proposed approach to address a grand challenge for AI and health equity, and may provide a useful evaluation framework not only during model development, but during pre-implementation and real-world monitoring stages, eg, in the form of health equity dashboards. We hold that the strength of the HEAL framework is in its future application to various AI tools and use cases and its refinement in the process. Finally, we acknowledge that a successful approach towards understanding the impact of AI technologies on health equity needs to be more than a set of metrics. It will require a set of goals agreed upon by a community that represents those who will be most impacted by a model. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The research described here is joint work across many teams at Google. We are grateful to all our co-authors: Terry Spitz, Malcolm Pyles, Heather Cole-Lewis, Ellery Wulczyn, Stephen R. Pfohl, Donald Martin, Jr., Ronnachai Jaroensri, Geoff Keeling, Yuan Liu, Stephanie Farquhar, Qinghan Xue, Jenna Lester, Cían Hughes, Patricia Strachan, Fraser Tan, Peggy Bui, Craig H. Mermel, Lily H. Peng, Yossi Matias, Greg S. Corrado, Dale R. Webster, Sunny Virmani, Christopher Semturs, Yun Liu, and Po-Hsuan Cameron Chen. We also thank Lauren Winer, Sami Lachgar, Ting-An Lin, Aaron Loh, Morgan Du, Jenny Rizk, Renee Wong, Ashley Carrick, Preeti Singh, Annisah Um&#39;rani, Jessica Schrouff, Alexander Brown, and Anna Iurchenko for their support of this project.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/977556648557231190/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type =&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;edit&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html&quot; rel=&quot;alternate&quot; title=&quot;HEAL: A framework for health equity assessment of machine learning performance&quot; type =&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIxRz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s72 -c/HEAL-Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total >;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7868032799856333119&lt;/id>;&lt;published>;2024-03-14T12:38:00.000-07:00&lt;/published>; &lt;updated>;2024-03-14T12:38:11.597-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/ category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NeurIPS&quot;>;&lt;/category>;&lt;title type=&quot; text&quot;>;Cappy: Outperforming and boosting large multi-task language models with a small scorer&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yun Zhu and Lijuan Liu, Software Engineers , Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0EjUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up-Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s320/Cappy%20hero.jpg&quot; style= &quot;display: none;&quot; />; &lt;p>; 大型语言模型 (LLM) 的进步催生了一种新范式，将各种自然语言处理 (NLP) 任务统一在指令跟踪框架内。这种范式以最近的多任务法学硕士为例，例如 &lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;、&lt;a href=&quot;https://arxiv.org/ abs/2210.11416&quot;>;FLAN&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2212.12017&quot;>;OPT-IML&lt;/a>;。首先，收集多任务数据，每个任务都遵循特定于任务的模板，其中每个标记的示例都转换为指令（例如，&lt;em>;“&lt;/em>;将概念放在一起形成一个句子：滑雪，山, skier&lt;em>;”&lt;/em>;) paired with a corresponding response (eg, &lt;em>;&quot;&lt;/em>;Skier skis down the mountain&lt;em>;&quot;&lt;/em>;). These instruction-response pairs are used to train the LLM, resulting in a conditional generation model that takes an instruction as input and generates a response. Moreover, multi-task LLMs have exhibited remarkable task-wise generalization capabilities as they can address unseen tasks by understanding and solving brand-new instructions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt-ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawA5VYDEj6VtSyOTNGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s640/Cappy%20instruction-following.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;177&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt-ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawA5VYDEj6VtSyOTNGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s16000/Cappy%20instruction-following.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;The demonstration of the instruction-following pre-training of multi-task LLMs, eg, FLAN. Pre-training tasks under this paradigm improves the performance for unseen tasks.&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 由于仅使用指令理解和解决各种任务的复杂性，多任务 LLM 的大小通常从数十亿个参数到数千亿个参数（例如，&lt;a href=&quot;https ://arxiv.org/abs/2210.11416&quot;>;FLAN-11B&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0-11B&lt;/a>; and &lt;a href= &quot;https://arxiv.org/abs/2212.12017&quot;>;OPT-IML-175B&lt;/a>;). As a result, operating such sizable models poses significant challenges because they demand considerable computational power and impose substantial requirements on the memory capacities of GPUs and TPUs, making their training and inference expensive and inefficient. Extensive storage is required to maintain a unique LLM copy for each downstream task. Moreover, the most powerful multi-task LLMs (eg, FLAN-PaLM-540B) are closed-sourced, making them impossible to be adapted. However, in practical applications, harnessing a single multi-task LLM to manage all conceivable tasks in a zero-shot manner remains difficult, particularly when dealing with complex tasks, personalized tasks and those that cannot be succinctly defined using instructions. On the other hand, the size of downstream training data is usually insufficient to train a model well without incorporating rich prior knowledge. Hence, it is long desired to adapt LLMs with downstream supervision while bypassing storage, memory, and access issues. &lt;/p>; &lt;p>; Certain &lt;em>;parameter-efficient tuning&lt;/em>; strategies, including &lt;a href=&quot;https://aclanthology.org/2021.acl-long.353.pdf&quot;>;prompt tuning&lt;/a>; and &lt;a href=&quot;https://openreview.net/pdf?id=nZeVKeeFYf9&quot;>;adapters&lt;/a>;, substantially diminish storage requirements, but they still perform back-propagation through LLM parameters during the tuning process, thereby keeping their memory demands high. Additionally, some &lt;em>;&lt;a href=&quot;https://arxiv.org/pdf/2301.00234.pdf&quot;>;in-context learning&lt;/a>;&lt;/em>; techniques circumvent parameter tuning by integrating a limited number of supervised examples into the instruction. However, these techniques are constrained by the model&#39;s maximum input length, which permits only a few samples to guide task resolution. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2311.06720&quot;>;Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer&lt;/a>;”, presented at &lt;a href=&quot;https://nips.cc/virtual/2023/index.html&quot;>;NeurIPS 2023&lt;/a>;, we propose a novel approach that enhances the performance and efficiency of multi-task LLMs. We introduce a lightweight pre-trained scorer, Cappy, based on continual pre-training on top of &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>; with merely 360 million parameters. Cappy takes in an instruction and a candidate response as input, and produces a score between 0 and 1, indicating an estimated correctness of the response with respect to the instruction. Cappy functions either independently on classification tasks or serves as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy efficiently enables downstream supervision without requiring any finetuning, which avoids the need for back-propagation through LLM parameters and reduces memory requirements. Finally, adaptation with Cappy doesn&#39;t require access to LLM parameters as it is compatible with closed-source multi-task LLMs, such as those only accessible via WebAPIs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIRNHe7JKZuR4ETQbHrM5h1knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s1999/Cappy %20overview.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;975&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIRNHe7JKZuR4ETQbHrM5h1knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s16000/Cappy%20overview.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Cappy takes an instruction and response pair as input and outputs a score ranging from 0 to 1, indicating an estimation of the correctness of the response with respect to the instruction.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>; Pre-training&lt;/h2>; &lt;p>; We begin with the same dataset collection, which includes 39 diverse datasets from &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>; that were used训练&lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;。 This collection encompasses a wide range of task types, such as question answering, sentiment analysis, and summarization. Each dataset is associated with one or more templates that convert each instance from the original datasets into an instruction paired with its ground truth response. &lt;/p>; &lt;p>; Cappy&#39;s regression modeling requires each pre-training data instance to include an instruction-response pair along with a correctness annotation for the response, so we produce a dataset with correctness annotations that range from 0 to 1. For every instance within a generation task, we leverage an existing multi-task LLM to generate multiple responses by sampling, conditioned on the given instruction. Subsequently, we assign an annotation to the pair formed by the instruction and every response, using the similarity between the response and the ground truth response of the instance. Specifically, we employ &lt;a href=&quot;https://aclanthology.org/W04-1013/&quot;>;Rouge-L&lt;/a>;, a commonly-used metric for measuring overall multi-task performance that has demonstrated a strong alignment with human evaluation, to calculate this similarity as a form of weak supervision. &lt;/p>; &lt;p>; As a result, we obtain an effective regression dataset of 160 million instances paired with correctness score annotations.最终的cappy模型是使用&lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>; Roberta &lt;/a>;模型的回归数据集进行连续预训练的结果。 The pre-training of Cappy is conducted on Google&#39;s &lt;a href=&quot;https://arxiv.org/abs/2304.01433&quot;>;TPU-v4&lt;/a>;, with &lt;a href=&quot;https://arxiv.org/pdf/2310.16355.pdf&quot;>;RedCoast&lt;/a>;, a lightweight toolkit for automating distributed training. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZymVyougwQqhCoE7Iio6DvIzdIK_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s1999/Cappy%20data%20augmentation .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;438&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZymVyougwQqhCoE7Iio6DvIzdIK_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s16000/Cappy%20data%20augmentation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Data augmentation with a multi-task LLM to construct a weakly supervised regression dataset for Cappy&#39;s pre-training and fine-tuning.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Applying Cappy&lt;/h2>; &lt;p>; Cappy solves practical tasks within a candidate-selection mechanism. More specifically, given an instruction and a set of candidate responses, Cappy produces a score for each candidate response. This is achieved by inputting the instruction alongside each individual response, and then assigning the response with the highest score as its prediction.在分类任务中，所有候选响应均在本质上是预定义的。 For example, for an instruction of a sentiment classification task (eg, “Based on this review, would the user recommend this product?: &#39;Stunning even for the non-gamer.&#39;”), the candidate responses are “Yes” or “不”。 In such scenarios, Cappy functions independently. On the other hand, in generation tasks, candidate responses are not pre-defined, requiring an existing multi-task LLM to yield the candidate responses. In this case, Cappy serves as an auxiliary component of the multi-task LLM, enhancing its decoding. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Adapting multi-task LLMs with Cappy &lt;/h3>; &lt;p>; When there is available downstream training data, Cappy enables effective and efficient adaptation of multi-task LLMs on downstream tasks. Specifically, we fine-tune Cappy to integrate downstream task information into LLM predictions.该过程涉及创建针对下游培训数据的单独的回归数据集，该数据集使用用于构建预训练数据的相同数据注释过程。因此，经过微调的 Cappy 与多任务 LLM 协作，提高了 LLM 在下游任务上的性能。 &lt;/p>; &lt;p>; In contrast to other LLM tuning strategies, adapting LLMs with Cappy significantly reduces the high demand for device memory as it avoids the need for back-propagation through LLM parameters for downstream tasks. Moreover, Cappy adaptation does not rely on the access to LLM parameters, making it compatible with closed-source multi-task LLMs, such as the ones only accessible via WebAPIs. Compared with in-context learning approaches, which circumvent model tuning by attaching training examples to the instruction prefix, Cappy is not restricted by the LLM&#39;s maximum input length. Thus, Cappy can incorporate an unlimited number of downstream training examples. Cappy can also be applied with other adaptation methods, such as fine-tuning and in-context learning, further boosting their overall performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncRdCyfIRa3KlLx3786yfSXfP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s1999/Cappy%20downstream%20adaptation .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1040&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncRdCyfIRa3KlLx3786yfSXfP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s16000/Cappy%20downstream%20adaptation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Downstream adaptation comparison between Cappy and approaches that rely on an LLM&#39;s parameters, such as fine-tuning and prompt tuning. Cappy&#39;s application enhances multi-task LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We assess Cappy&#39;s performance across eleven held-out language understanding classification tasks from &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>;. We demonstrate that Cappy, with 360M parameters, outperforms OPT-175B and OPT-IML-30B, and matches the accuracy of the best existing multi-task LLMs (T0-11B and OPT-IML-175B). These findings highlight Cappy&#39;s capabilities and parameter efficiency, which can be credited to its scoring-based pre-training strategy that integrates contrastive information by differentiating between high-quality and low-quality responses. On the contrary, previous multi-task LLMs depend exclusively on &lt;a href=&quot;https://en.wikipedia.org/wiki/Teacher_forcing&quot;>;teacher-forcing training&lt;/a>; that utilizes only the ground truth responses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGIWWUJ-Xo-1uVQUmN8uu0_5dLAERYPvOFfahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s1999/Cappy%20accuracy .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGIWWUJ-Xo-1uVQUmN8uu0_5dLAERYPvOFfahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s16000/Cappy%20accuracy.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The overall accuracy averaged over eleven test tasks from PromptSource. “RM” refers to a &lt;a href=&quot;https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2&quot;>;pre-trained RLHF reward model&lt;/a>;. Cappy matches the best ones among existing multi-task LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also examine the adaptation of multi-task LLMs with Cappy on complex tasks from &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;, a set of manually curated tasks that are considered beyond the capability of many LLMs. We focus on all the 45 generation BIG-Bench tasks, specifically those that do not offer pre-established answer choices. We evaluate the performance using the Rouge-L score (representing the overall similarity between model generations and corresponding ground truths) on every test set, reporting the average score across 45 tests. In this experiment, all variants of FLAN-T5 serve as the backbone LLMs, and the foundational FLAN-T5 models are frozen. These results, shown below, suggest that Cappy enhances the performance of FLAN-T5 models by a large margin, consistently outperforming the most effective baseline achieved through sample selection using self-scoring of the LLM itself. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhX Efus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s1999/Cappy%20平均%20Rouge-L%20score.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1625&quot; data-original-width=&quot;1999 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhXEfus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s16000/Cappy%20averaged%20Rouge-L%20score.png&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The averaged Rouge-L score over 45 complex tasks within BIG-Bench. The x-axis refers to FLAN-T5 models of different sizes. Every dashed line represents an approach working on FLAN-T5s. Self-scoring refers to using the cross-entropy of LLM to select responses. Cappy enhances the performance of FLAN-T5 models by a large margin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduce Cappy, a novel approach that enhances the performance and efficiency of multi-task LLMs. In our experiments, we adapt a single LLM to several domains with Cappy. In the future, Cappy as a pre-trained model can potentially be used in other creative ways beyond on single LLMs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;Thanks to Bowen Tan, Jindong Chen, Lei Meng, Abhanshu Sharma and Ewa Dominowska for their valuable feedback. We would also like to thank Eric Xing and Zhiting Hu for their suggestions. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7868032799856333119/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html&quot; rel=&quot;alternate&quot; title=&quot;Cappy: Outperforming and boosting large multi-task language models with a small scorer&quot; type =&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0EjUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up -Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s72-c/Cappy%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr: total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6090481872694489715&lt;/id>;&lt;published>;2024-03-12T14:15:00.000 -07：00 &lt;/publined>; &lt;更新>; 2024-03-19T09：12：05.568-07：00 &lt;/updated>; &lt;category scheme =“ http://www.blogger.com/atom/atom/ns/ns#” term = &quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www. blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Talk like a graph: Encoding graphs for large language models&lt;/stitle>;&lt;content type=&quot;html &quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Bahare Fatemi and Bryan Perozzi, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s1600/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Imagine all the things around you — your friends, tools in your kitchen, or even the parts of your bike. They are all connected in different ways. In computer science, the term &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graph&lt;/a>; &lt;/em>;is used to describe connections between objects. Graphs consist of nodes (the objects themselves) and edges (connections between two nodes, indicating a relationship between them). Graphs are everywhere now. The internet itself is a giant graph of websites linked together. Even the knowledge search engines use is organized in a graph-like way. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Furthermore, consider the remarkable advancements in artificial intelligence — such as chatbots that can write stories in seconds, and even software that can interpret medical reports. This exciting progress is largely thanks to large language models (LLMs). New LLM technology is constantly being developed for different uses. &lt;/p>; &lt;p>; Since graphs are everywhere and LLM technology is on the rise, in “&lt;a href=&quot;https://openreview.net/forum?id=IuXR1CCrSi&quot;>;Talk like a Graph: Encoding Graphs for Large Language Models&lt;/a>;”, presented at &lt;a href=&quot;https://iclr.cc/&quot;>;ICLR 2024&lt;/a>;, we present a way to teach powerful LLMs how to better reason with graph information. Graphs are a useful way to organize information, but LLMs are mostly trained on regular text. The objective is to test different techniques to see what works best and gain practical insights. Translating graphs into text that LLMs can understand is a remarkably complex task. The difficulty stems from the inherent complexity of graph structures with multiple nodes and the intricate web of edges that connect them. Our work studies how to take a graph and translate it into a format that an LLM can understand. We also design a benchmark called &lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>; to study different approaches on different graph reasoning problems and show how to &lt;em>;phrase&lt;/em>; a graph-related problem in a way that enables the LLM to solve the graph problem. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: 1) the graph encoding method, 2) the nature of the graph task itself, and 3) interestingly, the very structure of the graph considered.这些发现为我们提供了如何最好地表示法学硕士图表的线索。 Picking the right method can make the LLM up to 60% better at graph tasks! &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s1600/image7.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4 MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s16000/image7.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;如图所示，使用两种不同的方法将图形编码为文本，并将文本和有关图形的问题提供给法学硕士。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;图形文本&lt;/h2>; &lt;p>; 能够系统地找出翻译图形的最佳方式为了文本，我们首先设计一个名为 &lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>; 的基准测试。将 GraphQA 视为一项考试，旨在评估针对特定图问题的强大法学硕士。我们希望了解法学硕士能够如何很好地理解和解决涉及不同设置中的图形的问题。为了为法学硕士创建全面且真实的考试，我们不只使用一种类型的图表，而是使用多种图表的混合，以确保连接数量的广度。这主要是因为不同的图类型使解决此类问题变得更容易或更困难。通过这种方式，GraphQA 可以帮助揭露法学硕士对图表的看法的偏见，并且整个考试更接近法学硕士在现实世界中可能遇到的现实设置。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9S r3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s1368/image6.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;291&quot; data-original-width=&quot;1368&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJ xUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;我们使用 LLM 进行图形推理的框架概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; GraphQA 专注于与图形相关的简单任务，例如检查边缘是否存在，计算节点或边的数量，查找连接到特定节点的节点，并检查图中的循环。这些任务可能看起来很基本，但它们需要理解节点和边之间的关系。通过涵盖从识别模式到创建新连接等不同类型的挑战，GraphQA 可以帮助模型学习如何有效地分析图形。这些基本任务对于更复杂的图推理至关重要，例如寻找节点之间的最短路径、检测社区或识别有影响力的节点。此外，GraphQA 还包括使用各种算法生成随机图，例如 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős-Rényi&lt;/ a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;无标度网络&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki /Barab%C3%A1si%E2%80%93Albert_model&quot;>;Barabasi-Albert 模型&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_block_model&quot;>;随机块模型&lt;/a>; >;，以及更简单的图结构，如路径、完整图和星图，为训练提供了多样化的数据集。 &lt;/p>; &lt;p>; 在处理图表时，我们还需要找到方法来提出法学硕士可以理解的与图表相关的问题。 &lt;em>;提示启发式&lt;/em>;是实现此目的的不同策略。让我们分解一下常见的：&lt;/p>; &lt;ul>; &lt;li>;&lt;em>;零射击&lt;/em>;：简单地描述任务（“这张图中有循环吗？”）并告诉法学硕士去为了它。没有提供示例。 &lt;/li>;&lt;li>;&lt;em>;Few-shot&lt;/em>;：这就像在真正的交易之前给法学硕士进行一次小型练习测试。我们提供了一些示例图形问题及其正确答案。 &lt;/li>;&lt;li>;&lt;em>;Chain-of-Thought&lt;/em>;: Here, we show the LLM how to break down a problem step-by-step with examples.目标是教它在面对新图表时生成自己的“思维过程”。 &lt;/li>;&lt;li>;&lt;em>;Zero-CoT&lt;/em>;：与 CoT 类似，但我们没有提供训练示例，而是给 LLM 一个简单的提示，例如“让我们逐步思考”，以触发其自己解决问题的崩溃。 &lt;/li>;&lt;li>;&lt;em>;BAG（构建图形）&lt;/em>;：这是专门用于图形任务的。我们在描述中添加了短语“让我们构建一个图...”，帮助法学硕士专注于图结构。 &lt;/li>; &lt;/ul>; &lt;p>; 我们探索了将图表转换为法学硕士可以使用的文本的不同方法。我们的关键问题是： &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;节点编码&lt;/em>;：我们如何表示各个节点？测试的选项包括简单的&lt;a href=&quot;https://en.wikipedia.org/wiki/Integer&quot;>;整数&lt;/a>;、常用名称（人物、字符）和字母。 &lt;/li>;&lt;li>;&lt;em>;Edge encoding&lt;/em>;: How do we describe the relationships between nodes?方法涉及括号符号、“是朋友”等短语以及箭头等符号表示。 &lt;/li>; &lt;/ul>; &lt;p>; 各种节点和边编码被系统地组合起来。这导致了如下图所示的函数： &lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh -Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZchDY-oS1ts/s855/ image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-宽度=“855”高度=“302”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh-Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7 151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZcHDY-oS1ts/w640-h302/image1.png&quot;宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用于通过文本对图形进行编码的图形编码函数示例。&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;分析与结果&lt;/h2>; &lt;p>;我们进行了三项关键实验：一项是测试 LLM 如何处理图形任务，两项是了解 LLM 的大小和不同图形形状如何影响性能。我们在 GraphQA 上运行所有实验。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;LLM 如何处理图形任务&lt;/h3>; &lt;p>; 在这个实验中，我们测试了 pre 的效果如何。经过训练的法学硕士可以解决图形问题，例如识别连接、循环和节点度。以下是我们了解到的内容： &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;LLM 的挣扎：&lt;/em>;在大多数基本任务上，LLM 的表现并不比随机猜测好多少。 &lt;/li>;&lt;li>;&lt;em>;编码非常重要&lt;/em>;：我们如何将图形表示为文本对 LLM 性能有很大影响。一般来说，“事件”编码对于大多数任务都表现出色。 &lt;/li>; &lt;/ul>; &lt;p>; 我们的结果总结在下面的图表中。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8 BVaKgw5ObjwN6_zS54BOALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s1864/image8 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8BVaKgw5ObjwN6_zS54BOAL M_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据各种图形编码器函数在不同图形任务上的准确性进行比较。该图的主要结论是图形编码函数非常重要。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h3>;越大（通常）越好&lt;/h3>; &lt;p>; 在这个实验中，我们想看看 LLM 的大小（就参数数量而言）是否会影响他们处理图形问题的能力。为此，我们在 &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;PaLM 2&lt;/a>; 的 XXS、XS、S 和 L 尺寸上测试了相同的图形任务。 Here is a summary of our findings: &lt;/p>; &lt;ul>; &lt;li>;In general, bigger models did better on graph reasoning tasks.额外的参数似乎给了他们学习更复杂模式的空间。奇怪的是，对于“边存在”任务（找出图中的两个节点是否相连）来说，大小并不那么重要。 &lt;/li>;&lt;li>;即使是最大的法学硕士也无法在循环检查问题（找出图表是否包含循环）上始终击败简单的基线解决方案。这表明法学硕士在某些图形任务方面仍有改进的空间。 &lt;/li>; &lt;/ul>; &lt;table align =“ Center” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container” style&#39;style =“ margin-left：auto; auto; margin-right：auto; auto;&#39;&#39; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc4 2M4f3jpo8v17AddfoORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/s1227 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;959&quot; data-original-width=&quot;1227&quot; height=&quot; 500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc42M4f3jpo8v17AddfoORP liE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/w640-h500/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;模型容量对 PaLM 2-XXS、XS、S 和 L 的图形推理任务的影响。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;不同的图形形状是否会让法学硕士感到困惑&lt;/h3 >; &lt;p>; 我们想知道图的“形状”（节点如何连接）是否会影响法学硕士解决问题的能力。将下图视为图形形状的不同示例。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOafKwr7_w9dHTUD1xtnI6IMAswp0py ManGDEO1ej1WeH9yByu-5ivtlfU5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s1400/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;195&quot; data-original-width=&quot;1400&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOAfKwr7_w9dHTUD1xtnI6IMAswp0pyManGDEO1ej1WeH9yByu-5ivtlfU5N -7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Samples of graphs generated with different graph generators from GraphQA. ER、BA、SBM 和 SFN 指的是 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a >;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;巴拉巴西-阿尔伯特&lt;/a>;、&lt;a href=&quot;https://en .wikipedia.org/wiki/Stochastic_block_model&quot;>;随机块模型&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;无标度网络&lt;/a>;分别.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们发现图结构对LLM性能有很大影响。例如，在询问循环是否存在的任务中，法学硕士在紧密互连的图（循环在那里很常见）上表现出色，但在路径图（循环永远不会发生）上表现不佳。有趣的是，提供一些混合的例子有助于它适应。例如，对于循环检查，我们在提示中添加了一些包含循环的示例和一些不包含循环的示例作为少样本示例。其他任务也出现类似的模式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKeTHgfhItKeXo8P3M4sGSQRZJJsXMA VFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7lAym4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s1864/image5.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKETHgfhItKeXo8P3M4sGSQRZJJsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7LAym 4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;比较不同图形任务上的不同图形生成器。这里的主要观察是图结构对法学硕士的表现有重大影响。 ER、BA、SBM 和 SFN 指的是 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a >;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;巴拉巴西-阿尔伯特&lt;/a>;、&lt;a href=&quot;https://en .wikipedia.org/wiki/Stochastic_block_model&quot;>;随机块模型&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;无标度网络&lt;/a>;分别.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;简而言之，我们深入研究了如何最好地将图表表示为文本，以便法学硕士能够理解它们。我们发现三个主要因素会产生影响：&lt;/p>; &lt;ul>; &lt;li>;&lt;em>;如何将图表转换为文本&lt;/em>;：我们如何将图表表示为文本会显着影响法学硕士的表现。一般来说，事件编码对于大多数任务都表现出色。&lt;/li>;&lt;li>;&lt;em>;任务类型&lt;/em>;：某些类型的图形问题对于法学硕士来说往往更难，即使从图形到图形的良好翻译也是如此。文本。 &lt;/li>;&lt;li>;&lt;em>;图结构&lt;/em>;：令人惊讶的是，我们进行推理的图的“形状”（连接密集、稀疏等）会影响法学硕士的表现。 &lt;/li>; &lt;/ul>; &lt;p>; 这项研究揭示了如何为法学硕士准备图表的关键见解。正确的编码技术可以显着提高法学硕士在图问题上的准确性（提高约 5% 到 60% 以上）。我们的新基准 GraphQA 将有助于推动该领域的进一步研究。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们谨向我们的合作伙伴表示感谢——作者 Jonathan Halcrow，感谢他对本书做出的宝贵贡献。我们衷心感谢 Anton Tsitsulin、Dustin Zelle、Silvio Lattanzi、Vahab Mirrokni 以及 Google Research 的整个图挖掘团队，他们富有洞察力的评论、彻底的校对和建设性的反馈极大地提高了我们的工作质量。我们还要特别感谢 Tom Small 创建本文中使用的动画。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6090481872694489715 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like- graph-encoding-graphs-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds /8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html&quot; rel =&quot;alternate&quot; title=&quot;像图一样说话：为大型语言模型编码图&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger .com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6 tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s72-c/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot;宽度=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id >;标签：blogger.com，1999：blog-8474926331452026626.post-470840348983280912&lt;/id>;&lt;已发布>;2024-03-11T12:08:00.000-07:00&lt;/已发布>;&lt;更新>;2024-03-11T12:13 ：03.824-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Chain-of-table：在推理链中不断演化表格以实现表格理解&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：云 AI 团队学生研究员王子龙和研究科学家李振宇&lt;/span>; &lt;img src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUTIhHxVvsBjbPxvZLcNc D_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s832/Chain-of-Table.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人们每天都使用表格以结构化、易于访问的格式组织和解释复杂的信息。由于此类表格无处不在，表格数据的推理长期以来一直是自然语言处理 (NLP) 的中心主题。该领域的研究人员致力于利用语言模型来帮助用户回答问题、验证陈述以及基于表格分析数据。然而，语言模型是在大量纯文本上进行训练的，因此表格数据固有的结构化性质可能很难让语言模型完全理解和利用。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 最近，&lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot;>;大型语言模型&lt;/a>;（法学硕士）通过生成可靠的推理链，在各种自然语言理解（NLU）任务中取得了出色的表现，如作品所示就像&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;思想链&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2205.10625&quot;>;最少到-大多数&lt;/a>;。然而，法学硕士对表格数据进行推理的最合适方式仍然是一个悬而未决的问题。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2401.04398&quot;>;Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding&lt;/a>;”中，我们提出一个解决表格理解任务的框架，我们训练法学硕士逐步概述他们的推理，迭代更新给定的表格以反映思维过程的每个部分，类似于人们如何解决基于表格的问题。这使得LLM能够将表格转化为更简单、更易于管理的段，以便能够深入理解和分析表格的每个部分。这种方法取得了重大改进，并在 &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;、&lt;a href=&quot;https: //arxiv.org/abs/1909.02164&quot;>;TabFact&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>;FeTaQA&lt;/a>; 基准。下图显示了所提出的表链和其他方法的高级概述。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo 6om2M2rX5d9wqOWsXWVAa9o0S75bIt7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s1478/image2.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1478&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo6om2M2rX5d9wqOWsXWVAa9o0S75bIt7 qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;给定一个复杂的表格，其中骑车人的国籍和姓名位于同一单元格中，（a）通用的多步骤推理无法提供正确的答案（b）程序辅助推理生成并执行程序（例如、SQL 查询）来提供答案，但无法准确解决问题。相比之下，(c) Chain-of-Table 对一系列操作进行迭代采样，有效地将复杂的表转换为专门针对问题定制的版本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Chain-of-Table&lt;/h2>; &lt;p>; 在 Chain-of-Table 中，我们指导法学硕士使用&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;情境学习&lt;/a>; 迭代生成操作并更新表以表示其基于表格数据的推理链。这使得法学硕士能够根据之前操作的结果动态规划下一个操作。该表的这种不断演变形成了一条链，它为给定问题的推理过程提供了更加结构化和清晰的表示，并使法学硕士能够做出更准确和可靠的预测。 &lt;/p>; &lt;p>; 例如，当被问到“哪位演员获得最多的全国有色人种协进会形象奖？” the Chain-of-Table framework prompts an LLM to generate tabular operations mirroring tabular reasoning processes.它首先标识相关列。然后，它根据共享内容聚合行。最后，它对汇总结果进行重新排序，以生成明确回答所提出问题的最终表格。 &lt;/p>; &lt;p>; 这些操作会转换表格以与提出的问题保持一致。为了平衡大表上的性能和计算费用，我们根据表格行的子集构建操作链。同时，逐步操作通过显示表格操作的中间结果来揭示底层推理过程，促进增强可解释性和理解性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08 eo9ZEqymWYszwNyKfZjcx0T-wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s1999/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;983&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08eo9ZEqymWYszwNyKfZjcx0T-w YwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Chain-of-Table 中表格推理过程的图示。这个迭代过程涉及动态规划操作链并将中间结果准确地存储在转换后的表中。这些中间表格作为表格思维过程，可以指导法学硕士更可靠地找到正确答案。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Chain-of-表由三个主要阶段组成。在第一阶段，它指示法学硕士通过上下文学习动态规划下一步操作。具体来说，提示涉及三个组成部分，如下图所示：&lt;/p>; &lt;ol>; &lt;li>;问题&lt;em>;Q&lt;/em>;：“哪个国家的自行车运动员进入前三名的人数最多？” &lt;/li>;&lt;li>;操作历史&lt;em>;链&lt;/em>;：&lt;code>;f_add_col(Country)&lt;/code>;和&lt;code>;f_select_row(1, 2, 3)&lt;/code>;。 &lt;/li>;&lt;li>;最新中间表&lt;em>;T&lt;/em>;：转换后的中间表。 &lt;/li>; &lt;/ol>; &lt;p>; 通过在提示中提供三元组&lt;em>;(T, Q, chain)&lt;/em>;，LLM可以观察之前的表格推理过程，并从操作中选择下一个操作pool逐步完成推理链。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdi yuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s1958/image1.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1233&quot; data-original-width=&quot;1958&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjiIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZ kT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Chain-of-Table 如何从操作池中选择下一个操作并生成该操作的参数。(a) Chain-of-Table 对操作池中的下一个操作进行采样。 (b) 它将选定的操作作为输入并生成其参数。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 在下一个操作之后&lt;em>;f&lt;/em>;确定后，在第二阶段，我们需要生成参数。如上所述，Chain-of-Table 考虑提示中的三个组成部分，如图所示：(1) 问题，(2) 所选操作及其所需参数，以及 (3) 最新的中间表。 &lt;/p>; &lt;p>; 例如，当选择操作&lt;code>;f_group_by&lt;/code>;时，它需要一个标头名称作为其参数。 &lt;/p>; &lt;p>; 法学硕士在表中选择合适的标题。配备了选定的操作和生成的参数，Chain-of-Table 执行操作并构造一个新的中间表以进行以下推理。 &lt;/p>; &lt;p>; Chain-of-Table 迭代前两个阶段来计划下一个操作并生成所需的参数。在此过程中，我们创建一个操作链作为表格推理步骤的代理。这些操作生成中间表，向法学硕士呈现每个步骤的结果。 Consequently, the output table contains comprehensive information about the intermediate phases of tabular reasoning.在最后阶段，我们使用此输出表来制定最终查询，并提示法学硕士以及问题以获得最终答案。 &lt;/p>; &lt;br />; &lt;h2>;实验设置&lt;/h2>; &lt;p>;我们使用&lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2-S&lt;/a>;&amp;nbsp ；以及&lt;a href=&quot;https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>; 作为 LLM 的骨干和在三个公共表理解基准上进行实验：&lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/1909.02164 &quot;>;TabFact&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>;FeTaQA&lt;/a>;。 WikiTQ 和 FeTaQA 是基于表格的问答的数据集。 TabFact is a table-based fact verification benchmark.在这篇博文中，我们将重点关注 WikiTQ 和 TabFact 上的结果。我们将 Chain-of-Table 与通用推理方法（例如，End-to-End QA、Few-Shot QA 和 &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-思想&lt;/a>;）和程序辅助方法（例如，&lt;a href=&quot;https://arxiv.org/abs/2204.00498&quot;>;文本到SQL&lt;/a>;、&lt;a href=&quot;https: //arxiv.org/abs/2210.02875&quot;>;活页夹&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;日期&lt;/a>;）。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;答案更准确&lt;/h3>; &lt;p>;与通用推理方法和程序辅助推理相比方法中，Chain-of-Table 在 &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; 和&lt;a href=&quot;https://openai 上实现了更好的性能.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>;。这归因于动态采样操作和信息丰富的中间表。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3mu AAn2k0cjfKV9hBSRaIooRI30qkAbn9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s1018/ChainOfTableUnderstanding.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;755&quot; data-original-width=&quot;1018&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3muAAn2k0cjfKV9hBSRaIooRI30qkAbn 9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s16000/ChainOfTableUnderstanding.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;了解 WikiTQ 和 TabFact 上使用 PaLM 2 和 GPT 3.5 与各种模型的比较结果。&lt;/span>;&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;在较难的问题上具有更好的稳健性&lt;/h3>; &lt;p>; 在 Chain-of 中-表格，操作链越长，说明题及其对应表格的难度和复杂度越高。我们根据 Chain-of-Table 中的操作长度对测试样本进行分类。我们将 Chain-of-Table 与 Chain-of-Thought 和 Dater 进行比较，作为代表性的通用和程序辅助推理方法。我们使用 &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; 在 &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>; 上的结果来说明这一点维基百科&lt;/a>;。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrS ZEzXlLIly40_nr65KOcgQA96rC8Pz744FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIAEsHJohvlrSna7/s1548/CoTOpChainLength.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;624&quot; data-original-width=&quot;1548&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrSZEzXlLIly40_nr65KOcgQA96rC8Pz744 FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIAEsHJohvlrSna7/s16000/CoTOpChainLength.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;对于需要不同长度操作链的问题，思想链、日期器和 WikiTQ 上建议的表链的性能。我们提出的原子操作显着提高了通用和程序辅助推理对应物的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 值得注意的是，Chain-of-Table 始终超越了两个基线跨所有操作链长度的方法，与 &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>; 相比，显着提升高达 11.6%，高达 7.9%与&lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;日期&lt;/a>;相比。此外，与其他基线方法相比，表链的性能随着操作数量的增加而缓慢下降，当操作数量从 4 增加到 5 时，仅表现出最小的下降。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;更大的表格具有更好的鲁棒性&lt;/h3>; &lt;p>;我们对表格进行分类&lt;a href= &quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>; 根据代币数量分为三组：小型（&lt;2000 个代币）、中型（2000 到 4000 个代币）和大型（>;4000 个代币） 。然后我们将 Chain-of-Table 与 &lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;Datar&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2210.02875&quot; 进行比较>;Binder&lt;/a>;，两个最新且最强的基线。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsai xakttU_-ha88xZhHSbg6M_I4VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1008&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsaixakttU_-ha88xZhHSbg6M_I4 VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Binder、Dater 和建议的 Chain-of-Table 在小型（&lt;2000 个代币）、中型上的性能来自 WikiTQ 的（2000 到 4000 个标记）和大型（>; 4000 个标记）表。我们观察到，性能随着输入表的增大而降低，而表链则优雅地减小，与竞争方法相比取得了显着的改进。 （如上，下划线文字表示性能第二好；粗体表示性能最好。）&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; Binder性能、 Dater 以及来自 WikiTQ 的小型（&lt;2000 个令牌）、中型（2000 到 4000 个令牌）和大型（>;4000 个令牌）表上提议的 Chain-of-Table。我们观察到，性能随着输入表的增大而降低，而表链则优雅地减小，与竞争方法相比取得了显着的改进。 （如上所述，下划线文本表示第二佳性能；粗体表示最佳性能。） &lt;/p>; &lt;p>; 正如预期的那样，性能随着输入表的增大而下降，因为模型需要通过较长的上下文进行推理。尽管如此，所提出的 Chain-of-Table 的性能会优雅地下降，在处理大型表时，比第二佳竞争方法实现了 10+% 的显着改进。这证明了推理链在处理长表格输入方面的有效性。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出的表链方法通过利用表格结构来表达基于表格的推理的中间步骤，从而增强了 LLM 的推理能力。它指示法学硕士根据输入表及其相关问题动态规划操作链。这种不断发展的表格设计为促进法学硕士对表格的理解提供了新的思路。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由王子龙、张浩、李春亮、朱利安·马丁·艾森施洛斯、文森特·佩罗、王子峰、莱斯利·米库里奇进行, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister.感谢 Chih-Kuan Yeh 和 Sergey Ioffe 提供的宝贵反馈。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/470840348983280912/comments/default&quot; rel =&quot;回复&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/chain-of-table-evolving-tables- in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /470840348983280912&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/470840348983280912&quot; rel=&quot;self&quot; 类型=“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/03/chain-of-table-volving-tables-in.html- “Chain-of-table：在推理链中不断演化表格以实现表格理解” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUTIhHxVvsBjbPxvZLcNc D_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s72-c/Chain-of-Table.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1106624361649572376&lt;/id>;&lt;已发布>;2024-03-08T11:33:00.000-08:00&lt;/已发布>;&lt;更新>;2024-03-13T09:18:01.747-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;健康&quot;>;&lt;/category>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“图像分类”>;&lt;/类别>;&lt;title type =“text”>;皮肤病学和病理学的健康特定嵌入工具&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google Health 临床研究科学家 Dave Steiner 和 Google Research 产品经理 Rory Pilgrim&lt;/span>; &lt;img src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiA eG4J4ca6Pl8s9Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s16000/Path%20+%20Derm%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 全球范围内缺乏跨专业的医学影像专家解读，包括&lt;a href=&quot;https://www.rsna.org/news/2022/may/Global-Radiograph-Shortage&quot;>;放射学&lt;/ a>;、&lt;a href=&quot;https://www.aad.org/dw/monthly/2021/december/feature-running-dry&quot;>;皮肤科&lt;/a>;和&lt;a href=&quot;https://proscia. com/infographic-the-state-of-the-pathology-workforce-2022/&quot;>;病理学&lt;/a>;。机器学习 (ML) 技术可以通过支持工​​具帮助减轻这一负担，使医生能够更准确、更高效地解读这些图像。然而，此类机器学习工具的开发和实施通常受到高质量数据、机器学习专业知识和计算资源的可用性的限制。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 促进机器学习在医学成像中的应用的一种方法是通过特定领域的模型，利用深度学习 (DL) 来捕获医学图像中的信息作为压缩的数值向量（称为嵌入）。这些嵌入代表了对图像中重要特征的一种预先学习的理解。与&lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_Dimensionity&quot;>;处理高维数据&lt;相比，识别嵌入中的模式可以减少训练高性能模型所需的数据量、专业知识和计算量/a>;，如图像，直接。事实上，这些嵌入可用于执行专业领域内的各种下游任务（请参见下面的动画图）。这种利用预先学习的理解来解决相关任务的框架类似于经验丰富的吉他手通过耳朵快速学习新歌曲的框架。因为吉他手已经建立了技能和理解的基础，所以他们可以快速掌握新歌曲的模式和节奏。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xB x3fBi1u2ke10iE4THphEkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPSH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s1600/路径%20+% 20Derm%20train%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xBx3fBi1u2ke10iE4THph EkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPsH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s16000/Path%20+%20Derm%20train%20LP.gif&quot;/>;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Path Foundation 用于将（图像，标签）对的小数据集转换为（嵌入，标签）对。然后，这些对可用于使用线性探针（即轻量级线性分类器）来训练特定于任务的分类器（如图所示），或使用嵌入作为输入的其他类型的模型。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-右：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag- ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUpplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s1600/Path%20+%20Derm%20-%20评估%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= &quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag-ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s16000/Path%20+%20Derm%20-%20evaluate %20LP.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Once the linear probe is trained, it can be used to make predictions on embeddings from new images. These predictions can be compared to ground truth information in order to evaluate the linear probe&#39;s performance.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In order to make this type of embedding model available and drive further development of ML tools in medical imaging, we are excited to release two domain-specific tools for research use: &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/derm-foundation&quot;>;Derm Foundation&lt;/a>; and &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/path-foundation&quot;>;Path Foundation&lt;/a>;. This follows on the strong response we&#39;ve already received from researchers using the &lt;a href=&quot;https://blog.research.google/2022/07/simplified-transfer-learning-for-chest.html&quot;>;CXR Foundation&lt;/a>; embedding tool for chest radiographs and represents a portion of our expanding research offerings across multiple medical-specialized modalities. These embedding tools take an image as input and produce a numerical vector (the embedding) that is specialized to the domains of dermatology and digital pathology images, respectively. By running a dataset of chest X-ray, dermatology, or pathology images through the respective embedding tool, researchers can obtain embeddings for their own images, and use these embeddings to quickly develop new models for their applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Path Foundation&lt;/h2>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2310.13259&quot;>;Domain-specific optimization and diverse evaluation of self-supervised models for histopathology&lt;/a>;”, we showed that self-supervised learning (SSL) models for pathology images outperform traditional pre-training approaches and enable efficient training of classifiers for downstream tasks. This effort focused on &lt;a href=&quot;https://en.wikipedia.org/wiki/H%26E_stain&quot;>;hematoxylin and eosin&lt;/a>; (H&amp;amp;E) stained slides, the principal tissue stain in diagnostic pathology that enables pathologists to visualize cellular features under a microscope.使用SSL模型的输出训练的线性分类器的性能与先前在数量级训练的DL模型的数据相匹配。 &lt;/p>; &lt;p>;由于数字病理图像和“自然图像”照片之间存在实质性差异，这项工作涉及模型训练期间的几种病理特定优化。一个关键要素是，&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/pmc7522141/&quot;>;全部slide Images &lt;/a>;（WSIS）在病理学中可以是100,000个像素（比典型的智能手机照片大数千倍），并由多个：缩放水平的专家分析。因此，WSI通常被分解为用于计算机视觉和DL应用的较小瓷砖或补丁。所得的图像是信息密集的，该图像密集，分布在整个框架中，而不是具有不同的语义对象或前景和背景变化，从而为强大的SSL和特征提取带来了独特的挑战。此外，物理（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/microtome&quot;>;切割&lt;/a>;）和化学（例如，&lt;a href =“ https：///en.wikipedia。 org/wiki/fixation_（组织学）>;修复&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/staining&quot;>;染色&lt;/a>;）用于准备样本的过程可能会影响图像外观急剧。 &lt;/p>; &lt;p>;考虑了这些重要方面，病理学特异性的SSL优化包括帮助模型学习&lt;a href=&quot;https://arxiv.org/abs/2206.12694&quot;>; stain-agragnostic特征&lt;/a>; ，将模型概括为来自多个大型贴片，&lt;a href=&quot;https://blog.research.google/2020/02/generating-diverse-synthetic-medical.html&quot;>;增强&lt;/a>;和图像后处理以及自定义数据平衡，以改善SSL培训的输入异质性。使用涉及12种不同任务的17种不同组织类型的广泛基准任务对这些方法进行了广泛的评估。 &lt;/p>; &lt;p>;利用视觉变压器（&lt;a href=&quot;https://github.com/google-research/vision_transformer&quot;>; vit-s/16 &lt;/a>;）体系结构，被选为路径基础来自上述优化和评估过程的最佳性能模型（并在下图中进行了说明）。因此，该模型在性能和模型大小之间提供了重要的平衡，以使在许多大病理WSIS的许多单独的图像贴片上生成嵌入，以实现有价值的可扩展用途。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJb79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s1999/Path%20+%20Derm% 20SSSL.JPG“ style =”边距 - 左：自动; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 1097” data-Original-width =“ 1999” src =“ src =” https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJb79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s16000/Path%20+%20Derm%20SSL.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SSL training with pathology-specific optimizations for Path Foundation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The域特异性图像表示的值也可以在下图中看到，该图显示了路径基础的线性探测性能改善（如&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/receiver/receiver_operating_characteristic &quot;>; AUROC &lt;/a>;）与自然图像的传统预训练（&lt;a href=&quot;https://arxiv.org/abs/2104.10972&quot;>; imagenet-21k &lt;/a>;）。这包括评估诸如&lt;a href=&quot;https://jamanetwork.com/journals/jama/jama/fullarticle/2665774&quot;>;淋巴结中的转移性乳腺癌检测&lt;/a>;，&lt;a a href =“ https：// https：// https：// jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>; prostate癌症分级&lt;/a>;和&lt;a href=&quot;https://wwwww.nature.com/articles/articles/S41523-022-022-00478-Y>;等级&lt;/a>;，等等。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg-FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s1999/Path ％20+％20derm％20 Embeding.png“ style =”边距 - 左：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 890” data-Original-width =“ 1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg-FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s16000/Path%20+%20Derm%20embeddings.png&quot; />;&lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;路径基础嵌入显着超过传统的成像网嵌入，如线性探测在组织病理学中的多个评估任务所评估的。 。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; derm Foundation &lt;/ h2>; &lt;p>; &lt;a href=&quot;https://github.com/google-health/image-research-reakearch/tree/master/master/derm-foundation&quot;>; Derm Foundation &lt;/a>;是从我们的研究中得出的嵌入工具applying DL to &lt;a href=&quot;https://blog.research.google/2019/09/using-deep-learning-to-inform.html&quot;>;interpret images of dermatology conditions&lt;/a>; and includes our recent work that adds &lt;a href=&quot;https://arxiv.org/abs/2402.15566&quot;>;improvements to generalize better to new datasets&lt;/a>;.由于其皮肤科特定的预训练，因此对皮肤状况图像中存在的特征具有潜在的理解，可用于快速开发模型以对皮肤状况进行分类。 API的基础模型是&lt;a href=&quot;https://github.com/google-research/big_transfer&quot;>; BIT RESNET-101X3 &lt;/a>;在两个阶段进行了培训。第一个前训练阶段使用对比度学习，类似于&lt;a href=&quot;https://arxiv.org/abs/2010.00747 &quot;>; convirt &lt;/a>;，进行大量image-text对训练=“ https://blog.research.google/2017/07/revisiting-unrenoalable-effectives.html”>;来自Internet &lt;/a>;。在第二阶段，该预训练模型的图像组成部分是通过临床数据集（例如远程表现服务的临床数据集）进行微调进行微调分类的。 &lt;/p>; &lt;p>;与组织病理学图像不同，皮肤病学图像更类似于用于训练许多当今计算机视觉模型的现实世界图像。但是，对于专门的皮肤病学任务，创建高质量的模型仍然需要大型数据集。借助Derm Foundation，研究人员可以使用自己的较小数据集检索域特异性嵌入，并使用这些数据集来构建较小的模型（例如，线性分类器或其他小型非线性模型），使他们能够验证其研究或产品创意。为了评估这种方法，我们使用远程表现数据训练了下游任务的模型。模型培训涉及不同的数据集大小（12.5％，25％，50％，100％），以比较基于嵌入的线性分类器与微调。 &lt;/p>; &lt;p>;所考虑的建模变体是：&lt;/p>; &lt;ul>; &lt;li>; &lt;a href=&quot;https://github.com/google-research/google-research/big_transfer&quot;>; &lt;a href=&quot;https://github_transfer&quot;>; &lt;/p>; &lt;ul>; &lt;li>; BiT-M&lt;/a>; (a standard pre-trained image model) &lt;/li>;&lt;li>;Fine-tuned version of BiT-M with an extra dense layer for the downstream task &lt;/li>;&lt;li>;A linear classifier在Derm Foundation API API &lt;/li>; &lt;li>;模型的微调版本中，Derm Foundation API的微调版本具有额外层的下游任务&lt;/li>; &lt;/ul>; &lt;p>;我们发现该模型built on top of the Derm Foundation embeddings for dermatology-related tasks achieved significantly higher quality than those built solely on embeddings or fine tuned from BiT-M.发现该优势对于较小的培训数据集尺寸最为明显。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s1240/Path%20+%20Derm%20task% 20ACCURACY.PNG“ style =”边缘左左右：自动; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 842” data-Original-width =“ 1240” src =“ src =” https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s16000/Path%20+%20Derm%20task%20accuracy.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; TD类=“ Tr-Caption”样式=“ Text-Align：Center;”>;这些结果表明，Derm Foundation Tooi可以作为加速与皮肤相关的建模任务的有用起点。我们的目标是使其他研究人员能够建立模型所学的皮肤病学的基本特征和表示。 &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;但是，此分析存在局限性。我们仍在探索这些嵌入的跨任务类型，患者人群和图像设置的概括程度。使用DERM基础构建的下游模型仍然需要仔细评估，以了解其在预期设置中的预期性能。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Access Path and Derm Foundation&lt;/h2>; &lt;p>; We envision that the Derm Foundation and Path Foundation嵌入工具将实现一系列用例，包括有效开发用于诊断任务的模型，质量保证和分析前的工作流程改进，图像索引和策展以及生物标志物发现和验证。我们将这两个工具释放到研究界，以便他们可以探索嵌入的效用，以获取自己的皮肤病学和病理学数据。 &lt;/p>; &lt;p>; To get access, please sign up to each tool&#39;s terms of service using the following Google Forms.在a>; &lt;/li>; &lt;li>; &lt;a href =“ https://docs.google.com/forms/d/1auyo2vkzlzuiaxavzy1awuyqhaqo7t3blk-7ofk-7ofkugk-7ofkug/editiT？ 2“>;路径基础访问表格&lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>;获得每个工具的访问后，您可以使用API​​从已存储在Google Cloud中的皮肤病学图像或数字病理图像中检索嵌入。批准的用户只是好奇地看到该模型和嵌入在行动中的嵌入可以使用提供的示例COLAB笔记本电脑使用公共数据进行培训模型进行分类&lt;a href =“ https://github.com/google-health/image-image-image-image-image-research/- blob/master/master/derm-foundation/derm_foundation_demo.ipynb“>;六个常见的皮肤状况&lt;/a>;或在&lt;a href =”中识别肿瘤，路径发现/线性分类器demo.ipynb“>;组织病理学贴片&lt;/a>;。我们期待看到这些工具可以解锁的用例范围。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;我们要感谢许多帮助的合作者make this work possible including Yun Liu, Can Kirmizi, Fereshteh Mahvar, Bram Sterling, Arman Tajback, Kenneth Philbrik, Arnav Agharwal, Aurora Cheung, Andrew Sellergren, Boris Babenko, Basil Mustafa, Jan Freyberg, Terry Spitz, Yuan Liu, Pinal Bavishi, Ayush Jain, Amit Talreja, Rajeev Rikhye, Abbi Ward, Jeremy Lai, Faruk Ahmed, Supriya Vijay,Tiam Jaroensri, Jessica Loo, Saurabh Vyawahare, Saloni Agarwal, Ellery Wulczyn, Jonathan Krause, Fayaz Jamil, Tom Small, Annisah Um&#39;rani, Lauren Winer，Sami Lachgar，Yossi Matias，Greg Corrado和Dale Webster。 “ rel =”回答“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/health-phealth-specific-specific-specific-embedding-tools-tools--tools--tools--tools-tools-- for.html＃comment-form“ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926263314520262626262626262626/posts/posts/defeault /1106624361649572376&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1106624361649572376&quot; rel=&quot;self&quot; type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/03/health-specific-embedding-tools-tools-for.html” - 皮肤病学和病理学的特定嵌入工具“ type =” text/html“/>; &lt;aunder>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile.com/profile/1209862651477777777777777777777777777777777777777777777752661 &lt;/uri>; &lt;/uri>; &lt;Email>; noreply@blogger.com &lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/2005#thumbnail” src =“ https：//img1.blog1.blogblog .com/img/b16-round.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/rution>; &lt;媒体：thumbnail height =“ 72” url =“ https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiAeG4J4ca6Pl8s9Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s72-c/Path%20+%20Derm%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/ Mrss/“>; &lt;/媒体：Thumbnail>; &lt;THR：THR>; 0 &lt;/thr：Total>; &lt;/entry>; &lt;/entry>; &lt;ID>; &lt;id>;标签：blogger.com，1999：Blog-8474926331452026626.post-17659719719068432739 &lt;/id >; &lt;Ebtumed>; 2024-03-07T10：15：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-03-07T10：19：31.177-08：00 www.blogger.com/atom/ns#“ term =“大语言模型”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/atom/ns#” term =“机器智能”>; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“自然语言处理”>; &lt;/category>; &lt;/cattory>; &lt;title type =“ text”>;社交学习：与协作学习：大型语言模型&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由研究实习生Amirkeivan Mohtashami和Florian Hartmann，Google Research &lt;/span Research>; &lt;img src = &lt;img src = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot; style=&quot;display: none;&quot; />; &lt;p>;大语言模型（LLMS）已大大改善了使用自然语言指定的任务的最新技术状态，通常可以在接近人的情况下达到表现。随着这些模型越来越促进辅助代理，对他们有效地学习可能是有益的，就像人们在社交环境中所做的一样，这将使基于LLM的代理人能够改善彼此的表现。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>;讨论人类，班杜拉和沃尔特斯的学习过程。向他人学习的一种常见方法是通过A &lt;em>;口头指导&lt;/em>;（例如，来自教师），描述了如何参与特定行为。另外，通过模仿行为的实例，学习可以通过&lt;em>;实时模型&lt;/em>;进行。 &lt;/p>; &lt;p>;鉴于LLM的成功模仿了人类交流的成功，在我们的论文中，“ &lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;社会学习：与大语言模型进行协作学习： a>;”，我们研究LLM是否能够使用社会学习互相学习。为此，我们概述了社会学习的框架，其中LLM使用自然语言以隐私感知方式共享知识。 We evaluate the effectiveness of our framework on various datasets, and propose quantitative methods that measure privacy in this setting.与以前的协作学习方法相反，例如Common &lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;联合学习&lt;/a>;通常依靠的方法&lt;/a>;在梯度上，在我们的框架中，代理商纯粹使用自然语言互相教导。 &lt;/p>; &lt;br />; &lt;h2>; llms的社会学习&lt;/h2>; &lt;p>;将社交学习扩展到语言模型，我们考虑了学生llm应该学会从已经已经已经是已经已经已经已经已经是已经已经已经有多个已经已经已经有多个老师实体的，知道那个任务。在我们的论文中，我们评估了学生在各种任务上的表现，例如&lt;a href=&quot;https://dl.acm.org/doi/10.1145/2034691.2034742&quot;>;垃圾邮件检测&lt;/a>;在简短的短信中（ SMS），求解&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;小学数学问题&lt;/a>;和&lt;a href=&quot;https://arxiv.org.org/abs/1905.10044&quot;>;根据给定的文本回答问题&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1jP3Awu09-DVRHBE_Urf58yzm5tDBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s900/image3 。 //blogger.googleusercontent.com/img/b/r29vz2xl/avvxsegandq_mjavbs4j3lmxex71nmrclpaasklndzye8f7yj3 slyaauzwnauzwnauzw4yrxi_ncg7_ncg7ssp5jablaxraxraxrraxrraxrraxrhaxraxprhndbhdbhdbhdbhndbrhdbhndbhdbrhndbhdbhndbhdbhndbhrdwrdwrdwrdwrd.2 y.dbhdwrd.ncrfrndw.bmd.rvhrd.rvhrd.rvhrd.rvhrd.rvhrd.rvhrd.2 y1 BBPM-AIBZXMGA9O6CYSCCRDSMQG7VJ-OU07JHA0OU0YIXCXRB0Q3APMQBN8VZ5REBP70ZNOGH/s16000/image3.gif“/>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; =“ tr-caption”样式=“ text-align：center;”>;社会学习过程的可视化：教师模型在不共享其私人数据的情况下向学生模型提供指令或几个示例。&lt;/td>; &lt;/td>; &lt; /tr>; &lt;/tbody>; &lt;/table>; &lt;p>;语言模型显示出仅给定少数示例执行任务的出色能力 - 一个称为&lt;a href =的过程“>;几乎没有学习的学习&lt;/a>;。考虑到这一点，我们提供了一项任务的人体标签示例，该示例使教师模型可以教给学生。当这些示例无法与应得的归因于隐私问题的学生直接共享时，就会出现社会学习的主要用例之一。 &lt;/p>; &lt;p>;为了说明这一点，让我们看一个垃圾邮件检测任务的假设示例。教师模型位于设备上，一些用户自愿将收到的传入消息标记为“垃圾邮件”或“不垃圾邮件”。这是有用的数据，可以帮助培训学生模型以区分垃圾邮件而不是垃圾邮件，但是与其他用户共享个人消息是违反隐私的，因此应避免。为了防止这种情况，社交学习过程可以将知识从教师模型转移到学生，因此它可以了解垃圾邮件消息的外观，而无需共享用户的个人短信。 &lt;/p>; &lt;p>;我们通过类比与我们上面讨论的既定人类社会学习理论进行了类比，调查了这种社会学习方法的有效性。 In these experiments, we use &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-S&lt;/a>; models for both老师和学生。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 1117” data-Original-width =“ 1999” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “文本主修：中心；”>;社会学习的系统视图：在培训时，多位老师教学生。在推理时，学生正在使用从老师那里学到的东西。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;合成示例&lt;/h3>; &lt;p>;对于为传统社会学习描述的现场教学模型，我们提出了一种学习方法，教师为任务生成新的合成示例并与学生分享。这是由于一个人可以创建一个与原始示例完全不同的新例子的想法，但同样具有教育意义。确实，我们观察到，我们生成的示例与真实的示例完全不同，可以保留隐私，同时仍可以使性能与使用原始示例实现的绩效相当。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeKpICUiYU0uoqftlq-1bvLXfwlzPFhsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s1456/image5.png “ style =”边距 - 左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Original-height =“ 880” data-Original-width =“ 1456” src =“ https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeKpICUiYU0uoqftlq-1bvLXfwlzPFhsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-字幕“ style =” text-align：中心;“>; 8个生成的示例以及几个任务的原始数据（请参阅我们的＆nbsp; &lt;a href=&quot;https://arxiv.org/abs/abs/2312.114441&quot;>; paper &lt;/a>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We evaluate the efficacy of learning through synthetic examples on our task suite.特别是当示例数量足够高时，例如，n = 16时，我们观察到通过社交学习共享原始数据和与合成数据的教学之间没有统计学上的显着差异，这表明隐私改善不必出现以模型质量为代价。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHr8sU3WkHsPKVlsQmVnzW-YAop1plz6oxYvTQyxEirorXE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s1456/image4 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHr8sU3WkHsPKVlsQmVnzW-YAop1plz6oxYvTQyxEirorXE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr caption” style =“ text-align：center;”>;生成16而不是8个示例进一步缩小了相对于原始示例的性能差距。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table >; &lt;br />; &lt;p>;一个例外是垃圾邮件检测，其中合成数据的教学得出的精度较低。这可能是因为当前模型的训练程序使它们偏向仅生成非垃圾邮件示例。在&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>; paper &lt;/a>;中，我们还研究了聚集方法，以选择良好的示例子集使用。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>;指导模型也可以通过让教师为任务生成指令来自然地适应语言模型。我们的实验表明，提供这种生成的指令有效地改善了零拍的性能，达到了与原始示例相当的精确度。但是，我们确实发现教师模型可能会在某些任务上失败以提供良好的指示，例如，由于输出的格式需要复杂。 &lt;/p>; &lt;p>;为&lt;a href=&quot;https://arxiv.org/abs/1606.06031&quot;>; lambada &lt;/a>;，&lt;a href=&quot;https://arxiv.org.org/abs/2110.14168&quot;>; gsm8k &lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;随机插入&lt;/a>;，提供合成示例的性能要比提供生成的指令更好，而在其他任务中生成的指令获得的指令获得了，更高的精度。该观察结果表明，教学模型的选择取决于手头的任务，类似于人们最有效的教学方法因任务而变化。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuLHHFDC-d_FVS9DzxGQNzEHy8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s1451/image1 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuLHHFDC-d_FVS9DzxGQNzEHy8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depending on the task, generating instructions can work better than generating new examples.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;私人示例的记忆&lt;/h2>; &lt;p>;我们希望社交学习中的老师在不透露原始数据的细节的情况下教学生。 To quantify how prone this process is to leaking information, we used &lt;a href=&quot;https://research.google/pubs/the-secret-sharer-evaluating-and-testing-unintended-memorization-in-neural-networks/&quot;>;Secret Sharer&lt;/a>;, a popular method for quantifying to what extent a model memorizes its training data, and adapted it to the social learning setting.我们选择了这种方法，因为它以前曾是&lt;a href=&quot;https://blog.search.google/2023/03/distributed-differential-privacy-for.html&quot;>;使用&lt;/a>;用于评估联合学习中的记忆。 &lt;/p>; &lt;p>;为了将秘密共享方法应用于社会学习，我们设计了“金丝雀”数据点，以便我们可以具体地衡量训练过程记忆的多少。这些数据点包含在教师为生成新示例的数据集中。在社会学习过程完成后，我们可以衡量学生在使用教师使用的秘密数据中更有信心，而与教师也没有共享的类似数据相比。 &lt;/p>; &lt;p>;在我们的分析中，在&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>; paper &lt;/a>;中详细讨论。我们的结果表明，学生对老师使用的金融店只有更加自信。相比之下，当原始数据点与学生直接共享时，对随附的金丝雀的信心远高于持有的集合。这支持了这样一个结论，即教师确实使用其数据来教书而不简单地复制它。 &lt;/p>; &lt;br />; &lt;h2>;结论和下一步&lt;/h2>; &lt;p>;我们引入了一个社交学习框架，该框架允许具有访问私人数据的语言模型通过文本通信传输知识，同时保持其隐私数据。在此框架中，我们确定了共享示例并将指令分享为基本模型，并在多个任务上对其进行了评估。此外，我们将秘密共享者指标改编为我们的框架，提出了用于测量数据泄漏的指标。 &lt;/p>; &lt;p>;作为下一步，我们正在寻找改善教学过程的方法，例如，添加反馈循环和迭代。此外，我们希望使用社会学习对文本以外的其他方式进行调查。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要承认并感谢Matt Sharifi，Sian Gooding，Lukas Zilka和Blaise Aguera Y Arcas，他们都是合着者on the paper.此外，我们要感谢VictorCărbune，Zachary Garrett，Tautvydas Misiunas，Sofia Neata和John Platt的反馈，这极大地改善了论文。我们还要感谢汤姆·斯莫尔（Tom Small）创建动画人物。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/1765359719068432739/comments/comments/comments/default” rel =“回复” title =“ post注释” type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/03/social-learning-collaborative-collaborative-collaborative-learning.html ＃comment-form“ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/feeds/8474926331452026626/ rel =“ edit” type =“ application/atom+xml”/>; &lt;link href =“ http://www.blogger.com/feeds/847492633145202626/posts/posts/posts/default/176535971971906844444444444444432739 /atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/social-learning-collaborative-learning.html“大语言模型“ type =” text/html“/>; &lt;under>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147752666161 &lt;/urei>; blogger.com &lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” b16-rounded.gif“ width =” 16“>; &lt;/gd：image>; &lt;/ruter>; &lt;媒体：缩略图高度=“ 72” url =“ https：//blogger.googleusercontent.com/img/img/b/r29vz2xl/ AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s72-c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0 &lt;/thr：Total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：Blogger.com，1999：Blog-8474926331452026626.POST-8393293293208018757284 &lt;/id>; 00 &lt;/publined>; &lt;更新>; 2024-03-06T14：44：03.387-08：00 &lt;/updated>; &lt;category scheme =“ http://www.blogger.com/atom/atom/ns#” >; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“ dataSets”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http://www.blogger.com/ atom/ns＃“ term =“ ml”>; &lt;/category>; &lt;title type =“ text”>;羊角面：ML-Ready数据集的元数据格式&lt;/stitle>; &lt;content type type type =“ html”>; &lt;span class =“ Byline-author“>;由Google Research的软件工程师Omar Benjelloun和Google Core ML和MLCommons Association的软件工程师Peter Mattson和Peter Mattson &lt;/span>; &lt;img src =” img/b/R29vZ2xl/AVvXsEj09uSTHgWmPgOkD9W1nZZj5i8uW_-pgxm-T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyAAN_LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r5r_uAU2P4srnP/s1600/CroissantHero.png&quot; style=&quot;display: none;&quot; />; &lt;p>;机器学习（ML）的从业人员希望重复使用现有数据集来训练ML模型，通常会花费大量时间了解数据，了解其组织，或者找出要用作功能的子集。实际上，这么多时间以至于ML领域的进展受到基本障碍的阻碍：各种各样的数据表示。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>; ml数据集涵盖了广泛的内容类型，从文本和结构化数据到图像，音频和视频。即使在涵盖相同类型内容的数据集中，每个数据集都具有唯一的&lt;em>; Ad Hoc &lt;/em>;文件和数据格式的布置。从查找数据到培训模型，这项挑战可以降低整个ML开发过程的生产率。它还阻碍了与数据集合作的急需工具的开发。 &lt;/p>; &lt;p>;数据集有通用元数据格式，例如&lt;a href=&quot;http://schema.org/dataset&quot;>; schema.org &lt;/a>; &lt;/a>;和&lt;a href =“ https：// www.w3.org/TR/vocab-dcat-3/&quot;>;DCAT&lt;/a>;.但是，这些格式设计用于数据发现，而不是用于ML数据的特定需求，例如从结构化和非结构化源中提取和组合数据的能力，以包括将启用&lt;a href =的元数据， 。 &lt;/p>; &lt;p>;今天，我们正在介绍&lt;a href=&quot;https://mlcommons.org/croissant&quot;>; croissant &lt;/a>;，一种新的元数据格式，用于ML-Ready数据集。 Croissant was developed collaboratively by a community from industry and academia, as part of the &lt;a href=&quot;https://mlcommons.org/&quot;>;MLCommons&lt;/a>; effort.羊角面包格式不会改变实际数据的表示方式（例如，图像或文本文件格式），它提供了描述和组织它的标准方法。羊角面包以&lt;a href=&quot;https://schema.org/&quot;>; schema.org &lt;/a>;为基础，这是在Web上发布结构化数据的事实上的标准，该标准已被超过40m的数据集使用。羊角面包将其用全面的层次增加，用于ML相关的元数据，数据资源，数据组织和默认的ML语义。 &lt;/p>; &lt;p>;此外，我们还宣布了主要工具和存储库的支持：今天，三个广泛使用的ML数据集的集合 -  &lt;a href=&quot;http://www.kaggle.com/datasets&quot;>; kaggle &lt; /a>;，&lt;a href=&quot;https://huggingface.co/datasets？ ？type = data“>; OpenMl &lt;/a>;  - 将开始支持其托管数据集的羊角面包格式； &lt;a href=&quot;http://g.co/datasetsearch&quot;>;数据集搜索&lt;/a>;工具使用户可以在网络上搜索羊角面包数据集；和流行的ML框架，包括&lt;a href=&quot;https://www.tensorflow.org/&quot;>; tensorflow &lt;/a>;，&lt;a href=&quot;https://pytorch.org/&quot;>; pytorch &lt;/a>; and &lt;a href=&quot;https://github.com/google/jax&quot;>; jax &lt;/a>;，可以使用&lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>; jax &lt;/a>;可以轻松地加载羊角面包数据集TensorFlow数据集&lt;/a>;（TFDS）软件包。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;羊角面包&lt;/h2>; &lt;p>;此1.0羊角面包的释放包括一个完整的&lt;a href =“ https” https ：//mlcommons.org/croissant/1.0“>;规范&lt;/a>;格式，一组&lt;a href=&quot;https://github.com/github.com/mlcommons/croissant/croissant/croissant/croissant/tree/main/main/main/main/datasets&quot;>;示例数据集&lt;/a>;，开源&lt;a href=&quot;https://github.com/mlcommons/croissant/croissant/tree/main/main/main/python/mlcroissant&quot;>; Python Library &lt;/a>;开源&lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/editor&quot;>; Visual Editor &lt;/a>;以直观的方式加载，检查和创建羊角面包数据集描述。 &lt;/p>; &lt;p>;支持负责任的AI（RAI）是一开始就羊角面包的关键目标。我们还发布了&lt;a href=&quot;https://mlcommons.org/croissant/rai/1.0&quot;>;杂交式rai votabulary &lt;/a>;延伸的&lt;a href=&quot;https://mlcommons.org/croissant/rai/1.0&quot;>;延伸诸如数据生命周期管理，数据标签，参与式数据，ML安全性和公平评估，解释性和合规性等案例。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;为什么ML数据共享格式？&lt;/h2>; &lt;p>;大多数ML工作实际上是数据工作。培训数据是决定模型行为的“代码”。数据集可以从用于训练大型语言模型（LLM）的文本集合到用于训练汽车避免碰撞系统的驾驶场景（注释视频）的集合。但是，开发ML模型的步骤通常遵循相同的以数据为中心的过程：（1）查找或收集数据，（2）清洁和完善数据，（3）在数据上训练模型，（4）测试有关更多数据的模型，（5）发现该模型不起作用，（6）分析数据以找出原因，（7）重复直到实现可行的模型。由于缺乏通用格式，许多步骤都更加困难。对于资源有限的研究和早期企业家努力，这种“数据开发负担”尤其沉重。 &lt;/p>; &lt;p>; The goal of a format like Croissant is to make this entire process easier.例如，可以通过搜索引擎和数据集存储库来利用元数据，以使找到正确的数据集变得更加容易。数据资源和组织信息使开发清洁，完善和分析数据的工具变得更加容易。这些信息和默认的ML语义使ML框架可以使用数据使用最少的代码来训练和测试模型。这些改进共同大大减轻了数据开发负担。 &lt;/p>; &lt;p>;此外，数据集作者还关心其数据集的可发现性和易用性。采用羊角面包可以提高其数据集的价值，同时只需最少的努力，这要归功于ML数据平台的可用创建工具和支持。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;今天羊角面包可以做什么？&lt;/h2>; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“边距 - 左：自动; margin-right：auto;”>; &lt;tbody>; &lt;try>; &lt;tr>; &lt;tr>; &lt;td style =“ text-align：center; center; center;>;>; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s908/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left ：auto; margin-right：auto;“>; &lt;img border =“ 0” data-eriginal-height =“ 540” data-eriginal-width =“ 908” src =“ https://blogger.googleusercercontent.com/img /b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style =“ Text-Align：Center;”>;羊角面包生态系统：用户可以搜索羊角面包数据集，从主要存储库中下载它们，然后轻松地将其加载到他们喜欢的ML Frameworks中。他们可以使用羊角面包编辑器。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;今天，用户可以在：&lt;/p>; &lt;ul>; &lt;&lt;/p>; &lt;/pdbody>; &lt;p>;上创建，检查和修改羊角面包元数据。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>; li>; Google &lt;a href=&quot;https://datasetsearch.research.google.com/&quot;>;数据集搜索&lt;/a>;，提供了一个羊角面过滤器。 &lt;/li>; &lt;li>; &lt;a href=&quot;https://huggingface.co/datasets？other = croissant＆amp; sort = trending&quot;>; huggingface &lt;/a>; //kaggle.com/datasets&quot;>; kaggle &lt;/a>; &lt;/li>; &lt;li>; &lt;a href=&quot;https://openml.org/search?type=data&quot;>; opentml &lt;/a>; &lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>;使用羊角面包数据集，可以通过&lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>; tensorflow轻松获取数据。数据集&lt;/a>;用于在流行的ML框架中使用，例如&lt;a href=&quot;https://www.tensorflow.org/&quot;>; tensorflow &lt;/a>;，&lt;a href=&quot;https://pytorch.org/&quot;>; pytorch &lt;/a>;和&lt;a href=&quot;https://github.com/google/jax&quot;>; jax &lt;/a>;。 &lt;/li>; &lt;li>;使用&lt;a href=&quot;https://huggingface.co/spaces/mlcommons/croissant-editor&quot;>; Croissant Editor ui &lt;/a>;（&lt;a href =“ https） ：//github.com/mlcommons/croissant/tree/main/editor“>; github &lt;/a>;）。 &lt;/li>; &lt;/ul>; &lt;p>;要发布羊角面包数据集，用户可以：&lt;/p>; &lt;ul>; &lt;li>;使用&lt;a href =“ https://huggingface.co/spaces/mlcommons/croissant -Editor“>;牛角编辑UI &lt;/a>;（&lt;a href=&quot;https://github.com/mlcommons/croissant/croissant/croissant/tree/main/editor&quot;>; github &lt;/a>;）生成大部分杂色元数据通过分析用户提供的数据并填充重要的元数据字段（例如RAI属性）来自动自动。 &lt;/li>; &lt;li>;作为其数据集网页的一部分发布羊角面信息，以使其可发现和重复使用。 &lt;/li>; &lt;li>;在支持羊角面包的一个存储库中发布他们的数据，例如Kaggle，Huggingface和OpenML，并自动生成羊角面包元数据。 &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; We are excited about Croissant&#39;s potential to help ML从业者，但是使这种格式真正有用需要社区的支持。我们鼓励数据集创建者考虑提供羊角面包元数据。我们鼓励托管数据集的平台提供羊角面包文件，以便在数据集网页中下载并嵌入羊角面包元数据，以便可以通过数据集搜索引擎发现它们。帮助用户使用ML数据集的工具，例如标签或数据分析工具，也应考虑支持羊角面包数据集。我们可以一起减轻数据开发负担，并实现ML研发的更丰富的生态系统。 &lt;/p>; &lt;p>;我们鼓励社区参加&lt;a href=&quot;http://mlcommons.org/croissant&quot;>;加入我们&lt;/a>;为这项工作做出了贡献。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>; benspectledgments &lt;/h2>; &lt;p>; &lt;p>; &lt;em>; Croissant由&lt;a a href =“ https开发：//datasetsearch.research.google.com/“>; dataset search &lt;/a>;，&lt;a href=&quot;https://www.kaggle.com/&quot;>; kaggle &lt;/a>;和&lt;a href =“ https：https：https： //www.tensorflow.org/datasets&quot;>;TensorFlow Datasets&lt;/a>; teams from Google, as part of an &lt;a href=&quot;http://mlcommons.org&quot;>;MLCommons&lt;/a>; community working group, which also包括来自这些组织的贡献者：拜耳，Ctuning Foundation，Dans-knaw，Dotphoton，Harvard，Husgaft，Hugging Face，Kings College London，List，Meta，NASA，NASA，NASA，NASA，NASA，北卡罗来纳州立大学，开放数据研究所，加泰罗尼亚大学开放大学，Sage Bionetworks和Sage Bionetworks和Sage Bionetworks和tu eindhoven。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/8393293208018757284/comments/comments/comments/comments/default/default/default” =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/03/croissant-metadata-format-format-for-ml-ml-ml-ready.html#comment-comment-form” rel =“ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/posts/default/839329329329329320801875757284应用程序/atom+xml“/>; &lt;link href =” http://www.blogger.com/feeds/847492633145202626/posts/posts/posts/default/8393293293293208018757284 link href =“ http://blog.research.google/2024/03/croissant-metadata-format-format-for-ml-ml-ready.html” rel =“替代” title =“ croissant：obsoissant：ml-Ready DataSets的元数据格式“ type =” text/html”/>; &lt;under>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777775266161 &lt;/uri>; &lt;/email>; &lt;gd：Image Height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” 。 -T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyAAN_LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r5r_uAU2P4srnP/s72-c/CroissantHero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; THR：THR>; 0 &lt;/thr：Total>; &lt;/entry>; &lt;entry>; &lt;ID>;标签：Blogger.com，1999：Blog-8474926331452026626.POST-POST-2754526782497247497 &lt;/id>; ：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-03-05T08：40：45.490-08：00 &lt;/updated>; &lt;category scheme =&#39; term =“会议”>; &lt;/category>; &lt;类别scheme =“ http://www.blogger.com/atom/ns#” term =“ conferences”>; &lt;/caterory>; &lt;category>; &lt;category scheme =“ http：// http：// wwwww .blogger.com/atom/ns＃“ term =“ physics”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” >; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“量子计算”>; &lt;/category>; &lt;/cattory>; &lt;title type =“ text”>; google at aps 2024 &lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由凯特·韦伯（Kate Weber）和香农·莱昂（Shannon Leon）发表，Google Research，Quantum AI Team &lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s1600/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>;今天&lt;a href=&quot;https://www.aps.org/meetings/meeting/meeting.cfm?name=mar24&quot;>; 2024 3月会议&lt;/a>; /www.aps.org/&quot;>;美国物理社会&lt;/a>;（APS）在明尼苏达州明尼阿波利斯（Minneapolis）开幕。 APS 2024跨物理和相关领域的主题大会汇集了研究人员，学生和行业专业人员分享他们的发现并建立合作伙伴关系，以实现与物理相关科学和技术的基本进步。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;今年，Google在APS上拥有强大的影响力，由Google &lt;a href =“ https://quantumai.google/”托管的展位。 >; Quantum ai &lt;/a>;团队，在整个会议期间进行了50多次谈判，并参加会议组织活动，特殊会议和活动。亲自参加APS 2024？快来访问Google的量子AI展位，以了解有关我们为解决该领域最有趣的挑战所做的激动人心的工作的更多信息。 &lt;！ - 访问&lt;a href =“ https://twitter.com/googleai”>; @googleai &lt;/a>; x（twitter）帐户以了解有关Google Booth Activity（例如，demos and q＆amp; a sessions） .-->; &lt;/p>; &lt;p>; You can learn more about the latest cutting edge work we are presenting at the conference along with our schedule of booth events below (Googlers listed in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; >;会议椅包括：&lt;strong>; Aaron Szasz &lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;展位活动&lt; /h2>; &lt;div style =“边距左：20px;”>; &lt;p>; &lt;em>;此时间表可能会更改。请访问Google Quantum AI展位以获取更多信息。&lt;/em>; &lt;/p>; &lt;p>;崩溃：一种可视化QEC电路的原型交互式工具&lt;br />;主持人：&lt;strong>; Matt McEwen &lt;/strong>; &lt;br/ >; Tue, Mar 5 | CST上午11:00 &lt; /p>; &lt;p>; Qualtran：一个开源库，用于有效的资源估计容宽算法&lt;br />;主持人：&lt;strong>; tanuj khattar &lt; /strong>; &lt;br />; | 2:30 pm CST &lt; /p>; &lt;p>; Qualtran：一个开源库，用于有效资源估计容宽算法的资源&lt;br />;主持人：&lt;strong>; tanuj khattar &lt; /strong>; &lt;br />; | 11:00 AM CST &lt; /p>; &lt;p>; $ 5M Xprize /Google Quantum AI竞赛，以加速量子应用Q＆amp; a &lt;br />;主持人：&lt;strong>; ryan babbush &lt; /strong>; &lt;br />; | 11:00 AM CST &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;谈话&lt;/h2>; &lt;h3>;星期一&lt;/h3>; >; &lt;div style =“ margin-left：20px;”>; &lt;p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/a45.1&quot;>;从少数少数人身上认识高度enterge单量测量&lt;/a>; &lt;br />;主持人：&lt;strong>; hsin-yuan huang &lt; /strong>; &lt;br />;作者：&lt;strong>; hsin-yuan huang &lt; /strong>; &lt;br />; &lt;br />; &lt;br />; &lt;em>; A45：机器学习量子物理学的新边界&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.ops.org/meeting/mar24/session/session/a51.2&quot;>;用超导码头的模拟量子模拟&lt;/a>; &lt;br />;主持人：&lt;strong>; trond andersen &lt;/strong>; &lt;br />;作者：&lt;strong>; trond i andersen &lt;/strong>;，&lt;strong>; xiao mi &lt;/strong >;，&lt;strong>; Amir H Karamlou &lt;/strong>;，&lt;strong>; Nikita Astrakhantsev &lt;/strong>;，&lt;strong>; Andrey Klots &lt;/strong>;，&lt;strong>; Julia Berndtsson &lt;/strong>; strong>;，&lt;strong>; dmitry abanin &lt;/strong>;，&lt;strong>; lev b ioffe &lt;/strong>;，&lt;strong>; yu chen &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>;，&lt;strong>; pedram roushan &lt; /strong>; &lt;br />; &lt;em>;会话A51：嘈杂量子硬件上的应用程序i &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.ops.org/meeting/mar24/sessision /B50.6&quot;>;在表面代码电路上下文中的电路错误&lt;/a>; &lt;br />;主持人：&lt;strong>; dripto m debroy &lt;/strong>; &lt;br />;作者：&lt;strong>; dripto m debroy &lt;/strong &lt;/strong &lt;/strong &lt;/strong >;，&lt;strong>; Jonathan A Gross &lt;/strong>;，&lt;strong>;éliegenois &lt;/strong>;，&lt;strong>; Zhang jiang &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session b50：用qCVV技术表征噪声&lt;/em em em em em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/b51.6&quot;>;量子计算惯性融合目标设计的停止功率量量经典算法的限制&lt;/a>; &lt;br />;主持人：Andrew D. Baczewski &lt;br />;作者：&lt;strong>; Nicholas C. Rubin &lt; /strong>;，Dominic W. Berry，Alina Kononov，&lt;strong>; fionn D. Malone &lt;/strong>;，&lt;strong>; Tanuj Khattar &lt;/strong>;，Alec White，&lt;strong>; Joonho Lee &lt;/strong>;，&lt;strong>; Hartmut Neven &lt;/strong>;，&lt;strong>; Ryan Babbush &lt;/strong>;，Andrew D. Baczewski &lt;br />; &lt;em>;Session B51: Heterogeneous Design for Quantum Applications&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.12352.pdf&quot;>;Link to Paper&lt; /a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B51.7&quot;>;Quantum computation of stopping power for inertial fusion target design II: Physics overview以及经典算法的极限&lt;/a>; &lt;br />;主持人：&lt;strong>; nicholas C. rubin &lt;/strong>; &lt;br />;作者：&lt;strong>; nicholas C. Rubin &lt;/strong>;，Dominic W. Berry，Dominic W. Berry， Alina Kononov，&lt;strong>; Fionn D. Malone &lt;/strong>;，&lt;strong>; Tanuj Khattar &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; Joonho Lee &lt;/strong>;，&lt;strong>; hartmut neven neven &lt;/strong>;，&lt;strong &lt;strong >; Ryan Babbush &lt;/strong>;，Andrew D. Baczewski &lt;br />; &lt;em>; session B51：量子应用的异质设计&lt;/em>; &lt;br />; &lt;br />; 2308.12352.pdf“>;链接到纸张&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/b56.4&quot;>; nisq to Fart bolance &lt;/a>; &lt;br />;主持人：&lt;strong>; sabrina s hong &lt; /strong>; &lt;br />;作者：&lt;strong>; sabrina s hong &lt; /strong>; &lt;br />; &lt;br />; From NISQ to Fault Tolerance&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B31.9&quot;>;Measurement and feedforward induced entanglement negativity transition&lt; /a>; &lt;br />; Presenter: &lt;strong>;Ramis Movassagh&lt;/strong>; &lt;br />; Authors: Alireza Seif, Yu-Xin Wang,&lt;strong>; Ramis Movassagh&lt;/strong>;, Aashish A. Clerk &lt;br />; &lt;em>;Session B31: Measurement Induced Criticality in Many-Body Systems&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2310.18305.pdf&quot;>;Link to Paper&lt;/a>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B52.9&quot;>;Effective quantum volume, fidelity and computational cost of noisy quantum processing experiments&lt;/a>; &lt; br />; Presenter: &lt;strong>;Salvatore Mandra&lt;/strong>; &lt;br />; Authors: &lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;, &lt;strong>;Sergei V Isakov&lt;/strong>;, &lt;strong>;Salvatore Mandra&lt;/strong>; , &lt;strong>;Benjamin Villalonga&lt;/strong>;, &lt;strong>;X. Mi&lt;/strong>;, &lt;strong>;Sergio Boixo&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>; &lt;br />; &lt;em>;Session B52: Quantum Algorithms and Complexity&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2306.15970.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/D60.4&quot;>;Accurate thermodynamic tables for solids using Machine Learning Interaction Potentials and Covariance of Atomic Positions&lt;/a>; &lt;br />; Presenter: Mgcini K Phuthi &lt;br />; Authors: Mgcini K Phuthi, Yang Huang, Michael Widom, &lt;strong>;Ekin D Cubuk&lt;/strong>;, Venkat Viswanathan &lt;br />; &lt;em>;Session D60: Machine Learning of Molecules and Materials: Chemical Space and Dynamics&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Tuesday&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/F50.4&quot;>;IN-Situ Pulse Envelope Characterization Technique (INSPECT)&lt;/a>; &lt;br />; Presenter: &lt;strong>;Zhang Jiang&lt;/strong>; &lt;br />; Authors: &lt;strong>;Zhang Jiang&lt;/strong>;, &lt;strong>;Jonathan A Gross&lt;/strong>;, &lt;strong>;Élie Genois&lt;/strong>; &lt;br />; &lt;em>;Session F50: Advanced Randomized Benchmarking and Gate Calibration&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/F50.11&quot;>;Characterizing two-qubit gates with dynamical decoupling&lt;/a>; &lt;br />; Presenter: &lt;strong>;Jonathan A Gross&lt;/strong>; &lt;br />; Authors: &lt;strong>;Jonathan A Gross&lt;/strong>;, &lt;strong>;Zhang Jiang&lt;/strong>;, &lt;strong>;Élie Genois, Dripto M Debroy&lt;/strong>;, Ze-Pei Cian*, &lt;strong>;Wojciech Mruczkiewicz&lt;/strong>; &lt;br />; &lt;em>;Session F50: Advanced Randomized Benchmarking and Gate Calibration&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/EE01.2&quot;>;Statistical physics of regression with quadratic models&lt;/a>; &lt;br />; Presenter: Blake Bordelon &lt;br />; Authors: Blake Bordelon, Cengiz Pehlevan, &lt;strong>;Yasaman Bahri&lt;/strong>; &lt;br />; &lt;em>;Session EE01: V: Statistical and Nonlinear Physics II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G51.2&quot;>;Improved state preparation for first-quantized simulation of electronic structure&lt;/a>; &lt;br />; Presenter: &lt;strong>;William J Huggins&lt;/strong>; &lt;br />; Authors: &lt;strong>;William J Huggins&lt;/strong>;, &lt;strong>;Oskar Leimkuhler&lt;/strong>;, &lt;strong>;Torin F Stetina&lt;/strong>;, &lt;strong>;Birgitta Whaley&lt;/strong>; &lt;br />; &lt;em>;Session G51: Hamiltonian Simulation&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G30.2&quot;>;Controlling large superconducting quantum processors&lt;/a>; &lt;br />; Presenter: &lt;strong>;Paul V. Klimov&lt;/strong>; &lt;br />; Authors: &lt;strong>;Paul V. Klimov&lt;/strong>;, &lt;strong>;Andreas Bengtsson&lt;/strong>;, &lt;strong>;Chris Quintana&lt;/strong>;, &lt;strong>;Alexandre Bourassa&lt;/strong>;, &lt;strong>;Sabrina Hong&lt;/strong>;, &lt;strong>;Andrew Dunsworth&lt;/strong>;, &lt;strong>;Kevin J. Satzinger&lt;/strong>;, &lt;strong>;William P. Livingston&lt;/strong>;, &lt;strong>;Volodymyr Sivak&lt;/strong>;, &lt;strong>;Murphy Y. Niu&lt;/strong>;, &lt;strong>;Trond I. Andersen&lt;/strong>;, &lt;strong>;Yaxing Zhang&lt;/strong>;, &lt;strong>;Desmond Chik&lt;/strong>;, &lt;strong>;Zijun Chen&lt;/strong>;, &lt;strong>;Charles Neill&lt;/strong>;, &lt;strong>;Catherine Erickson&lt;/strong>;, &lt;strong>;Alejandro Grajales Dau&lt;/strong>;, &lt;strong>;Anthony Megrant&lt;/strong>;, &lt;strong>;Pedram Roushan&lt;/strong>;, &lt;strong>;Alexander N. Korotkov&lt;/strong>;, &lt;strong>;Julian Kelly&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>;, &lt;strong>;Yu Chen&lt;/strong>;, &lt;strong>;Hartmut Neven&lt;/strong>; &lt;br />; &lt;em>;Session G30: Commercial Applications of Quantum Computing&lt;/em>;&lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.02321.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G50.5&quot;>;Gaussian boson sampling: Determining quantum advantage&lt;/a>; &lt;br />; Presenter: Peter D Drummond &lt;br />; Authors: Peter D Drummond, Alex Dellios, Ned Goodman, Margaret D Reid, &lt;strong>;Ben Villalonga&lt;/strong>; &lt;br />; &lt;em>;Session G50: Quantum Characterization, Verification, and Validation II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G50.8&quot;>;Attention to complexity III: learning the complexity of random quantum circuit states&lt;/a>; &lt;br />; Presenter: Hyejin Kim &lt;br />; Authors: Hyejin Kim, Yiqing Zhou, Yichen Xu, Chao Wan, Jin Zhou, &lt;strong>;Yuri D Lensky&lt;/strong>;, Jesse Hoke, &lt;strong>;Pedram Roushan&lt;/strong>;, Kilian Q Weinberger, Eun-Ah Kim &lt;br />; &lt;em>;Session G50: Quantum Characterization, Verification, and Validation II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/K48.10&quot;>;Balanced coupling in superconducting circuits&lt;/a>; &lt;br />; Presenter: &lt;strong>;Daniel T Sank&lt;/strong>; &lt;br />; Authors: &lt;strong>;Daniel T Sank&lt;/strong>;, &lt;strong>;Sergei V Isakov&lt;/strong>;, &lt;strong>;Mostafa Khezri&lt;/strong>;, &lt;strong>;Juan Atalaya&lt;/strong>; &lt;br />; &lt;em>;Session K48: Strongly Driven Superconducting Systems&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/K49.12&quot;>;Resource estimation of Fault Tolerant algorithms using Qᴜᴀʟᴛʀᴀɴ&lt;/a>; &lt;br />; Presenter: &lt;strong>;Tanuj Khattar&lt;/strong>; &lt;br />; Author: &lt;strong>;Tanuj Khattar&lt;/strong>;, &lt;b>;Matthew Harrigan&lt;/b>;, &lt;b>;Fionn D. Malone&lt;/b>;, &lt;b>;Nour Yosri&lt;/b>;, &lt;b>;Nicholas C. Rubin&lt;/b>;&lt;br />; &lt;em>;Session K49: Algorithms and Implementations on Near-Term Quantum Computers&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Wednesday&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/M24.1&quot;>;Discovering novel quantum dynamics with superconducting qubits&lt;/a>; &lt;br />; Presenter: &lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; Author: &lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; &lt;em>;Session M24: Analog Quantum Simulations Across Platforms&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/M27.7&quot;>;Deciphering Tumor Heterogeneity in Triple-Negative Breast Cancer: The Crucial Role of Dynamic Cell-Cell and Cell-Matrix Interactions&lt;/a>; &lt;br />; Presenter: Susan Leggett &lt;br />; Authors: Susan Leggett, Ian Wong, Celeste Nelson, Molly Brennan, &lt;strong>;Mohak Patel&lt;/strong>;, Christian Franck, Sophia Martinez, Joe Tien, Lena Gamboa, Thomas Valentin, Amanda Khoo, Evelyn K Williams &lt;br />; &lt;em>;Session M27: Mechanics of Cells and Tissues II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N48.2&quot;>;Toward implementation of protected charge-parity qubits&lt;/a>; &lt;br />; Presenter: Abigail Shearrow &lt;br />; Authors: Abigail Shearrow, Matthew Snyder, Bradley G Cole, Kenneth R Dodge, Yebin Liu, Andrey Klots, &lt;strong>;Lev B Ioffe&lt;/strong>;, Britton L Plourde, Robert McDermott &lt;br />; &lt;em>;Session N48: Unconventional Superconducting Qubits&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N48.3&quot;>;Electronic capacitance in tunnel junctions for protected charge-parity qubits&lt;/a>; &lt;br />; Presenter: Bradley G Cole &lt;br />; Authors: Bradley G Cole, Kenneth R Dodge, Yebin Liu, Abigail Shearrow, Matthew Snyder, &lt;strong>;Andrey Klots&lt;/strong>;, &lt;strong>;Lev B Ioffe&lt;/strong>;, Robert McDermott, BLT Plourde &lt;br />; &lt;em>;Session N48: Unconventional Superconducting Qubits&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N51.7&quot;>;Overcoming leakage in quantum error correction&lt;/a>; &lt;br />; Presenter: &lt;strong>;Kevin C. Miao&lt;/strong>; &lt;br />; Authors: &lt;strong>;Kevin C. Miao&lt;/strong>;, &lt;strong>;Matt McEwen&lt;/strong>;, &lt;strong>;Juan Atalaya&lt;/strong>;, &lt;strong>;Dvir Kafri&lt;/strong>;, &lt;strong>;Leonid P. Pryadko&lt;/strong>;, &lt;strong>;Andreas Bengtsson&lt;/strong>;, &lt;strong>;Alex Opremcak&lt;/strong>;, &lt;strong>;Kevin J. Satzinger&lt;/strong>;, &lt;strong>;Zijun Chen&lt;/strong>;, &lt;strong>;Paul V. Klimov&lt;/strong>;, &lt;strong>;Chris Quintana&lt;/strong>;, &lt;strong>;Rajeev Acharya&lt;/strong>;, &lt;strong>;Kyle Anderson&lt;/strong>;, &lt;strong>;Markus Ansmann&lt;/strong>;, &lt;strong>;Frank Arute&lt;/strong>;, &lt;strong>;Kunal Arya&lt;/strong>;, &lt;strong>;Abraham Asfaw&lt;/strong>;, &lt;strong>;Joseph C. Bardin&lt;/strong>;, &lt;strong>;Alexandre Bourassa&lt;/strong>;, &lt;strong>;Jenna Bovaird&lt;/strong>;, &lt;strong>;Leon Brill&lt;/strong>;, &lt;strong>;Bob B. Buckley&lt;/strong>;, &lt;strong>;David A. Buell&lt;/strong>;, &lt;strong>;Tim Burger&lt;/strong>;, &lt;strong>;Brian Burkett&lt;/strong>;, &lt;strong>;Nicholas Bushnell&lt;/strong>;, &lt;strong>;Juan Campero&lt;/strong>;, &lt;strong>;Ben Chiaro&lt;/strong>;, &lt;strong>;Roberto Collins&lt;/strong>;, &lt;strong>;Paul Conner&lt;/strong>;, &lt;strong>;Alexander L. Crook&lt;/strong>;, &lt;strong>;Ben Curtin&lt;/strong>;, &lt;strong>;Dripto M. Debroy&lt;/strong>;, &lt;strong>;Sean Demura&lt;/strong>;, &lt;strong>;Andrew Dunsworth&lt;/strong>;, &lt;strong>;Catherine Erickson&lt;/strong>;, &lt;strong>;Reza Fatemi&lt;/strong>;, &lt;strong>;Vinicius S. Ferreira&lt;/strong>;, &lt;strong>;Leslie Flores Burgos&lt;/strong>;, &lt;strong>;Ebrahim Forati&lt;/strong>;, &lt;strong>;Austin G. Fowler&lt;/strong>;, &lt;strong>;Brooks Foxen&lt;/strong>;, &lt;strong>;Gonzalo Garcia&lt;/strong>;, &lt;strong>;William Giang&lt;/strong>;, &lt;strong>;Craig Gidney&lt;/strong>;, &lt;strong>;Marissa Giustina&lt;/strong>;, &lt;strong>;Raja Gosula&lt;/strong>;, &lt;strong>;Alejandro Grajales Dau&lt;/strong>;, &lt;strong>;Jonathan A. Gross&lt;/strong>;, &lt;strong>;Michael C. Hamilton&lt;/strong>;, &lt;strong>;Sean D. Harrington&lt;/strong>;, &lt;strong>;Paula Heu&lt;/strong>;, &lt;strong>;Jeremy Hilton&lt;/strong>;, &lt;strong>;Markus R. Hoffmann&lt;/strong>;, &lt;strong>;Sabrina Hong&lt;/strong>;, &lt;strong>;Trent Huang&lt;/strong>;, &lt;strong>;Ashley Huff&lt;/strong>;, &lt;strong>;Justin Iveland&lt;/strong>;, &lt;strong>;Evan Jeffrey&lt;/strong>;, &lt;strong>;Zhang Jiang&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>;, &lt;strong>;Julian Kelly&lt;/strong>;, &lt;strong>;Seon Kim&lt;/strong>;, &lt;strong>;Fedor Kostritsa&lt;/strong>;, &lt;strong>;John Mark Kreikebaum&lt;/strong>;, &lt;strong>;David Landhuis&lt;/strong>;, &lt;strong>;Pavel Laptev&lt;/strong>;, &lt;strong>;Lily Laws&lt;/strong>;, &lt;strong>;Kenny Lee&lt;/strong>;, &lt;strong>;Brian J. Lester&lt;/strong>;, &lt;strong>;Alexander T. Lill&lt;/strong>;, &lt;strong>;Wayne Liu&lt;/strong>;, &lt;strong>;Aditya Locharla&lt;/strong>;, &lt;strong>;Erik Lucero&lt;/strong>;, &lt;strong>;Steven Martin&lt;/strong>;, &lt;strong>;Anthony Megrant&lt;/strong>;, &lt;strong>;Xiao Mi&lt;/strong>;, &lt;strong>;Shirin Montazeri&lt;/strong>;, &lt;strong>;Alexis Morvan&lt;/strong>;, &lt;strong>;Ofer Naaman&lt;/strong>;, &lt;strong>;Matthew Neeley&lt;/strong>;, &lt;strong>;Charles Neill&lt;/strong>;, &lt;strong>;Ani Nersisyan&lt;/strong>;, &lt;strong>;Michael Newman&lt;/strong>;, &lt;strong>;Jiun How Ng&lt;/strong>;, &lt;strong>;Anthony Nguyen&lt;/strong>;, &lt;strong>;Murray Nguyen&lt;/strong>;, &lt;strong>;Rebecca Potter&lt;/strong>;, &lt;strong>;Charles Rocque&lt;/strong>;, &lt;strong>;Pedram Roushan&lt;/strong>;, &lt;strong>;Kannan Sankaragomathi&lt;/strong>;, &lt;strong>;Christopher Schuster&lt;/strong>;, &lt;strong>;Michael J. Shearn&lt;/strong>;, &lt;strong>;Aaron Shorter&lt;/strong>;, &lt;strong>;Noah Shutty&lt;/strong>;, &lt;strong>;Vladimir Shvarts&lt;/strong>;, &lt;strong>;Jindra Skruzny&lt;/strong>;, &lt;strong>;W. Clarke Smith&lt;/strong>;, &lt;strong>;George Sterling&lt;/strong>;, &lt;strong>;Marco Szalay&lt;/strong>;, &lt;strong>;Douglas Thor&lt;/strong>;, &lt;strong>;Alfredo Torres&lt;/strong>;, &lt;strong>;Theodore White&lt;/strong>;, &lt;strong>;Bryan WK Woo&lt;/strong>;, &lt;strong>;Z. Jamie Yao&lt;/strong>;, &lt;strong>;Ping Yeh&lt;/strong>;, &lt;strong>;Juhwan Yoo&lt;/strong>;, &lt;strong>;Grayson Young&lt;/strong>;, &lt;strong>;Adam Zalcman&lt;/strong>;, &lt;strong>;Ningfeng Zhu&lt;/strong>;, &lt;strong>;Nicholas Zobrist&lt;/strong>;, &lt;strong>;Hartmut Neven&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>;, &lt;strong>;Andre Petukhov&lt;/strong>;, &lt;strong>;Alexander N. Korotkov&lt;/strong>;, &lt;strong>;Daniel Sank&lt;/strong>;, &lt;strong>;Yu Chen&lt;/strong>; &lt;br />; &lt;em>;Session N51: Quantum Error Correction Code Performance and Implementation I&lt;/em>; &lt;br />; &lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N51.11&quot;>;Modeling the performance of the surface code with non-uniform error distribution: Part 1&lt;/a>; &lt;br />; Presenter: &lt;strong>;Yuri D Lensky&lt;/strong>; &lt;br />; Authors: &lt;strong>;Yuri D Lensky&lt;/strong>;, &lt;strong>;Volodymyr Sivak&lt;/strong>;, &lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;, &lt;strong>;Igor Aleiner&lt;/strong>; &lt;br />; &lt;em>;Session N51: Quantum Error Correction Code Performance and Implementation I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N51.12&quot;>;Modeling the performance of the surface code with non-uniform error distribution: Part 2&lt;/a>; &lt;br />; Presenter: &lt;strong>;Volodymyr Sivak&lt;/strong>; &lt;br />; Authors: &lt;strong>;Volodymyr Sivak&lt;/strong>;, &lt;strong>;Michael Newman&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>;, &lt;strong>;Henry Schurkus&lt;/strong>;, &lt;strong>;Dvir Kafri&lt;/strong>;, &lt;strong>;Yuri D Lensky&lt;/strong>;, &lt;strong>;Paul Klimov&lt;/strong>;, &lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>; &lt;br />; &lt;em>;Session N51: Quantum Error Correction Code Performance and Implementation I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Q51.7&quot;>;Highly optimized tensor network contractions for the simulation of classically challenging quantum computations&lt;/a>; &lt;br />; Presenter: &lt;strong>;Benjamin Villalonga&lt;/strong>; &lt;br />; Author: &lt;strong>;Benjamin Villalonga&lt;/strong>; &lt;br />; &lt;em>;Session Q51: Co-evolution of Quantum Classical Algorithms&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Q61.7&quot;>;Teaching modern quantum computing concepts using hands-on open-source software at all levels&lt;/a>; &lt;br />; Presenter: &lt;strong>;Abraham Asfaw&lt;/strong>; &lt;br />; Author: &lt;strong>;Abraham Asfaw&lt;/strong>; &lt;br />; &lt;em>;Session Q61: Teaching Quantum Information at All Levels II&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Thursday&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S51.1&quot;>;New circuits and an open source decoder for the color code&lt;/a>; &lt;br />; Presenter: &lt;strong>;Craig Gidney&lt;/strong>; &lt;br />; Authors: &lt;strong>;Craig Gidney&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>; &lt;br />; &lt;em>;Session S51: Quantum Error Correction Code Performance and Implementation II&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2312.08813.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S18.2&quot;>;Performing Hartree-Fock many-body physics calculations with large language models&lt;/a>; &lt;br />; Presenter: &lt;strong>;Eun-Ah Kim&lt;/strong>; &lt;br />; Authors: &lt;strong>;Eun-Ah Kim&lt;/strong>;, Haining Pan, &lt;strong>;Nayantara Mudur&lt;/strong>;, William Taranto,&lt;strong>; Subhashini Venugopalan&lt;/strong>;, &lt;strong>;Yasaman Bahri&lt;/strong>;, &lt;strong>;Michael P Brenner&lt;/strong>; &lt;br />; &lt;em>;Session S18: Data Science, AI and Machine Learning in Physics I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S51.5&quot;>;New methods for reducing resource overhead in the surface code&lt;/a>; &lt;br />; Presenter: &lt;strong>;Michael Newman&lt;/strong>; &lt;br />; Authors: &lt;strong>;Craig M Gidney&lt;/strong>;, &lt;strong>;Michael Newman&lt;/strong>;, &lt;strong>;Peter Brooks&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>; &lt;br />; &lt;em>;Session S51: Quantum Error Correction Code Performance and Implementation II&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2312.04522.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S49.10&quot;>;Challenges and opportunities for applying quantum computers to drug design&lt;/a>; &lt;br />; Presenter: Raffaele Santagati &lt;br />; Authors: Raffaele Santagati, Alan Aspuru-Guzik, &lt;strong>;Ryan Babbush&lt;/strong>;, Matthias Degroote, Leticia Gonzalez, Elica Kyoseva, Nikolaj Moll, Markus Oppel, Robert M. Parrish, &lt;strong>;Nicholas C. Rubin&lt;/strong>;, Michael Streif, Christofer S. Tautermann, Horst Weiss, Nathan Wiebe, Clemens Utschig-Utschig &lt;br />; &lt;em>;Session S49: Advances in Quantum Algorithms for Near-Term Applications&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2301.04114.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/T45.1&quot;>;Dispatches from Google&#39;s hunt for super-quadratic quantum advantage in new applications&lt;/a>; &lt;br />; Presenter: &lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; Author: &lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; &lt;em>;Session T45: Recent Advances in Quantum Algorithms&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/T48.11&quot;>;Qubit as a reflectometer&lt;/a>; &lt;br />; Presenter: &lt;strong>;Yaxing Zhang&lt;/strong>; &lt;br />; Authors: &lt;strong>;Yaxing Zhang&lt;/strong>;, &lt;strong>;Benjamin Chiaro&lt;/strong>; &lt;br />; &lt;em>;Session T48: Superconducting Fabrication, Packaging, &amp;amp; Validation&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W14.3&quot;>;Random-matrix theory of measurement-induced phase transitions in nonlocal Floquet quantum circuits&lt;/a>; &lt;br />; Presenter: Aleksei Khindanov &lt;br />; Authors: Aleksei Khindanov, &lt;strong>;Lara Faoro&lt;/strong>;, &lt;strong>;Lev Ioffe&lt;/strong>;, &lt;strong>;Igor Aleiner&lt;/strong>; &lt;br />; &lt;em>;Session W14: Measurement-Induced Phase Transitions&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W58.5&quot;>;Continuum limit of finite density many-body ground states with MERA&lt;/a>; &lt;br />; Presenter: Subhayan Sahu &lt;br />; Authors: Subhayan Sahu, &lt;strong>;Guifré Vidal&lt;/strong>; &lt;br />; &lt;em>;Session W58: Extreme-Scale Computational Science Discovery in Fluid Dynamics and Related Disciplines II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W50.8&quot;>;Dynamics of magnetization at infinite temperature in a Heisenberg spin chain&lt;/a>; &lt;br />; Presenter: &lt;strong>;Eliott Rosenberg&lt;/strong>; &lt;br />; Authors: &lt;strong>;Eliott Rosenberg&lt;/strong>;, &lt;strong>;Trond Andersen&lt;/strong>;, Rhine Samajdar, &lt;strong>;Andre Petukhov&lt;/strong>;, Jesse Hoke*,&lt;strong>; Dmitry Abanin&lt;/strong>;, &lt;strong>;Andreas Bengtsson&lt;/strong>;, &lt;strong>;Ilya Drozdov&lt;/strong>;, &lt;strong>;Catherine Erickson&lt;/strong>;,&lt;strong>; Paul Klimov&lt;/strong>;, &lt;strong>;Xiao Mi&lt;/strong>;, &lt;strong>;Alexis Morvan&lt;/strong>;, &lt;strong>;Matthew Neeley&lt;/strong>;, &lt;strong>;Charles Neill&lt;/strong>;, &lt;strong>;Rajeev Acharya&lt;/strong>;, &lt;strong>;Richard Allen&lt;/strong>;, &lt;strong>;Kyle Anderson&lt;/strong>;, &lt;strong>;Markus Ansmann&lt;/strong>;, &lt;strong>;Frank Arute&lt;/strong>;, &lt;strong>;Kunal Arya&lt;/strong>;, &lt;strong>;Abraham Asfaw&lt;/strong>;, &lt;strong>;Juan Atalaya&lt;/strong>;, &lt;strong>;Joseph Bardin&lt;/strong>;, &lt;strong>;A. Bilmes&lt;/strong>;, &lt;strong>;Gina Bortoli&lt;/strong>;, &lt;strong>;Alexandre Bourassa&lt;/strong>;, &lt;strong>;Jenna Bovaird&lt;/strong>;, &lt;strong>;Leon Brill&lt;/strong>;, &lt;strong>;Michael Broughton&lt;/strong>;, &lt;strong>;Bob B. Buckley&lt;/strong>;, &lt;strong>;David Buell&lt;/strong>;, &lt;strong>;Tim Burger&lt;/strong>;, &lt;strong>;Brian Burkett&lt;/strong>;, &lt;strong>;Nicholas Bushnell&lt;/strong>;, &lt;strong>;Juan Campero&lt;/strong>;, &lt;strong>;Hung-Shen Chang&lt;/strong>;, &lt;strong>;Zijun Chen&lt;/strong>;, &lt;strong>;Benjamin Chiaro&lt;/strong>;, &lt;strong>;Desmond Chik&lt;/strong>;, &lt;strong>;Josh Cogan&lt;/strong>;, &lt;strong>;Roberto Collins&lt;/strong>;, &lt;strong>;Paul Conner&lt;/strong>;, &lt;strong>;William Courtney&lt;/strong>;, &lt;strong>;Alexander Crook&lt;/strong>;, &lt;strong>;Ben Curtin&lt;/strong>;, &lt;strong>;Dripto Debroy&lt;/strong>;, &lt;strong>;Alexander Del Toro Barba&lt;/strong>;, &lt;strong>;Sean Demura&lt;/strong>;, &lt;strong>;Agustin Di Paolo&lt;/strong>;, &lt;strong>;Andrew Dunsworth&lt;/strong>;, &lt;strong>;Clint Earle&lt;/strong>;, &lt;strong>;E. Farhi&lt;/strong>;, &lt;strong>;Reza Fatemi&lt;/strong>;, &lt;strong>;Vinicius Ferreira&lt;/strong>;, &lt;strong>;Leslie Flores&lt;/strong>;, &lt;strong>;Ebrahim Forati&lt;/strong>;, &lt;strong>;Austin Fowler&lt;/strong>;, &lt;strong>;Brooks Foxen&lt;/strong>;, &lt;strong>;Gonzalo Garcia&lt;/strong>;, &lt;strong>;Élie Genois&lt;/strong>;, &lt;strong>;William Giang&lt;/strong>;, &lt;strong>;Craig Gidney&lt;/strong>;, &lt;strong>;Dar Gilboa&lt;/strong>;, &lt;strong>;Marissa Giustina&lt;/strong>;, &lt;strong>;Raja Gosula&lt;/strong>;, &lt;strong>;Alejandro Grajales Dau&lt;/strong>;, &lt;strong>; Jonathan Gross&lt;/strong>;, &lt;strong>;Steve Habegger&lt;/strong>;, &lt;strong>;Michael Hamilton&lt;/strong>;, &lt;strong>;Monica Hansen&lt;/strong>;, &lt;strong>;Matthew Harrigan&lt;/strong>;, &lt;strong>; Sean Harrington&lt;/strong>;, &lt;strong>;Paula Heu&lt;/strong>;, &lt;strong>;Gordon Hill&lt;/strong>;, &lt;strong>;Markus Hoffmann&lt;/strong>;, &lt;strong>;Sabrina Hong&lt;/strong>;, &lt;strong>; Trent Huang&lt;/strong>;, &lt;strong>;Ashley Huff&lt;/strong>;, &lt;strong>;William Huggins&lt;/strong>;, &lt;strong>;Lev Ioffe&lt;/strong>;, &lt;strong>;Sergei Isakov&lt;/strong>;, &lt;strong>; Justin Iveland&lt;/strong>;, &lt;strong>;Evan Jeffrey&lt;/strong>;, &lt;strong>;Zhang Jiang&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>;, &lt;strong>;Pavol Juhas&lt;/strong>;, &lt;strong>; D . Kafri&lt;/strong>;, &lt;strong>;Tanuj Khattar&lt;/strong>;, &lt;strong>;Mostafa Khezri&lt;/strong>;, &lt;strong>;Mária Kieferová&lt;/strong>;, &lt;strong>;Seon Kim&lt;/strong>;, &lt;strong>;Alexei Kitaev&lt;/strong>;, &lt;strong>;Andrey Klots&lt;/strong>;, &lt;strong>;Alexander Korotkov&lt;/strong>;, &lt;strong>;Fedor Kostritsa&lt;/strong>;, &lt;strong>;John Mark Kreikebaum&lt;/strong>;, &lt;strong>;David Landhuis&lt;/strong>;, &lt;strong>;Pavel Laptev&lt;/strong>;, &lt;strong>;Kim Ming Lau&lt;/strong>;, &lt;strong>;Lily Laws&lt;/strong>;, &lt;strong>;Joonho Lee&lt;/strong>;, &lt;strong>;Kenneth Lee&lt;/strong>;, &lt;strong>;Yuri Lensky&lt;/strong>;, &lt;strong>;Brian Lester&lt;/strong>;, &lt;strong>;Alexander Lill&lt;/strong>;, &lt;strong>;Wayne Liu&lt;/strong>;, &lt;strong>;William P. Livingston&lt;/strong>;, &lt;strong>;A. Locharla&lt;/strong>;, &lt;strong>;Salvatore Mandrà&lt;/strong>;, &lt;strong>;Orion Martin&lt;/strong>;, &lt;strong>;Steven Martin&lt;/strong>;, &lt;strong>;Jarrod McClean&lt;/strong>;, &lt;strong>;Matthew McEwen&lt;/strong>;, &lt;strong>;Seneca Meeks&lt;/strong>;, &lt;strong>;Kevin Miao&lt;/strong>;, &lt;strong>;Amanda Mieszala&lt;/strong>;, &lt;strong>;Shirin Montazeri&lt;/strong>;, &lt;strong>;Ramis Movassagh&lt;/strong>;, &lt;strong>;Wojciech Mruczkiewicz&lt;/strong>;, &lt;strong>;Ani Nersisyan&lt;/strong>;, &lt;strong>;Michael Newman&lt;/strong>;, &lt;strong>;Jiun How Ng&lt;/strong>;, &lt;strong>;Anthony Nguyen&lt;/strong>;, &lt;strong>;Murray Nguyen&lt;/strong>;, &lt;strong>;M. Niu&lt;/strong>;, &lt;strong>;Thomas O&#39;Brien&lt;/strong>;, &lt;strong>;Seun Omonije&lt;/strong>;, &lt;strong>;Alex Opremcak&lt;/strong>;, &lt;strong>;Rebecca Potter&lt;/strong>;, &lt;strong>;Leonid Pryadko&lt;/strong>;, &lt;strong>;Chris Quintana&lt;/strong>;, &lt;strong>;David Rhodes&lt;/strong>;, &lt;strong>;Charles Rocque&lt;/strong>;, &lt;strong>;N. Rubin&lt;/strong>;, &lt;strong>;Negar Saei&lt;/strong>;, &lt;strong>;Daniel Sank&lt;/strong>;, &lt;strong>;Kannan Sankaragomathi&lt;/strong>;, &lt;strong>;Kevin Satzinger&lt;/strong>;, &lt;strong>;Henry Schurkus&lt;/strong>;, &lt;strong>;Christopher Schuster&lt;/strong>;, &lt;strong>;Michael Shearn&lt;/strong>;, &lt;strong>;Aaron Shorter&lt;/strong>;, &lt;strong>;Noah Shutty&lt;/strong>;, &lt;strong>;Vladimir Shvarts&lt;/strong>;, &lt;strong>;Volodymyr Sivak&lt;/strong>;, &lt;strong>;Jindra Skruzny&lt;/strong>;, &lt;strong>;Clarke Smith&lt;/strong>;, &lt;strong>;Rolando Somma&lt;/strong>;, &lt;strong>;George Sterling&lt;/strong>;, &lt;strong>;Doug Strain&lt;/strong>;, &lt;strong>;Marco Szalay&lt;/strong>;, &lt;strong>;Douglas Thor&lt;/strong>;, &lt;strong>;Alfredo Torres&lt;/strong>;, &lt;strong>;Guifre Vidal&lt;/strong>;, &lt;strong>;Benjamin Villalonga&lt;/strong>;, &lt;strong>;Catherine Vollgraff Heidweiller&lt;/strong>;, &lt;strong>;Theodore White&lt;/strong>;, &lt;strong>;Bryan Woo&lt;/strong>;, &lt;strong>;Cheng Xing&lt;/strong>;, &lt;strong>;Jamie Yao&lt;/strong>;, &lt;strong>;Ping Yeh&lt;/strong>;, &lt;strong>;Juhwan Yoo&lt;/strong>;, &lt;strong>;Grayson Young&lt;/strong>;, &lt;strong>;Adam Zalcman&lt;/strong>;, &lt;strong>;Yaxing Zhang&lt;/strong>;, &lt;strong>;Ningfeng Zhu&lt;/strong>;, &lt;strong>;Nicholas Zobrist&lt;/strong>;, &lt;strong>;Hartmut Neven&lt;/strong>;, &lt;strong>;Ryan Babbush&lt;/strong>;, &lt;strong>;Dave Bacon&lt;/strong>;, &lt;strong>;Sergio Boixo&lt;/strong>;, &lt;strong>;Jeremy Hilton&lt;/strong>;, &lt;strong>;Erik Lucero&lt;/strong>;, &lt;strong>;Anthony Megrant&lt;/strong>;, &lt;strong>;Julian Kelly&lt;/strong>;, &lt;strong>;Yu Chen&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>;, Vedika Khemani, Sarang Gopalakrishnan,&lt;strong>; Tomaž Prosen&lt;/strong>;, &lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; &lt;em>;Session W50: Quantum Simulation of Many-Body Physics&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2306.09333.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W50.13&quot;>;The fast multipole method on a quantum computer&lt;/a>; &lt;br />; Presenter: Kianna Wan &lt;br />; Authors: Kianna Wan, Dominic W Berry, &lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; &lt;em>;Session W50: Quantum Simulation of Many-Body Physics&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Friday&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Y43.1&quot;>;The quantum computing industry and protecting national security: what tools will work?&lt;/a>; &lt;br />; Presenter: &lt;strong>;Kate Weber&lt;/strong>; &lt;br />; Author: &lt;strong>;Kate Weber&lt;/strong>; &lt;br />; &lt;em>;Session Y43: Industry, Innovation, and National Security: Finding the Right Balance&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Y46.3&quot;>;Novel charging effects in the fluxonium qubit&lt;/a>; &lt;br />; Presenter: &lt;strong>;Agustin Di Paolo&lt;/strong>; &lt;br />; Authors: &lt;strong>;Agustin Di Paolo&lt;/strong>;, Kyle Serniak, Andrew J Kerman, &lt;strong>;William D Oliver&lt;/strong>; &lt;br />; &lt;em>;Session Y46: Fluxonium-Based Superconducting Quibits&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Z46.3&quot;>;Microwave Engineering of Parametric Interactions in Superconducting Circuits&lt;/a>; &lt;br />; Presenter: &lt;strong>;Ofer Naaman&lt;/strong>; &lt;br />; Author: &lt;strong>;Ofer Naaman&lt;/strong>; &lt;br />; &lt;em>;Session Z46: Broadband Parametric Amplifiers and Circulators&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Z62.3&quot;>;Linear spin wave theory of large magnetic unit cells using the Kernel Polynomial Method&lt;/a>; &lt;br />; Presenter: Harry Lane &lt;br />; Authors: Harry Lane, Hao Zhang, David A Dahlbom, Sam Quinn, &lt;strong>;Rolando D Somma&lt;/strong>;, Martin P Mourigal, Cristian D Batista, Kipton Barros &lt;br />; &lt;em>;Session Z62: Cooperative Phenomena, Theory&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;b>;*&lt;/b>;&lt;/sup>;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2754526782497247497/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/google-at-aps-2024.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2754526782497247497&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2754526782497247497&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/google-at-aps-2024.html&quot; rel=&quot;alternate&quot; title=&quot;Google at APS 2024&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1695264277638670894&lt;/id>;&lt;published>;2024-02-22T12:05:00.000-08:00&lt;/published>;&lt;updated>;2024-02-23T10:07:08.500-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VideoPrism: A foundational visual encoder for video understanding&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Long Zhao, Senior Research Scientist, and Ting Liu, Senior Staff Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s450/VideoPrismSample.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; An astounding number of videos are available on the Web, covering a variety of content from everyday moments people share to historical moments to scientific observations, each of which contains a unique record of the world. The right tools could help researchers analyze these videos, transforming how we understand the world around us. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Videos offer dynamic visual content far more rich than static images, capturing movement, changes, and dynamic relationships between entities. Analyzing this complexity, along with the immense diversity of publicly available video data, demands models that go beyond traditional image understanding. Consequently, many of the approaches that best perform on video understanding still rely on specialized models tailor-made for particular tasks. Recently, there has been exciting progress in this area using video foundation models (ViFMs), such as &lt;a href=&quot;https://arxiv.org/abs/2109.14084&quot;>;VideoCLIP&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2212.03191&quot;>;InternVideo&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2212.04979&quot;>;VideoCoCa&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>;. However, building a ViFM that handles the sheer diversity of video data remains a challenge. &lt;/p>; &lt;p>; With the goal of building a single model for general-purpose video understanding, we introduce “&lt;a href=&quot;https://arxiv.org/abs/2402.13217&quot;>;VideoPrism: A Foundational Visual Encoder for Video Understanding&lt;/a>;”. VideoPrism is a ViFM designed to handle a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering (QA). We propose innovations in both the pre-training data as well as the modeling strategy. We pre-train VideoPrism on a massive and diverse dataset: 36 million high-quality video-text pairs and 582 million video clips with noisy or machine-generated parallel text. Our pre-training approach is designed for this hybrid data, to learn both from video-text pairs and the videos themselves. VideoPrism is incredibly easy to adapt to new video understanding challenges, and achieves state-of-the-art performance using a single frozen model. &lt;/p>;&lt;p>;&lt;/p>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/garyzhao/videoprism-blog/raw/main/teaser.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism is a general-purpose video encoder that enables state-of-the-art results over a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering, by producing video representations from a single frozen model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Pre-training data&lt;/h2>; &lt;p>; A powerful ViFM needs a very large collection of videos on which to train — similar to other foundation models (FMs), such as those for large language models (LLMs). Ideally, we would want the pre-training data to be a representative sample of all the videos in the world. While naturally most of these videos do not have perfect captions or descriptions, even imperfect text can provide useful information about the semantic content of the video. &lt;/p>; &lt;p>; To give our model the best possible starting point, we put together a massive pre-training corpus consisting of several public and private datasets, including &lt;a href=&quot;https://rowanzellers.com/merlot/&quot;>;YT-Temporal-180M&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2307.06942&quot;>;InternVid&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2204.00679&quot;>;VideoCC&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2007.14937&quot;>;WTS-70M&lt;/a>;, etc. This includes 36 million carefully selected videos with high-quality captions, along with an additional 582 million clips with varying levels of noisy text (like auto-generated transcripts). To our knowledge, this is the largest and most diverse video training corpus of its kind. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s1999/image18 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;779&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s16000/image18.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Statistics on the video-text pre-training data. The large variations of the&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2104.14806&quot;>;CLIP similarity scores&lt;/a>;&amp;nbsp;(the higher, the better) demonstrate the diverse caption quality of our pre-training data, which is a byproduct of the various ways used to harvest the text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Two-stage training&lt;/h2>; &lt;p>; The VideoPrism model architecture stems from the standard &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;vision transformer&lt;/a>; (ViT) with a factorized design that sequentially encodes spatial and temporal information following &lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;ViViT&lt;/a>;. Our training approach leverages both the high-quality video-text data and the video data with noisy text mentioned above. To start, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Self-supervised_learning#Contrastive_self-supervised_learning&quot;>;contrastive learning&lt;/a>; (an approach that minimizes the distance between positive video-text pairs while maximizing the distance between negative video-text pairs) to teach our model to match videos with their own text descriptions, including imperfect ones. This builds a foundation for matching semantic language content to visual content. &lt;/p>; &lt;p>; After video-text contrastive training, we leverage the collection of videos without text descriptions. Here, we build on the &lt;a href=&quot;https://arxiv.org/abs/2212.04500&quot;>;masked video modeling framework&lt;/a>; to predict masked patches in a video, with a few improvements. We train the model to predict both the video-level global embedding and token-wise embeddings from the first-stage model to effectively leverage the knowledge acquired in that stage. We then randomly shuffle the predicted tokens to prevent the model from learning shortcuts. &lt;/p>; &lt;p>; What is unique about VideoPrism&#39;s setup is that we use two complementary pre-training signals: text descriptions and the visual content within a video. Text descriptions often focus on what things look like, while the video content provides information about movement and visual dynamics. This enables VideoPrism to excel in tasks that demand an understanding of both appearance and motion. &lt;/p>; &lt;br />; &lt;h2>;Results&lt;/h2>; &lt;p>; We conduct extensive evaluation on VideoPrism across four broad categories of video understanding tasks, including video classification and localization, video-text retrieval, video captioning, question answering, and scientific video understanding. VideoPrism achieves state-of-the-art performance on 30 out of 33 video understanding benchmarks — all with minimal adaptation of a single, frozen model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/s1999 /image20.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot; 1959&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/w628-h640/image20.png&quot; width=&quot;628 &quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism compared to the previous best-performing FMs.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />; &lt;/div>; &lt;h3>;Classification and localization&lt;/h3>; &lt;p>; We evaluate VideoPrism on an existing large-scale video understanding benchmark (&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;VideoGLUE&lt;/a>;) covering classification and localization tasks. We find that (1) VideoPrism outperforms all of the other state-of-the-art FMs, and (2) no other single model consistently came in second place. This tells us that VideoPrism has learned to effectively pack a variety of video signals into one encoder — from semantics at different granularities to appearance and motion cues — and it works well across a variety of video sources. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s1816/image12.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1816&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;VideoPrism outperforms state-of-the-art approaches (including &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;, &lt;a href=&quot; https://arxiv.org/abs/2104.11178&quot;>;VATT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2212.03191&quot;>;InternVideo&lt;/a>;, and &lt;a href=&quot;https ://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>;) on the &lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;video understanding benchmark&lt;/a>;. In this plot, we show the absolute score differences compared with the previous best model to highlight the relative improvements of VideoPrism. On &lt;a href=&quot;http://vuchallenge.org/charades.html&quot;>;Charades&lt;/a>;, &lt;a href=&quot;http://activity-net.org/&quot;>;ActivityNet&lt;/a>;, &lt;a href=&quot;https://research.google.com/ava/&quot;>;AVA&lt;/a>;, and &lt;a href=&quot;https://research.google.com/ava/&quot;>;AVA-K&lt;/a>;, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision&quot;>;mean average precision&lt;/a>; (mAP) as the evaluation metric. On the other datasets, we report top-1 accuracy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Combining with LLMs&lt;/h3>; &lt;p>; We further explore combining VideoPrism with LLMs to unlock its ability to handle various video-language tasks. In particular, when paired with a text encoder (following &lt;a href=&quot;https://arxiv.org/abs/2111.07991&quot;>;LiT&lt;/a>;) or a language decoder (such as &lt;a href=&quot;https://arxiv.org/abs/2305.10403&quot;>;PaLM-2&lt;/a>;), VideoPrism can be utilized for video-text retrieval, video captioning, and video QA tasks. We compare the combined models on a broad and challenging set of vision-language benchmarks. VideoPrism sets the new state of the art on most benchmarks. From the visual results, we find that VideoPrism is capable of understanding complex motions and appearances in videos (eg, the model can recognize the different colors of spinning objects on the window in the visual examples below). These results demonstrate that VideoPrism is strongly compatible with language models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/s1028/VideoPrismResults.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;932&quot; data-original-width=&quot;1028&quot; height=&quot;580&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/w640-h580/VideoPrismResults.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism achieves competitive results compared with state-of-the-art approaches (including &lt;a href=&quot;https:// arxiv.org/abs/2212.04979&quot;>;VideoCoCa&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>; and &lt;a href=&quot;https://arxiv. org/abs/2204.14198&quot;>;Flamingo&lt;/a>;) on multiple video-text retrieval (top) and video captioning and video QA (bottom) benchmarks. We also show the absolute score differences compared with the previous best model to highlight the relative improvements of VideoPrism. We report the Recall@1 on &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video- and-language/&quot;>;MASRVTT&lt;/a>;, &lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>;VATEX&lt;/a>;, and &lt;a href=&quot; https://cs.stanford.edu/people/ranjaykrishna/densevid/&quot;>;ActivityNet&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>;CIDEr score&lt;/a>; on &lt; a href=&quot;https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/&quot;>;MSRVTT- Cap&lt;/a>;, &lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>;VATEX-Cap&lt;/a>;, and &lt;a href=&quot;http:// youcook2.eecs.umich.edu/&quot;>;YouCook2&lt;/a>;, top-1 accuracy on &lt;a href=&quot;https://github.com/xudejing/video-question-answering&quot;>;MSRVTT-QA&lt;/a>; and &lt;a href=&quot;https://github.com/xudejing/video-question-answering&quot;>;MSVD-QA&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/cmp-lg /9406033&quot;>;WUPS index&lt;/a>; on &lt;a href=&quot;https://doc-doc.github.io/docs/nextqa.html&quot;>;NExT-QA&lt;/a>;.&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com /garyzhao/videoprism-blog/raw/main/snowball_water_bottle_drum.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width =&quot;100%&quot;>; &lt;source src=&quot;https://github.com/garyzhao/videoprism-blog/raw/main/spin_roller_skating.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt; video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/garyzhao/videoprism-blog/raw/main/making_ice_cream_ski_lifting.mp4 &quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show qualitative results using VideoPrism with a text encoder for video-text retrieval (first row) and adapted to a language decoder for video QA (second and third row). For video-text retrieval examples, the blue bars indicate the embedding similarities between the videos and the text queries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Scientific applications&lt;/h3>; &lt;p>; Finally, we test VideoPrism on datasets used by scientists across domains, including fields such as ethology, behavioral neuroscience, and ecology. These datasets typically require domain expertise to annotate, for which we leverage existing scientific datasets open-sourced by the community including &lt;a href=&quot;https://data.caltech.edu/records/zrznw-w7386&quot;>;Fly vs. Fly&lt;/a>;, &lt;a href=&quot;https://data.caltech.edu/records/s0vdx-0k302&quot;>;CalMS21&lt;/a>;, &lt;a href=&quot;https://shirleymaxx.github.io/ChimpACT/&quot;>;ChimpACT&lt;/a>;, and &lt;a href=&quot;https://dirtmaxim.github.io/kabr/&quot;>;KABR&lt;/a>;. VideoPrism not only performs exceptionally well, but actually surpasses models designed specifically for those tasks. This suggests tools like VideoPrism have the potential to transform how scientists analyze video data across different fields. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1 -s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/s1200/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200 &quot; height=&quot;397&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/w640-h397/image5.png&quot; width =&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism outperforms the domain experts on various scientific benchmarks 。 We show the absolute score differences to highlight the relative improvements of VideoPrism. We report mean average precision (mAP) for all datasets, except for KABR which uses class-averaged top-1 accuracy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; With VideoPrism, we introduce a powerful and versatile video encoder that sets a new standard for general-purpose video understanding. Our emphasis on both building a massive and varied pre-training dataset and innovative modeling techniques has been validated through our extensive evaluations. Not only does VideoPrism consistently outperform strong baselines, but its unique ability to generalize positions it well for tackling an array of real-world applications. Because of its potential broad use, we are committed to continuing further responsible research in this space, guided by our &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. We hope VideoPrism paves the way for future breakthroughs at the intersection of AI and video analysis, helping to realize the potential of ViFMs across domains such as scientific discovery, education, and healthcare. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This blog post is made on behalf of all the VideoPrism authors: Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, and Boqing Gong 。 We sincerely thank David Hendon for their product management efforts, and Alex Siegman, Ramya Ganeshan, and Victor Gomes for their program and resource management efforts. We also thank Hassan Akbari, Sherry Ben, Yoni Ben-Meshulam, Chun-Te Chu, Sam Clearwater, Yin Cui, Ilya Figotin, Anja Hauth, Sergey Ioffe, Xuhui Jia, Yeqing Li, Lu Jiang, Zu Kim, Dan Kondratyuk, Bill Mark, Arsha Nagrani, Caroline Pantofaru, Sushant Prakash, Cordelia Schmid, Bryan Seybold, Mojtaba Seyedhosseini, Amanda Sadler, Rif A. Saurous, Rachel Stigler, Paul Voigtlaender, Pingmei Xu, Chaochao Yan, Xuan Yang, and Yukun Zhu for the discussions, support, and feedback that greatly contributed to this work. We are grateful to Jay Yagnik, Rahul Sukthankar, and Tomas Izo for their enthusiastic support for this project. Lastly, we thank Tom Small, Jennifer J. Sun, Hao Zhou, Nitesh B. Gundavarapu, Luke Friedman, and Mikhail Sirotenko for the tremendous help with making this blog post.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1695264277638670894/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html&quot; rel=&quot;alternate&quot; title=&quot;VideoPrism: A foundational visual encoder for video understanding&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s72-c/VideoPrismSample.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4343235509909091741&lt;/id>;&lt;published>;2024-02-21T12:15:00.000-08:00&lt;/published>;&lt;updated>;2024-02-21T12:15:36.694-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Gboard&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Advances in private training for production on-device language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zheng Xu, Research Scientist, and Yanxiang Zhang, Software Engineer, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s1600/GBoard%20PrivacyHero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Language models (LMs) trained to predict the next word given input text are the key technology for many applications [&lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;1&lt;/a>;, &lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;2&lt;/a>;]. In &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;amp;hl=en_US&amp;amp;gl=US&quot;>;Gboard&lt;/a>;, LMs are used to improve users&#39; typing experience by supporting features like &lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;next word prediction&lt;/a>; (NWP), &lt;a href=&quot;https:// support.google.com/gboard/answer/7068415&quot;>;Smart Compose&lt;/a>;,&lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>; smart completion&lt;/a>; and &lt; a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;suggestion&lt;/a>;, &lt;a href=&quot;https://support.google.com/gboard/answer/2811346&quot;>;slide to type&lt;/a>;&lt;span style=&quot;text-decoration: underline;&quot;>;,&lt;/span>; and &lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;proofread&lt;/一个>;。 Deploying models on users&#39; devices rather than enterprise servers has advantages like lower latency and better privacy for model usage. While training on-device models directly from user data effectively improves the utility performance for applications such as NWP and &lt;a href=&quot;https://blog.research.google/2021/11/predicting-text-selections-with.html&quot;>;smart text selection&lt;/a>;, protecting the privacy of user data for model training is important. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s1996/image45.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1600&quot; data-original-width=&quot;1996&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s16000/image45.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Gboard features powered by on-device language models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In this blog we discuss how years of research advances now power the private training of Gboard LMs, since the proof-of-concept development of &lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;federated learning&lt;/a>; (FL) in 2017 and formal &lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;differential privacy&lt;/a>; (DP) guarantees in 2022. &lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;FL&lt;/a>; enables mobile phones to collaboratively learn a model while keeping all the training data on device, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;DP&lt;/a>; provides a quantifiable measure of data anonymization. Formally, DP is often characterized by (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) with smaller values representing stronger guarantees. Machine learning (ML) models are considered to have &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;reasonable DP guarantees for ε=10 and strong DP guarantees for ε=1&lt;/a>; when &lt;em>;δ&lt;/em>; is small. &lt;/p>; &lt;p>; As of today, all NWP neural network LMs in Gboard are trained with FL with formal DP guarantees, and all future launches of Gboard LMs trained on user data require DP. These 30+ Gboard on-device LMs are launched in 7+ languages and 15+ countries, and satisfy (&lt;em>;ɛ&lt;/em>;, &lt;em>;δ&lt;/em>;)-DP guarantees of small &lt;em>;δ&lt;/em>; of 10&lt;sup>;-10&lt;/sup>; and ɛ between 0.994 and 13.69. To the best of our knowledge, this is the largest known deployment of user-level DP in production at Google or anywhere, and the first time a strong DP guarantee of &lt;em>;ɛ&lt;/em>; &amp;lt; 1 is announced for models trained directly on user data. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Privacy principles and practices in Gboard&lt;/h2>; &lt;p>; In “&lt;a href=&quot;https ://arxiv.org/abs/2306.14793&quot;>;Private Federated Learning in Gboard&lt;/a>;”, we discussed how different &lt;a href=&quot;https://queue.acm.org/detail.cfm?id=3501293&quot; >;privacy principles&lt;/a>; are currently reflected in production models, including: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Transparency and user control&lt;/em>;: We provide disclosure of what data is used, what purpose it is used for, how it is processed in various channels, and how Gboard users can easily &lt;a href=&quot;https://support.google.com/gboard/answer/12373137&quot;>;configure&lt;/a>; the data usage in learning楷模。 &lt;/li>;&lt;li>;&lt;em>;Data minimization&lt;/em>;: FL immediately aggregates only focused updates that improve a specific model. &lt;a href=&quot;https://eprint.iacr.org/2017/281.pdf&quot;>;Secure aggregation&lt;/a>; (SecAgg) is an encryption method to further guarantee that only aggregated results of the ephemeral updates can be accessed. &lt;/li>;&lt;li>;&lt;em>;Data anonymization&lt;/em>;: DP is applied by the server to prevent models from memorizing the unique information in individual user&#39;s training data. &lt;/li>;&lt;li>;&lt;em>;Auditability and verifiability&lt;/em>;: We have made public the key algorithmic approaches and privacy accounting in open-sourced code (&lt;a href=&quot;https://github.com/tensorflow/federated/blob/main/tensorflow_federated/python/aggregators/differential_privacy.py&quot;>;TFF aggregator&lt;/a>;, &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/dp_query/tree_aggregation_query.py&quot;>;TFP DPQuery&lt;/a>;, &lt;a href=&quot;https://github.com/google-research/federated/blob/master/dp_ftrl/blogpost_supplemental_privacy_accounting.ipynb&quot;>;DP accounting&lt;/a>;, and &lt;a href=&quot;https://github.com/google/federated-compute&quot;>;FL system&lt;/a>;). &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;A brief history&lt;/h3>; &lt;p>; In recent years, FL has become the default method for training &lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;Gboard on-device LMs&lt;/a>; from user data. In 2020, a DP mechanism that &lt;a href=&quot;https://arxiv.org/abs/1710.06963&quot;>;clips and adds noise&lt;/a>; to model updates was used to &lt;a href=&quot;https://arxiv.org/abs/2009.10031&quot;>;prevent memorization&lt;/a>; for training the Spanish LM in Spain, which satisfies finite DP guarantees (&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;Tier 3&lt;/a>; described in “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML“&lt;/a>; guide). In 2022, with the help of the &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-Follow-The-Regularized-Leader (DP-FTRL) algorithm&lt;/a>;, the Spanish LM became the first production neural network trained directly on user data announced with &lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;a formal DP guarantee of (ε=8.9, δ=10&lt;sup>;-10&lt;/sup>;)-DP&lt;/a>; (equivalent to the reported &lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;ρ=0.81&lt;/a>;&lt;/em>; &lt;a href=&quot;https://arxiv.org/abs/1605.02065&quot;>;zero-Concentrated-Differential-Privacy&lt;/a>;), and therefore satisfies &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;reasonable privacy guarantees&lt;/a>; (&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;Tier 2&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Differential privacy by default in federated learning &lt;/h2>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;Federated Learning of Gboard Language Models with Differential Privacy&lt;/a>;”, we announced that all the NWP neural network LMs in Gboard have DP guarantees, and all future launches of Gboard LMs trained on user data require DP guarantees. DP is enabled in FL by applying the following practices: &lt;/p>; &lt;ul>; &lt;li>;Pre-train the model with the &lt;a href=&quot;https://arxiv.org/abs/2010.11934&quot;>;multilingual&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;C4&lt;/a>; dataset. &lt;/li>;&lt;li>;Via simulation experiments on public datasets, find a large DP-noise-to-signal ratio that allows for high utility. Increasing the number of clients contributing to one round of model update improves privacy while keeping the noise ratio fixed for good utility, up to the point the DP target is met, or the maximum allowed by the system and the size of the population. &lt;/li>;&lt;li>;Configure the parameter to restrict the frequency each client can contribute (eg, once every few days) based on computation budget and estimated population in &lt;a href=&quot;https://arxiv.org/abs/1902.01046&quot;>;the FL system&lt;/a>;. &lt;/li>;&lt;li>;Run &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>; training with limits on the magnitude of per-device updates chosen either via &lt;a href=&quot;https://github.com/tensorflow/federated/commit/ee9d08368828ea730662e5e2b3a90e103368b6b6&quot;>;adaptive clipping&lt;/a>;, or fixed based on experience. &lt;/li>; &lt;/ul>; &lt;p>; SecAgg can be additionally applied by adopting the &lt;a href=&quot;https://blog.research.google/2023/03/distributed-differential-privacy-for.html&quot;>;advances in improving computation and communication for scales and sensitivity&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mUlO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N /s1600/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;1600&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mUlO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Federated learning with differential privacy and (SecAgg).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Reporting DP guarantees&lt;/h3>; &lt;p>; The DP guarantees of launched Gboard NWP LMs are visualized in the barplot below 。 The &lt;em>;x&lt;/em>;-axis shows LMs labeled by language-locale and trained on corresponding populations; the &lt;em>;y&lt;/em>;-axis shows the &lt;em>;ε&lt;/em>; value when &lt;em>;δ&lt;/em>; is fixed to a small value of 10&lt;sup>;-10&lt;/sup>; for &lt;a href=&quot;https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf&quot;>;(ε, δ)-DP&lt;/a>; (lower is better). The utility of these models are either significantly better than previous non-neural models in production, or comparable with previous LMs without DP, measured based on user-interactions metrics during A/B testing. For example, by applying the best practices, the DP guarantee of the Spanish model in Spain is improved from &lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;ε=8.9&lt;/a>;&lt;/em>; to &lt;em>;ε&lt;/em>;=5.37. SecAgg is additionally used for training the Spanish model in Spain and English model in the US. More details of the DP guarantees are reported in &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;the appendix &lt;/a>;following the &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;guidelines outlined&lt;/a>; in “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML&lt;/a>;”. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Towards stronger DP guarantees&lt;/h2>; &lt;p>; The &lt;em>;ε&lt;/em>;~10 DP guarantees of many launched LMs are already considered &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;reasonable&lt;/a>; for ML models in practice, while the journey of DP FL in Gboard continues for improving user typing experience while protecting data privacy. We are excited to announce that, for the first time, production LMs of Portuguese in Brazil and Spanish in Latin America are trained and launched with a DP guarantee of &lt;em>;ε&lt;/em>; ≤ 1, which satisfies &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;Tier 1 strong privacy guarantees&lt;/a>;. Specifically, the (&lt;em>;ε&lt;/em>;=0.994, &lt;em>;δ&lt;/em>;=10&lt;sup>;-10&lt;/sup>;)-DP guarantee is achieved by running the advanced &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;Matrix Factorization DP-FTRL&lt;/a>; (MF-DP-FTRL) algorithm, with 12,000+ devices participating in every training round of server model update larger than the &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;common setting of 6500+ devices&lt;/a>;, and a carefully configured policy to restrict each client to at most participate twice in the total 2000 rounds of training in 14 days in the large Portuguese user population of Brazil. Using a similar setting, the es-US Spanish LM was trained in a large population combining multiple countries in Latin America to achieve (&lt;em>;ε&lt;/em>;=0.994, &lt;em>;δ&lt;/em>;=10&lt;sup>;-10&lt;/sup>;)-DP. The &lt;em>;ε&lt;/em>; ≤ 1 es-US model significantly improved the utility in many countries, and launched in Colombia, Ecuador, Guatemala, Mexico, and Venezuela. For the smaller population in Spain, the DP guarantee of es-ES LM is improved from &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;ε=5.37&lt;/a>;&lt;/em>; to &lt;em>;ε&lt;/em>;=3.42 by only replacing &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>; without increasing the number of devices participating every round. More technical details are disclosed in the &lt;a href=&quot;https://colab.sandbox.google.com/github/google-research/federated/blob/master/mf_dpftrl_matrices/privacy_accounting.ipynb&quot;>;colab&lt;/a>; for privacy会计。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s1999/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;709&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DP guarantees for Gboard NWP LMs (the purple bar represents the first es-ES launch of ε=8.9; cyan bars represent privacy improvements for models trained with &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>;; &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;tiers &lt;/a>;are from “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML&lt;/a>;“ guide; en-US* and es-ES* are additionally trained with SecAgg).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Discussion and next steps&lt;/h2>; &lt;p>; Our experience suggests that DP can be achieved in practice through system algorithm co-design on client participation, and that both privacy and utility can be strong when populations are large &lt;em>;and&lt;/em>; a large number of devices&#39; contributions are aggregated. Privacy-utility-computation trade-offs can be improved by &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;using public data&lt;/a>;, the &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;new MF-DP-FTRL algorithm&lt;/a>;, &lt;a href=&quot;https://github.com/google/differential-privacy&quot;>;and tightening accounting&lt;/a>;. With these techniques, a strong DP guarantee of &lt;em>;ε&lt;/em>; ≤ 1 is possible but still challenging. Active research on empirical privacy auditing [&lt;a href=&quot;https://arxiv.org/abs/2302.03098&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2305.08846&quot;>;2&lt;/a>;] suggests that DP models are potentially more private than the worst-case DP guarantees imply. While we keep pushing the frontier of algorithms, which dimension of privacy-utility-computation should be prioritized? &lt;/p>; &lt;p>; We are actively working on all privacy aspects of ML, including extending DP-FTRL to &lt;a href=&quot;https://blog.research.google/2023/03/distributed-differential-privacy-for.html&quot;>;distributed DP&lt;/a>; and improving &lt;a href=&quot;https://arxiv.org/abs/2306.14793&quot;>;auditability and verifiability&lt;/a>;. &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot;>;Trusted Execution Environment&lt;/a>; opens the opportunity for substantially increasing the model size with verifiable privacy. The recent &lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;breakthrough in large LMs&lt;/a>; (LLMs) motivates us to &lt;a href=&quot;https://arxiv.org/abs/2305.12132&quot;>;rethink&lt;/a>; the usage of &lt;a href=&quot;https://arxiv.org/abs/2212.06470&quot;>;public&lt;/a>; information in private training and more future interactions between LLMs, on-device LMs, and Gboard production. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The authors would like to thank Peter Kairouz, Brendan McMahan, and Daniel Ramage for their early feedback on the blog post itself, Shaofeng Li and Tom Small for helping with the animated figures, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The collaborators below directly contribute to the presented results:&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Research and algorithm development: Galen Andrew, Stanislav Chiknavaryan, Christopher A. Choquette-Choo, Arun Ganesh, Peter Kairouz, Ryan McKenna, H. Brendan McMahan, Jesse Rosenstock, Timon Van Overveldt, Keith Rush, Shuang Song, Thomas Steinke, Abhradeep Guha Thakurta, Om Thakkar, and Yuanbo Zhang.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Infrastructure, production and leadership support: Mingqing Chen, Stefan Dierauf, Billy Dou, Hubert Eichner, Zachary Garrett, Jeremy Gillula, Jianpeng Hou, Hui Li, Xu Liu, Wenzhi Mao, Brett McLarnon, Mengchen Pei, Daniel Ramage, Swaroop Ramaswamy, Haicheng Sun, Andreas Terzis, Yun Wang, Shanshan Wu, Yu Xiao, and Shumin Zhai.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4343235509909091741/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4343235509909091741&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4343235509909091741&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;Advances in private training for production on-device language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s72-c/GBoard%20PrivacyHero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5605933033299261025&lt;/id>;&lt;published>;2024-02-14T10:32:00.000-08:00&lt;/published>;&lt;updated>;2024-02-14T10:32:25.557-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Learning the importance of training data under concept drift&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Nishant Jain, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s1600/temporalreweightinghero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The constantly changing nature of the world around us poses a significant challenge for the development of AI models. Often, models are trained on longitudinal data with the hope that the training data used will accurately represent inputs the model may receive in the future. More generally, the default assumption that all training data are equally relevant often breaks in practice. For example, the figure below shows images from the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; nonstationary learning benchmark, and it illustrates how visual features of objects evolve significantly over a 10 year span (a phenomenon we refer to as &lt;em>;slow concept drift&lt;/em>;), posing a challenge for object categorization models. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;662&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Sample images from the CLEAR benchmark. (Adapted from Lin et al&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;.&lt;/a>;)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Alternative approaches, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Online_machine_learning&quot;>;online&lt;/a>; and &lt;a href=&quot;https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning&quot;>;continual learning&lt;/a>;, repeatedly update a model with small amounts of recent data in order to keep it current. This implicitly prioritizes recent data, as the learnings from past data are gradually erased by subsequent updates. However in the real world, different kinds of information lose relevance at different rates, so there are two key issues: 1) By design they focus &lt;em>;exclusively&lt;/em>; on the most recent data and lose any signal from older data that is erased. 2) Contributions from data instances decay &lt;em>;uniformly over time&lt;/em>; irrespective of the contents of the data. &lt;/p>; &lt;p>; In our recent work, “&lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;Instance-Conditional Timescales of Decay for Non-Stationary Learning&lt;/a>;”, we propose to assign each instance an importance score during training in order to maximize model performance on future data. To accomplish this, we employ an auxiliary model that produces these scores using the training instance as well as its age. This model is jointly learned with the primary model. We address both the above challenges and achieve significant gains over other robust learning methods on a range of benchmark datasets for nonstationary learning. For instance, on a &lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;recent large-scale benchmark&lt;/a>; for nonstationary learning (~39M photos over a 10 year period), we show up to 15% relative accuracy gains through learned reweighting of training data. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;The challenge of concept drift for supervised learning&lt;/h2>; &lt;p>; To gain quantitative insight into slow concept drift, we built classifiers on a &lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;recent photo categorization task&lt;/a>;, comprising roughly 39M photographs sourced from social media websites over a 10 year period. We compared offline training, which iterated over all the training data multiple times in random order, and continual training, which iterated multiple times over each month of data in sequential (temporal) order. We measured model accuracy both during the training period and during a subsequent period where both models were frozen, ie, not updated further on new data (shown below). At the end of the training period (left panel, x-axis = 0), both approaches have seen the same amount of data, but show a large performance gap. This is due to &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0079742108605368&quot;>;catastrophic forgetting&lt;/a>;, a problem in continual learning where a model&#39;s knowledge of data from early on in the training sequence is diminished in an uncontrolled manner. On the other hand, forgetting has its advantages — over the test period (shown on the right), the continual trained model degrades much less rapidly than the offline model because it is less dependent on older data. The decay of both models&#39; accuracy in the test period is confirmation that the data is indeed evolving over time, and both models become increasingly less relevant. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s1554/image2.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;616&quot; data-original-width=&quot;1554&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparing offline and continually trained models on the photo classification task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt; div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Time-sensitive reweighting of training data&lt;/h2>; &lt;p>; We design a method combining the benefits of offline learning (the flexibility of effectively reusing all available data) and continual learning (the ability to downplay older data) to address slow concept drift. We build upon offline learning, then add careful control over the influence of past data and an optimization objective, both designed to reduce model decay in the future. &lt;/p>; &lt;p>; Suppose we wish to train a model, &lt;em>;M&lt;/em>;,&lt;em>; &lt;/em>;given some training data collected over time. We propose to also train a helper model that assigns a weight to each point based on its contents and age. This weight scales the contribution from that data point in the training objective for &lt;em>;M&lt;/em>;. The objective of the weights is to improve the performance of &lt;em>;M&lt;/em>; on future data. &lt;/p>; &lt;p>; In &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;our work&lt;/a>;, we describe how the helper model can be &lt;em>;meta-learned, &lt;/em>;ie, learned alongside &lt;em>;M&lt;/em>; in a manner that helps the learning of the model &lt;em>;M&lt;/em>; itself. A key design choice of the helper model is that we separated out instance- and age-related contributions in a factored manner. Specifically, we set the weight by combining contributions from multiple different fixed timescales of decay, and learn an approximate “assignment” of a given instance to its most suited timescales. We find in our experiments that this form of the helper model outperforms many other alternatives we considered, ranging from unconstrained joint functions to a single timescale of decay (exponential or linear), due to its combination of simplicity and expressivity. Full details may be found in the &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Instance weight scoring&lt;/h2>; &lt;p>; The top figure below shows that our learned helper model indeed up-weights more modern-looking objects in the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR object recognition challenge&lt;/a>;; older-looking objects are correspondingly down-weighted. On closer examination (bottom figure below, gradient-based &lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;feature importance&lt;/a>; assessment), we see that the helper model focuses on the primary object within the image, as opposed to, eg, background features that may spuriously be correlated with instance age. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s1999/image1.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;499&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Sample images from the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; benchmark (camera &amp;amp; computer categories) assigned the highest and lowest weights respectively by our helper model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Feature importance analysis of our helper model on sample images from the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; benchmark.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Gains on large-scale data &lt;/h3>; &lt;p>; We first study the large-scale &lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;photo categorization task&lt;/a>; (PCAT) on the &lt;a href=&quot;https://arxiv.org/abs/1503.01817&quot;>;YFCC100M dataset&lt;/a>; discussed earlier, using the first five years of data for training and the next five years as test data. Our method (shown in red below) improves substantially over the no-reweighting baseline (black) as well as many other robust learning techniques. Interestingly, our method deliberately trades off accuracy on the distant past (training data unlikely to reoccur in the future) in exchange for marked improvements in the test period. Also, as desired, our method degrades less than other baselines in the test period. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s800/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text- align: center;&quot;>;Comparison of our method and relevant baselines on the PCAT dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h3>;Broad applicability&lt;/h3>; &lt;p>; We validated our findings on a wide range of nonstationary learning challenge datasets sourced from the academic literature (see &lt;a href=&quot;https://arxiv .org/abs/2108.09020&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;2&lt;/a>;, &lt;a href=&quot;https://arxiv.org /abs/2211.14238&quot;>;3&lt;/a>;, &lt;a href=&quot;https://proceedings.mlr.press/v206/awasthi23b/awasthi23b.pdf&quot;>;4&lt;/a>; for details) that spans data sources and modalities (photos, satellite images, social media text, medical records, sensor readings, tabular data) and sizes (ranging from 10k to 39M instances). We report significant gains in the test period when compared to the nearest published benchmark method for each dataset (shown below). Note that the previous best-known method may be different for each dataset. These results showcase the broad applicability of our approach. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s765/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;552&quot; data-original-width=&quot;765&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class= &quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance gain of our method on a variety of tasks studying natural concept drift. Our reported gains are over the previous best-known method for each dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Extensions to continual learning&lt;/h3>; &lt;p>; Finally, we consider an interesting extension of our work. The work above described how offline learning can be extended to handle concept drift using ideas inspired by continual learning. However, sometimes offline learning is infeasible — for example, if the amount of training data available is too large to maintain or process. We adapted our approach to continual learning in a straightforward manner by applying temporal reweighting &lt;em>;within the context of &lt;/em>;each bucket of data being used to sequentially update the model. This proposal still retains some limitations of continual learning, eg, model updates are performed only on most-recent data, and all optimization decisions (including our reweighting) are only made over that data. Nevertheless, our approach consistently beats regular continual learning as well as a wide range of other continual learning algorithms on the photo categorization benchmark (see below). Since our approach is complementary to the ideas in many baselines compared here, we anticipate even larger gains when combined with them. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s800/image6.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Results of our method adapted to continual learning, compared to the latest baselines.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We addressed the challenge of data drift in learning by combining the strengths of previous approaches — offline learning with its effective reuse of data, and continual learning with its emphasis on more recent data. We hope that our work helps improve model robustness to concept drift in practice, and generates increased interest and new ideas in addressing the ubiquitous problem of slow concept drift. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank Mike Mozer for many interesting discussions in the early phase of this work, as well as very helpful advice and feedback during its development.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5605933033299261025/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/learning-importance-of-training-data.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5605933033299261025&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5605933033299261025&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/learning-importance-of-training-data.html&quot; rel=&quot;alternate&quot; title=&quot;Learning the importance of training data under concept drift&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s72-c/temporalreweightinghero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5933365460125094774&lt;/id>;&lt;published>;2024-02-13T14:11:00.000-08:00&lt;/published>;&lt;updated>;2024-02-13T14:11:49.258-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;DP-Auditorium: A flexible library for auditing differential privacy&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Mónica Ribero Díaz, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;Differential privacy&lt;/a>; (DP) is a property of randomized mechanisms that limit the influence of any individual user&#39;s information while processing and analyzing data. DP offers a robust solution to address growing concerns about data protection, enabling technologies &lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;across&lt;/a>; &lt;a href=&quot;https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf&quot;>;industries&lt;/a>; and government applications (eg, &lt;a href=&quot;https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance/differential-privacy.html&quot;>;the US census&lt;/a>;) without compromising individual user identities. As its adoption increases, it&#39;s important to identify the potential risks of developing mechanisms with faulty implementations. Researchers have recently found errors in the mathematical proofs of private mechanisms, and their implementations. For example, &lt;a href=&quot;https://arxiv.org/pdf/1603.01699.pdf&quot;>;researchers compared&lt;/a>; six sparse vector technique (SVT) variations and found that only two of the six actually met the asserted privacy保证。 Even when mathematical proofs are correct, the code implementing the mechanism is vulnerable to human error. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, practical and efficient DP auditing is challenging primarily due to the inherent randomness of the mechanisms and the probabilistic nature of the tested guarantees. In addition, a range of guarantee types exist, (eg, &lt;a href=&quot;https://dl.acm.org/doi/10.1007/11681878_14&quot;>;pure DP&lt;/a>;, &lt;a href=&quot;https://link.springer.com/chapter/10.1007/11761679_29&quot;>;approximate DP&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/pdf/1603.01887.pdf&quot;>;concentrated DP&lt;/a>;), and this diversity contributes to the complexity of formulating the auditing problem. Further, debugging mathematical proofs and code bases is an intractable task given the volume of proposed mechanisms. While &lt;em>;ad hoc&lt;/em>; testing techniques exist under specific assumptions of mechanisms, few efforts have been made to develop an extensible tool for testing DP mechanisms. &lt;/p>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>;DP-Auditorium: A Large Scale Library for Auditing Differential Privacy&lt;/a>;”, we introduce an &lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/python/dp_auditorium&quot;>;open source library&lt;/a>; for auditing DP guarantees with only black-box access to a mechanism (ie, without any knowledge of the mechanism&#39;s internal properties). DP-Auditorium is implemented in Python and provides a flexible interface that allows contributions to continuously improve its testing capabilities. We also introduce new testing algorithms that perform divergence optimization over function spaces for Rényi DP, pure DP, and approximate DP. We demonstrate that DP-Auditorium can efficiently identify DP guarantee violations, and suggest which tests are most suitable for detecting particular bugs under various privacy guarantees. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP guarantees&lt;/h2>; &lt;p>; The output of a DP mechanism is a sample drawn from a probability distribution (&lt;em>;M&lt;/em>; (&lt;em>;D&lt;/em>;)) that satisfies a mathematical property ensuring the privacy of user data. A DP guarantee is thus tightly related to properties between pairs of probability distributions. A mechanism is differentially private if the probability distributions determined by &lt;i>;M&lt;/i>; on dataset &lt;em>;D&lt;/em>; and a neighboring dataset &lt;em>;D&#39;&lt;/em>;, which differ by only one record, are &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_indistinguishability&quot;>;indistinguishable&lt;/a>;&lt;/em>; under a given divergence metric. &lt;/p>; &lt;p>; For example, the classical &lt;a href=&quot;https://software.imdea.org/~federico/pubs/2013.ICALP.pdf&quot;>;approximate DP&lt;/a>; definition states that a mechanism is approximately DP with parameters (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) if the &lt;a href=&quot;https://arxiv.org/pdf/1508.00335.pdf&quot;>;hockey-stick divergence&lt;/a>; of order &lt;em>;e&lt;sup>;ε&lt;/sup>;&lt;/em>;, between &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>;and &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;), is at most &lt;em>;δ&lt;/em>;. Pure DP is a special instance of approximate DP where &lt;em>;δ = 0&lt;/em>;. Finally, a mechanism is considered &lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>; with parameters (&lt;em>;𝛼&lt;/em>;, &lt;em>;ε)&lt;/em>; if the &lt;a href=&quot;https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy&quot;>;Rényi divergence&lt;/a>; of order &lt;em>;𝛼&lt;/em>;, is at most &lt;em>;ε&lt;/em>; (where &lt;em>;ε&lt;/em>; is a small positive value). In these three definitions, &lt;em>;ε &lt;/em>;is not interchangeable but intuitively conveys the same concept; larger values of &lt;em>;ε&lt;/em>; imply larger divergences between the two distributions or less privacy, since the two distributions are easier to distinguish. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP-Auditorium&lt;/h2>; &lt;p>; DP-Auditorium comprises two main components: property testers and dataset finders. Property testers take samples from a mechanism evaluated on specific datasets as input and aim to identify privacy guarantee violations in the provided datasets. Dataset finders suggest datasets where the privacy guarantee may fail. By combining both components, DP-Auditorium enables (1) automated testing of diverse mechanisms and privacy definitions and, (2) detection of bugs in privacy-preserving mechanisms. We implement various private and non-private mechanisms, including simple mechanisms that compute the mean of records and more complex mechanisms, such as different SVT and &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;gradient descent&lt;/a>; mechanism variants. &lt;/p>; &lt;p>; &lt;strong>;Property testers&lt;/strong>; determine if evidence exists to reject the hypothesis that a given divergence between two probability distributions, &lt;em>;P&lt;/em>; and &lt;em>;Q&lt;/em>;, is bounded by a prespecified budget determined by the DP guarantee being tested. They compute a lower bound from samples from &lt;em>;P&lt;/em>; and &lt;em>;Q,&lt;/em>; rejecting the property if the lower bound value exceeds the expected divergence. No guarantees are provided if the result is indeed bounded. To test for a range of privacy guarantees, DP-Auditorium introduces three novel testers: (1) HockeyStickPropertyTester, (2) RényiPropertyTester, and (3) MMDPropertyTester. Unlike other approaches, these testers don&#39;t depend on explicit histogram approximations of the tested distributions. They rely on variational representations of the hockey-stick divergence, Rényi divergence, and &lt;a href=&quot;https://jmlr.csail.mit.edu/papers/v13/gretton12a.html&quot;>;maximum mean discrepancy&lt;/a>; (MMD) that enable the estimation of divergences through optimization over function spaces. As a baseline, we implement &lt;a href=&quot;https://arxiv.org/abs/1806.06427&quot;>;HistogramPropertyTester&lt;/a>;, a commonly used approximate DP tester. While our three testers follow a similar approach, for brevity, we focus on the HockeyStickPropertyTester in this post. &lt;/p>; &lt;p>; Given two neighboring datasets, &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>;, the HockeyStickPropertyTester finds a lower bound,&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>; &amp;nbsp;for the hockey-stick divergence between &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>;and &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;) that holds with high probability. Hockey-stick divergence enforces that the two distributions &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>;and &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;) are close under an approximate DP guarantee. Therefore, if a privacy guarantee claims that the hockey-stick divergence is at most &lt;em>;δ&lt;/em>;, and&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; >; &lt;em>;δ&lt;/em>;, then with high probability the divergence is higher than what was promised on &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>; and the mechanism cannot satisfy the given approximate DP guarantee 。 The lower bound&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; is computed as an empirical and tractable counterpart of a variational formulation of the hockey-stick divergence (see &lt;a href=&quot;https://arxiv.org/pdf/2307.05608.pdf&quot;>;the paper&lt;/a>; for more details) 。 The accuracy of&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; increases with the number of samples drawn from the mechanism, but decreases as the variational formulation is simplified. We balance these factors in order to ensure that&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>; &amp;nbsp; is both accurate and easy to compute. &lt;/p>; &lt;p>; &lt;strong>;Dataset finders&lt;/strong>; use &lt;a href=&quot;https://arxiv.org/pdf/2207.13676.pdf&quot;>;black-box optimization&lt;/a>; to find datasets &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>; that maximize&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;, a lower bound on the divergence value &lt;em>;δ&lt;/em>;. Note that black-box optimization techniques are specifically designed for settings where deriving gradients for an objective function may be impractical or even impossible. These optimization techniques oscillate between exploration and exploitation phases to estimate the shape of the objective function and predict areas where the objective can have optimal values. In contrast, a full exploration algorithm, such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search&quot;>;grid search method&lt;/a>;, searches over the full space of neighboring datasets &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>;. DP-Auditorium implements different dataset finders through the open sourced black-box optimization library &lt;a href=&quot;https://github.com/google/vizier&quot;>;Vizier&lt;/a>;. &lt;/p>; &lt;p>; Running existing components on a new mechanism only requires defining the mechanism as a Python function that takes an array of data &lt;em>;D&lt;/em>; and a desired number of samples &lt;em>;n&lt;/em>; to be output by the mechanism computed on &lt;em>;D&lt;/em>;. In addition, we provide flexible wrappers for testers and dataset finders that allow practitioners to implement their own testing and dataset search algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Key results&lt;/h2>; &lt;p>; We assess the effectiveness of DP-Auditorium on five private and nine non-private mechanisms with diverse output spaces. For each property tester, we repeat the test ten times on fixed datasets using different values of &lt;em>;ε&lt;/em>;, and report the number of times each tester identifies privacy bugs. While no tester consistently outperforms the others, we identify bugs that would be missed by previous techniques (HistogramPropertyTester). Note that the HistogramPropertyTester is not applicable to SVT mechanisms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s785/image22.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;409&quot; data-original-width=&quot;785&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s16000/image22.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Number of times each property tester finds the privacy violation for the tested non-private mechanisms. NonDPLaplaceMean and NonDPGaussianMean mechanisms are faulty implementations of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Laplace_Mechanism&quot;>;Laplace&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Gaussian_Mechanism&quot;>;Gaussian&lt;/a>; mechanisms for computing the mean.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also analyze the implementation of a &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py&quot;>;DP gradient descent algorithm&lt;/a>; (DP-GD) in TensorFlow that computes gradients of the loss function on private data. To preserve privacy, DP-GD employs a clipping mechanism to bound the &lt;a href=&quot;https://mathworld.wolfram.com/L2-Norm.html&quot;>;l2-norm&lt;/a>; of the gradients by a value &lt;em>;G&lt;/em>;, followed by the addition of Gaussian noise. This implementation incorrectly assumes that the noise added has a scale of &lt;em>;G&lt;/em>;, while in reality, the scale is &lt;em>;sG&lt;/em>;, where &lt;em>;s&lt;/em>; is a positive scalar 。 This discrepancy leads to an approximate DP guarantee that holds only for values of &lt;em>;s&lt;/em>; greater than or equal to 1. &lt;/p>; &lt;p>; We evaluate the effectiveness of property testers in detecting this bug and show that HockeyStickPropertyTester and RényiPropertyTester exhibit superior performance in identifying privacy violations, outperforming MMDPropertyTester and HistogramPropertyTester. Notably, these testers detect the bug even for values of &lt;em>;s&lt;/em>; as high as 0.6. It is worth highlighting that &lt;em>;s &lt;/em>;= 0.5 corresponds to a &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/308cbda4db6ccad5d1e7d56248727274e4c0c79e/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy_lib.py#L445C1-L446C1&quot;>;common error&lt;/a>; in literature that involves missing a factor of two when accounting for the privacy budget &lt;em>;ε&lt;/em>;. DP-Auditorium successfully captures this bug as shown below. For more details see section 5.6 &lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;here&lt;/a>;. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s836/image21.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;332&quot; data-original-width=&quot;836&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s16000/image21.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Estimated divergences and test thresholds for different values of &lt;em>;s&lt;/em>; when testing DP-GD with the HistogramPropertyTester (&lt;strong>;left&lt;/strong>;) and the HockeyStickPropertyTester (&lt;strong>;right&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s828/image20.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;333&quot; data-original-width=&quot;828&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s16000/image20.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Estimated divergences and test thresholds for different values of &lt;em>;s&lt;/em>; when testing DP-GD with the RényiPropertyTester (&lt;strong>;left&lt;/strong>;) and the MMDPropertyTester (&lt;strong>;right&lt;/strong>;)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To test dataset finders, we compute the number of datasets explored before finding a privacy violation. On average, the majority of bugs are discovered in less than 10 calls to dataset finders. Randomized and exploration/exploitation methods are more efficient at finding datasets than grid search. For more details, see the &lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; DP is one of the most powerful frameworks for data protection. However, proper implementation of DP mechanisms can be challenging and prone to errors that cannot be easily detected using traditional unit testing methods. A unified testing framework can help auditors, regulators, and academics ensure that private mechanisms are indeed private. &lt;/p>; &lt;p>; DP-Auditorium is a new approach to testing DP via divergence optimization over function spaces. Our results show that this type of function-based estimation consistently outperforms previous black-box access testers. Finally, we demonstrate that these function-based estimators allow for a better discovery rate of privacy bugs compared to histogram estimation. By &lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/python/dp_auditorium&quot;>;open sourcing&lt;/a>; DP-Auditorium, we aim to establish a standard for end-to-end testing of new differentially private algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described here was done jointly with Andrés Muñoz Medina, William Kong and Umar Syed. We thank Chris Dibak and Vadym Doroshenko for helpful engineering support and interface suggestions for our library.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5933365460125094774/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5933365460125094774&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5933365460125094774&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html&quot; rel=&quot;alternate&quot; title=&quot;DP-Auditorium: A flexible library for auditing differential privacy&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3264694155710647310&lt;/id>;&lt;published>;2024-02-06T11:17:00.000-08:00&lt;/published>;&lt;updated>;2024-02-06T11:17:53.968-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graph Mining&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TensorFlow&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Graph neural networks in TensorFlow&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s1600/TFGNN%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Objects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation — take for example transportation networks, production networks, knowledge graphs, or social networks 。 Discrete mathematics and computer science have a long history of formalizing such networks as &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graphs&lt;/a>;&lt;/em>;, consisting of &lt;em>;nodes&lt;/em>; connected by &lt;em>;edges&lt;/em>; in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;Graph neural networks&lt;/a>;, or GNNs for short, have emerged as a powerful technique to leverage both the graph&#39;s connectivity (as in the older algorithms &lt;a href=&quot;http://perozzi.net/projects/deepwalk/&quot;>;DeepWalk&lt;/a>; and &lt;a href=&quot;https://snap.stanford.edu/node2vec/&quot;>;Node2Vec&lt;/a>;) and the input features on the various nodes and edges. GNNs can make predictions for graphs as a whole (Does this molecule react in a certain way?), for individual nodes (What&#39;s the topic of this document, given its citations?) or for potential edges (Is this product likely to be purchased together with that product?). Apart from making predictions about graphs, GNNs are a powerful tool used to bridge the chasm to more typical neural network use cases. They encode a graph&#39;s &lt;em>;discrete&lt;/em>;, &lt;em>;relational&lt;/em>; information in a &lt;em>;continuous&lt;/em>; way so that it can be included naturally in another deep learning system. &lt;/p>; &lt;p>; We are excited to announce the release of &lt;a href=&quot;https://github.com/tensorflow/gnn&quot;>;TensorFlow GNN 1.0&lt;/a>; (TF-GNN), a production-tested library for building GNNs at large scales. It supports both modeling and training in TensorFlow as well as the extraction of input graphs from huge data stores. TF-GNN is built from the ground up for heterogeneous graphs, where types of objects and relations are represented by distinct sets of nodes and edges. Real-world objects and their relations occur in distinct types, and TF-GNN&#39;s heterogeneous focus makes it natural to represent them. &lt;/p>; &lt;p>; Inside TensorFlow, such graphs are represented by objects of type &lt;code>;tfgnn.GraphTensor&lt;/code>;. This is a composite tensor type (a collection of tensors in one Python class) accepted as a &lt;a href=&quot;https://en.wikipedia.org/wiki/First-class_citizen&quot;>;first-class citizen&lt;/a>; in &lt;code>;tf.data.Dataset&lt;/code>;, &lt;code>;tf.function&lt;/code>;, etc. It stores both the graph structure and its features attached to nodes, edges and the graph as a whole. Trainable transformations of GraphTensors can be defined as Layers objects in the high-level &lt;a href=&quot;https://www.tensorflow.org/guide/keras&quot;>;Keras API&lt;/a>;, or directly using the &lt;code>;tfgnn.GraphTensor&lt;/code>; primitive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;GNNs: Making predictions for an object in context&lt;/h2>; &lt;p>; For illustration, let&#39;s look at one typical application of TF-GNN: predicting a property of a certain type of node in a graph defined by cross-referencing tables of a huge database. For example, a citation database of Computer Science (CS) arXiv papers with one-to-many cites and many-to-one cited relationships where we would like to predict the subject area of each paper. &lt;/p>; &lt;p>; Like most neural networks, a GNN is trained on a dataset of many labeled examples (~millions), but each training step consists only of a much smaller batch of training examples (say, hundreds). To scale to millions, the GNN gets trained on a stream of reasonably small subgraphs from the underlying graph. Each subgraph contains enough of the original data to compute the GNN result for the labeled node at its center and train the model. This process — typically referred to as subgraph sampling — is extremely consequential for GNN training. Most existing tooling accomplishes sampling in a batch way, producing static subgraphs for training. TF-GNN provides tooling to improve on this by sampling dynamically and interactively. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s800/image2.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Pictured, the process of subgraph sampling where small, tractable subgraphs are sampled from a larger graph to create input examples for GNN training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;p>; TF-GNN 1.0 debuts a flexible Python API to configure dynamic or batch subgraph sampling at all relevant scales: interactively in a Colab notebook (like &lt;a href=&quot;https://colab.research.google.com/github /tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;this one&lt;/a>;), for efficient sampling of a small dataset stored in the main memory of a single training host, or distributed by &lt;a href =&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>; for huge datasets stored on a network filesystem (up to hundreds of millions of nodes and billions of edges). For details, please refer to our user guides for &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/inmemory_sampler.md&quot;>;in-memory&lt;/a>; and &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/beam_sampler.md&quot;>;beam-based&lt;/a>; sampling, respectively. &lt;/p>; &lt;p>; On those same sampled subgraphs, the GNN&#39;s task is to compute a hidden (or latent) state at the root node; the hidden state aggregates and encodes the relevant information of the root node&#39;s neighborhood. One classical approach is &lt;a href=&quot;https://research.google/pubs/neural-message-passing-for-quantum-chemistry/&quot;>;message-passing neural networks&lt;/a>;. In each round of message passing, nodes receive messages from their neighbors along incoming edges and update their own hidden state from them. After &lt;em>;n&lt;/em>; rounds, the hidden state of the root node reflects the aggregate information from all nodes within &lt;em>;n&lt;/em>; edges (pictured below for &lt;em>;n&lt;/em>; = 2) 。 The messages and the new hidden states are computed by hidden layers of the neural network. In a heterogeneous graph, it often makes sense to use separately trained hidden layers for the different types of nodes and edges &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s573/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;511&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Pictured, a simple message-passing neural network where, at each step, the node state is propagated from outer to inner nodes where it is pooled to compute new node states. Once the root node is reached, a final prediction can be made.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The training setup is completed by placing an output layer on top of the GNN&#39;s hidden state for the labeled nodes, computing the &lt;em>;loss &lt;/em>;(to measure the prediction error), and updating model weights by backpropagation, as usual in any neural network training. &lt;/p>; &lt;p>; Beyond supervised training (ie, minimizing a loss defined by labels), GNNs can also be trained in an unsupervised way (ie, without labels). This lets us compute a &lt;em>;continuous&lt;/em>; representation (or &lt;em>;embedding&lt;/em>;) of the &lt;em>;discrete&lt;/em>; graph structure of nodes and their features. These representations are then typically utilized in other ML systems. In this way, the discrete, relational information encoded by a graph can be included in more typical neural network use cases. TF-GNN supports a fine-grained specification of unsupervised objectives for heterogeneous graphs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Building GNN architectures&lt;/h2>; &lt;p>; The TF-GNN library supports building and training GNNs at various levels of abstraction. &lt;/p>; &lt;p>; At the highest level, users can take any of the predefined models bundled with the library that are expressed in Keras layers. Besides a small collection of models from the research literature, TF-GNN comes with a highly configurable model template that provides a curated selection of modeling choices that we have found to provide strong baselines on many of our in-house problems. The templates implement GNN layers; users need only to initialize the Keras layers. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s1400/TFGNN%20code1 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;865&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s16000/TFGNN%20code1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; At the lowest level, users can write a GNN model from scratch in terms of primitives for passing data around the graph, such as broadcasting data from a node to all its outgoing edges or pooling data into a node from all its incoming edges (eg, computing the sum of incoming messages). TF-GNN&#39;s graph data model treats nodes, edges and whole input graphs equally when it comes to features or hidden states, making it straightforward to express not only node-centric models like the MPNN discussed above but also more general forms of &lt;a href=&quot;https://arxiv.org/abs/1806.01261&quot;>;GraphNets&lt;/a>;. This can, but need not, be done with Keras as a modeling framework on the top of core TensorFlow. For more details, and intermediate levels of modeling, see the TF-GNN &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/gnn_modeling.md&quot;>;user guide&lt;/a>; and &lt;a href=&quot;https://github.com/tensorflow/gnn/tree/main/tensorflow_gnn/models&quot;>;model collection&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training orchestration&lt;/h2>; &lt;p>; While advanced users are free to do custom model training, the &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md&quot;>;TF-GNN Runner&lt;/a>; also provides a succinct way to orchestrate the training of Keras models in the common cases. A simple invocation may look like this: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s1400/TFGNN%20code2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;508&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s16000/TFGNN%20code2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The Runner provides ready-to-use solutions for ML pains like distributed training and &lt;code>;tfgnn.GraphTensor&lt;/code>; padding for fixed shapes on Cloud TPUs. Beyond training on a single task (as shown above), it supports joint training on multiple (two or more) tasks in concert. For example, unsupervised tasks can be mixed with supervised ones to inform a final continuous representation (or embedding) with application specific inductive biases. Callers only need substitute the task argument with a mapping of tasks: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s1400/TFGNN%20code3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;392&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s16000/TFGNN%20code3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Additionally, the TF-GNN Runner also includes an implementation of &lt;a href=&quot;https://www.tensorflow.org/tutorials/interpretability/integrated_gradients&quot;>;integrated gradients&lt;/a>; for use in model attribution. Integrated gradients output is a GraphTensor with the same connectivity as the observed GraphTensor but its features replaced with gradient values where larger values contribute more than smaller values in the GNN prediction. Users can inspect gradient values to see which features their GNN uses the most. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In short, we hope TF-GNN will be useful to advance the application of GNNs in TensorFlow at scale and fuel further innovation in the field. If you&#39;re curious to find out more, please try our &lt;a href=&quot;https://colab.sandbox.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;Colab demo&lt;/a>; with the popular OGBN-MAG benchmark (in your browser, no installation required), browse the rest of our &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/overview.md&quot;>;user guides and Colabs&lt;/a>;, or take a look at our &lt;a href=&quot;https://arxiv.org/abs/2207.03522&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The TF-GNN release 1.0 was developed by a collaboration between Google Research: Sami Abu-El-Haija, Neslihan Bulut, Bahar Fatemi, Johannes Gasteiger, Pedro Gonnet, Jonathan Halcrow, Liangze Jiang, Silvio Lattanzi, Brandon Mayer, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, Dustin Zelle, Google Core ML: Arno Eigenwillig, Oleksandr Ferludin, Parth Kothari, Mihir Paradkar, Jan Pfeifer, Rachael Tamakloe, and Google DeepMind:&lt;strong>; &lt;/strong>;Alvaro Sanchez-Gonzalez and Lisa Wang.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3264694155710647310/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html&quot; rel=&quot;alternate&quot; title=&quot;Graph neural networks in TensorFlow&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s72-c/TFGNN%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6956767920612914706&lt;/id>;&lt;published>;2024-02-02T11:07:00.000-08:00&lt;/published>;&lt;updated>;2024-02-07T16:05:00.722-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Cloud Platform&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A decoder-only foundation model for time-series forecasting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rajat Sen and Yichen Zhou, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;Time-series&lt;/a>; forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural科学。 In retail use cases, for example, it has been observed that &lt;a href=&quot;https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning&quot;>;improving demand forecasting accuracy&lt;/a>; can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (eg, DL models performed well in the &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0169207021001874&quot;>;M5 competition&lt;/a>;). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; At the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;>;translation&lt;/a>;, &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/&quot;>;retrieval-augmented generation&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligent_code_completion&quot;>;code completion&lt;/a>;. These models are trained on massive amounts of &lt;em>;textual &lt;/em>;data derived from a variety of sources like &lt;a href=&quot;https://commoncrawl.org/&quot;>;common crawl&lt;/a>; and open-source code that allows them to identify patterns in languages. This makes them very powerful &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-shot_learning&quot;>;zero-shot&lt;/a>; tools; for instance, &lt;a href=&quot;https://blog.google/products/bard/google-bard-try-gemini-ai/&quot;>;when paired with retrieval&lt;/a>;, they can answer questions about and summarize current events 。 &lt;/p>; &lt;p>; Despite DL-based forecasters largely &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;outperforming&lt;/a>; traditional methods and progress being made in &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;reducing training and inference costs&lt;/a>;, they face challenges: most DL architectures require &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;long and involved training and validation cycles&lt;/a>; before a customer can test the model on a new time-series. A foundation model for time-series forecasting, in contrast, can provide decent out-of-the-box forecasts on unseen time-series data with no additional training, enabling users to focus on refining forecasts for the actual downstream task like &lt;a href=&quot;https://en.wikipedia.org/wiki/Customer_demand_planning&quot;>;retail demand planning&lt;/a>;. &lt;/p>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/pdf/2310.10688.pdf&quot;>;A decoder-only foundation model for time-series forecasting&lt;/a>;”, we introduce TimesFM, a single forecasting model pre-trained on a large time-series corpus of 100 billion real world time-points. Compared to the latest large language models (LLMs), TimesFM is much smaller (200M parameters), yet we show that even at such scales, its zero-shot performance on a variety of unseen datasets of different domains and temporal granularities come close to the state-of-the-art supervised approaches trained explicitly on these datasets. Later this year we plan to make this model available for external customers in &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/train-model#aiplatform_create_training_pipeline_tabular_forecasting_sample-python&quot;>;Google Cloud Vertex AI&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A decoder-only foundation model for time-series forecasting&lt;/h2>; &lt;p>; LLMs are usually trained in a &lt;a href=&quot;https://arxiv.org/pdf/1801.10198.pdf&quot;>;decoder-only&lt;/a>; fashion that involves three steps. First, text is broken down into subwords called tokens. Then, the tokens are fed into stacked causal &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;transformer&lt;/a>; layers that produce an output corresponding to each input token (it cannot attend to future tokens) 。 Finally, the output corresponding to the &lt;em>;i&lt;/em>;-th token summarizes all the information from previous tokens and predicts the (&lt;em>;i&lt;/em>;+1)-th token. During inference, the LLM generates the output one token at a time. For example, when prompted with “What is the capital of France?”, it might generate the token “The”, then condition on “What is the capital of France? The” to generate the next token “capital” and so on until it generates the complete answer: “The capital of France is Paris”. &lt;/p>; &lt;p>; A foundation model for time-series forecasting should adapt to variable context (what we observe) and horizon (what we query the model to forecast) lengths, while having enough capacity to encode all patterns from a large pretraining数据集。 Similar to LLMs, we use stacked transformer layers (self-attention and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;>;feedforward&lt;/a>; layers) as the main building blocks for the TimesFM model 。 In the context of time-series forecasting, we treat a patch (a group of contiguous time-points) as a token that was popularized by a recent &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;long-horizon forecasting work&lt;/a>;. The task then is to forecast the (&lt;em>;i&lt;/em>;+1)-th patch of time-points given the &lt;em>;i&lt;/em>;-th output at the end of the stacked transformer layers. &lt;/p>; &lt;p>; However, there are several key differences from language models. Firstly, we need a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; block with residual connections to convert a patch of time-series into a token that can be input to the transformer layers along with &lt;a href=&quot;https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/&quot;>;positional encodings&lt;/a>; (PE). For that, we use a residual block similar to our prior work in &lt;a href=&quot;https://arxiv.org/abs/2304.08424&quot;>;long-horizon forecasting&lt;/a>;. Secondly, at the other end, an output token from the stacked transformer can be used to predict a longer length of subsequent time-points than the input patch length, ie, the output patch length can be larger than the input patch length. &lt;/p>; &lt;p>; Consider a time-series of length 512 time-points being used to train a TimesFM model with input patch length 32 and output patch length 128. During training, the model is simultaneously trained to use the first 32 time-points to forecast the next 128 time-points, the first 64 time-points to forecast time-points 65 to 192, the first 96 time-points to forecast time-points 97 to 224 and so on. During inference, suppose the model is given a new time-series of length 256 and tasked with forecasting the next 256 time-points into the future. The model will first generate the future predictions for time-points 257 to 384, then condition on the initial 256 length input plus the generated output to generate time-points 385 to 512. On the other hand, if in our model the output patch length was equal to the input patch length of 32 then for the same task we would have to go through eight generation steps instead of just the two above. This increases the chances of more errors accumulating and therefore, in practice, we see that a longer output patch length yields better performance for long-horizon forecasting &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s1084/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;674&quot; data-original-width=&quot;1084&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;TimesFM architecture.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Pretraining data&lt;/h2>; &lt;p>; Just like LLMs get better with more tokens, TimesFM requires a large volume of legitimate time series data to learn and improve. We have spent a great amount of time creating and assessing our training datasets, and the following is what we have found works best: &lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;Synthetic data helps with the basics.&lt;/strong>; Meaningful synthetic time-series data can be generated using statistical models or physical simulations. These basic temporal patterns can teach the model the grammar of time series forecasting. &lt;/p>;&lt;/div>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;Real-world data adds real-world flavor.&lt;/strong>; We comb through available public time series datasets, and selectively put together a large corpus of 100 billion time-points. Among these datasets there are &lt;a href=&quot;https://trends.google.com/trends/&quot;>;Google Trends&lt;/a>; and &lt;a href=&quot;https://meta.wikimedia.org/wiki/Research:Page_view&quot;>;Wikipedia Pageviews&lt;/a>;, which track what people are interested in, and that nicely mirrors trends and patterns in many other real-world time series. This helps TimesFM understand the bigger picture and generalize better when provided with domain-specific contexts not seen during training. &lt;/p>;&lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Zero-shot evaluation results&lt;/h2>; &lt;p>; We evaluate TimesFM zero-shot on data not seen during training using popular time-series benchmarks. We observe that TimesFM performs better than most statistical methods like &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average&quot;>;ARIMA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_smoothing&quot;>;ETS&lt;/a>; and can match or outperform powerful DL models like &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>; that have been &lt;em>;explicitly trained&lt;/em>; on the target time-series. &lt;/p>; &lt;p>; We used the &lt;a href=&quot;https://huggingface.co/datasets/monash_tsf&quot;>;Monash Forecasting Archive&lt;/a>; to evaluate TimesFM&#39;s out-of-the-box performance. This archive contains tens of thousands of time-series from various domains like traffic, weather, and demand forecasting covering frequencies ranging from few minutes to yearly data. Following existing literature, we inspect the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;mean absolute error&lt;/a>; (MAE) &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;appropriately scaled&lt;/a>; so that it can be averaged across the datasets. We see that zero-shot (ZS) TimesFM is better than most supervised approaches, including recent deep learning models. We also compare TimesFM to &lt;a href=&quot;https://platform.openai.com/docs/models/gpt-3-5&quot;>;GPT-3.5&lt;/a>; for forecasting using a specific prompting technique proposed by &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime(ZS)&lt;/a>;. We demonstrate that TimesFM performs better than llmtime(ZS) despite being orders of magnitude smaller. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s1476/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;876&quot; data-original-width=&quot;1476&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Scaled MAE (the lower the better) of TimesFM(ZS) against other supervised and zero-shot approaches on Monash datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Most of the Monash datasets are short or medium horizon, ie, the prediction length is not too long. We also test TimesFM on popular benchmarks for long horizon forecasting against a recent state-of-the-art baseline &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>; (and other long-horizon forecasting baselines). In the next figure, we plot the MAE on &lt;a href=&quot;https://paperswithcode.com/dataset/ett&quot;>;ETT&lt;/a>; datasets for the task of predicting 96 and 192 time-points into the future. The metric has been calculated on the last test window of each dataset (as done by the &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime&lt;/a>; paper). We see that TimesFM not only surpasses the performance of llmtime(ZS) but also matches that of the supervised PatchTST model explicitly trained on the respective datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s735/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;433&quot; data-original-width=&quot;735&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Last window MAE (the lower the better) of TimesFM(ZS) against llmtime(ZS) and long-horizon forecasting baselines on ETT datasets.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We train a decoder- only foundation model for time-series forecasting using a large pretraining corpus of 100B real world time-points, the majority of which was search interest time-series data derived from Google Trends and pageviews from Wikipedia. We show that even a relatively small 200M parameter pretrained model that uses our TimesFM architecture displays impressive zero-shot performance on a variety of public benchmarks from different domains and granularities. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a collaboration between several individuals across Google Research and Google Cloud, including (in alphabetical order): Abhimanyu Das, Weihao Kong, Andrew Leach, Mike Lawrence, Alex Martin, Rajat Sen, Yang Yang, Skander Hannachi, Ivan Kuznetsov and Yichen Zhou.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6956767920612914706/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html&quot; rel=&quot;alternate&quot; title=&quot;A decoder-only foundation model for time-series forecasting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2254287928040727502&lt;/id>;&lt;published>;2024-02-02T09:49:00.000-08:00&lt;/published>;&lt;updated>;2024-02-02T09:49:36.211-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML Fairness&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Intervening on early readouts for mitigating spurious features and simplicity bias&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s1600/SiFer%20Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Machine learning models in the real world are often trained on limited data that may contain unintended &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias_(statistics)&quot;>;statistical biases&lt;/a >;。 For example, in the &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CELEBA&lt;/a>; celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting “blond” as the hair color for most female faces — here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as &lt;a href=&quot;https://www.researchgate.net/publication/362524426_Addressing_fairness_in_artificial_intelligence_for_medical_imaging&quot;>;medical diagnosis&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Surprisingly, recent work has also discovered an inherent tendency of deep networks to &lt;em>;amplify such statistical biases&lt;/em>;, through the so-called &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf&quot;>;simplicity bias&lt;/a>; of deep learning. This bias is the tendency of deep networks to identify weakly predictive features early in the training, and continue to anchor on these features, failing to identify more complex and potentially more accurate features. &lt;/p>; &lt;p>; With the above in mind, we propose simple and effective fixes to this dual challenge of spurious features and simplicity bias by applying &lt;em>;early readouts&lt;/em>; and &lt;em>;feature forgetting&lt;/em>; 。 First, in “&lt;a href=&quot;https://arxiv.org/abs/2310.18590&quot;>;Using Early Readouts to Mediate Featural Bias in Distillation&lt;/a>;”, we show that making predictions from early layers of a deep network (referred to as “early readouts”) can automatically signal issues with the quality of the learned representations. In particular, these predictions are more often wrong, and more confidently wrong, when the network is relying on spurious features. We use this erroneous confidence to improve outcomes in &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;>;model distillation&lt;/a>;, a setting where a larger “teacher” model guides the training of a smaller “student” model. Then in “&lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;Overcoming Simplicity Bias in Deep Networks using a Feature Sieve&lt;/a>;”, we intervene directly on these indicator signals by making the network “forget” the problematic features and consequently look for better, more predictive features. This substantially improves the model&#39;s ability to generalize to unseen domains compared to previous approaches. Our &lt;a href=&quot;https://ai.google/responsibility/principles&quot;>;AI Principles&lt;/a>; and our &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; guide how we research and develop these advanced applications and help us address the challenges posed by statistical biases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s1080/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;1080&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation comparing hypothetical responses from two models trained with and without the feature sieve.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Early readouts for debiasing distillation&lt;/h2>; &lt;p>; We first illustrate the diagnostic value of &lt;em>;early readouts&lt;/em >; and their application in debiased distillation, ie, making sure that the student model inherits the teacher model&#39;s resilience to feature bias through distillation. We start with a standard distillation framework where the student is trained with a mixture of label matching (minimizing the &lt;a href=&quot;https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e&quot;>;cross-entropy loss&lt;/a>; between student outputs and the ground-truth labels) and teacher matching (minimizing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;KL divergence&lt;/a>; loss between student and teacher outputs for any given input). &lt;/p>; &lt;p>; Suppose one trains a linear decoder, ie, a small auxiliary neural network named as &lt;em>;Aux,&lt;/em>; on top of an intermediate representation of the student model. We refer to the output of this linear decoder as an early readout of the network representation. Our finding is that early readouts make more errors on instances that contain spurious features, and further, the confidence on those errors is higher than the confidence associated with other errors. This suggests that confidence on errors from early readouts is a fairly strong, automated indicator of the model&#39;s dependence on potentially spurious features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s1128/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;796&quot; data-original-width=&quot;1128&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrating the usage of early readouts (ie, output from the auxiliary layer) in debiasing distillation. Instances that are confidently mispredicted in the early readouts are upweighted in the distillation loss.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We used this signal to modulate the contribution of the teacher in the distillation loss on a per-instance basis, and found significant improvements in the trained student model as a result. &lt;/p>; &lt;p>; We evaluated our approach on standard benchmark datasets known to contain spurious correlations (&lt;a href=&quot;https://arxiv.org/pdf/1911.08731.pdf&quot;>;Waterbirds&lt;/a>;, &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CelebA&lt;/a>;, &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/civil_comments&quot;>;CivilComments&lt;/a>;, &lt;a href=&quot;https://cims.nyu.edu/~sbowman/multinli/&quot;>;MNLI&lt;/a>;). Each of these datasets contain groupings of data that share an attribute potentially correlated with the label in a spurious manner. As an example, the CelebA dataset mentioned above includes groups such as {blond male, blond female, non-blond male, non-blond female}, with models typically performing the worst on the {non-blond female} group when predicting hair color 。 Thus, a measure of model performance is its &lt;em>;worst group accuracy&lt;/em>;, ie, the lowest accuracy among all known groups present in the dataset. We improved the worst group accuracy of student models on all datasets; moreover, we also improved overall accuracy in three of the four datasets, showing that our improvement on any one group does not come at the expense of accuracy on other groups. More details are available in our &lt;a href=&quot;https://arxiv.org/pdf/2310.18590.pdf&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/s1270/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1270&quot; height=&quot;511&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/w640-h511/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Worst Group Accuracies of different distillation techniques relative to that of the Teacher model. Our method outperforms other methods on all datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overcoming simplicity bias with a feature sieve&lt;/h2>; &lt;p>; In a second, closely related project, we intervene directly on the information provided by early readouts, to improve &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;>;feature learning&lt;/a>; and &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/generalization/video-lecture&quot;>;generalization&lt;/a>;. The workflow alternates between &lt;em>;identifying &lt;/em>;problematic features and &lt;em>;erasing identified features&lt;/em>; from the network. Our primary hypothesis is that early features are more prone to simplicity bias, and that by erasing (“sieving”) these features, we allow richer feature representations to be learned. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s1098/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;604&quot; data-original-width=&quot;1098&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;Training workflow with feature sieve. We alternate between identifying problematic features (using training iteration) and erasing them from the network (using forgetting iteration).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We describe the identification and erasure steps in more detail: &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;Identifying simple features&lt;/b>;: We train the primary model and the readout model (AUX above) in conventional fashion via forward- and back-propagation. Note that feedback from the auxiliary layer does not back-propagate to the main network. This is to force the auxiliary layer to learn from already-available features rather than create or reinforce them in the main network. &lt;/li>;&lt;li>;&lt;b>;Applying the feature sieve&lt;/b>;: We aim to erase the identified features in the early layers of the neural network with the use of a novel &lt;em>;forgetting loss&lt;/em>;,&lt;em>; L&lt;sub>;f &lt;/sub>;&lt;/em>;, which is simply the cross-entropy between the readout and a uniform distribution over labels. Essentially, all information that leads to nontrivial readouts are erased from the primary network. In this step, the auxiliary network and upper layers of the main network are kept unchanged. &lt;/li>; &lt;/ul>; &lt;p>; We can control specifically how the feature sieve is applied to a given dataset through a small number of configuration parameters. By changing the position and complexity of the auxiliary network, we control the complexity of the identified- and erased features. By modifying the mixing of learning and forgetting steps, we control the degree to which the model is challenged to learn more complex features. These choices, which are dataset-dependent, are made via &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter search&lt;/a>; to maximize validation accuracy, a standard measure of generalization. Since we include “no-forgetting” (ie, the baseline model) in the search space, we expect to find settings that are at least as good as the baseline. &lt;/p>; &lt;p>; Below we show features learned by the baseline model (middle row) and our model (bottom row) on two benchmark datasets — biased activity recognition (&lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;) and animal categorization (&lt;a href=&quot;https://arxiv.org/pdf/1906.02899v3.pdf&quot;>;NICO&lt;/a>;). Feature importance was estimated using post-hoc gradient-based importance scoring (&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GRAD-CAM&lt;/a>;), with the orange-red end of the spectrum indicating high importance, while green-blue indicates low importance. Shown below, our trained models focus on the primary object of interest, whereas the baseline model tends to focus on background features that are simpler and spuriously correlated with the label. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s1616/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;850&quot; data-original-width=&quot;1616&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Feature importance scoring using GRAD-CAM on activity recognition (BAR) and animal categorization (NICO) generalization benchmarks. Our approach (last row) focuses on the relevant objects in the image, whereas the baseline (ERM; middle row) relies on background features that are spuriously correlated with the label.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Through this ability to learn better, generalizable features, we show substantial gains over a range of relevant baselines on real-world spurious feature benchmark datasets: &lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2104.06885.pdf&quot;>;CelebA Hair&lt;/a>;, &lt;a href=&quot;https://nico.thumedialab.com/&quot;>;NICO&lt;/a>; and &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/imagenet_a&quot;>;ImagenetA&lt;/a>;, by margins up to 11% (see figure below). More details are available in &lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;our paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/s1082/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1082&quot; data-original-width=&quot;844&quot; height=&quot;640&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/w501-h640/image1.png&quot; width=&quot;501&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our feature sieve method improves accuracy by significant margins relative to the nearest baseline for a range of feature generalization benchmark datasets.&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We hope that our work on early readouts and their use in feature sieving for generalization will both spur the development of a new class of adversarial feature learning approaches and help improve the generalization capability and robustness of deep learning systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;The work on applying early readouts to debiasing distillation was conducted in collaboration with our academic partners Durga Sivasubramanian, Anmol Reddy and Prof. Ganesh Ramakrishnan at &lt;a href=&quot;https://www.iitb.ac.in/&quot;>;IIT Bombay&lt;/a>;. We extend our sincere gratitude to Praneeth Netrapalli and Anshul Nasery for their feedback and recommendations. We are also grateful to Nishant Jain, Shreyas Havaldar, Rachit Bansal, Kartikeya Badola, Amandeep Kaur and the whole cohort of pre-doctoral researchers at Google Research India for taking part in research discussions. Special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2254287928040727502/comments/default&quot; rel =&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for. html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502 &quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502&quot; rel=&quot;self&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for.html&quot; rel=&quot;alternate&quot; title=&quot;Intervening on early readouts for mitigating spurious features and simplicity bias&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;Email>; noreply@blogger.com &lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/2005#thumbnail” src =“ https：//img1.blog1.blogblog .com/img/b16-round.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/rution>; &lt;媒体：thumbnail height =“ 72” url =“ https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s72-c/SiFer%20Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media :thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5966553114967673984&lt;/id>;&lt;published>;2024-01 -31T13:59:00.000-08:00&lt;/published>;&lt;updated>;2024-01-31T13:59:36.056-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme= &quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MobileDiffusion: Rapid text-to-image generation on-device&lt; /stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s1600/InstantTIGO%20hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Text-to-image &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (eg, &lt;a href=&quot;https://stability.ai/news/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>;, &lt;a href=&quot;https://openai.com/research/dall-e&quot;>;DALL·E&lt;/a>;, and &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;). While recent advancements in inference solutions on &lt;a href=&quot;https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html&quot;>;Android&lt;/a>; via MediaPipe and &lt;a href=&quot;https://github.com/apple/ml-stable-diffusion&quot;>;iOS&lt;/a>; via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices&lt;/a>;”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s800/image2.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style =&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s800/image5.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rapid text-to-image generation on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires &lt;a href=&quot;https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html&quot;>;iterative denoising&lt;/a>; to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature. &lt;/p>; &lt;p>; The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (eg, &lt;a href=&quot;https://arxiv.org/abs/2206.00927&quot;>;DPM&lt;/a>;) or distillation techniques (eg, &lt;a href=&quot;https://arxiv.org/abs/2202.00512&quot;>;progressive distillation&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.01469&quot;>;consistency distillation&lt;/a>;), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.&quot;>;Adversarial Diffusion Distillation&lt;/a>;, even reduce to a single necessary step. &lt;/p>; &lt;p>; However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (eg, &lt;a href=&quot;https://snap-research.github.io/SnapFusion/&quot;>;SnapFusion&lt;/a>;). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MobileDiffusion&lt;/h2>; &lt;p>; Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model&#39;s architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion&#39;s &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;UNet architecture&lt;/a>;. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion. &lt;/p>; &lt;p>; The design of MobileDiffusion follows that of &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;latent diffusion models&lt;/a>;. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP-ViT/L14&lt;/a>;, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Diffusion UNet&lt;/h3>; &lt;p>; As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (eg, data, optimizer) to study the effects of different architectures. &lt;/p>; &lt;p>; In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of &lt;a href=&quot;https://arxiv.org/abs/2301.11093&quot;>;UViT&lt;/a>; architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV /s915/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;249&quot; data-original-width=&quot;915&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Convolution blocks, in particular &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is &lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;>;separable convolution&lt;/a>;. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance. &lt;/p>; &lt;p>; In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of &lt;a href=&quot;https://arxiv.org/pdf/2110.12894.pdf&quot;>;FLOPs&lt;/a>; (floating-point operations) and number of parameters. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Comparison of some diffusion UNets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h3>;Image decoder&lt;/h3>; &lt;p>; In addition to the UNet, we also optimized the image decoder. We trained a &lt;a href=&quot;https://arxiv.org/abs/2012.03715&quot;>;variational autoencoder&lt;/a>; (VAE) to encode an &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size. To further enhance efficiency, we design a lightweight decoder architecture by pruning the original&#39;s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our &lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;789&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Decoder&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;#Params (M)&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;PSNR↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;SSIM↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;LPIPS↓&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;SD&lt;/b>; &lt;/td>; &lt;td>;49.5 &lt;/td>; &lt;td>;26.7 &lt;/td>; &lt;td>;0.76 &lt;/td>; &lt;td>;0.037 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours&lt;/b>; &lt;/td>; &lt;td>;39.3 &lt;/td>; &lt;td>;30.0 &lt;/td>; &lt;td>;0.83 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours-Lite&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;9.8 &lt;/td>; &lt;td>;30.2 &lt;/td>; &lt;td>;0.84 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quality evaluation of VAE decoders. Our lite decoder is much smaller than SD, with better quality metrics, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;peak signal-to-noise ratio&lt;/a>; (PSNR), &lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_similarity&quot;>;structural similarity index measure&lt;/a>; (SSIM), and &lt;a href=&quot;https://arxiv.org/abs/1801.03924&quot;>;Learned Perceptual Image Patch Similarity&lt;/a>; (LPIPS).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;One-step sampling&lt;/h3>; &lt;p>; In addition to optimizing the model architecture, we adopt a &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN hybrid&lt;/a>; to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (eg, &lt;a href=&quot;https://arxiv.org/abs/2301.09515&quot;>;StyleGAN-T&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.05511&quot;>;GigaGAN&lt;/a>;) confront similar complexities, resulting in highly intricate and expensive training. &lt;/p>; &lt;p>; To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training. &lt;/p>; &lt;p>; The figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ -UILKw/s960/image7.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;960 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of DiffusionGAN fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Below we show example images generated by our MobileDiffusion with DiffusionGAN one-step sampling 。 With such a compact model (520M parameters in total), MobileDiffusion can generate high-quality diverse images for various domains. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1728&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images generated by our MobileDiffusion&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We measured the performance of our MobileDiffusion on both iOS and Android devices, using different runtime optimizers. The latency numbers are reported below. We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image. This lightning speed potentially enables many interesting use cases on mobile devices. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1184&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Latency measurements (&lt;b>;s&lt;/b>;) on mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; With superior efficiency in terms of latency and size, MobileDiffusion has the potential to be a very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts. And we will ensure any application of this technology will be in-line with Google&#39;s &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;responsible AI practices&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.&lt; /em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5966553114967673984/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http: //blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html&quot; rel=&quot;alternate&quot; title=&quot;MobileDiffusion: Rapid text-to-image generation on-device&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd :image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot; 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s72-c /InstantTIGO%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5144906729109253495&lt;/id>;&lt;published>;2024-01-26T11:56:00.000-08:00&lt;/published>;&lt;updated >;2024-01-26T11:56:23.553-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term =&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Mixed-input matrix multiplication performance optimizations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Manish Gupta , Staff Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s1180/matrixhero.png&quot; style=&quot;display: none ;” />; &lt;p>; AI-driven technologies are weaving themselves into the fabric of our daily routines, with the potential to enhance our access to knowledge and boost our overall productivity. The backbone of these applications lies in large language models (LLMs). LLMs are memory-intensive and typically require specialized hardware accelerators to efficiently deliver &lt;a href=&quot;https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e&quot;>;tens of exaflops&lt;/a>; of computing power. This blog post shows how we can start addressing the computational challenges by utilizing memory more effectively. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The bulk of an LLM&#39;s memory and compute are consumed by &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;>;weights&lt;/a>; in &lt;a href=&quot;https://arxiv.org/pdf/2006.16668.pdf&quot;>;matrix multiplication&lt;/a>; operations. Using narrower &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Primitive_data_type&quot;>;data types&lt;/a>;&lt;/em>; reduces memory consumption. For example, storing weights in the 8-bit &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_(computer_science)&quot;>;integer&lt;/a>; (ie, U8 or S8) data type reduces the memory footprint by 4× relative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Single-precision_floating-point_format&quot;>;single-precision&lt;/a>; (F32) and 2× relative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Half-precision_floating-point_format&quot;>;half-precision&lt;/a>; (F16) or &lt;a href=&quot;https://en.wikipedia.org/wiki/Bfloat16_floating-point_format&quot;>;bfloat16&lt;/a>; (BF16). Furthermore, &lt;a href=&quot;https://arxiv.org/pdf/2206.01861.pdf&quot;>;previous work has&lt;/a>; shown that LLM models running matrix multiplications with &lt;em>;weights&lt;/em>; in S8 and &lt;em>;input&lt;/em>; in F16 (preserving higher precision of the user-input) is an effective method for increasing the efficiency with acceptable trade-offs in accuracy. This technique is known as &lt;em>;weight-only quantization&lt;/em>; and requires efficient implementation of matrix multiplication with &lt;em>;mixed-inputs&lt;/em>;, eg, half-precision input multiplied with 8-bits integer. Hardware accelerators, including GPUs, support a fixed set of data types, and thus, mixed-input matrix multiplication requires software transformations to map to the hardware operations. &lt;/p>; &lt;p>; To that end, in this blog we focus on mapping mixed-input matrix multiplication onto the &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/&quot;>;NVIDIA Ampere architecture&lt;/a>;. We present software techniques addressing data type conversion and layout conformance to map mixed-input matrix multiplication efficiently onto hardware-supported data types and layouts. Our results show that the overhead of additional work in software is minimal and enables performance close to the peak hardware capabilities. The software techniques described here are released in the open-source &lt;a href=&quot;https://github.com/NVIDIA/cutlass/pull/1084&quot;>;NVIDIA/CUTLASS&lt;/a>; repository. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s1999 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1159&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Memory footprint for an 175B parameter LLM model with various data types formats.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The matrix-multiply-accumulate operation&lt;/h2>; &lt;p>; Modern AI hardware accelerators such as &lt;a href= &quot;https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works&quot;>;Google&#39;s TPU&lt;/a>; and &lt;a href=&quot;https://www.nvidia.com/en-us/ data-center/tensor-cores/&quot;>;NVIDIA&#39;s GPU&lt;/a>; multiply matrices natively in the hardware by targeting Tensor Cores, which are specialized processing elements to accelerate matrix operations, particularly for AI workloads. In this blog, we focus on NVIDIA Ampere Tensor Cores, which provide the &lt;em>;matrix-multiply-accumulate&lt;/em>; (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma&quot;>;mma&lt;/a>;&lt;/code>;) operation. For the rest of the blog the reference to &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; is for Ampere Tensor Cores. The supported data types, shapes, and data layout of the two input matrices (called operands) for the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation are fixed in hardware 。 This means that matrix multiplications with various data types and larger shapes are implemented in the software by tiling the problem onto hardware-supported data types, shapes, and layouts. &lt;/p>; &lt;p>; The Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation is defined by specifying two input matrices (eg, &lt;em>;A&lt;/em>; &amp;amp; &lt;em>;B&lt;/em>;, shown below) to produce a result matrix, &lt;em>;C&lt;/em>;. The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation natively supports mixed-precision. &lt;em>;&lt;a href=&quot;https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/&quot;>;Mixed-precision Tensor Cores&lt;/a>;&lt;/em>; allow mixing input (&lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>;) data type with the result (&lt;em>;C&lt;/em>;) data type. In contrast, &lt;em>;mixed-input &lt;/em>;matrix multiplication involves mixing the input data types, and it is not supported by the hardware, so it needs to be implemented in the software. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s1039/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;668&quot; data-original-width=&quot;1039&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Tensor Core operation of M-by-N-by-K on input matrix A of M-by-K and matrix B of K-by-N produces output matrix C of M-by-N.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Challenges of mixed -input matrix multiplication&lt;/h2>; &lt;p>; To simplify the discussion, we restrict to a specific example of mixed-input matrix multiplication: F16 for user input and U8 for the model weights (written as F16 * U8). The techniques described here work for various combinations of mixed-input data types. &lt;/p>; &lt;p>; A GPU programmer can access a &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;hierarchy of memory&lt;/a>;, including global memory, shared memory, and registers, which are arranged in order of decreasing capacity but increasing speed. NVIDIA Ampere Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operations consume input matrices from registers. Furthermore, input and output matrices are required to conform to a layout of data within a group of 32 threads known as a &lt;em>;warp&lt;/em>;. The supported data type &lt;em>;and&lt;/em>; layout within a warp are fixed for an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation, so to implement mixed-input multiplication efficiently, it is necessary to solve the challenges of data type conversion and layout conformance in software. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Data type conversion &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation requires two input matrices with the same data type. Thus, mixed-input matrix multiplication, where one of the operands is stored in U8 in global memory and other in F16, requires a data type conversion from U8 to F16. The conversion will bring two operands to F16, mapping the &lt;em>;mixed-input&lt;/em>; matrix multiplication to hardware-supported &lt;em>;mixed-precision&lt;/em>; Tensor Cores. Given the large number of weights, there are a large number of such operations, and our techniques show how to reduce their latency and improve performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Layout conformance &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation also requires the layout of two input matrices, within the registers of a warp, to be conformat with hardware specification. The layout for the input matrix &lt;em>;B&lt;/em>; of U8 data type in mixed-input matrix multiplication (F16 * U8) needs to conform with the converted F16 data type. This is called &lt;em>;layout conformance&lt;/em>; and needs to be achieved in the software. &lt;/p>; &lt;p>; The figure below shows an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation consuming matrix &lt;em>;A&lt;/em>; and matrix &lt;em>;B&lt;/em>; from registers to produce matrix &lt;em>;C&lt;/em>; in registers, distributed across one warp. The thread &lt;em>;T0&lt;/em>; is highlighted and zoomed in to show the weight matrix &lt;em>;B&lt;/em>; goes through data type conversion and needs a layout conformance to be able to map to the hardware-supported Tensor Core手术。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s1999/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1240&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;The mapping of mixed-input (F32 = F16 * U8) operation in software to natively supported warp-level Tensor Cores in hardware (F32 = F16 * F16). (Original figure source &lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/&quot;>;Developing CUDA kernels to push Tensor Cores to the Absolute Limit on NVIDIA A100&lt;/a>;.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Software strategies addressing challenges&lt;/h2>; &lt;p>; A typical data type conversion involves a sequence of operations on 32-bit registers, shown below. Each rectangular block represents a register and the adjoining text are the operations. The entire sequence shows the conversion from 4xU8 to 2x(2xF16). The sequence involves roughly 10 operations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s947/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;836&quot; data-original-width=&quot;947&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760&quot;>;NumericArrayConvertor&lt;/a>;&lt; /code>; from 4xU8 to 2x(2xF16) in 32-bit registers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There are many ways of achieving layout conformance. Two of the existing solutions are: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Narrower bitwidth shared memory loads&lt;/em>;: In this approach, threads issue narrow bitwidth memory loads moving the U8 data from shared memory to registers. This results in &lt;em>;two&lt;/em>; 32-bit registers, with each register containing 2xF16 values (shown above for the matrix &lt;em>;B&lt;/em>;&#39;s thread &lt;em>;T0&lt;/em>;). The narrower shared memory load achieves layout conformance directly into registers without needing any shuffles; however, it does not utilize the full shared memory bandwidth. &lt;/li>;&lt;li>;&lt;em>;Pre-processing in global memory&lt;/em>;: An &lt;a href=&quot;https://arxiv.org/pdf/2211.10017.pdf&quot;>;alternative strategy&lt;/a>; involves rearranging the data within the global memory (one level above the shared memory in &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;memory hierarchy&lt;/a>;), allowing wider shared memory loads. This approach maximizes the shared memory bandwidth utilization and ensures that the data is loaded in a conformant layout directly in the registers. Although the rearrangement process can be executed offline prior to the LLM deployment, ensuring no impact on the application performance, it introduces an additional, non-trivial hardware-specific pre-processing step that requires an extra program to rearrange the data. &lt;a href=&quot;https://github.com/NVIDIA/FasterTransformer&quot;>;NVIDIA/FasterTransformer&lt;/a>; adopts this method to effectively address layout conformance challenges. &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Optimized software strategies&lt;/h2>; &lt;p>; To further optimize and reduce the overhead of data type conversion and layout conformance, we have implemented &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;FastNumericArrayConvertor&lt;/a>;&lt;/code>; and &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h#L120&quot;>;FragmentShuffler&lt;/a>;&lt;/code>;, respectively. &lt;/p>;&lt;p>; &lt;code>;FastNumericArrayConvertor&lt;/code>; operates on 4xU8 in 32-bit registers without unpacking individual 1xU8 values. Furthermore, it uses less expensive arithmetic operations which reduces the number of instructions and increases the speed of the conversion. &lt;/p>; &lt;p>; The conversion sequence for &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>; is shown below. The operations use packed 32b registers, avoiding explicit unpacking and packing. &lt;code>;FastNumericArrayConvertor&lt;/code>; uses the &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prmt&quot;>;permute byte&lt;/a>;&lt;/code>; to rearrange bytes of 4xU8 into two registers. Additionally, &lt;code>;FastNumericArrayConvertor&lt;/code>; does not use expensive integer to floating-point conversion instructions and employs vectorized operations to obtain the packed results in &lt;em>;two&lt;/em>; 32-bit registers containing 2x(2xF16) values. The &lt;code>;FastNumericArrayConvertor&lt;/code>; for U8-to-F16 approximately uses six operations, a 1.6× reduction relative to the approach shown above. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s1392/image201.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;733&quot; data-original-width=&quot;1392&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s16000/image201.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;&lt;code>;FastNumericArrayConvertor&lt;/code>; utilizes &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index .html#data-movement-and-conversion-instructions-prmt&quot;>;permute bytes&lt;/a>;&lt;/code>; and packed arithmetic, reducing the number of instructions in the data type conversion.&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; &lt;code>;FragmentShuffler&lt;/code>; handles the layout conformance by shuffling data in a way that allows the use of wider bitwidth load operation, increasing shared memory bandwidth utilization and reducing the total number of operations 。 &lt;/p>; &lt;p>; NVIDIA Ampere architecture provides a load matrix instruction (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix&quot;>;ldmatrix&lt;/a>;&lt;/code>;). The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; is a warp-level operation, where 32 threads of a warp move the data from shared memory to registers in the &lt;em>;shape&lt;/em>; and &lt;em>;layout&lt;/em>; that &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>; consume. The use of &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; &lt;em>;reduces&lt;/em>; the number of load instructions and &lt;em>;increases&lt;/em>; the memory bandwidth utilization. Since the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; instruction moves U8 data to registers, the layout after the load conforms with U8*U8 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation, and not with F16*F16 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation 。 We implemented &lt;code>;FragmentShuffler&lt;/code>; to rearrange the data within registers using shuffle (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions&quot;>;shfl.sync&lt;/a>;)&lt;/code>; operations to achieve the layout conformance. &lt;/p>;&lt;p>; The most significant contribution of this work is to achieve layout conformance through register shuffles, avoiding offline pre-processing in global memory or narrower bitwidth shared memory loads. Furthermore, we provide implementations for &lt;code>;FastNumericArrayConvertor&lt;/code>; covering data type conversion from &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2448&quot;>;S8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2546&quot;>;U8-to-BF16&lt;/a>;, and &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2588&quot;>;S8-to-BF16&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance results&lt;/h2>; &lt;p>; We measured the performance of eight mixed-input variants of &lt;em>;our method&lt;/em>; (shown below in blue and red; varying the data types of matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>;) and two &lt;em>;mixed-precision&lt;/em>; data types (shown in green) on an NVIDIA A100 SXM chip. The performance results are shown in &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; (higher is better). Notably, the first eight matrix-multipications require additional operations relative to the last two, because the mixed-precision variants directly target hardware-accelerated Tensor Core operations and do not need data type conversion and layout conformance. Even so, our approach demonstrates mixed-input matrix multiplication performance only slightly below or on par with mixed-precision. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s1999/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1180&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Mixed-input matrix multiplication performance on NVIDIA A100 40GB SMX4 chip for a compute-bound matrix problem shape &lt;code>;m=3456, n=4096, k=2048.&lt;/ code>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p >; &lt;em>;We would like to mention several folks who have contributed through technical brainstorming and improving the blog post including, Quentin Colombet, Jacques Pienaar, Allie Culp, Calin Cascaval, Ashish Gondimalla, Matt Walsh, Marek Kolodziej, and Aman Bhatia. We would like to thank our NVIDIA partners Rawn Henry, Pradeep Ramani, Vijay Thakkar, Haicheng Wu, Andrew Kerr, Matthew Nicely, and Vartika Singh.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http:/ /blog.research.google/feeds/5144906729109253495/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2024/01/mixed-input-matrix-multiplication.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/5144906729109253495&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/5144906729109253495&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html &quot; rel=&quot;alternate&quot; title=&quot;Mixed-input matrix multiplication performance optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s72-c/matrixhero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1418736582601940076&lt;/id>;&lt;published >;2024-01-23T14:27:00.000-08:00&lt;/published>;&lt;updated>;2024-01-23T14:27:09.785-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;Exphormer: Scaling transformers for graph-structured data&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s1600/EXPHORMER%2005large.gif&quot;样式=“显示：无；” />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;Graphs&lt;/a>;, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; A common approach to learning on graphs are &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;graph neural networks&lt;/a>; (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a &lt;a href=&quot;https://wandb.ai/graph-neural-networks/spatial/reports/An-Introduction-to-Message-Passing-Graph-Neural-Networks--VmlldzoyMDI2NTg2&quot;>;message-passing&lt;/a>; framework, whereby each layer aggregates the representation of a node with those of its immediate neighbors. &lt;/p>; &lt;p>; Recently, &lt;a href=&quot;https://arxiv.org/abs/2012.09699&quot;>;graph transformer models&lt;/a>; have emerged as a popular alternative to message-passing GNNs. These models build on the success of &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine-learning_model)&quot;>;Transformer architectures&lt;/a>; in natural language processing (NLP), adapting them to graph-structured data. The attention mechanism in graph transformers can be modeled by an interaction graph, in which edges represent pairs of nodes that attend to each other. Unlike message passing architectures, graph transformers have an interaction graph that is separate from the input graph. The typical interaction graph is a complete graph, which signifies a full attention mechanism&lt;em>; &lt;/em>;that models direct interactions between all pairs of nodes. However, this creates quadratic computational and memory bottlenecks that limit the applicability of graph transformers to datasets on small graphs with at most a few thousand nodes. Making graph transformers scalable has been considered one of the most important research directions in the field (see &lt;a href=&quot;https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0&quot;>;the first open problem here&lt;/a>;). &lt;/p>; &lt;p>; A natural remedy is to use a &lt;em>;sparse&lt;/em>; interaction graph with fewer edges. &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3530811&quot;>;Many sparse and efficient transformers have been proposed&lt;/a>; to eliminate the quadratic bottleneck for sequences, however, they do not generally extend to graphs in a principled manner. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.06147&quot;>;Exphormer: Sparse Transformers for Graphs&lt;/a>;”, presented at &lt;a href=&quot;https://icml.cc/Conferences/2023/Dates&quot;>;ICML 2023&lt;/a>;, we address the scalability challenge by introducing a sparse attention framework for transformers that is designed specifically for graph data. The Exphormer framework makes use of expander graphs, a powerful tool from &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_graph_theory&quot;>;spectral graph theory&lt;/a>;, and is able to achieve strong empirical results on a wide variety of datasets. Our implementation of Exphormer is now available on &lt;a href=&quot;https://github.com/hamed1375/Exphormer&quot;>;GitHub&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Expander graphs&lt;/h2>; &lt;p>; A key idea at the heart of Exphormer is the use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Expander_graph&quot;>;expander graphs&lt;/a>;, which are sparse yet well-connected graphs that have some useful properties — 1) the matrix representation of the graphs have similar linear-algebraic properties as a complete graph, and 2) they exhibit rapid mixing of random walks, ie, a small number of steps in a random walk from any starting node is enough to ensure convergence to a “stable” distribution on the nodes of the graph. Expanders have found applications to diverse areas, such as algorithms, pseudorandomness, complexity theory, and error-correcting codes. &lt;/p>; &lt;p>; A common class of expander graphs are &lt;em>;d&lt;/em>;-regular expanders, in which there are &lt;em>;d&lt;/em>; edges from every node (ie, every node has degree &lt;em>;d&lt;/em>;). The quality of an expander graph is measured by its &lt;em>;spectral gap&lt;/em>;, an algebraic property of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Adjacency_matrix&quot;>;adjacency matrix&lt;/a>; (a matrix representation of the graph in which rows and columns are indexed by nodes and entries indicate whether pairs of nodes are connected by an edge). Those that maximize the spectral gap are known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Ramanujan_graph&quot;>;Ramanujan graphs&lt;/a>; — they achieve a gap of &lt;em>;d&lt;/em>; - 2*√(&lt;em>;d&lt;/em>;-1), which is essentially the best possible among &lt;em>;d&lt;/em>;-regular graphs. A number of deterministic and randomized constructions of Ramanujan graphs have been proposed over the years for various values of &lt;em>;d&lt;/em>;. We use a &lt;a href=&quot;https://arxiv.org/abs/cs/0405020&quot;>;randomized expander construction of Friedman&lt;/a>;, which produces near-Ramanujan graphs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s843/image1.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;800&quot; height=&quot;320&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s320/image1.gif&quot; width=&quot;304&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-2920b38b-7fff-2fa8-a3cd-06dfd3ba9968&quot;>;&lt;span face=&quot;Arial, sans- serif&quot; style=&quot;font-size: 10pt; font-style: italic;字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse: preserve;&quot;>;Expander graphs are at the heart of Exphormer. A good expander is sparse yet exhibits rapid mixing of random walks, making its global connectivity suitable for an interaction graph in a graph transformer model.&lt;/span>;&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;Exphormer replaces the dense, fully-connected interaction graph of a standard Transformer with edges of a sparse &lt;em>;d&lt;/em>;-regular expander graph. Intuitively, the spectral approximation and mixing properties of an expander graph allow distant nodes to communicate with each other after one stacks multiple attention layers in a graph transformer architecture, even though the nodes may not attend to each other directly. Furthermore, by ensuring that &lt;em>;d&lt;/em>; is constant (independent of the size of the number of nodes), we obtain a linear number of edges in the resulting interaction graph.&lt;/p>; &lt;br />; &lt;h2>;Exphormer: Constructing a sparse interaction graph&lt;/h2>; &lt;p>; Exphormer combines expander edges with the input graph and virtual nodes. More specifically, the sparse attention mechanism of Exphormer builds an interaction graph consisting of three types of edges: &lt;/p>; &lt;ul>; &lt;li>;Edges from the input graph (&lt;em>;local attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from a constant-degree expander graph (&lt;em>;expander attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from every node to a small set of virtual nodes (&lt;em>;global attention&lt;/em>;) &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-ac11d16d-7fff-62da-cf18-7ba830f677d3&quot;>;&lt;span face=&quot;Arial, sans-serif&quot; style=&quot;font-size: 10pt; font-style: italic;字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse: preserve;&quot;>;Exphormer builds an interaction graph by combining three types of edges. The resulting graph has good connectivity properties and retains the inductive bias of the input dataset graph while still remaining sparse.&lt;/span>;&lt;/ span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Each component serves a specific purpose: the edges from the input graph retain the inductive bias from the input graph structure (which typically gets lost in a fully-connected attention module). Meanwhile, expander edges allow good global connectivity and random walk mixing properties (which spectrally approximate the complete graph with far fewer edges). Finally, virtual nodes serve as global “memory sinks” that can directly communicate with every node. While this results in additional edges from each virtual node equal to the number of nodes in the input graph, the resulting graph is still sparse. The degree of the expander graph and the number of virtual nodes are hyperparameters to tune for improving the quality指标。 &lt;/p>; &lt;p>; Furthermore, since we use an expander graph of constant degree and a small constant number of virtual nodes for the global attention, the resulting sparse attention mechanism is linear in the size of the original input graph, ie, it models a number of direct interactions on the order of the total number of nodes and edges. &lt;/p>; &lt;p>; We additionally show that Exphormer is as expressive as the dense transformer and obeys universal approximation properties. In particular, when the sparse attention graph of Exphormer is augmented with self loops (edges connecting a node to itself), it can universally approximate continuous functions [&lt;a href=&quot;https://arxiv.org/abs/1912.10077&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.04862&quot;>;2&lt;/a>;]. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Relation to sparse Transformers for sequences&lt;/h3>; &lt;p>; It is interesting to compare Exphormer to sparse attention methods for sequences. Perhaps the architecture most conceptually similar to our approach is &lt;a href=&quot;https://blog.research.google/2021/03/constructing-transformers-for-longer.html&quot;>;BigBird&lt;/a>;, which builds an interaction graph by combining different components. BigBird also uses virtual nodes, but, unlike Exphormer, it uses window attention and random attention from an &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős-Rényi&lt;/a>; random graph model for the remaining components. &lt;/p>; &lt;p>; Window attention in BigBird looks at the tokens surrounding a token in a sequence — the local neighborhood attention in Exphormer can be viewed as a generalization of window attention to graphs. &lt;/p>; &lt;p>; The Erdős-Rényi graph on &lt;em>;n&lt;/em>; nodes, &lt;em>;G(n, p)&lt;/em>;, which connects every pair of nodes independently with probability &lt;em>;p&lt;/em>;, also functions as an expander graph for suitably high &lt;em>;p&lt;/em>;. However, a superlinear number of edges (Ω(&lt;em>;n&lt;/em>; log &lt;em>;n&lt;/em>;)) is needed to ensure that an Erdős-Rényi graph is connected, let alone a good expander. On the other hand, the expanders used in Exphormer have only a &lt;em>;linear&lt;/em>; number of edges. &lt;/p>; &lt;br />; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; Earlier works have shown the use of full graph Transformer-based models on datasets with graphs of size up to 5,000 nodes. To evaluate the performance of Exphormer, we build upon the celebrated &lt;a href=&quot;https://github.com/rampasek/GraphGPS&quot;>;GraphGPS framework&lt;/a>; [&lt;a href=&quot;https://arxiv.org/abs/2205.12454&quot;>;3&lt;/a>;], which combines both message passing and graph transformers and achieves state-of-the-art performance on a number of datasets. We show that replacing dense attention with Exphormer for the graph attention component in the GraphGPS framework allows one to achieve models with comparable or better performance, often with fewer trainable parameters. &lt;/p>; &lt;p>; Furthermore, Exphormer notably allows graph transformer architectures to scale well beyond the usual graph size limits mentioned above. Exphormer can scale up to datasets of 10,000+ node graphs, such as the &lt;a href=&quot;https://arxiv.org/abs/1811.05868&quot;>;Coauthor dataset&lt;/a>;, and even beyond to larger graphs such as the well-known &lt;a href=&quot;https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv&quot;>;ogbn-arxiv dataset&lt;/a>;, a citation network, which consists of 170K nodes and 1.1 million edges. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance .png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot;0&quot; data-original- height=&quot;190&quot; data-original-width=&quot;1522&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance.png&quot; / >;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results comparing Exphormer to standard GraphGPS on the five &lt;a href=&quot; https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of the paper&#39;s publication.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot;0&quot; data-original-height=&quot;202&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results comparing Exphormer to standard GraphGPS on the five &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of publication.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td align=&quot;left&quot;>;&lt;strong>;Model&amp;nbsp;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;PascalVOC-SP&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;F1 score &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;COCO-SP&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;F1 score &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Func&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;AP &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Struct&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;MAE &lt;/font>;&lt;strong>;↓&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;PCQM-Contact&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;MRR &lt;/font>;&lt;strong>;↑&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>;&lt;td colspan=&quot;6&quot;>;&lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />;&lt;/div>;&lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;Standard GraphGPS&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.375 ± 0.011&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.341 ± 0.004&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;&lt;strong>;0.654 ± 0.004&lt;/strong>; &amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.250 ± 0.001&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.334 ± 0.001 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Exphormer (ours)&amp;nbsp;&lt;/em>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.398 ± 0.004&amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.346 ± 0.001 &amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;em>;&amp;nbsp;0.653 ± 0.004&amp;nbsp;&lt;/em>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong >;&lt;em>;&amp;nbsp;0.248 ± 0.001&amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.364 ± 0.002&lt;/em>;&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>;-->; &lt;p>; Finally, we observe that Exphormer, which creates an overlay graph of small diameter via expanders, exhibits the ability to effectively learn long-range dependencies 。 The &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>;&amp;nbsp;is a suite of five graph learning datasets designed to measure the ability of models to capture long-range interactions 。 Results show that Exphormer-based models outperform standard GraphGPS models (which were previously state-of-the-art on four out of five datasets at the time of publication). &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Graph transformers have emerged as an important architecture for ML that adapts the highly successful sequence-based transformers used in NLP to graph-structured data. Scalability has, however, proven to be a major challenge in enabling the use of graph transformers on datasets with large graphs. In this post, we have presented Exphormer, a sparse attention framework that uses expander graphs to improve scalability of graph transformers. Exphormer is shown to have important theoretical properties and exhibit strong empirical performance, particularly on datasets where it is crucial to learn long range dependencies. For more information, we point the reader to a short presentation &lt;a href=&quot;https://icml.cc/virtual/2023/poster/23782&quot;>;video&lt;/a>; from ICML 2023. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our research collaborators Hamed Shirzad and Danica J. Sutherland from The University of British Columbia as well as Ali Kemal Sinop from Google Research. Special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1418736582601940076/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html&quot; rel=&quot;alternate&quot; title=&quot;Exphormer: Scaling transformers for graph-structured data&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s72-c/EXPHORMER%2005large.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-220849549986709693&lt;/id>;&lt;published>;2024-01-18T10:03:00.000-08:00&lt;/published>;&lt;updated>;2024-01-18T10:03:43.608-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Introducing ASPIRE for selective prediction in LLMs&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jiefeng Chen, Student Researcher, and Jinsung Yoon, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7NqezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_-o4sPK8wYe/s320/ASPIRE%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; In the fast-evolving landscape of artificial intelligence, large language models (LLMs) have revolutionized the way we interact with machines, pushing the boundaries of natural language understanding and generation to unprecedented heights. Yet, the leap into high-stakes decision-making applications remains a chasm too wide, primarily due to the inherent uncertainty of model predictions. Traditional LLMs generate responses recursively, yet they lack an intrinsic mechanism to assign a confidence score to these responses. Although one can derive a confidence score by summing up the probabilities of individual tokens in the sequence, traditional approaches typically fall short in reliably distinguishing between correct and incorrect answers. But what if LLMs could gauge their own confidence and only make predictions when they&#39;re sure? &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2017/hash/4a8423d5e91fda00bb7e46540e2b0cf1-Abstract.html&quot;>;Selective prediction&lt;/a>; aims to do this by enabling LLMs to output an answer along with a selection score, which indicates the probability that the answer is correct. With selective prediction, one can better understand the reliability of LLMs deployed in a variety of applications. Prior research, such as &lt;a href=&quot;https://openreview.net/pdf?id=VD-AYtP0dve&quot;>;semantic uncertainty&lt;/a>; and &lt;a href=&quot;https://arxiv.org/pdf/2207.05221.pdf&quot;>;self-evaluation&lt;/a>;, has attempted to enable selective prediction in LLMs. A typical approach is to use heuristic prompts like “Is the proposed answer True or False?” to trigger self-evaluation in LLMs. However, this approach may not work well on challenging &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;question answering&lt;/a>; (QA) tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIwlr34r2t085uhaOx2IbBulTJX2FB2g8LkhTgrKgycgb8cDZaRuht0cFPqmlgSkT5jHOx-rrWywmYAEEfJ0FxlC7ammU8ewrZaVo_My7cCpBNYlfgERRKgFYnF-8LhsWhcyS3KTFdBnhyphenhyphencrenwQxBkbjM8UriPKzji8zDkXYv-5rhRXiE0SlvGmXUV-jt/s1999/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1534&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIwlr34r2t085uhaOx2IbBulTJX2FB2g8LkhTgrKgycgb8cDZaRuht0cFPqmlgSkT5jHOx-rrWywmYAEEfJ0FxlC7ammU8ewrZaVo_My7cCpBNYlfgERRKgFYnF-8LhsWhcyS3KTFdBnhyphenhyphencrenwQxBkbjM8UriPKzji8zDkXYv-5rhRXiE0SlvGmXUV-jt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The &lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>;OPT-2.7B&lt;/a>; model incorrectly answers a question from the &lt;a href=&quot;https://aclanthology.org/P17-1147/&quot;>;TriviaQA&lt;/a>; dataset: “Which vitamin helps regulate blood clotting?” with “Vitamin C”. Without selective prediction, LLMs may output the wrong answer which, in this case, could lead users to take the wrong vitamin. With selective prediction, LLMs will output an answer along with a selection score. If the selection score is low (0.1), LLMs will further output “I don&#39;t know!” to warn users not to trust it or verify it using other sources.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In &quot;&lt;a href=&quot;https://aclanthology.org/2023.findings-emnlp.345.pdf&quot;>;Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs&lt;/a>;&quot;, presented at &lt;a href=&quot;https://2023.emnlp.org/program/accepted_findings/&quot;>;Findings of EMNLP 2023&lt;/a>;, we introduce ASPIRE — a novel framework meticulously designed to enhance the selective prediction capabilities of LLMs. ASPIRE fine-tunes LLMs on QA tasks via &lt;a href=&quot;https://huggingface.co/blog/peft&quot;>;parameter-efficient fine-tuning, &lt;/a>;and trains them to evaluate whether their generated answers are correct. ASPIRE allows LLMs to output an answer along with a confidence score for that answer. Our experimental results demonstrate that ASPIRE significantly outperforms state-of-the-art selective prediction methods on a variety of QA datasets, such as the &lt;a href=&quot;https://aclanthology.org/Q19-1016.pdf&quot;>;CoQA benchmark &lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The mechanics of ASPIRE&lt;/h2>; &lt;p>; Imagine teaching an LLM to not only answer questions but also evaluate those answers — akin to a student verifying their answers in the back of the textbook. That&#39;s the essence of ASPIRE, which involves three stages: (1) task-specific tuning, (2) answer sampling, and (3) self-evaluation learning. &lt;/p>; &lt;p>; &lt;strong>;Task-specific tuning&lt;/strong>;: ASPIRE performs task-specific tuning to train adaptable parameters (θ&lt;sub>;p&lt;/sub>;) while freezing the LLM. Given a training dataset for a generative task, it fine-tunes the pre-trained LLM to improve its prediction performance. Towards this end, parameter-efficient tuning techniques (eg, &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243/&quot;>;soft prompt tuning&lt;/a>; and &lt;a href=&quot;https://openreview.net/forum?id=nZeVKeeFYf9&quot;>;LoRA&lt;/a>;) might be employed to adapt the pre-trained LLM on the task, given their effectiveness in obtaining strong generalization with small amounts of target task data. Specifically, the LLM parameters (θ) are frozen and adaptable parameters (θ&lt;sub>;p&lt;/sub>;) are added for fine-tuning. Only θ&lt;sub>;p&lt;/sub>; are updated to minimize the standard LLM training loss (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy#Cross-entropy_minimization&quot;>;cross-entropy&lt;/a>;). Such fine-tuning can improve selective prediction performance because it not only improves the prediction accuracy, but also enhances the likelihood of correct output sequences. &lt;/p>; &lt;p>; &lt;strong>;Answer sampling&lt;/strong>;: After task-specific tuning, ASPIRE uses the LLM with the learned θ&lt;sub>;p&lt;/sub>; to generate different answers for each training question and create a dataset for self-evaluation learning. We aim to generate output sequences that have a high likelihood. We use &lt;a href=&quot;https://en.wikipedia.org/wiki/Beam_search&quot;>;beam search&lt;/a>; as the decoding algorithm to generate high-likelihood output sequences and the &lt;a href=&quot;https://aclanthology.org/P04-1077/&quot;>;Rouge-L&lt;/a>; metric to determine if the generated output sequence is correct. &lt;/p>; &lt;p>; &lt;strong>;Self-evaluation learning&lt;/strong>;: After sampling high-likelihood outputs for each query, ASPIRE adds adaptable parameters (θ&lt;sub>;s&lt;/sub>;) and only fine-tunes θ&lt;sub>;s&lt;/sub>; for learning self-evaluation. Since the output sequence generation only depends on θ and θ&lt;sub>;p&lt;/sub>;, freezing θ and the learned θ&lt;sub>;p&lt;/sub>; can avoid changing the prediction behaviors of the LLM when learning self-evaluation. We optimize θ&lt;sub>;s&lt;/sub>; such that the adapted LLM can distinguish between correct and incorrect answers on their own. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKSO3s9PgcBr1MpkJR_PYI-ogQ79JD4A4-fJ_OgT8reSKEqIWSUPD7QUVSqIUuAhNfbgEA-XVrOne8S1oJSFaE6YIH4z43bn8jzsfG768qynW-G6lG7dwOvu15UCH6tdlIXEoe2dCUAHmT2bmNijwUvigF50W8vBCsCrjBA_FGYlnsmizHiyutHYZ1A-A2/s1999 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;933&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKSO3s9PgcBr1MpkJR_PYI-ogQ79JD4A4-fJ_OgT8reSKEqIWSUPD7QUVSqIUuAhNfbgEA-XVrOne8S1oJSFaE6YIH4z43bn8jzsfG768qynW-G6lG7dwOvu15UCH6tdlIXEoe2dCUAHmT2bmNijwUvigF50W8vBCsCrjBA_FGYlnsmizHiyutHYZ1A-A2/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The three stages of the ASPIRE framework. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In the proposed framework, θ&lt;sub>;p&lt;/sub>; and θ&lt;sub>;s&lt;/sub>; can be trained using any parameter-efficient tuning approach. In this work, we use &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243/&quot;>;soft prompt tuning&lt;/a>;, a simple yet effective mechanism for learning “&lt;a href=&quot; https://blog.research.google/2022/02/guiding-frozen-language-models-with.html&quot;>;soft prompts&lt;/a>;” to condition frozen language models to perform specific downstream tasks more effectively than traditional discrete text提示。 The driving force behind this approach lies in the recognition that if we can develop prompts that effectively stimulate self-evaluation, it should be possible to discover these prompts through soft prompt tuning in conjunction with targeted training objectives. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCX9MjJ_KqSauAvXOvcXCDWR1H-hoD3e6EaSCEbG5-EJoJYLmekytCRSXaXrhNGS5BH7DfwbZW7FWzUaTErsGxKSWWM8lLAOxxDX3M5U4Zv8gERXBk_uCY7OVshLexrKt5GTSwrkRdFW0dAcMaALHvrLIosv7Tn4pRd7Rh35-HGWQ13SAeHtJ5-wsNgMNt/s800/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;800&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCX9MjJ_KqSauAvXOvcXCDWR1H-hoD3e6EaSCEbG5-EJoJYLmekytCRSXaXrhNGS5BH7DfwbZW7FWzUaTErsGxKSWWM8lLAOxxDX3M5U4Zv8gERXBk_uCY7OVshLexrKt5GTSwrkRdFW0dAcMaALHvrLIosv7Tn4pRd7Rh35-HGWQ13SAeHtJ5-wsNgMNt/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Implementation of the ASPIRE framework via soft prompt tuning. We first generate the answer to the question with the first soft prompt and then compute the learned self-evaluation score with the second soft prompt.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; After training θ&lt;sub>;p&lt;/sub>; and θ&lt;sub>;s&lt;/sub>;, we obtain the prediction for the query via beam search decoding. We then define a selection score that combines the likelihood of the generated answer with the learned self-evaluation score (ie, the likelihood of the prediction being correct for the query) to make selective predictions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; To demonstrate ASPIRE&#39;s efficacy, we evaluate it across three question-answering datasets — &lt;a href=&quot;https://aclanthology.org/Q19-1016/&quot;>;CoQA&lt;/a>;, &lt;a href=&quot;https://aclanthology.org/P17-1147/&quot;>;TriviaQA&lt;/a>;, and &lt;a href=&quot;https://aclanthology.org/D16-1264/&quot;>;SQuAD&lt;/a>; — using various &lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>;open pre-trained transformer&lt;/a>; (OPT) models. By training θ&lt;sub>;p&lt;/sub>; with soft prompt tuning, we observed a substantial hike in the LLMs&#39; accuracy. For example, the &lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>;OPT-2.7B&lt;/a>; model adapted with ASPIRE demonstrated improved performance over the larger, pre-trained OPT-30B model using the CoQA and SQuAD datasets. These results suggest that with suitable adaptations, smaller LLMs might have the capability to match or potentially surpass the accuracy of larger models in some scenarios. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiT3H50FS2Ml9VoWJCuNnInzRQqtNEpyUXuwRnogIG-pDIlRduV0DmvyI9iQIHTziGdqkugV9SDjIcV8WPfnb8QyzQ3ACcusD7O1FQvyVe9U9C5iCbX-uS8xHNhbvg2uv_CPwe4UJASF_dPe8s-c-xz-1hplqXYYxw4wsLOw9dJ-vWrLz -Ei4BrrW-e9Wzc/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1500&quot; data-original-width= &quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiT3H50FS2Ml9VoWJCuNnInzRQqtNEpyUXuwRnogIG-pDIlRduV0DmvyI9iQIHTziGdqkugV9SDjIcV8WPfnb8QyzQ3ACcusD7O1FQvyVe9U9C5iCbX-uS8xHNhbvg2uv_CPwe4UJASF_dPe8s-c-xz-1hplqXYYxw4wsLOw9dJ-vWrLz-Ei4BrrW-e9Wzc/s16000/image4.png&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; When delving into the computation of selection scores with fixed model predictions, ASPIRE received a higher &lt;a href=&quot;https://openreview.net /pdf?id=VD-AYtP0dve&quot;>;AUROC&lt;/a>; score (the probability that a randomly chosen correct output sequence has a higher selection score than a randomly chosen incorrect output sequence) than baseline methods across all datasets. For example, on the CoQA benchmark, ASPIRE improves the AUROC from 51.3% to 80.3% compared to the baselines. &lt;/p>; &lt;p>; An intriguing pattern emerged from the TriviaQA dataset evaluations. While the pre-trained OPT-30B model demonstrated higher baseline accuracy, its performance in selective prediction did not improve significantly when traditional self-evaluation methods — &lt;a href=&quot;https://arxiv.org/abs/2207.05221&quot;>;Self-eval and P(True)&lt;/a>; — were applied. In contrast, the smaller OPT-2.7B model, when enhanced with ASPIRE, outperformed in this aspect. This discrepancy underscores a vital insight: larger LLMs utilizing conventional self-evaluation techniques may not be as effective in selective prediction as smaller, ASPIRE-enhanced models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQLSLnyNPe8LW9W3KLt7Tkp3gmLSLHkTbICIi5j__yWNt7aG9PmWyW5bE4vSs8q2iFqE5dlb0KOdtKzcmg1JuKdzZWFUxYXiatDPB7N-q1NhkkH9hEcgqlw33BFUh_v_8DQJnY5lMrXexv0HUvTPYRS_Gb-M75Rx_TBhxyvLrI-AhZJV243sUzo2gIPoh/s1999/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1599&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQLSLnyNPe8LW9W3KLt7Tkp3gmLSLHkTbICIi5j__yWNt7aG9PmWyW5bE4vSs8q2iFqE5dlb0KOdtKzcmg1JuKdzZWFUxYXiatDPB7N-q1NhkkH9hEcgqlw33BFUh_v_8DQJnY5lMrXexv0HUvTPYRS_Gb-M75Rx_TBhxyvLrI-AhZJV243sUzo2gIPoh/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our experimental journey with ASPIRE underscores a pivotal shift in the landscape of LLMs: The capacity of a language model is not the be-all and end-all of its performance. Instead, the effectiveness of models can be drastically improved through strategic adaptations, allowing for more precise, confident predictions even in smaller models. As a result, ASPIRE stands as a testament to the potential of LLMs that can judiciously ascertain their own certainty and decisively outperform larger counterparts in selective prediction tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In conclusion, ASPIRE is not just another framework; it&#39;s a vision of a future where LLMs can be trusted partners in decision-making. By honing the selective prediction performance, we&#39;re inching closer to realizing the full potential of AI in critical applications. &lt;/p>; &lt;p>; Our research has opened new doors, and we invite the community to build upon this foundation. We&#39;re excited to see how ASPIRE will inspire the next generation of LLMs and beyond. To learn more about our findings, we encourage you to read our paper and join us in this thrilling journey towards creating a more reliable and self-aware AI. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We gratefully acknowledge the contributions of Sayna Ebrahimi, Sercan O Arik, Tomas Pfister, and Somesh Jha.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/220849549986709693/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/introducing-aspire-for-selective.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/220849549986709693&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/220849549986709693&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/introducing-aspire-for-selective.html&quot; rel=&quot;alternate&quot; title=&quot;Introducing ASPIRE for selective prediction in LLMs&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7NqezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_-o4sPK8wYe/s72-c/ASPIRE%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1800430129205268706&lt;/id>;&lt;published>;2024-01-12T09:04:00.000-08:00&lt;/published>;&lt;updated>;2024-01-16T10:55:15.626-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AMIE: A research AI system for diagnostic medical reasoning and conversations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Alan Karthikesalingam and Vivek Natarajan, Research Leads, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/s16000/AMIE.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; The physician-patient conversation is a cornerstone of medicine, in which skilled and intentional communication drives diagnosis, management, empathy and trust. AI systems capable of such diagnostic dialogues could increase availability, accessibility, quality and consistency of care by being useful conversational partners to clinicians and patients alike. But approximating clinicians&#39; considerable expertise is a significant challenge. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Recent progress in large language models (LLMs) outside the medical domain has shown that they can plan, reason, and use relevant context to hold rich conversations. However, there are many aspects of good diagnostic dialogue that are unique to the medical domain. An effective clinician takes a complete “clinical history” and asks intelligent questions that help to derive a differential diagnosis. They wield considerable skill to foster an effective relationship, provide information clearly, make joint and informed decisions with the patient, respond empathically to their emotions, and support them in the next steps of care. While LLMs can accurately perform tasks such as medical summarization or answering medical questions, there has been little work specifically aimed towards developing these kinds of conversational diagnostic capabilities. &lt;/p>; &lt;p>; Inspired by this challenge, we developed &lt;a href=&quot;https://arxiv.org/abs/2401.05654&quot;>;Articulate Medical Intelligence Explorer (AMIE)&lt;/a>;, a research AI system based on a LLM and optimized for diagnostic reasoning and conversations. We trained and evaluated AMIE along many dimensions that reflect quality in real-world clinical consultations from the perspective of both clinicians and patients. To scale AMIE across a multitude of disease conditions, specialties and scenarios, we developed a novel self-play based simulated diagnostic dialogue environment with automated feedback mechanisms to enrich and accelerate its learning process. We also introduced an inference time chain-of-reasoning strategy to improve AMIE&#39;s diagnostic accuracy and conversation quality. Finally, we tested AMIE prospectively in real examples of multi-turn dialogue by simulating consultations with trained actors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s1200/AMIE%20GIF%201%20v2.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s16000/AMIE%20GIF%201%20v2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;AMIE was optimized for diagnostic conversations, asking questions that help to reduce its uncertainty and improve diagnostic accuracy, while also balancing this with other requirements of effective clinical communication, such as empathy, fostering a relationship, and providing information clearly.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Evaluation of conversational diagnostic AI&lt;/h2>; &lt;p>; Besides developing and optimizing AI systems themselves for diagnostic conversations, how to assess such systems is also an open question. Inspired by accepted tools used to measure consultation quality and clinical communication skills in real-world settings, we constructed a pilot evaluation rubric to assess diagnostic conversations along axes pertaining to history-taking, diagnostic accuracy, clinical management, clinical communication skills, relationship fostering and共情。 &lt;/p>; &lt;p>; We then designed a randomized, double-blind crossover study of text-based consultations with validated patient actors interacting either with board-certified primary care physicians (PCPs) or the AI system optimized for diagnostic dialogue. We set up our consultations in the style of an &lt;a href=&quot;https://en.wikipedia.org/wiki/Objective_structured_clinical_examination&quot;>;objective structured clinical examination&lt;/a>; (OSCE), a practical assessment commonly used in the real world to examine clinicians&#39; skills and competencies in a standardized and objective way. In a typical OSCE, clinicians might rotate through multiple stations, each simulating a real-life clinical scenario where they perform tasks such as conducting a consultation with a standardized patient actor (trained carefully to emulate a patient with a particular condition). Consultations were performed using a synchronous text-chat tool, mimicking the interface familiar to most consumers using LLMs today. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETClVRu8Hv3J9gQRd_WGYwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/s1350/image4.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1350&quot; height=&quot;568&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETClVRu8Hv3J9gQRd_WGYwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/w640-h568/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE is a research AI system based on LLMs for diagnostic reasoning and dialogue.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;br />; &lt;h2>;AMIE: an LLM-based conversational diagnostic research AI system &lt;/h2>; &lt;p>; We trained AMIE on real-world datasets comprising medical reasoning, medical summarization and real-world clinical conversations. &lt;/p>; &lt;p>; It is feasible to train LLMs using real-world dialogues developed by passively collecting and transcribing in-person clinical visits, however, two substantial challenges limit their effectiveness in training LLMs for medical conversations. First, existing real-world data often fails to capture the vast range of medical conditions and scenarios, hindering the scalability and comprehensiveness. Second, the data derived from real-world dialogue transcripts tends to be noisy, containing ambiguous language (including slang, jargon, humor and sarcasm), interruptions, ungrammatical utterances, and implicit references. &lt;/p>; &lt;p>; To address these limitations, we designed a self-play based simulated learning environment with automated feedback mechanisms for diagnostic medical dialogue in a virtual care setting, enabling us to scale AMIE&#39;s knowledge and capabilities across many medical conditions and contexts 。 We used this environment to iteratively fine-tune AMIE with an evolving set of simulated dialogues in addition to the static corpus of real-world data described. &lt;/p>; &lt;p>; This process consisted of two self-play loops: (1) an “inner” self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient simulator; and (2) an “outer” self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the inner loop again, creating a virtuous continuous learning cycle. &lt;/p>; &lt;p>; Further, we also employed an inference time chain-of-reasoning strategy which enabled AMIE to progressively refine its response conditioned on the current conversation to arrive at an informed and grounded reply. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRGLoGpkMV99TM1Ru0xpNSNWee-5xfkUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s1400/image1.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1400&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRGLoGpkMV99TM1Ru0xpNSNWee-5xfkUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;AMIE uses a novel self-play based simulated dialogue learning environment to improve the quality of diagnostic dialogue across a multitude of disease conditions, specialities and patient contexts.&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/s1834/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE uses a novel self-play based simulated dialogue learning environment to improve the quality of diagnostic dialogue across a multitude of disease conditions, specialities and patient contexts.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;br />; &lt;p>; We tested performance in consultations with simulated patients (played by trained actors), compared to those performed by 20 real PCPs using the randomized approach described above. AMIE and PCPs were assessed from the perspectives of both specialist attending physicians and our simulated patients in a randomized, blinded crossover study that included 149 case scenarios from OSCE providers in Canada, the UK and India in a diverse range of specialties and diseases. &lt;/p>; &lt;p>; Notably, our study was not designed to emulate either traditional in-person OSCE evaluations or the ways clinicians usually use text, email, chat or telemedicine. Instead, our experiment mirrored the most common way consumers interact with LLMs today, a potentially scalable and familiar mechanism for AI systems to engage in remote diagnostic dialogue. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s1999 /image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the randomized study design to perform a virtual remote OSCE with simulated patients via online multi-turn synchronous text chat.&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Performance of AMIE&lt;/h2>; &lt;p>; In this setting, we observed that AMIE performed simulated diagnostic conversations at least as well as PCPs when both were evaluated along multiple clinically-meaningful axes of consultation quality. AMIE had greater diagnostic accuracy and superior performance for 28 of 32 axes from the perspective of specialist physicians, and 24 of 26 axes from the perspective of patient actors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/s1834/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE outperformed PCPs on multiple evaluation axes for diagnostic dialogue in our evaluations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s1999/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;752&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Specialist-rated top-k diagnostic accuracy. AMIE and PCPs top-k differential diagnosis (DDx) accuracy are compared across 149 scenarios with respect to the ground truth diagnosis (a) and all diagnoses listed within the accepted differential diagnoses (b). Bootstrapping (n=10,000) confirms all top-k differences between AMIE and PCP DDx accuracy are significant with p &amp;lt;0.05 after&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/False_discovery_rate&quot;>;false discovery rate&lt;/a>;&amp;nbsp;(FDR) correction.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mLS635ryiJJOF7ZOuuA4t0rkj1OXWXB57GW-FQcNcYq_TKfTPyCLm-EV3Ivk5yPgYdjYKxT8-yxQnDz4mNJwKop4yS3XvyNpcUzVOrhm0MrJKs5DVfl-u8hgwhwkI6kZAvCto4Z4JGcnTb/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1864&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mLS635ryiJJOF7ZOuuA4t0rkj1OXWXB57GW-FQcNcYq_TKfTPyCLm-EV3Ivk5yPgYdjYKxT8-yxQnDz4mNJwKop4yS3XvyNpcUzVOrhm0MrJKs5DVfl-u8hgwhwkI6kZAvCto4Z4JGcnTb/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diagnostic conversation and reasoning qualities as assessed by specialist physicians. On 28 out of 32 axes, AMIE outperformed PCPs while being comparable on the rest.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4-4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpVgGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s1892/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1714&quot; data-original-width=&quot;1892&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4-4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpVgGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diagnostic conversation and reasoning qualities as assessed by specialist physicians. On 28 out of 32 axes, AMIE outperformed PCPs while being comparable on the rest.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;br />; &lt;h2>;Limitations&lt;/h2>; &lt;p>; Our research has several limitations and should be interpreted with appropriate caution. Firstly, our evaluation technique likely underestimates the real-world value of human conversations, as the clinicians in our study were limited to an unfamiliar text-chat interface, which permits large-scale LLM–patient interactions but is not representative of usual clinical practice. Secondly, any research of this type must be seen as only a first exploratory step on a long journey. Transitioning from a LLM research prototype that we evaluated in this study to a safe and robust tool that could be used by people and those who provide care for them will require significant additional research. There are many important limitations to be addressed, including experimental performance under real-world constraints and dedicated exploration of such important topics as health equity and fairness, privacy, robustness, and many more, to ensure the safety and reliability of the technology. &lt;/p>; &lt;br />; &lt;h2>;AMIE as an aid to clinicians&lt;/h2>; &lt;p>; In a &lt;a href=&quot;https://arxiv.org/abs/2312.00164&quot;>;recently released preprint&lt;/a>;, we evaluated the ability of an earlier iteration of the AMIE system to generate a DDx alone or as an aid to clinicians. Twenty (20) generalist clinicians evaluated 303 challenging, real-world medical cases sourced from the &lt;em>;&lt;a href=&quot;https://www.nejm.org/&quot;>;New England Journal of Medicine&lt;/a>;&lt;/em>; (NEJM) &lt;a href=&quot;https://www.nejm.org/case-challenges&quot;>;ClinicoPathologic Conferences&lt;/a>; (CPCs). Each case report was read by two clinicians randomized to one of two assistive conditions: either assistance from search engines and standard medical resources, or AMIE assistance in addition to these tools. All clinicians provided a baseline, unassisted DDx prior to using the respective assistive tools. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfvMTKASXUVZ5n_I4VytKfa0S3EN5vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s1999/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1022&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfvMTKASXUVZ5n_I4VytKfa0S3EN5vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Assisted randomized reader study setup to investigate the assistive effect of AMIE to clinicians in solving complex diagnostic case challenges from the New England Journal of Medicine.&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; AMIE exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% vs. 33.6%, p= 0.04). Comparing the two assisted study arms, the top-10 accuracy was higher for clinicians assisted by AMIE, compared to clinicians without AMIE assistance (24.6%, p&amp;lt;0.01) and clinicians with search (5.45%, p=0.02). Further, clinicians assisted by AMIE arrived at more comprehensive differential lists than those without AMIE assistance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/s1999/image7.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1424&quot; data-original-width=&quot;1999&quot; height=&quot;456&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/w640-h456/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In addition to strong standalone performance, using the AMIE system led to significant assistive effect and improvements in diagnostic accuracy of the clinicians in solving these complex case challenges.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; It&#39;s worth noting that NEJM CPCs are not representative of everyday clinical practice. They are unusual case reports in only a few hundred individuals so offer limited scope for probing important issues like equity or fairness. &lt;/p>; &lt;br />; &lt;h2>;Bold and responsible research in healthcare — the art of the possible &lt;/h2>; &lt;p>; Access to clinical expertise remains scarce around the world. While AI has shown great promise in specific clinical applications, engagement in the dynamic, conversational diagnostic journeys of clinical practice requires many capabilities not yet demonstrated by AI systems. Doctors wield not only knowledge and skill but a dedication to myriad principles, including safety and quality, communication, partnership and teamwork, trust, and professionalism. Realizing these attributes in AI systems is an inspiring challenge that should be approached responsibly and with care. AMIE is our exploration of the “art of the possible”, a research-only system for safely exploring a vision of the future where AI systems might be better aligned with attributes of the skilled clinicians entrusted with our care. It is early experimental-only work, not a product, and has several limitations that we believe merit rigorous and extensive further scientific studies in order to envision a future in which conversational, empathic and diagnostic AI systems might become safe, helpful and accessible. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The research described here is joint work across many teams at Google Research and Google Deepmind. We are grateful to all our co-authors - Tao Tu, Mike Schaekermann, Anil Palepu, Daniel McDuff, Jake Sunshine, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Sara Mahdavi, Karan Sighal, Shekoofeh Azizi, Nenad Tomasev, Yun Liu, Yong Cheng, Le Hou, Albert Webson, Jake Garrison, Yash Sharma, Anupam Pathak, Sushant Prakash, Philip Mansfield, Shwetak Patel, Bradley Green, Ewa Dominowska, Renee Wong, Juraj Gottweis, Dale Webster, Katherine Chou, Christopher Semturs, Joelle Barral, Greg Corrado and Yossi Matias. We also thank Sami Lachgar, Lauren Winer and John Guilyard for their support with narratives and the visuals. Finally, we are grateful to Michael Howell, James Manyika, Jeff Dean, Karen DeSalvo, Zoubin Ghahramani&amp;nbsp;and Demis Hassabis for their support during the course of this project&lt;/em>;. &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1800430129205268706/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html&quot; rel=&quot;alternate&quot; title=&quot;AMIE: A research AI system for diagnostic medical reasoning and conversations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/s72-c/AMIE.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;