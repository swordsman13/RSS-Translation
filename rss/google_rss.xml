<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-10-04T10:27:44.954-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Publications"></category><category term="Research"></category><category term="Machine Perception"></category><category term="TensorFlow"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Computational Photography"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="MOOC"></category><category term="ICLR"></category><category term="AI for Social Good"></category><category term="Machine Translation"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="accessibility"></category><category term="optimization"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="ACL"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="Android"></category><category term="ICML"></category><category term="TPU"></category><category term="Awards"></category><category term="ML"></category><category term="Structured Data"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="Responsible AI"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="RAI-HCT Highlights"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1287&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8807628089114947298&lt;/id>;&lt;发布>;2023-10-04T10:26:00.003-07:00&lt;/发布>;&lt;更新>;2023-10- 04T10:26:49.868-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI 促进社会公益&quot;>;&lt;/category>;&lt;category schema= &quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot; ICML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;用于科学应用的可扩展球形 CNN&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Carlos Esteves 和 Ameesh Makadia 发布、研究科学家、Google 研究中心、Athena 团队&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacd xHLuwUeW1Q2BIFi6bVw6c1au-9_umDgedCgx2RPogqqhPqDZaP-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s398/image2.gif&quot;样式=“显示：无；” />; &lt;p>; 计算机视觉的典型深度学习模型，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;卷积神经网络&lt;/a>; (CNN) 和&lt;a href=&quot; https://en.wikipedia.org/wiki/Vision_transformer&quot;>;视觉变换器&lt;/a>; (ViT)，假设平面（平坦）空间的处理信号。例如，数字图像表示为平面上的像素网格。然而，此类数据仅占我们在科学应用中遇到的数据的一小部分。从地球大气层采样的变量（例如温度和湿度）自然地呈现在球体上。某些类型的&lt;a href=&quot;https://en.wikipedia.org/wiki/Cosmic_microwave_background&quot;>;​​宇宙学数据&lt;/a>;和全景照片也是球形信号，因此更好地进行处理。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 使用为平面图像设计的方法来处理球形信号存在问题，原因有几个。首先，存在采样问题，即无法在球体上定义平面 CNN 和 ViT 所需的均匀网格，且不会产生严重失真。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU4VfEu7zoN6hcXN5a0xaKnCpLgMub71nN059lUTuQgHiW8shAUKxGjcO3ZQddZ-vBPsykL9WQbv4mK9iAE e5iSha93qzlansJKQJdPzxZfxMkX1PU0YP9j9_nqEJiehqpMAW5p7UmSt6XtQSYzJvOnReXQJTIf6lZcYnHhYZpBfnSzbGs9xVajOP7wVs3/s1692/SphericalDistortions.gif&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;1692&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiU4VfEu7zoN6hcXN5a0xaKnCpLgMub71nN059lUTuQgHiW8shAUKxGjcO3ZQddZ-vBPsykL9WQbv4mK9iAEe5iSha93qzlansJKQJdPzxZfxMkX1PU 0YP9j9_nqEJiehqpMAW5p7UmSt6XtQSYzJvOnReXQJTIf6lZcYnHhYZpBfnSzbGs9xVajOP7wVs3/s16000/SphericalDistortions.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;将球体投影到平面上时，红色圆圈表示的面片在两极附近严重扭曲。这种采样问题损害了传统 CNN 和 ViT 在球形输入上的准确性。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 其次，球体通常因旋转而变得复杂，因此模型需要一种方法来解决这个问题。我们希望与 3D 旋转具有&lt;a href=&quot;https://en.wikipedia.org/wiki/Equivariant_map&quot;>;等变&lt;/a>;，以确保学习到的特征遵循输入的旋转。这可以更好地利用模型参数，并允许使用更少的数据进行训练。 3D 旋转等变在大多数输入没有首选方向的设置中也很有用，例如 3D 形状和分子。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhzyhCxjMHyAkR3LfadfFYdzY2TDks15-Z_pJ9n7UZQgYmf8HDYHX09cl_Qd4j3yOmo7Fr3T-rzZ_OTX5jky3 Vhu1R_N9IPqsCeV1LWAB90tPGsYnQykUDKxwte9dd4ONfYzxmbe1l0JtLoIsvcPKq4XjBAPBe1MGZSc2yFAFRMTBlL3Xp33EhAVPGhwaF/s512/image3.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;256&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhzyhCxjMHyAkR3LfadfFYdzY2TDks15-Z_pJ9n7UZQgYmf8HDYHX09cl_Qd4j3yOmo7Fr3T-rzZ_OTX5jky3Vhu1R_N9IPqsCeV1LWAB90tPGsY nQykUDKxwte9dd4ONfYzxmbe1l0JtLoIsvcPKq4XjBAPBe1MGZSc2yFAFRMTBlL3Xp33EhAVPGhwaF/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;使用全景相机进行无人机竞赛。这里，急转弯会导致球形图像发生较大的 3D 旋转。我们希望我们的模型对于这种旋转具有鲁棒性。来源：&lt;a href=&quot;https://www.youtube.com/watch?v=_J7qXbbXY80&quot;>;https://www.youtube.com/watch?v=_J7qXbbXY80&lt;/a>;（已获得许可;&lt;a href=&quot;https://creativecommons.org/licenses/by/3.0/legalcode&quot;>;CC BY&lt;/a>;）&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcdTjU_u4KqbG40bY_qPZRkt8mS_wPR2cotwKueGkMqRnz2GCkyFNfZb6UPktZ7hIkELEhEEdSlabTFlryxDNicP7Uvv7e 8N96u8WKEe79lWNpeY6eeAPPdCLzrbWEcRcNGPLlZwjgQ16petAInpKme8UPjg-j3wIDO3W9NwxtHkuF5AIi4VqZa6-z_YVw/s711/image1.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;358&quot; data-original-width=&quot;711&quot; height=&quot;322&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcdTjU_u4KqbG40bY_qPZRkt8mS_wPR2cotwKueGkMqRnz2GCkyFNfZb6UPktZ7hIkELEhEEdSlabTFlryxDNicP7Uvv7e8N96u8WKEe79lWNpeY6eeAPP dCLzrbWEcRcNGPLlZwjgQ16petAInpKme8UPjg-j3wIDO3W9NwxtHkuF5AIi4VqZa6-z_YVw/w640-h322/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;在大气中，经常会看到相似的图案出现在不同的位置和方向。我们希望我们的模型能够共享参数来识别这些模式。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;考虑到上述挑战，在“&lt; a href=&quot;https://arxiv.org/abs/2306.05420&quot;>;缩放球形 CNN&lt;/a>;”，发表于 &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/ a>;，我们在 &lt;a href=&quot;https://jax.readthedocs.a>; 中引入了一个开源&lt;a href=&quot;https://github.com/google-research/spherical-cnn&quot;>;库&lt;/a>;。 io/en/latest/&quot;>;JAX&lt;/a>; 用于球面上的深度学习。我们演示了该库的应用程序如何在天气预报和分子特性预测基准（通常使用 Transformer 和图神经网络解决的任务）方面匹配或超越最先进的性能。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;球形 CNN 背景&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv .org/abs/1801.10130&quot;>;球形 CNN&lt;/a>; 利用球形解决了&lt;a href=&quot;https://arxiv.org/abs/1711.06721&quot;>;采样和旋转鲁棒性&lt;/a>;问题卷积和互相关运算，通常通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform#Compact_non-abelian_groups&quot;>;广义傅里叶变换&lt;/a>;计算。然而，对于平面，使用小滤波器的卷积速度更快，因为它可以在规则网格上执行，而无需使用傅立叶变换。迄今为止，球形输入的计算成本较高，限制了球形 CNN 在小型模型和数据集以及低分辨率数据集上的应用。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;我们的贡献&lt;/h2>; &lt;p>; 我们已经实现了 &lt;a href=&quot; 中的球形卷积https://arxiv.org/abs/2006.10731&quot;>;&lt;a href=&quot;https://jax.readthedocs.io/en/latest/&quot;>;JAX&lt;/a>; 中的自旋加权球形 CNN&lt;/a>;注重速度，并使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Data_parallelism&quot;>;数据并行&lt;/a>;对大量 TPU 进行分布式训练。我们还引入了新的相位塌陷激活和光谱批量归一化层，以及可提高准确性和效率的新残差块，从而可以训练比以前大 100 倍的更准确模型。我们将这些新模型应用于分子特性回归和天气预报。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggm83f_OaxfBc4k1uMFMGudMpAWMDMZQkAn9pKBLcdzolHk_3f0emRMupSiu10dcCoxWWEf3KexI6FokwtdkwujNcygfMwc2 p-OvFCSUAWTmM6RE6sAcRCjwRSZxcCoq9gK_sdqyH_lZVLOxe4rL-gFBTtVdZjWjDsxcLPO23bYhOXaMXTUvZoKSs9AVao/s640/image7.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;431&quot; data-original-width=&quot;640&quot; height=&quot;430&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggm83f_OaxfBc4k1uMFMGudMpAWMDMZQkAn9pKBLcdzolHk_3f0emRMupSiu10dcCoxWWEf3KexI6FokwtdkwujNcygfMwc2p-OvFCSUAWTmM6RE6sAcRCjwRS ZxcCoq9gK_sdqyH_lZVLOxe4rL-gFBTtVdZjWjDsxcLPO23bYhOXaMXTUvZoKSs9AVao/w640-h430/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;我们将球形 CNN 的特征尺寸最多扩展了两个数量级和模型容量，与文献相比：&lt;a href=&quot;https://arxiv.org/abs/1801.10130&quot;>;Cohen&#39;18&lt;/a>;、&lt;a href=&quot;https://arxiv. org/abs/1711.06721&quot;>;伊斯特维斯&#39;18&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2006.10731&quot;>;伊斯特维斯&#39;20&lt;/a>; 和&lt;a href=&quot; https://arxiv.org/abs/2010.11661&quot;>;Cobb&#39;21&lt;/a>;。&lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;>;VGG-19&lt;/a>;作为传统的 CNN 参考。我们最大的天气预报模型有 256 x 256 x 78 个输入和输出，在训练期间运行 96 个卷积层，最低内部分辨率为 128 x 128 x 256。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;br />; &lt;h2>;分子特性回归&lt;/h2>; &lt;p>;预测分子特性在&lt;a href=&quot;https://www.nature.com/articles/d41586-018- 05267-x&quot;>;药物发现&lt;/a>;，其目标是快速筛选大量分子，寻找具有所需特性的分子。类似的模型也可能与针对蛋白质之间相互作用的药物设计相关。当前计算或实验量子化学中的方法非常昂贵，这激励了机器学习的使用。 &lt;/p>; &lt;p>; 分子可以用一组原子及其在 3D 空间中的位置来表示；分子的旋转改变了位置，但不改变分子特性。由于球形 CNN 的旋转等方差性，这促进了其应用。然而，分子并没有被定义为球体上的信号，因此第一步是将它们映射到一组球函数。我们通过利用分子原子之间基于物理的相互作用来做到这一点。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBdDPSVCSfr2FQ4_Re4Pdhd0UQ4-1pFUROon3__vXr3-N2kOhWr7uN8K3QfukpYcDIIVj6hPCIAgYhltWpGHHtq5ywvJ _WGLMu5lSo0mbJYUZw8I0H0mw_CDb50D_dGQO__tPt-Jb81ooB6WLw-5Zx6C87jX4fKHgQaPxGy27kykUtrxpvlMuRhsV9Jr7i/s1600/image6 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;1600&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBdDPSVCSfr2FQ4_Re4Pdhd0UQ4-1pFUROon3__vXr3-N2kOhWr7uN8K3QfukpYcDIIVj6hPCIAgYhltWpGHHtq5ywvJ_WGLMu5lSo0mbJYUZw8I0H0m w_CDb50D_dGQO__tPt-Jb81ooB6WLw-5Zx6C87jX4fKHgQaPxGy27kykUtrxpvlMuRhsV9Jr7i/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;每个原子都由一组球形信号表示，这些信号积累了与每种类型的其他原子的物理相互作用（如右侧的三个面板所示）。例如，氧原子（O；上图）有一个氧气通道（由左侧标记为“O”的球体表示）和氢气通道（右侧为“H”）。相对于两个氢原子而言，氧原子上累积的&lt;a href=&quot;https://en.wikipedia.org/wiki/Coulomb%27s_law&quot;>;库仑力&lt;/a>;由红色阴影表示球体底部标记为“H”的区域。由于氧原子本身不产生任何力，因此“O”球体是均匀的。我们为&lt;a href=&quot;https://en.wikipedia.org/wiki/Van_der_Waals_force&quot;>;范德华力&lt;/a>;提供了额外的通道。&lt;/em>;&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 球形 CNN 应用于每个原子的特征，随后将结果组合起来以产生属性预测。这使得大多数属性都具有最先进的性能，通常在 &lt;a href=&quot;https://www.nature.com/articles/sdata201422&quot;>;QM9&lt;/a>; 基准中进行评估：&lt;/p>; &lt;表align =“center”cellpadding =“0”cellspacing =“0”class =“tr-caption-container”style =“margin-left：auto; margin-right：auto;”>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVQebJURRBto17o1rYA7wz-Ze-vwHt0gtDHHbloA51I9__iaHu9KtRm8eI639aChAIqEE-rlmy4v8Kyg4TERuNP4sJfzOkXYwaJ9 CC1DU6-vQf0nH85hBXnuGWYhP6mRMGr_Sns_WonWqAkTtM8bbvnx3QbOdrGFEYF60_f9Q7F6uSzLbzVptd_EnnixNZ/s1409/image4.png&quot;样式= “左边距：自动；右边距：自动；”&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;786&quot; data-original-width=&quot;1409&quot; src=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEhVQebJURRBto17o1rYA7wz-Ze-vwHt0gtDHHbloA51I9__iaHu9KtRm8eI639aChAIqEE-rlmy4v8Kyg4TERuNP4sJfzOkXYwaJ9CC1DU6-vQf0nH85hBXnuGWYh P6mRMGr_Sns_WonWqAkTtM8bbvnx3QbOdrGFEYF60_f9Q7F6uSzLbzVptd_EnnixNZ/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption &quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;在 QM9 的 12 个属性上与最新技术的错误比较（请参阅数据集&lt;a href= “http://www.nature.com/articles/sdata201422&quot;>;论文&lt;/a>;&amp;nbsp;了解详细信息）。我们展示&lt;a href=&quot;https://arxiv.org/abs/2202.02541&quot;>;TorchMD-Net&lt;/a>;&amp;nbsp;和&lt;a href=&quot;https://arxiv.org/abs/2102.03150&quot;>; PaiNN&lt;/a>;&amp;nbsp;结果，将 TorchMD-Net 错误标准化为 1.0（越低越好）。我们的模型（以绿色显示）在大多数目标中都优于基线。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;天气预报&lt;/h2>; &lt;p>;准确的气候预报是及时提供极端天气事件预警、实现有效水资源管理和指导明智的基础设施规划的宝贵工具。在一个日益&lt;a href=&quot;https://www.ncei.noaa.gov/access/billions/&quot;>;受到气候灾害威胁&lt;/a>;的世界中，迫切需要更快、更准确地提供预测比一般环流模型的时间跨度更长。预测模型对于预测应对气候变化努力的安全性和有效性也很重要，例如&lt;a href=&quot;https://www.ametsoc.org/index.cfm/ams/about-ams/ams-statements /statements-of-the-ams-in-force/climate-intervention/&quot;>;气候干预措施&lt;/a>;。当前最先进的技术使用基于流体动力学和热力学的昂贵&lt;a href=&quot;https://en.wikipedia.org/wiki/Numerical_weather_prediction&quot;>;数值模型&lt;/a>;，这些模型在经过一段时间后往往会发生漂移。几天。 &lt;/p>; &lt;p>; 鉴于这些挑战，机器学习研究人员迫切需要解决气候预测问题，因为数据驱动技术具有降低计算成本和提高长期准确性的潜力。球形 CNN 适合这项任务，因为大气数据本身就呈现在球体上。它们还可以有效地处理此类数据中常见的不同位置和方向的重复模式。 &lt;/p>; &lt;p>; 我们将我们的模型应用于多个天气预报基准，并且优于或匹配基于传统 CNN 的神经天气模型（具体来说，&lt;a href=&quot;https://arxiv.org/abs/2002.00469&quot;>;1&lt; /a>;、&lt;a href=&quot;https://arxiv.org/abs/2008.08626&quot;>;2&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2205.10972&quot;>;3&lt;/一个>;）。下面我们展示了&lt;a href=&quot;https://arxiv.org/abs/2202.07575&quot;>;测试设置&lt;/a>;中的结果，其中模型将许多大气变量作为输入，并提前六小时预测它们的值。然后，该模型迭代地应用于其自身的预测，以产生更长的预测。在训练期间，模型最多可提前三天进行预测，并可最多进行五天的评估。 &lt;a href=&quot;https://arxiv.org/abs/2202.07575&quot;>;Keisler&lt;/a>; 为该任务提出了一种图神经网络，但我们证明球形 CNN 在相同设置下可以匹配 GNN 的精度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS-sTrl2nwk9Lys2hVTJznTbLnl3ii-VodPHRD5QkKWRl71Ri5IW1N3j9XqVvdUbSx-vp2UudZiPGtk3Qy5 hfHCoktwbyHVwbONnK7hLA7SSn7wnpRTTJ5FC3Z51tMVRAvAocexLw9AJUqyTOEZPYyINqzGmw2Obgy5LlhQVbLUtJgHLep_POyn6ANTqQz/s1300/image5.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;353&quot; data-original-width=&quot;1300&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS-sTrl2nwk9Lys2hVTJznTbLnl3ii-VodPHRD5QkKWRl71Ri5IW1N3j9XqVvdUbSx-vp2UudZiPGtk3Qy5hfHCoktwbyHVwbONnK7hLA7SSn7w npRTTJ5FC3Z51tMVRAvAocexLw9AJUqyTOEZPYyINqzGmw2Obgy5LlhQVbLUtJgHLep_POyn6ANTqQz/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;使用球形 CNN 提前五天（120 小时）进行迭代天气预报。动画显示给定压力下的具体湿度预测及其误差。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot; 0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au- 9_umDgedCgx2RPogqqhPqDZaP-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s398/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; >;&lt;img border=&quot;0&quot; data-original-height=&quot;398&quot; data-original-width=&quot;361&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZl BUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au-9_umDgedCgx2RPogqqhPqDZaP -Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;使用球形 CNN 进行风速和温度预测。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;其他资源&lt; /h2>; &lt;p>; 我们用于高效球形 CNN 的 JAX 库现已&lt;a href=&quot;https://github.com/google-research/spherical-cnn&quot;>;可用&lt;/a>;。我们已经展示了在分子属性回归和天气预报方面的应用，我们相信该库将有助于其他科学应用以及计算机视觉和 3D 视觉。 &lt;/p>; &lt;p>; 天气预报是 Google 的一个活跃研究领域，其目标是构建更准确、更强大的模型 - 例如 &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;Graphcast&lt;/ a>;，一个最近基于机器学习的中期预测模型 - 并构建能够促进整个研究社区进一步发展的工具，例如最近发布的 &lt;a href=&quot;https://blog.research.google/2023/08 /weatherbench-2-benchmark-for-next.html&quot;>;WeatherBench 2&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是与 Jean-Jacques 合作完成的Slotine，基于之前与 Kostas Daniilidis 和 Christine Allen-Blanchette 的合作。我们感谢 Stephan Hoyer、Stephan Rasp 和 Ignacio Lopez-Gomez 在数据处理和评估方面提供的帮助，感谢 Fei Sha、Vivian Yang、Anudhyan Boral、Leonardo Zepeda-Núñez 和 Avram Hershko 的建议和讨论。我们感谢 Michael Riley 和 Corinna Cortes 对这个项目的支持和鼓励。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8807628089114947298/comments/default &quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/scalable-spherical-cnns-for- science.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /8807628089114947298&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8807628089114947298&quot; rel=&quot;self&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/scalable-spherical-cnns-for-scientific.html&quot; rel=&quot;alternate&quot; title=&quot;可扩展用于科学应用的球形 CNN&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au-9_umDgedCgx2RPogqqhPqDZa P-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s72-c/image2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>; &lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7111364624265266582&lt;/id>;&lt;发布>;2023-10-02T00: 51:00.004-07:00&lt;/发布>;&lt;更新>;2023-10-03T03:39:27.357-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns# “ term=”会议”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http://” www.blogger.com/atom/ns#&quot; term=&quot;ICCV&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 参加 ICCV 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot; byline-author&quot;>;发布者：Shaina Mehta，Google 项目经理&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVnC8dDc8c44GFT7cAZh8PfC8_Mpt4h1rhl-uNMNGoGlNCAZVsT51z89pMqEcVgwa7UPUuvXlr 07PpOJlxomCAyRRTOEssXQxDwm4SX8J4JC_63fKWKywHLuqPHBLRZLl4yYIC311eAXC4r47i1zeZoPg2OXhjxuBmzVXCFn5MrJtH7QhZtMLKzzFvXyvx/s320/ICCV%20hero.jpg “样式=“显示：无；” />; &lt;p>; Google 很荣幸成为 &lt;a href=&quot;&lt;a href=&quot;https://iccv2023.thecvf.com/iccv.2023.sponsors-93.php&quot;>;白金赞助商&lt;/a>; https://iccv2023.thecvf.com/&quot;>;国际计算机视觉会议&lt;/a>;（ICCV 2023），这是一项重要的年度会议，本周将于法国巴黎举行。作为计算机视觉研究领域的领导者，Google 在今年的会议上表现强劲，共收到 60 篇论文，并积极参与 27 个&lt;a href=&quot;https://iccv2023.thecvf.com/list.of.accepted.workshops-90 .php&quot;>;研讨会&lt;/a>;和&lt;a href=&quot;https://iccv2023.thecvf.com/list.of.accepted.tutorials-91.php&quot;>;教程&lt;/a>;。 Google 还很荣幸成为&lt;a href=&quot;https://www.latinxinai.org/iccv-2023&quot;>;简历中的拉丁语&lt;/a>;研讨会的白金赞助商。我们期待分享我们广泛的计算机视觉研究成果，并扩大我们与更广泛的研究界的合作伙伴关系。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 参加 ICCV 2023？我们希望您能够参观 Google 展位，与积极追求计算机视觉最新创新的研究人员交谈，并查看一些预定的展位活动（例如下面列出的演示和问答环节）。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter 帐户，了解有关 ICCV 2023 上 Google 展位活动的更多信息。&lt;/p>; &lt;p>; 请看下面了解有关在 ICCV 2023 上展示的 Google 研究的更多信息（Google 隶属关系以&lt;strong>;粗体&lt;/strong>;显示）。 &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;董事会及组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 总主席：&lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;br />; 财务主席：&lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>; &lt; br />; 劳资关系主席：&lt;strong>;&lt;em>;Rahul Sukthankar&lt;/em>;&lt;/strong>; &lt;br />; 宣传和社交媒体联合主席：&lt;strong>;&lt;em>;龚博清&lt;/em>;&lt;/strong >; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展位活动&lt;/h2>; &lt;div style= &quot;margin-left: 20px;&quot;>; &lt;p>; 标题：即时调整：即时个性化图像到图像生成 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Xuhui Jia&lt;/em>;&lt;/strong>;、&lt;strong >;&lt;em>;Suraj Kothawade&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 4 日星期三中午 12:30 &lt;/p>; &lt;p>; 标题：Open Images V7 (&lt;a href=&quot;https:// storage.googleapis.com/openimages/web_v7/2022_pointillism_arxiv.pdf&quot;>;论文&lt;/a>;，&lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;>;数据集&lt;/a>; , &lt;a href=&quot;https://blog.research.google/2022/10/open-images-v7-now-featuring-point.html&quot;>;博客文章&lt;/a>;) &lt;br />; 演讲者：&lt;strong >;&lt;em>;Rodrigo Benenson&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jasper Uijlings&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jordi Pont-Tuset&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 4 日星期三下午 3:30 &lt;/p>; &lt;p>; 标题：Pixel 8 Pro 上的新魔法橡皮擦&lt;br />; 演讲者：&lt;strong>;&lt;em>;Steven Hickson&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;Pedro Velez&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Albert Shaw&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 5 日星期四上午 10:30 &lt;/p>; &lt;p>; 标题：前言：用于少样本超高分辨率人脸合成的数据驱动体积先验&lt;br />; 演讲者：&lt;strong>;&lt;em>;Marcel Bühler&lt;/em>;&lt;/strong>;， &lt;strong>;&lt;em>;Kripasindhu Sarkar&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 5 日星期四中午 12:30 &lt;/p>; &lt;p>; 标题：哎哟！合成和合成图像的视觉和语言基准 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 5 日星期四下午 1:00 &lt;/ p>; &lt;p>; 标题：Fact Check Explorer 中的图像搜索（&lt;a href=&quot;https://blog.google/products/news/new-features-coming-to-fact-check-explorer/&quot;>;博客文章&lt; /a>;) &lt;br />; 演讲者：&lt;strong>;&lt;em>;Yair Alon&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Avneesh Sud&lt;/em>;&lt;/strong>; &lt;br />; 十月星期四欧洲中部夏令时间 5 日下午 3:30 &lt;/p>; &lt;p>; 标题：UnLoc：视频本地化任务的统一框架（&lt;a href=&quot;https://arxiv.org/pdf/2308.11062.pdf&quot;>;论文&lt;/a >;) &lt;br />; 演讲者：&lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Xuehan Xiong&lt;/em>;&lt;/strong>; &lt;br />; 10 月 6 日，星期五上午 10:30 CEST &lt;/p>; &lt;p>; 标题：逆问题的即时调整潜在扩散模型 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Hyungjin Chung&lt;/em>;&lt;/strong>; &lt;br />; 星期五，10 月 6 日中午 12:30 CEST &lt;/p>; &lt;p>; 标题：现实世界应用的神经隐式表示 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;、&lt;strong >;&lt;em>;Fabian Manhardt&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Marie-Julie Rakotosaona&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 6 日星期五下午 3:30 &lt;/p >; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;已接受论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px ;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.14383.pdf&quot;>;采用轻量级 ToF 传感器的单目密集 SLAM 的多模态神经辐射场&lt;/a>; &lt;br / >; &lt;em>;刘新阳&lt;/em>;、&lt;em>;李一进&lt;/em>;、&lt;em>;滕彦斌&lt;/em>;、&lt;em>;包胡军&lt;/em>;、&lt;em>;张国锋&lt;/em>; 、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;崔兆鹏&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// arxiv.org/pdf/2309.05569.pdf&quot;>;ITI-GEN：包容性文本到图像生成&lt;/a>; &lt;br />; &lt;em>;Cheng Chang&lt;/em>;、&lt;em>;Xuanbai Chen&lt;/em>;、 &lt;em>;柴思琪&lt;/em>;、&lt;em>;陈亨利吴&lt;/em>;、&lt;strong>;&lt;em>;Dmitry Lagun&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thabo Beeler&lt;/em>; &lt;/strong>;, &lt;em>;Fernando De la Torre&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.16201.pdf&quot;>;ASIC：在中对齐稀疏-狂野图像集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Kamal Gupta&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Varun Jampani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Carlos Esteves&lt;/em>;&lt;/strong>;、&lt;em>;Abhinav Shrivastava&lt;/em>;、&lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Noah Snavely&lt;/em>; em>;&lt;/strong>;，&lt;strong>;&lt;em>;阿布舍克·卡尔&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.06833.pdf&quot;>; VQ3D：在 ImageNet 上学习 3D 感知生成模型&lt;/a>; &lt;br />; &lt;em>;Kyle Sargent&lt;/em>;、&lt;em>;Jing Yu Koh&lt;/em>;、&lt;strong>;&lt;em>;Han Zhu&lt;/em>; em>;&lt;/strong>;,&lt;strong>; &lt;em>;张慧文&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Charles Herrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pratul Srinivasan&lt; /em>;&lt;/strong>;、&lt;em>;吴家军&lt;/em>;、&lt;strong>;&lt;em>;孙德清&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// arxiv.org/pdf/2302.11154.pdf&quot;>;开放域视觉实体识别：识别数百万个维基百科实体&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;胡何翔&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;鸾易&lt;/em>;&lt;/strong>;、&lt;em>;杨晨&lt;/em>;*、&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;曼达尔·乔希&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kristina Toutanova&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;Ming-Wei Chang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15343.pdf&quot;>;语言图像预训练的 Sigmoid 损失&lt;/ a>; &lt;br />; &lt;strong>;&lt;em>;翟晓华&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;巴兹尔·穆斯塔法&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;亚历山大·科列斯尼科夫&lt;/em>; em>;&lt;/strong>;，&lt;strong>;&lt;em>;卢卡斯·拜尔&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05422.pdf&quot;>;一次性追踪所有地点的所有内容&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;王倩倩&lt;/em>;&lt;/strong>;、&lt;em>;张彦宇&lt;/em>;、&lt;em>;蔡若金&lt;/ em>;、&lt;strong>;&lt;em>;李正琪&lt;/em>;&lt;/strong>;、&lt;em>;Bharath Hariharan&lt;/em>;、&lt;strong>;&lt;em>;Aleksander Holynski&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Noah Snavely&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06706.pdf&quot;>;Zip-NeRF：基于网格的抗锯齿神经辐射场&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;乔纳森·T.巴伦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;本·米尔登霍尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dor Verbin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pratul P. Srinivasan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Peter Hedman&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.07090.pdf&quot;>;Delta 去噪分数&lt;/a>; &lt;br />; &lt;em>;Amir Hertz&lt;/em>;*, &lt;strong >;&lt;em>;Kfir Aberman&lt;/em>;&lt;/strong>;、&lt;em>;丹尼尔·科恩-奥尔&lt;/em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13508 .pdf&quot;>;DreamBooth3D：主题驱动的文本到 3D 生成&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Amit Raj&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Srinivas Kaza&lt;/a>; &lt;em>;Srinivas Kaza&lt;/a>; &lt;em>;Amit Raj&lt;/em>;&lt;/strong>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;本·普尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迈克尔·尼迈耶&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;纳塔尼尔·鲁伊斯&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;本·米尔登霍尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Shiran Zada&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kfir Aberman &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迈克尔·鲁宾斯坦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔纳森·巴伦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;元祯李&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;瓦伦·贾帕尼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09224。 pdf&quot;>;百科全书式 VQA：有关细粒度类别详细属性的视觉问题&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Thomas Mensink&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jasper Uijlings&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;路易斯·卡斯特雷洪&lt;/em>;&lt;/strong>;、&lt;em>;阿鲁什·戈埃尔&lt;/em>;*、&lt;em>;费利佩·卡达尔&lt;/em>;*、&lt;strong>; &lt;em>;Howard Zhou&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;沙飞&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;Vittorio Ferrar&lt;/em>;i&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.05916.pdf&quot;>;GECCO：几何条件点扩散模型&lt;/a>; &lt;br />; &lt;em>;Michał J. Tyszkiewicz&lt;/em>;、&lt;em>;Pascal Fua&lt;/em>;、&lt;strong>;&lt;em>;Eduard Trulls&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11489.pdf&quot;>;从不成对多视图之间的语义对齐中学习以实现自我中心视频识别&lt;/a>; &lt;br />; &lt;em>;Qitong Wang&lt;/em >;、&lt;strong>;&lt;em>;赵龙&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;袁良哲&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘婷&lt;/em>;&lt;/ strong>;, &lt;em>;Xi Peng&lt;/​​em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17806.pdf&quot;>;用于逆向渲染的神经微面场&lt;/a>; &lt; br />; &lt;em>;Alexander Mai&lt;/em>;、&lt;strong>;&lt;em>;Dor Verbin&lt;/em>;&lt;/strong>;、&lt;em>;Falko Kuester&lt;/em>;、&lt;em>;Sara Fridovich-Keil&lt;/em>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09346.pdf&quot;>;Rosetta 神经元：挖掘模型动物园中的公共单元&lt;/a>; &lt;br />; &lt;em>; Amil Dravid&lt;/em>;、&lt;em>;Yossi Gandelsman&lt;/em>;、&lt;em>;Alexei A. Efros&lt;/em>;、&lt;strong>;&lt;em>;Assaf Shocher&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12066.pdf&quot;>;教 CLIP 数到十&lt;/a>; &lt;br />; &lt;em>;Roni Paiss&lt;/em>;*, &lt;strong>; &lt;em>;Ariel Ephrat&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Omer Tov&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shiran Zada&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;因巴尔·莫塞里&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;米哈尔·伊拉尼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;塔利·德克尔&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.12048.pdf&quot;>;Vox-E：3D 对象的文本引导体素编辑&lt;/a>; &lt;br />; &lt;em>;Etai Sella &lt;/em>;、&lt;em>;Gal Fiebelman&lt;/em>;、&lt;strong>;&lt;em>;Peter Hedman&lt;/em>;&lt;/strong>;、&lt;em>;Hadar Averbuch-Elor&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.12074.pdf&quot;>;CC3D：组合 3D 场景的布局条件生成&lt;/a>; &lt;br />; &lt;em>;Sherwin Bahmani&lt;/em>;，&lt;em >;Jeong Joon Park&lt;/em>;、&lt;em>;Despoina Paschalidou&lt;/em>;、&lt;em>;颜星光&lt;/em>;、&lt;em>;Gordon Wetzstein&lt;/em>;、&lt;em>;Leonidas Guibas&lt;/em>;、&lt; strong>;&lt;em>;Andrea Tagliasacchi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11607.pdf&quot;>;深入研究单目运动感知匹配3D 对象跟踪&lt;/a>; &lt;br />; &lt;em>;黄冠志&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi- Hsuan Tsai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01172.pdf&quot;>;用于 3D 感知图像生成的生成多平面神经辐射&lt;/a >; &lt;br />; &lt;em>;Amandeep Kumar&lt;/em>;、&lt;em>;Ankan Kumar Bhunia&lt;/em>;、&lt;em>;Sanath Narayan&lt;/em>;、&lt;em>;Hisham Cholakkal&lt;/em>;、&lt;em>;Rao穆罕默德·安维尔&lt;/em>;、&lt;em>;萨尔曼·汗&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;em>;Fahad Shahbaz Khan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.07313.pdf&quot;>;M2T：两次屏蔽变形金刚以加快解码速度&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Fabian Mentzer&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;Eirikur Agustsson&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Michael Tschannen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= &quot;https://arxiv.org/pdf/2304.02859.pdf&quot;>;MULLER：用于视觉的多层拉普拉斯调整器&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;屠正中&lt;/em>;&lt;/strong>;，&lt;strong >; &lt;em>;佩曼·米兰法&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;侯赛因·塔莱比&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2303.11305.pdf&quot;>;SVDiff：用于扩散微调的紧凑参数空间&lt;/a>; &lt;br />; &lt;em>;韩利公&lt;/em>;*，&lt;strong>;&lt;em>;李银晓&lt;/em>;&lt; /strong>;, &lt;strong>;&lt;em>;张涵&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;佩曼·米兰法&lt;/em>;&lt;/strong>;, &lt;em>;迪米特里斯·梅塔克萨斯&lt;/em>;, &lt;strong>; >;&lt;em>;Feng Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.08996.pdf&quot;>;利用迭代扩散模型实现真实人脸恢复Beyond&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;赵阳&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;侯廷波&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;余- 苏川&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;贾旭辉&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;李彦东&lt;/em>;&lt;/strong>;，&lt;strong>; &lt; em>;Matthias Grundmann&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08998.pdf&quot;>;利用视觉和语言模型进行统一视觉关系检测&lt;/ a>; &lt;br />; &lt;strong>;&lt;em>;赵龙&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;袁良哲&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;龚伯庆&lt;/ em>;&lt;/strong>;、&lt;strong>;&lt;em>;崔银&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Florian Schroff&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;明轩杨&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Hartwig Adam&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘婷&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03757.pdf&quot;>;3D 运动放大：可视化时变辐射场中的细微运动&lt;/a>; &lt;br />; &lt;em>;Brandon Y. Feng&lt;/ em>;、&lt;em>;哈迪·阿尔扎耶&lt;/em>;、&lt;strong>;&lt;em>;迈克尔·鲁宾斯坦&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;威廉·T.弗里曼&lt;/em>;&lt;/strong>;、&lt; em>;Jia-Bin Huang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.06954.pdf&quot;>;全局特征是图像检索和重新排名所需的全部&lt;/ a>; &lt;br />; &lt;em>;邵世豪&lt;/em>;、&lt;strong>;&lt;em>;陈凯峰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Arjun Karpur&lt;/em>;&lt;/strong>;、 &lt;em>;崔庆华&lt;/em>;、&lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;曹丙一&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.15827.pdf&quot;>;在基于提示的持续学习中引入语言指导&lt;/a>; &lt;br />; &lt;em>;Muhammad Gul Zain Ali Khan&lt;/em>;， &lt;em>;Muhammad Ferjad Naeem&lt;/em>;、&lt;em>;Luc Van Gool&lt;/em>;、&lt;em>;Didier Stricker&lt;/em>;、&lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;、&lt; em>;Muhammad Zeshan Afzal&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01789.pdf&quot;>;用于图像去模糊的多尺度结构引导扩散&lt;/a>; &lt;br / >; &lt;em>;任孟伟&lt;/em>;*、&lt;strong>;&lt;em>;毛里西奥·德尔布拉西奥&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;侯赛因·塔莱比&lt;/em>;&lt;/strong>;、&lt;em>;Guido Gerig&lt;/em>;，&lt;strong>;&lt;em>;Peyman Milanfar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.09711.pdf&quot;>;稳健挑战性条件下的单目深度估计&lt;/a>; &lt;br />; &lt;em>;Stefano Gasperini&lt;/em>;、&lt;em>;Nils Morbitzer&lt;/em>;、&lt;em>;HyunJun Jung&lt;/em>;、&lt;em>;Nassir Navab&lt; /em>;,&lt;strong>; &lt;em>;Federico Tombari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.11751.pdf&quot;>;基于分数扩散模型作为逆向成像的原则性先验&lt;/a>; &lt;br />; &lt;em>;Berthy T. Feng&lt;/​​em>;*，&lt;strong>; &lt;em>;Jamie Smith&lt;/em>;&lt;/strong>;，&lt;strong>;&lt; em>;Michael Rubinstein&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张惠文&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Katherine L. Bouman&lt;/em>;&lt;/strong>;、&lt; strong>; &lt;em>;William T. Freeman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2309.01858.pdf&quot;>;迈向通用图像嵌入：A大规模数据集和通用图像表示的挑战&lt;/a>; &lt;br />; &lt;em>;Nikolaos-Antonios Ypsilantis&lt;/em>;，&lt;strong>;&lt;em>;Kaifeng Chen&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Bingyi Cao&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mario Lipovsky&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Pelin Dogan-Schonberger&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Grzegorz Makosa&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Boris Bluntschli&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mojtaba Seyedhosseini&lt;/em>;&lt;/strong>; ，&lt;em>;Ondrej Chum&lt;/em>;，&lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.06383 .pdf&quot;>;U-RED：部分点云的无监督 3D 形状检索和变形&lt;/a>; &lt;br />; &lt;em>;Yan Di&lt;/em>;、&lt;em>;Chenyangguang Zhu&lt;/em>;、&lt;em>;Ruida张&lt;/em>;、&lt;strong>;&lt;em>;Fabian Manhardt&lt;/em>;&lt;/strong>;、&lt;em>;苏永志&lt;/em>;、&lt;em>;Jason Rambach&lt;/em>;、&lt;em>;Didier Stricker&lt;/em>; em>;、&lt;em>;纪向阳&lt;/em>;、&lt;strong>;&lt;em>;费德里科·汤巴里&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf /2303.17606.pdf&quot;>;AvatarCraft：通过参数化形状和姿势控制将文本转换为神经人类头像&lt;/a>; &lt;br />; &lt;em>;Ruixiang Jiang&lt;/em>;、&lt;em>;Can Wang&lt;/em>;、&lt;em>; >;张静波&lt;/em>;、&lt;strong>;&lt;em>;柴梦雷&lt;/em>;&lt;/strong>;、&lt;em>;何明明&lt;/em>;、&lt;em>;陈东东&lt;/em>;、&lt;em>;廖靖&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14700.pdf&quot;>;通过改进的 AR 模型学习多功能 3D 形状生成&lt;/a>; &lt;br />; &lt;em >;Simian Luo&lt;/em>;、&lt;em>;钱学林&lt;/em>;、&lt;em>;付彦伟&lt;/em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt;em>;泰英&lt;/em>;、&lt;em>;张振宇&lt;/em>;、&lt;em>;王成杰&lt;/em>;、&lt;em>;薛向阳&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// arxiv.org/pdf/2308.11198.pdf&quot;>;稀疏视图中手-物体交互的新颖视图合成和姿势估计&lt;/a>; &lt;br />; &lt;em>;Wentian Qu&lt;/em>;，&lt;em>;Zhaopeng Cui&lt; /em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt;em>;孟晨宇&lt;/em>;、&lt;em>;马翠霞&lt;/em>;、&lt;em>;邓晓明&lt;/em>; , &lt;em>;王红安&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05534.pdf&quot;>;PreSTU：场景文本理解预训练&lt;/a >; &lt;br />; &lt;em>;Jihyung Kil&lt;/em>;*、&lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陈曦&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;胡鹤翔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;塞巴斯蒂安·古德曼&lt;/em>;&lt;/strong>;、&lt;em>;赵伟伦&lt;/em>;、&lt;strong>;&lt; em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://v&quot;>;可变形物体的密集对应的隐式形状表示的自监督学习&lt;/a>; &lt; br />; &lt;em>;张宝文&lt;/em>;、&lt;em>;李嘉禾&lt;/em>;、&lt;em>;邓小明&lt;/em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、 &lt;em>;马翠霞&lt;/em>;、&lt;em>;王红安&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.06948.pdf&quot;>;自我调节提示：不忘初心的基础模型适应&lt;/a>; &lt;br />; &lt;em>;Muhammad Uzair Khattak&lt;/em>;、&lt;em>;Syed Talal Wasi&lt;/em>;、&lt;em>;Muzammal Nasee&lt;/em>;、&lt;em>;Salman Kha&lt;/em>;、&lt;strong>;&lt;em>;严明轩&lt;/em>;&lt;/strong>;、&lt;em>;Fahad Shahbaz Khan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2308.11015.pdf&quot;>;Spectral Graphmer：基于频谱图的变换器，使用多视图彩色图像进行以自我为中心的双手重建&lt;/a>; &lt;br />; &lt;em>;Tze Ho Elden Tse&lt;/em >;*、&lt;strong>;&lt;em>;Franziska Mueller&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;沉正阳&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;唐丹航&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;窦明松&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;张银达&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;Sasa Petrovic&lt;/em>;&lt;/strong>;、&lt;em>;Hyung Jin Chang&lt;/em>;、&lt;strong>;&lt;em>;乔纳森·泰勒&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Bardia Doosti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.12411.pdf&quot;>;在 3D 室内合成不同的人体动作场景&lt;/a>; &lt;br />; &lt;em>;赵凯峰&lt;/em>;、&lt;em>;张艳&lt;/em>;、&lt;em>;王少飞&lt;/em>;、&lt;strong>;&lt;em>;Thabo Beeler&lt;/em>; >;&lt;/strong>;, &lt;em>;Siyu Tang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06419.pdf&quot;>;通过 3D 模型估计未知物体进行跟踪视频中&lt;/a>; &lt;br />; &lt;em>;Denys Rozumnyi&lt;/em>;、&lt;em>;Jiri Matas&lt;/em>;、&lt;em>;Marc Pollefeys&lt;/em>;、&lt;strong>;&lt;em>;Vittorio Ferrari&lt;/ em>;&lt;/strong>;、&lt;em>;Martin R. Oswald&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11062.pdf&quot;>;UnLoc：统一框架视频本地化任务&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;沉彦&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;熊学涵&lt;/em>;&lt;/strong>;、&lt;strong>;&lt; em>;Arsha Nagrani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;王忠浩&lt;/em>;&lt;/strong>;*、&lt;strong >;&lt;em>;葛维娜&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;大卫·罗斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;科迪莉亚·施密德&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06708.pdf&quot;>;动词在行动：提高视频语言模型中的动词理解&lt;/a>; &lt;br />; &lt;em>;Liliane Momeni &lt;/em>;、&lt;strong>;&lt;em>;玛蒂尔德·卡隆&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿尔莎·纳格拉尼&lt;/em>;&lt;/strong>;、&lt;em>;安德鲁·齐瑟曼&lt;/em>;、&lt;强>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2309.06703v1.pdf&quot;>;VLSlice：交互式视觉和-语言切片发现&lt;/a>; &lt;br />; &lt;em>;Eric Slyman&lt;/em>;、&lt;strong>;&lt;em>;Minsuk Kahng&lt;/em>;&lt;/strong>;、&lt;em>;Stefan Lee&lt;/em>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09012.pdf&quot;>;是的，我们可以：基于局部特征的视觉定位的约束近似最近邻&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;Dror Aiger&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德烈·阿劳霍&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;西蒙·林宁&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05922.pdf&quot;>;视听掩码自动编码器&lt;/a>; &lt;br />; &lt;em>;Mariana-Iuliana Georgescu&lt;/em>;*, &lt;强>;&lt;em>;爱德华多·丰塞卡&lt;/em>;&lt;/strong>;、&lt;em>;拉杜·图多尔·伊内斯库&lt;/em>;、&lt;strong>;&lt;em>;马里奥·卢西奇&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;科迪莉亚·施密德&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;阿努拉格·阿纳布&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.11386 .pdf&quot;>;CLR：用于持续学习的通道式轻量级重编程&lt;/a>; &lt;br />; &lt;em>;葛云浩&lt;/em>;、&lt;em>;李岳成&lt;/em>;、&lt;em>;倪硕&lt;/em>; >;、&lt;strong>;&lt;em>;赵家平&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Laurent Itti&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05410.pdf&quot;>;LU-NeRF：通过同步本地未姿势 NeRF 进行场景和姿势估计&lt;/a>; &lt;br />; &lt;em>;泽州&lt;/em>; &lt;em>;程&lt;/em>;*,&lt;strong>; &lt;em>;卡洛斯&lt;/em>; &lt;em>;埃斯特维斯&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Varun&lt;/em>; &lt;em>;Jampani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Abhishek&lt;/em>; &lt;em>;Kar&lt;/em>;&lt;/strong>;、&lt;em>;Subhransu&lt;/em>; &lt;em>;Maji&lt;/em>;、&lt;strong>;&lt;em>;Ameesh&lt;/em>; &lt;em>;Makadia&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/2304.10075.pdf&quot;>;实时抗锯齿神经渲染的多尺度表示&lt;/a>; &lt;br />; &lt;em>;Dongting&lt;/em>; &lt;em>;Hu&lt;/em>;，&lt;em>;Zhenkai&lt; /em>; &lt;em>;张&lt;/em>;、&lt;strong>;&lt;em>;廷波&lt;/em>; &lt;em>;侯&lt;/em>;&lt;/strong>;、&lt;em>;铜梁&lt;/em>; &lt;em>;刘&lt;/em>; em>;, &lt;em>;欢&lt;/em>; &lt;em>;福&lt;/em>;, &lt;em>;明明&lt;/em>; &lt;em>;宫&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //arxiv.org/pdf/2304.10532.pdf&quot;>;Nerfbusters：从随意捕获的 NeRF 中移除幽灵文物&lt;/a>; &lt;br />; &lt;em>;Frederik Warburg&lt;/em>;、&lt;em>;Ethan Weber&lt;/em>;、 &lt;em>;Matthew Tancik&lt;/em>;、&lt;strong>;&lt;em>;Aleksander Holynski&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Angjoo Kanazawa&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05407.pdf&quot;>;在没有先验知识的情况下分割已知物体和看不见的未知物体&lt;/a>; &lt;br />; &lt;em>;Stefano Gasperini&lt;/em>;，&lt;em>;阿尔瓦罗·马科斯-拉米罗&lt;/em>;、&lt;em>;迈克尔·施密特&lt;/em>;、&lt;em>;纳西尔·纳瓦布&lt;/em>;、&lt;em>;本杰明·布萨姆&lt;/em>;、&lt;strong>;&lt;em>;费德里科·汤巴里&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14340.pdf&quot;>;SparseFusion：融合多模态稀疏表示用于多传感器 3D 对象检测&lt;/a >; &lt;br />; &lt;em>;谢一辰&lt;/em>;、&lt;em>;徐晨风&lt;/em>;、&lt;strong>;&lt;em>;Marie-Julie Rakotosaona&lt;/em>;&lt;/strong>;、&lt;em>;Patrick Rim&lt; /em>;、&lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;、&lt;em>;Kurt Keutzer&lt;/em>;、&lt;em>;富冢正义&lt;/em>;、&lt;em>;魏战&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15446.pdf&quot;>;SwiftFormer：基于 Transformer 的实时移动视觉应用的高效附加注意力&lt;/a>; &lt;br />; &lt;em>;Abdelrahman Shaker&lt;/em>;、&lt;em>;Muhammad Maa&lt;/em>;、&lt;em>;Hanoona Rashee&lt;/em>;、&lt;em>;Salman Kha&lt;/em>;、&lt;strong>;&lt;em>;严明轩&lt;/em>;&lt;/strong>;，&lt;em>;Fahad Shahbaz Kha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12948.pdf&quot;>;敏捷建模：来自在几分钟内从概念到分类器&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Otilia Stretcu&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Edward Vendrow&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Kenji Hata&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Krishnamurthy Viswanathan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;维托里奥·法拉利、&lt;/em>;&lt;/strong>; &lt;strong >;&lt;em>;Sasan Tavakkol&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;周文磊&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Aditya Avinash&lt;/em>;&lt;/strong>;、&lt;强>;&lt;em>;罗恩明&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尼尔戈登奥尔德林&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;穆罕默德侯赛因巴泰尼&lt;/em>;&lt;/strong>; ,&lt;strong>;&lt;em>;加布里埃尔·伯杰&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;安德鲁·邦纳&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;卢春达&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;哈维尔·A·雷伊&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;朱利亚·德萨尔沃&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;兰杰·克里希纳&lt;/em>; >;&lt;/strong>;、&lt;strong>;&lt;em>;Ariel Fuxman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09011.pdf&quot;>;CAD -Estate：RGB 视频中的大型 CAD 模型注释&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Kevis-Kokitsi Maninis&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Stefan Popov&lt;/em >;&lt;/strong>;、&lt;em>;马蒂亚斯·尼斯纳&lt;/em>;、&lt;strong>;&lt;em>;维托里奥·法拉利&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/2306.01209.pdf&quot;>;恶劣天气下的人数统计&lt;/a>; &lt;br />; &lt;em>;黄志凯&lt;/em>;、&lt;em>;陈伟霆&lt;/em>;、&lt;em>;袁-Chun Jiang&lt;/em>;、&lt;em>;Sy-Yen Kuo&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://arxiv.org/pdf/2304.06025.pdf&quot;>;DreamPose：具有稳定扩散的时尚视频合成&lt;/a>; &lt;br />; &lt;em>;Johanna Karras&lt;/em>;，&lt;strong>;&lt;em>;Aleksander Holynski&lt; /em>;&lt;/strong>;、&lt;em>;王庭春&lt;/em>;、&lt;em>;Ira Kemelmacher-Shlizerman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2301.09637.pdf&quot;>;InfiniCity：无限规模城市综合&lt;/a>; &lt;br />; &lt;em>;Chieh Hubert Lin&lt;/em>;、&lt;em>;Hsin-Ying Lee&lt;/em>;、&lt;em>;Willi Menapace&lt;/em>;、&lt;em>;Menglei Chai&lt;/em>;、&lt;em>;Aliaksandr Siarohin&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;em>;Sergey Tulyakov &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://faculty.ucmerced.edu/mhyang/papers/iccv2023_sampling.pdf&quot;>;采样：场景自适应分层多平面图像表示，用于从单幅图像&lt;/a>; &lt;br />; &lt;em>;周小宇&lt;/em>;、&lt;em>;林志伟&lt;/em>;、&lt;em>;单晓军&lt;/em>;、&lt;em>;王永涛&lt;/em>;、 &lt;strong>;&lt;em>;孙德清&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;http: //sifeiliu.net/NDLM_ICCV23tutorial/&quot;>;利用噪声和无标签数据学习无法分类的大型模型&lt;/a>; &lt;br />; &lt;em>;Sifei Liu&lt;/em>;，&lt;em>;洪旭尹&lt;/em>;，&lt; em>;Shalini De Mello&lt;/em>;、&lt;em>;Pavlo Molchanov&lt;/em>;、&lt;em>;Jose M. Alvarez&lt;/em>;、&lt;em>;Jan Kautz&lt;/em>;、&lt;em>;小龙&lt;/em>; &lt;em>;王&lt;/em>;、&lt;em>;Anima Anandkumar&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;em>;Trevor Darrell&lt;/em>; &lt;br / >; 演讲者：&lt;strong>;&lt;em>;Varun Jampani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.latinxinai.org/iccv-2023&quot;>;人工智能中的拉丁语&lt;/a>; &lt;br />; 白金赞助商&lt;br />; 小组成员：&lt;strong>;&lt;em>;Daniel Castro Chin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Andre Araujo&lt;/em>;&lt;/strong>; &lt;br />; 特邀演讲嘉宾：&lt;strong>; &lt;em>;Irfan Essa&lt;/em>;&lt;/strong>; &lt;br />; 志愿者：&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;袁良哲&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pedro Velez&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vincent Etter&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://sg2rl.github.io/index.html&quot;>;场景图和图表示学习&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Federico Tombari&lt; /em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://web.northeastern.edu/smilelab/amfg2023/&quot;>;人脸和手势分析与建模国际研讨会&lt;/a>; &lt; br />; 演讲者：&lt;strong>;&lt;em>;Todd Zickler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://3dv-in-ecommerce.github.io/&quot;>;3D电子商务中的愿景和建模挑战&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://bigmac- vision.github.io/&quot;>;BigMAC：计算机视觉的大模型适应&lt;/a>; &lt;br />;组织者：&lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://iccv23-arow.github.io/&quot;>;现实世界中的对抗鲁棒性（AROW）&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;白宇桐&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://geonet-challenge.github.io/ICCV2023/&quot;>;GeoNet：第一届跨地域鲁棒计算机视觉研讨会&lt;/a>; &lt;br />; 演讲者: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Tarun Kalluri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= “https://gkioxari.github.io/Tutorials/iccv2023/&quot;>;Quo Vadis，计算机视觉？ &lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;比尔·弗里曼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/ view/vshh/home&quot;>;去 NeRF 还是不去 NeRF：人类大脑的视图合成挑战&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>; &lt;br / >; 组织者：&lt;strong>;&lt;em>;Stefanos Zafeiriou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/nivt-iccv2023/&quot; >;视觉变形金刚的新想法&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;br />;组织者：&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://lsfsl.net/limit23/&quot;>;非常有限的图像表示学习：自我、合成和公式监督的潜力&lt;/a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Manel Baradad Jurjo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/rcv2023 /&quot;>;面向计算机视觉的资源高效深度学习&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>; &lt;br />;组织者：&lt;strong>;&lt;em>;余家辉&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rishabh Tiwari&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://cvaad-workshop.github.io/&quot;>;计算机视觉辅助建筑设计&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Noah Snavely&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://av4d.org/&quot;>;AV4D：空间声音的视觉学习&lt;/a>; &lt;br />;组织者：&lt;strong>;&lt;em>;David Harwath&lt;/em >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://wvlar.github.io/iccv23/&quot;>;视觉与语言算法推理&lt;/a>; &lt;br />; 演讲者：&lt;strong >;&lt;em>;François Chollet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://v&quot;>;自动驾驶和机器人技术的神经场&lt;/a>; &lt;br />;演讲者： &lt;strong>;&lt;em>;乔恩·巴伦&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://campworkshop.org/&quot;>;构图和多模态感知国际挑战&lt;/a>; &lt; br />; 组织者：&lt;strong>;&lt;em>;Ranjay Krishna&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://opensun3d.github.io/&quot;>;开放词汇 3D 场景理解 (OpenSUN3D)&lt;/a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Thomas Funkhouser&lt;/em>;&lt;/strong>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Francis Engelmann&lt;/em>;&lt;/强>;，&lt;强>;&lt;em>;约翰娜·沃尔德&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;费德里科·托姆巴里&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;列奥尼达斯·吉巴斯&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mono3d-iccv-workshop&quot;>;单目 3D 感知前沿：几何基础模型&lt;/a>; &lt; br />; 演讲者：&lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/perdream/home &quot;>;PerDream：通过多模式基础建模进行感知、决策和推理&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://cmp.felk.cvut.cz/sixd/workshop_2023/&quot;>;恢复 6D 物体姿势&lt;/a>; &lt;br />;演讲者：&lt;strong>;Fabian Manhardt&lt;/strong>;、&lt;strong>; Martin Sundermeyer&lt; /strong>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Martin Sundermeyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view /wicviccv2023/&quot;>;计算机视觉领域的女性 (WiCV)&lt;/a>; &lt;br />; 小组成员：&lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://languagefor3dscenes.github.io/ICCV2023/&quot;>;3D 场景语言&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://ai3dcc.github.io/&quot;>;人工智能用于 3D 内容创作&lt;/a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Kai-Hung Chang&lt;/em>;&lt;/strong >; &lt;br />; 组织者：&lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cv4metaverse /&quot;>;Metaverse 的计算机视觉&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Jon Barron&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thomas Funkhouser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.datacomp.ai/workshop.html#first&quot;>;迈向下一代计算机视觉数据集&lt;/a>; &lt;br />; 演讲者：&lt;strong>; &lt;em>;Tom Duerig&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style -span&quot; style=&quot;font-size:small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http:// blog.research.google/feeds/7111364624265266582/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google /2023/10/google-at-iccv-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www. blogger.com/feeds/8474926331452026626/posts/default/7111364624265266582&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/7111364624265266582&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-at-iccv-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICCV 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161 &lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https:// /img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEhVnC8dDc8c44GFT7cAZh8PfC8_Mpt4h1rhl-uNMNGoGlNCAZVsT51z89pMqEcVgwa7UPUuvXlr07PpOJlxomCAyRRTOEssXQxDwm4SX8J4JC_63fKWK ywHLuqPHBLRZLl4yYIC311eAXC4r47i1zeZoPg2OXhjxuBmzVXCFn5MrJtH7QhZtMLKzzFvXyvx/s72-c/ICCV%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media :thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签:blogger.com,1999:blog-8474926331452026626.post-3765645246777916540&lt;/id>;&lt;发布>;2023-09 -28T13:01:00.000-07:00&lt;/发布>;&lt;更新>;2023-09-28T13:01:53.849-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;计算摄影&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema= &quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;DynIBaR：动态场景视频的时空视图合成&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院研究科学家 Zengqi Li 和 Noah Snavely &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhGX53UfU9eWiSTRWdkrUWNln3KGyagkwfUi_38zEihOJ2qLkmQ-3yNsuJJ7SkJ-BTLlVrxJlyoEYl7-tAer6v4MnIAw49TWLGwa8cFgl_c_2WUJqw1O3J8n VhU-VtVwO5Z-bq7rH6pZj7APe5yDZCUSZyeDO39shlGFkVsSQDd1ZWYUT0eDmeJ9JLoWlA3/s320/hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 手机相机是捕捉日常生活瞬间的强大工具。然而，使用单个相机捕​​捉动态场景从根本上是有限的。例如，如果我们想要调整摄像机运动或录制视频的时间（例如，在扫动摄像机以突出戏剧性时刻时冻结时间），我们通常需要带有同步摄像机装备的昂贵的好莱坞设置。在没有好莱坞预算的情况下，仅通过手机摄像头拍摄的视频是否有可能实现类似的效果？ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2211.11082&quot;>;DynIBaR：基于神经动态图像的渲染&lt;/a”中>;”，&lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/Awards&quot;>;最佳论文荣誉奖&lt;/a>;，位于 &lt;a href=&quot;https://cvpr2023.thecvf.com/ Conferences/2023&quot;>;CVPR 2023&lt;/a>;，我们描述了一种新方法，可以从复杂动态场景的单个视频生成逼真的自由视点渲染。基于神经动态图像的渲染 (DynIBaR) 可用于生成一系列视频效果，例如“&lt;a href=&quot;https://en.wikipedia.org/wiki/Bullet_time&quot;>;子弹时间&lt;/a>;”使用手机相机拍摄的单个视频的效果（时间暂停并且相机以正常速度围绕场景移动）、视频稳定、景深和慢动作。我们证明，DynIBaR 显着改进了复杂移动场景的视频渲染，为新型视频编辑应用打开了大门。我们还在 DynIBaR &lt;a href=&quot;https://dynibar.github.io/&quot;>;项目页面上发布了&lt;a href=&quot;https://github.com/google/dynibar&quot;>;代码&lt;/a>; &lt;/a>;，所以你可以自己尝试一下。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;对于复杂动态场景的野外视频，DynIBaR 可以冻结时间，同时允许摄像机继续在场景中自由移动。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt; /table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;背景&lt;/h2>; &lt;p>; 过去几年，计算机取得了巨大进步使用&lt;a href=&quot;https://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot;>;神经辐射场&lt;/a>; (NeRF) 进行重建和渲染的视觉技术静态（非移动）3D 场景。然而，人们用移动设备拍摄的大多数视频都描绘了移动的物体，例如人、宠物和汽车。这些移动场景导致了更具挑战性的 4D（3D + 时间）场景重建问题，无法使用标准视图合成方法来解决。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;标准视图合成方法在应用于动态场景的视频时会输出模糊、不准确的渲染。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 其他最近处理视图的方法使用时空神经辐射场合成动态场景（即&lt;em>;&lt;a href=&quot;https://www.cs.cornell.edu/~zl548/NSFF/&quot;>;动态 NeRF&lt;/a>;&lt;/em >;），但这些方法仍然表现出固有的局限性，阻碍了它们应用于随意捕捉的野外视频。特别是，他们很难从具有长时间持续时间、不受控制的摄像机路径和复杂物体运动的视频中呈现高质量的新颖视图。 &lt;/p>; &lt;p>; 关键的缺陷是它们将复杂的移动场景存储在单个数据结构中。特别是，它们以多层感知器（MLP）神经网络的权重对场景进行编码。 MLP 可以近似任何函数 - 在本例中，是映射 4D 时空点的函数（&lt;em>;x&lt;/em>;、&lt;em>;y&lt;/em>;、&lt;em>;z&lt;/em>;、&lt;em>; >;t&lt;/em>;）转换为我们可以在渲染场景图像时使用的 RGB 颜色和密度。然而，该 MLP 的容量（由其神经网络中的参数数量定义）必须根据视频长度和场景复杂性而增加，因此，在野外视频上训练此类模型在计算上可能很困难。结果，我们得到了模糊、不准确的渲染，就像 &lt;a href=&quot;https://free-view-video.github.io/&quot;>;DVS&lt;/a>; 和 &lt;a href=&quot;https:// 生成的渲染一样。 www.cs.cornell.edu/~zl548/NSFF/&quot;>;NSFF&lt;/a>;（如下所示）。 DynIBaR 通过采用不同的渲染范例来避免创建如此大的场景模型。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-0.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;对于复杂动态场景的视频，与之前的动态视图合成方法（&lt;strong>;顶行&lt;/strong>;）相比，DynIBaR（&lt;strong>;底行&lt;/strong>;）显着提高了渲染质量。先前的方法会产生模糊的渲染，因为它们需要将整个移动场景存储在 MLP 数据结构中。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;基于图像的渲染 (IBR)&lt;/h2>; &lt;p>; DynIBaR 背后的一个关键见解是，我们实际上不需要将所有场景内容存储在巨型 MLP 中的视频。相反，我们直接使用附近输入视频帧的像素数据来渲染新视图。 DynIBaR 构建于名为 &lt;a href= 的&lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Image-based_modeling_and_rendering&quot;>;基于图像的渲染&lt;/a>;&lt;/em>; (IBR) 方法之上“https://ibrnet.github.io/&quot;>;IBRNet&lt;/a>; 专为静态场景的视图合成而设计。 IBR 方法认识到场景的新目标视图应该与附近的源图像非常相似，因此通过从附近的源帧动态选择和扭曲像素来合成目标，而不是提前重建整个场景。特别是，IBRNet 学习将附近的图像混合在一起，以在体积渲染框架内重新创建场景的新视图。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DynIBaR：将 IBR 扩展到复杂的动态视频&lt;/h2>; &lt;p>; 将 IBR 扩展到动态视频场景中，我们需要在渲染过程中考虑场景运动。因此，作为重建输入视频的一部分，我们求解每个 3D 点的运动，其中我们使用 MLP 编码的运动轨迹场来表示场景运动。与之前在 MLP 中存储整个场景外观和几何形状的动态 NeRF 方法不同，我们只存储运动（一种更平滑和稀疏的信号），并使用输入视频帧来确定渲染新视图所需的所有其他内容。 &lt;/p>; &lt;p>; 我们通过获取每个输入视频帧、使用体积渲染渲染光线以形成 2D 图像来优化给定视频的 DynIBaR（如 &lt;a href=&quot;https://ai.googleblog.com/2023 /06/reconstructing-indoor-spaces-with-nerf.html&quot;>;NeRF&lt;/a>;），并将渲染图像与输入帧进行比较。也就是说，我们的优化表示应该能够完美地重建输入视频。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVxjezPKVF8ofpfJP7WtIN3-wOMHJPXISABUaoBbnS9wMtko3lTDTfMfntgTTkcNfG8DxFP63DobaehzDY2QA3OcNf-tJW 6JJk1ghWO2oma_XWYD2FcKeylPDYPUhqKax8FBCy9qxZ3RzizzQluECi6mV2lkuOkd3r16wRaG5mRcpFt_JCSDyUt6Q8BTUb/s660/image3.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;660&quot; height=&quot;465&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVxjezPKVF8ofpfJP7WtIN3-wOMHJPXISABUaoBbnS9wMtko3lTDTfMfntgTTkcNfG8DxFP63DobaehzDY2QA3OcNf-tJW6JJk1ghWO2oma_XWYD2FcKeyl PDYPUhqKax8FBCy9qxZ3RzizzQluECi6mV2lkuOkd3r16wRaG5mRcpFt_JCSDyUt6Q8BTUb/w640-h465/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们说明了 DynIBaR 如何渲染动态场景的图像。为了简单起见，我们展示了一个 2D 世界，如从上面看到的。 (&lt;strong>;a&lt;/strong>;) 一组输入源视图（&lt;strong>;三角形&lt;/strong>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Viewing_frustum&quot;>;相机 frusta&lt;/a >;) 观察一个立方体在场景中移动（&lt;strong>;动画方块&lt;/strong>;）。每个摄像机都标有其时间戳（&lt;em>;t&lt;/em>;-2、&lt;em>;t&lt;/em>;-1 等）。 (&lt;strong>;b&lt;/strong>;) 为了渲染时间 &lt;em>;t&lt;/em>; 时相机的视图，DynIBaR 会通过每个像素（&lt;strong>;蓝线&lt;/strong>;）发射虚拟光线，并计算颜色以及沿该射线的样本点的不透明度。为了计算这些属性，DyniBaR 通过多视图几何将这些样本投影到其他视图中，但首先，我们必须补偿每个点的估计运动（&lt;strong>;红虚线&lt;/strong>;）。 (&lt;strong>;c&lt;/strong>;) 使用此估计运动，DynIBaR 将 3D 中的每个点移动到相关时间，然后将其投影到相应的源相机中，以对用于渲染的颜色进行采样。 DynIBaR 优化每个场景点的运动，作为学习如何合成场景新视图的一部分。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 但是，重建和派生新视图复杂、移动场景的视图是一个非常不适定的问题，因为有许多解决方案可以解释输入视频 - 例如，它可能会为每个时间步创建断开连接的 3D 表示。因此，仅优化 DynIBaR 来重建输入视频是不够的。为了获得高质量的结果，我们还引入了其他几种技术，包括一种称为跨时间渲染的方法。跨时间渲染是指使用 4D 表示在某一时刻的状态来渲染不同时刻的图像，这有助于 4D 表示随着时间的推移保持连贯。为了进一步提高渲染保真度，我们自动将场景分解为两个组件，静态组件和动态组件，分别通过时不变和时变场景表示进行建模。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;创建视频效果&lt;/h2>; &lt;p>; DynIBaR 支持各种视频效果。我们在下面展示了几个例子。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;视频稳定&lt;/h3>; &lt;p>; 我们使用摇晃的手持输入视频来比较 DynIBaR 的视频稳定性能优于现有 2D 视频稳定和动态 NeRF 方法，包括 &lt;a href=&quot;https://alex04072000.github.io/FuSta/&quot;>;FuSta&lt;/a>;、&lt;a href=&quot;https://dl.acm .org/doi/abs/10.1145/3363550&quot;>;DIFRINT&lt;/a>;、&lt;a href=&quot;https://hypernerf.github.io/&quot;>;HyperNeRF&lt;/a>; 和 &lt;a href=&quot;https:// /www.cs.cornell.edu/~zl548/NSFF/&quot;>;NSFF&lt;/a>;。我们证明 DynIBaR 可以产生更平滑的输出，具有更高的渲染保真度和更少的伪影（例如，闪烁或模糊的结果）。特别是，FuSta 会产生残留的相机抖动，DIFRINT 会在对象边界周围产生闪烁，而 HyperNeRF 和 NSFF 会产生模糊的结果。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-4.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h3>;同时视图合成和慢动作&lt;/h3>; &lt;p>; DynIBaR 可以同时在空间和时间上进行视图合成，产生流畅的 3D 电影效果。下面，我们演示了 DynIBaR 可以接受视频输入并生成使用新颖的相机路径渲染的平滑的 5 倍慢动作视频。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h3>;视频散景&lt;/h3>; &lt;p>; DynIBaR 还可以通过合成视频生成高质量的&lt;a href=&quot;https://en.wikipedia.org/wiki/Bokeh&quot;>;视频散景&lt;/a>;动态变化的&lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_of_field&quot;>;景深&lt;/a>;。给定全焦点输入视频，DynIBar 可以生成具有不同离焦区域的高质量输出视频，这些区域引起人们对移动（例如，奔跑的人和狗）和静态内容（例如，树木和建筑物）的注意场景中。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;结论&lt;/h2>; &lt;p>; DynIBaR 是我们从新摄像机路径渲染复杂移动场景的能力的一次飞跃。虽然它目前涉及每个视频的优化，但我们设想可以在野外视频上部署更快的版本，以便为使用移动设备的消费者视频编辑提供新的效果。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;DynIBaR 是以下研究人员合作的成果：谷歌研究中心和康奈尔大学。本文中介绍的工作的主要贡献者包括Zhengqi Li、Qianqian Wang、Forrester Cole、Richard Tucker 和Noah Snavely。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog. Research.google/feeds/3765645246777916540/comments/default&quot; rel=&quot;replies&quot; title=&quot;帖子评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023 /09/dynibar-space-time-view-synthesis-from.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http:// www.blogger.com/feeds/8474926331452026626/posts/default/3765645246777916540&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626 /posts/default/3765645246777916540&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/dynibar-space-time-view- Synthesis-from.html&quot; rel=&quot;alternate&quot; title=&quot;DynIBaR：动态场景视频的时空视图合成&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri >;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google. com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:缩略图高度=“72”网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGX53UfU9eWiSTRWdkrUWNln3KGyagkwfUi_38zEihOJ2qLkmQ-3yNsuJJ7SkJ-BTLlVrxJlyoEYl7-tAer6v4MnIAw49TWLGWa8 cfgl_c_2WUJqw1O3J8nVhU-VtVwO5Z-bq7rH6pZj7APe5yDZCUSZyeDO39shlGFkVsSQDd1ZWYUT0eDmeJ9JLoWlA3/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns: media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com ,1999:blog-8474926331452026626.post-2311847718846675808&lt;/id>;&lt;发布>;2023-09-28T11:16:00.002-07:00&lt;/发布>;&lt;更新>;2023-09-28T11:22:44.163-07 :00 &lt;/更新>; &lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“深度学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/” atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type= &quot;text&quot;>;通过分布鲁棒优化重新加权梯度下降&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Ramnath Kumar，博士前研究员和 Arun Sai Suggala，研究科学家, Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONjsYNtXi5AX4ZZ6qPfdY2Gwt2W5xZy1PF1m9ZaxucZtRZ5ZzmsNzHNNWkCKcpX1_n15DkZCR59ivF_uDCUUmbdijaAuiK 3tlC9HahtKc1s1r6pQWwbwMhnuNK3Guu5G5QigWU6p4yaJXCBLvIosDHXwtyhxMVfplzK4wTXiwCwOGo1B2aFidcgtaZfN_/s320/hero%20RGD.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>;深度神经网络 (DNN) 已成为解决标准监督学习 (&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Tokens- to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.pdf&quot;>;使用 ViT 进行图像分类&lt;/a>;）到&lt;a href=&quot;https://arxiv.org/pdf/2201.11775.pdf&quot;>;元学习&lt;/a>;。学习 DNN 最常用的范例是&lt;em>; &lt;a href=&quot;http://web.mit.edu/6.962/www/www_spring_2001/emin/slt.pdf&quot;>;经验风险最小化&lt;/a>; (ERM ）&lt;/em>;，旨在确定一个能够最小化平均&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss训练数据点的“损失”。多种算法，包括&lt;a href=&quot;https://www2.isye.gatech.edu/~nemirovs/Nemirovskii_Yudin_1983.pdf&quot;>;随机梯度下降&lt;/a>; (SGD)、&lt;a href=&quot;https://arxiv .org/pdf/1412.6980.pdf&quot;>;Adam&lt;/a>; 和 &lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;>;Adagrad&lt;/a>;，有被提出来解决ERM。然而，ERM的一个缺点是它对所有样本进行平均加权，往往会忽略稀有和较困难的样本，而关注较容易和丰富的样本。这会导致在未见过的数据上表现不佳，尤其是在训练数据稀缺的情况下。&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了克服这一挑战，最近的工作开发了数据重新加权技术提高 ERM 绩效。然而，这些方法侧重于特定的学习任务（例如&lt;a href=&quot;https://arxiv.org/pdf/1708.02002.pdf&quot;>;分类&lt;/a>;）和/或需要学习&lt;a href=&quot;https ://arxiv.org/pdf/1902.07379.pdf&quot;>;额外的元模型&lt;/a>;，用于预测每个数据点的权重。额外模型的存在显着增加了训练的复杂性，并使它们在实践中变得笨拙。&lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2306.09222&quot;>;随机重新加权中通过分布稳健优化进行梯度下降&lt;/a>;”，我们引入了经典&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent#&quot;>;SGD算法&lt;/a>;的一种变体，用于重新加权数据每个优化步骤中根据其难度进行评分。随机重加权梯度下降（RGD）是一种轻量级算法，具有简单的封闭式表达式，只需两行代码即可应用于解决任何学习任务。在学习过程的任何阶段，RGD 只是将数据点重新加权为其损失的指数。我们凭经验证明，RGD 重新加权算法提高了从监督学习到元学习等各种任务中众多学习算法的性能。值得注意的是，我们在 &lt;a href=&quot;https://arxiv.org/abs/2007.01434&quot;>;DomainBed&lt;/a>; 和 &lt;a href=&quot;https://arxiv. org/abs/2206.08564&quot;>;表格分类&lt;/a>;。此外，RGD 算法还使用 &lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>; 提高了 &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; 的性能GLUE&lt;/a>; 基准测试和&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.pdf&quot;>;ImageNet-1K 上的 ViT&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;分布式稳健优化&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/1610.03425.pdf&quot;>;分布式鲁棒优化&lt;/a>; (DRO) 是一种假设可能发生“最坏情况”数据分布变化的方法，这可能会损害模型的性能。如果模型专注于识别少量虚假特征进行预测，这些“最坏情况”数据分布变化可能会导致样本错误分类，从而导致性能下降。 DRO 优化了“最坏情况”分布中样本的损失，使模型能够抵抗数据分布中的扰动（例如，从数据集中删除一小部分点、数据点的较小上/下加权等） 。在分类的背景下，这迫使模型减少对噪声特征的重视，而更多地重视有用和预测的特征。因此，使用 DRO 优化的模型往往在未见过的样本上具有&lt;a href=&quot;https://arxiv.org/pdf/1610.03425.pdf&quot;>;更好的泛化保证和更强的性能&lt;/a>;。 &lt;/p>; &lt;p>; 受这些结果的启发，我们开发了 RGD 算法作为解决 DRO 目标的技术。具体来说，我们专注于基于 &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;Kullback–Leibler divergence&lt;/a>; 的 DRO，其中添加扰动来创建分布接近 KL 散度度量中的原始数据分布，使模型能够在所有可能的扰动下表现良好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhAuznJBUTcLrt6urFhWEkWCLviEMHf3KsvQAGd1JZSusTYW1kbTYg7mz-mmHUQtmYCHuclpML5JqBhCHB0OmYChuI62CJ BB5nJdefCX0idWac3WKaJgb9UfvljskRgcYPrLnakAdnaNFRsWI8nQ3cPTy4pSdABX8k4tg3UrXaFbcT3I9pLuKxbl9UpQ80/s1999/image7.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1236&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjhAuznJBUTcLrt6urFhWEkWCLviEMHf3KsvQAGd1JZSusTYW1kbTYg7mz-mmHUQtmYCHuclpML5JqBhCHB0OmYChuI62CJBB5nJdefCX0idWac3WKaJgb9UfvljskRgcY PrLnakAdnaNFRsWI8nQ3cPTy4pSdABX8k4tg3UrXaFbcT3I9pLuKxbl9UpQ80/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;图示 DRO 的图。与 &lt;a href=&quot;http://web.mit.edu/6.962/www/www_spring_2001/emin/slt.pdf&quot;>;ERM&lt;/a>; 相比，ERM 学习一个最小化原始数据分布的预期损失的模型， DRO 学习的模型在原始数据分布的多个扰动版本上表现良好。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot; >; &lt;br />; &lt;/div>; &lt;h2>;随机重新加权梯度下降&lt;/h2>; &lt;p>; 考虑样本的随机子集（称为小批量），其中每个数据点都有一个关联的&lt;a href= “https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss&quot;>;损失&lt;/a>; &lt;i>;L&lt;sub>;i&lt;/sub>;&lt; /i>;.像 SGD 这样的传统算法对小批量中的所有样本给予同等的重视，并通过沿着这些样本损失的平均梯度下降来更新模型的参数。通过 RGD，我们重新加权小批量中的每个样本，并更加重视模型识别为更困难的点。准确地说，我们使用损失作为代理来计算一个点的难度，并通过其损失的指数重新加权。最后，我们通过沿着样本梯度的加权平均值下降来更新模型参数。 &lt;/p>; &lt;p>; 出于稳定性考虑，在我们的实验中，我们在计算损失指数之前对损失进行裁剪和缩放。具体来说，我们将损失限制在某个阈值&lt;em>;T&lt;/em>;，并将其乘以与阈值成反比的标量。 RGD 的一个重要方面是它的简单性，因为它不依赖元模型来计算数据点的权重。此外，它可以用两行代码来实现，并与任何流行的优化器结合使用（例如&lt;a href=&quot;https://www2.isye.gatech.edu/~nemirovs/Nemirovskii_Yudin_1983.pdf&quot;>;SGD&lt;/a >;、&lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;>;亚当&lt;/a>;和&lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/ duchi11a.pdf&quot;>;Adagrad&lt;/a>;。&lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellpacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg71xwsL38tNMVHtrY7NqXEzpFmHH- o6E9lVSsVVTks8JnrU1uHBg9CA8ZaIBpX-bi0F1w-LBZAcnco0JXxfdOup-6Qn4fk_DC0MAVT1ytwoi8TCeYfNqBPAGwBe6raUde9O7FibdDNjz4_7pMA3poNl-o_6bZM0r3VbWfzSacWhY019yvTobe DrmSSh1sb/s1973/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1541&quot; 数据-original-width=&quot;1973&quot; height=&quot;500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg71xwsL38tNMVHtrY7NqXEzpFmHH-o6E9lVSsVVTks8JnrU1uHBg9CA8ZaIBpX-bi0F1w-LBZAcnco0JXx fdOup-6Qn4fk_DC0MAVT1ytwoi8TCeYfNqBPAGwBe6raUde9O7FibdDNjz4_7pMA3poNl-o_6bZM0r3VbWfzSacWhY019yvTobeDrmSSh1sb/w640-h500/image5。 png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;图解了背后的直观想法二元分类设置中的 RGD。特征 1 和特征 2 是模型可用于预测数据点标签的特征。 RGD 增加了模型错误分类的高损失数据点的权重。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们提出了将 RGD 与标准监督学习和领域适应的最先进技术进行比较的实证结果（请参阅 &lt;a href=&quot; https://arxiv.org/abs/2306.09222&quot;>;元学习结果论文&lt;/a>;）。在我们所有的实验中，我们使用保留的验证集来调整优化器的裁剪级别和学习率。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;监督学习&lt;/h3>; &lt;p>; 我们在多个监督学习任务上评估 RGD，包括语言、视觉和表格分类。对于语言分类任务，我们将 RGD 应用于在 &lt;a href=&quot;https://gluebenchmark 上训练的 &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; 模型。 com/leaderboard&quot;>;通用语言理解评估&lt;/a>; (&lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>;GLUE&lt;/a>;) 基准，表明 RGD 比 BERT 基准高出 +1.94%标准偏差为 0.42%。为了评估 RGD 在视觉分类上的性能，我们将 RGD 应用于在 ImageNet-1K 数据集上训练的 ViT-S 模型，结果表明 RGD 的性能优于 ViT-S 基线 +1.01%，标准差为 0.23%。此外，我们还执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;假设检验&lt;/a>;，以确认这些结果具有统计显着性，p 值小于 0.05。&lt; /p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto ;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioW7ubyangDExeb1OJ8vWgXuU9VnDkeSwOqY1pmRHq22Pyym63a8_WHjd6W3yiXLUDPgPEXQkS31aOnkwgp COgzhBRzNLyV0OkHlZskXpqOOmCV-xZyZ6NQpuC85jen-YxDfhF-usexsm -lZDlGnoenyoEwS9slSplJOHmoPeySmwvi4Qr7c_KBko4PjaK/s1999/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1334&quot; data-original-width=&quot;1999 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioW7ubyangDExeb1OJ8vWgXuU9VnDkeSwOqY1pmRHq22Pyym63a8_WHjd6W3yiXLUDPgPEXQkS31aOnkwgpCOgzhBRzNLyV0OkHlZskXpqOOmCV -xZyZ6NQpuC85jen-YxDfhF-usexsm-lZDlGnoenyoEwS9slSplJOHmoPeySmwvi4Qr7c_KBko4PjaK/s16000/image8.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RGD 使用 GLUE 和 Imagenet-1K 基准测试在语言和视觉分类方面的性能。请注意，MNLI、QQP、QNLI、SST-2、MRPC、RTE 和 COLA 是构成 GLUE 基准的不同数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 对于表格分类，我们使用&lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;MET&lt;/a>;作为我们的基线，并考虑来自&lt;a href=&quot;https://archive.ics的各种二进制和多类数据集.uci.edu/&quot;>;加州大学欧文分校的机器学习存储库&lt;/a>;。我们表明，将 RGD 应用于 &lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;MET&lt;/a>; 框架，其在二元和多类表格分类上的性能分别提高了 1.51% 和 1.27% ，在该领域实现了最先进的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYobZ13Q2uashBWpbo58LiuXs2OO0UGKorkJ5VMm0rUUlJS92R6_kBrjRbAPAFRVj88xBEptVFuDSHtlbzjNQKNpfKI0lOO p_KJI_rXdxka8qddjeP5MuQATCRDCbnBAkfEYxqUK44LQ8cTZEUHDThR4aVqZ5_rdfx6931HvMVuY4OPh4G_Z96l6sjWuw9/s1999/image4.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjYobZ13Q2uashBWpbo58LiuXs2OO0UGKorkJ5VMm0rUUlJS92R6_kBrjRbAPAFRVj88xBEptVFuDSHtlbzjNQKNpfKI0lOOp_KJI_rXdxka8qddjeP5MuQATCRDCbnBAkfEYxqUK4 4LQ8cTZEUHDThR4aVqZ5_rdfx6931HvMVuY4OPh4G_Z96l6sjWuw9/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0Uh_DCijOhzBjSzNyJoi92YCp2jD3n2k-l9aLuUWA8geS7QfkgzxZFj8WOseCbOfJu6hZxNFpAFDNFmBLSL7o0_bFHV6O0iZ6GPzderDKYpnW8QTY PLLLFGO0R6XzCITFBUVJFsMXfLUWK4qEffp70g4tEPloFdirGRpgClA0blEOZMPvf7TmxaKS9Vyw/s1999/image6.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEi0Uh_DCijOhzBjSzNyJoi92YCp2jD3n2k-l9aLuUWA8geS7QfkgzxZFj8WOseCbOfJu6hZxNFpAFDNFmBLSL7o0_bFHV6O0iZ6GPzderDKYpnW8QTYPRLLFGO0R6XzCITFBUVJFs MXfLUWK4qEffp70g4tEPloFdirGRpgClA0blEOZMPvf7TmxaKS9Vyw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RGD 对各种表格数据集分类的性能。&lt;/td>;&lt;/ tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;领域泛化&lt;/h3>; &lt;p>; 评估RGD的泛化能力，我们使用标准的&lt;a href=&quot;https://arxiv.org/abs/2007.01434&quot;>;DomainBed&lt;/a>;基准，该基准通常用于研究模型的域外性能。我们应用RGD 到 &lt;a href=&quot;https://arxiv.org/abs/2210.01360&quot;>;FRR&lt;/a>;，这是一种改进域外基准的最新方法，并表明 RGD 与 FRR 的平均性能为 0.7%优于 FRR 基线。此外，我们通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;假设检验&lt;/a>;确认大多数基准测试结果（Office Home 除外）具有统计显着性p 值小于 0.05。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhv_KdUdYK72dakGtm0fkZJGsJqe3jRCL1EXAdpC98gHDcHpXirdr2qiMMQfLxFyUrEi5NLr L9Rrx3sqx8CIME00srUKH51MYvsvXzSWImhDBvjokiPE7Oz76CZHr4Ty3RRHX6IW4BcpQRQhbilp1- nw6D0BqPxNuvpx1vaB5Auv2Gtb-y_INiFNTCJmaV9/s1999/image3.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhv_KdUdYK72dakGtm0fkZJGsJqe3jRCL1EXAdpC98gHDcHpXirdr2qiMMQfLxFyUrEi5NLrL9Rrx3sqx8CIME00srUKH51MYvsvXzSWImhDBvjokiPE7Oz76CZHr4T y3RRHX6IW4BcpQRQhbilp1-nw6D0BqPxNuvpx1vaB5Auv2Gtb-y_INiFNTCJmaV9/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;RGD 在 DomainBed 分布偏移基准上的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h3>;类别不平衡和公平性&lt;/h3>; &lt;p>; 为了证明使用 RGD 学习的模型在存在类别不平衡（数据集中某些类别代表性不足）的情况下仍表现良好，我们将 RGD 与 ERM 在 &lt;a href= 上的性能进行比较“https://openaccess.thecvf.com/content_CVPR_2019/html/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.html&quot;>;长尾 CIFAR-10&lt;/a>;。我们报告，RGD 将基线 ERM 的准确性平均提高了 2.55%标准偏差为 0.23%。此外，我们还执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;假设检验&lt;/a>;并确认这些结果具有统计显着性，p 值小于 0.05。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBesU8uGmy9DLXZMZyC94wiLHs_swM94ZcUxhyphenhyphen6IbKHwzpDjwH-L8q2E-8iRQ9TmDC2QCWSNqZKm8pF KynE_4kcU1u3ERg_s35uKaRvc2PqvNMOPTBBcMu4ciXtkwudlY7wFXR0DHvu0lUccgcRNi5ZQTHZdaQyKXl7AXwl9sUBSjoLpdUHxztgUzVWgHw/s1999/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1356&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBesU8uGmy9DLXZMZyC94wiLHs_swM94ZcUxhyphenhyphen6IbKHwzpDjwH-L8q2E-8iRQ9TmDC2QCWSNqZKm8pFKynE_4kcU1u3ERg_s35uKaRvc2P qvNMOPTBBcMu4ciXtkwudlY7wFXR0DHvu0lUccgcRNi5ZQTHZdaQyKXl7AXwl9sUBSjoLpdUHxztgUzVWgHw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;RGD 在类别不平衡领域的长尾 Cifar-10 基准上的性能。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;限制&lt;/h2>; &lt;p>; RGD 算法是使用流行的研究数据集开发的，这些数据集已经被精心设计以消除损坏（例如，噪声和不正确的标签）。因此，在训练数据有大量损坏的情况下，RGD 可能无法提供性能改进。处理此类场景的一种可能方法是将&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/375663.375668&quot;>;异常值去除技术&lt;/a>;应用于 RGD 算法。这种异常值去除技术应该能够从小批量中过滤掉异常值，并将剩余的点发送到我们的算法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; RGD 已被证明在各种任务上都有效，包括域外泛化、表格表示学习和类不平衡。它实现简单，只需更改两行代码即可无缝集成到现有算法中。总的来说，RGD 是一种很有前途的提高 DNN 性能的技术，并且可以帮助突破各个领域的界限。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这篇博文中描述的论文是由Ramnath Kumar、Arun Sai Suggala、Dheeraj Nagaraj 和 Kushal Majmundar。我们衷心感谢匿名审稿人 Prateek Jain、Pradeep Shenoy、Anshul Nasery、Lovish Madaan 以及 Google 印度研究院机器学习和优化团队的众多敬业成员，感谢他们对这项工作的宝贵反馈和贡献。&lt;/ em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2311847718846675808/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论“ type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/re-weighted-gradient-descent-via.html#comment-form&quot; rel=&quot;回复&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2311847718846675808&quot; rel=&quot;edit&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2311847718846675808&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt; link href=&quot;http://blog.research.google/2023/09/re-weighted-gradient-descent-via.html&quot; rel=&quot;alternate&quot; title=&quot;通过分布鲁棒优化重新加权梯度下降&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONjsYNtXi5AX4ZZ6qPfdY2Gwt2W5xZy1PF1m9ZaxucZtRZ5ZzmsNzHNNWkCKcpX1_n15DkZCR5 9ivF_uDCUUmbdijaAuiK3tlC9HahtKc1s1r6pQWwbwMhnuNK3Guu5G5QigWU6p4yaJXCBLvIosDHXwtyhxMVfplzK4wTXiwCwOGo1B2aFidcgtaZfN_/s72-c/ Hero%20RGD.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/条目>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-109886617977245321&lt;/id>;&lt;已发布>;2023-09-26T07:10:00.000-07:00&lt;/已发布>;&lt;已更新>; 2023-09-26T07:10:39.655-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category方案=“http://www.blogger.com/atom/ns#”术语=“机器学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=&quot;神经网络&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 研究人员着手绘制小鼠大脑图&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google Research 研究科学家 Michał Januszewski&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFf6_wflZxOT_rmYJu0VCRvigHGMvE2QXwYJeh6F7GHmxnEtg7_bEraidqQ8uii_-N5EtBr2LfOGV _ccWS2g1Qcjssq2HmmGCqbaL_ij-UroaO6JtJVhyNbxK5PN57OfVlmkfXGSg3DMFawUlj1btghs5Nb9tmvQVSZH3pDoLK0SoKlhMh028164YArWcc/s320/connectome%20600x580.gif&quot;样式=“显示：无；” />; &lt;p>; 人脑可能是现存计算最复杂的机器，由 &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_brain#:~:text=brainstem.%5B37%5D -,显微解剖学,-%5Bedit%5D&quot;>;数十亿个细胞组成的网络&lt;/a>;。研究人员目前还不了解其网络机制的故障如何导致精神疾病和其他疾病（例如痴呆症）的全貌。然而，新兴的&lt;a href=&quot;https://en.wikipedia.org/wiki/Connectomics&quot;>;连接组学&lt;/a>;领域旨在精确绘制大脑中每个细胞之间的连接，可以帮助解决这个问题。虽然地图只是为更简单的生物体创建的，但绘制更大大脑的技术进步可以使我们了解人脑的工作原理以及如何治疗脑部疾病。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 今天，我们&lt;a href=&quot;https://www.ninds.nih.gov/news-events/highlights-announcements/nih -brain-initiative-launches-projects-develop-innovative-technologies-map-brain-incredible-detail&quot;>;很高兴地宣布&lt;/a>; &lt;a href=&quot;https://research.google/teams/connectomics/ &quot;>;Google 研究中心的连接组学团队&lt;/a>;和我们的合作者正在启动一个&lt;a href=&quot;https://reporter.nih.gov/project-details/10665380&quot;>;耗资 3300 万美元的项目&lt;/a>;来拓展前沿未来五年的连接组学。由 &lt;a href=&quot;https://www.nih.gov/&quot;>;通过推进创新神经技术进行大脑研究&lt;/a>; (BRAIN) 计划的支持/&quot;>;美国国立卫生研究院&lt;/a>; (NIH) 和&lt;a href=&quot;https://lichtmanlab.fas.harvard.edu/&quot;>;哈佛大学&lt;/a>;的研究人员领导，我们将致力于与来自&lt;a href=&quot;https://alleninstitute.org/division/brain-science/&quot;>;艾伦研究所&lt;/a>;的多学科专家团队一起，&lt;a href=&quot;https://fietelab.mit.edu /&quot;>;麻省理工学院&lt;/a>;，&lt;a href=&quot;https://www2.mrc-lmb.cam.ac.uk/group-leaders/h-to-m/gregory-jefferis/&quot;>;剑桥大学&lt;/ a>;、&lt;a href=&quot;https://pni.princeton.edu/&quot;>;普林斯顿大学&lt;/a>;和&lt;a href=&quot;https://www.jhuapl.edu/&quot;>;约翰霍普金斯大学&lt;/a>; >;，顾问来自&lt;a href=&quot;https://www.janelia.org/lab/hess-lab&quot;>;HHMI 珍妮莉亚研究园区&lt;/a>;。我们的项目目标是应对神经科学领域的巨大挑战：绘制小鼠大脑的一小部分（2-3%）的图谱。我们将特别针对&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5690575/&quot;>;海马&lt;/a>;区域，该区域负责编码记忆、注意力和空间导航。该项目是 NIH 1.5 亿美元资助的 11 个项目之一&lt;a href=&quot;https://braininitiative.nih.gov/funding-opportunies/brain-initiative-connectivity-across-scales-brain-connects-compressive-0&quot;>;大脑倡议跨尺度连接（BRAIN CONNECTS）计划。谷歌研究中心正在为这项工作贡献计算和分析资源，并且不会从美国国立卫生研究院获得任何资助。我们的项目提出了一个关键问题：我们能否扩展和加速我们的技术，以绘制小鼠大脑的整个连接组？ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;连接组学的现代时代&lt;/h2>; &lt;p>; 这项绘制小型连接组图的工作小鼠大脑的一部分建立在该领域十年的创新基础上，其中包括谷歌研究院连接组学团队发起的许多进展。我们希望完成类似于&lt;a href=&quot;https://www.genome.gov/ human-genome-project&quot;>;人类基因组计划&lt;/a>;早期的事情，当时科学家们花了数年时间对人类基因组的一小部分，因为他们改进了技术，使他们能够完成基因组的其余部分。 &lt;/p>; &lt;p>; 2021 年，我们和哈佛大学的合作者成功&lt;a href=&quot;https://blog.research.google/2021/06/a-browsable-petascale-reconstruction-of.html&quot;>;绘制了一个人类大脑的立方毫米&lt;/a>;，我们将其作为 &lt;a href=&quot;https://h01-release.storage.googleapis.com/landing.html&quot;>;H01&lt;/a>; 数据集发布，该数据集是研究人脑和扩展连接组学技术。但绘制整个人脑连接组需要收集和分析多达泽字节（十亿太字节）的数据，这超出了现有技术的当前能力。 &lt;/p>; &lt;p>; 分析鼠标连接组是第二好的事情。它足够小，在技术上是可行的，并且有可能提供与我们自己的想法相关的见解；神经科学家已经使用小鼠来研究人类大脑的功能和功能障碍。通过共同绘制 10-15 立方毫米的小鼠大脑图谱，我们希望开发出新的方法，使我们能够绘制小鼠大脑的整个其余部分以及此后的人类大脑图谱。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEgZ3vCYyxz8BDdHUzAEYteV7DWtaSXT7q3ajif8uJXF-Aqfw4-fNrAaVxXU4xnevMe74eCKHtw_OGG3wC7wZUCn4hcN3snTPgqFfD9HrYqft0SoyZ1bN2cGFFEjGoGEU_ GV3go90LahMLyCGrPFGVRaniC-ngxMgNiQoGM_03aln2kTqIDGUqXPMZcgH1-W/s960/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “540”数据原始宽度=“960”高度=“360”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ3vCYyxz8BDdHUzAEYteV7DWtaSXT7q3ajif8uJXF-Aqfw4-fNrAaVxXU4xnevMe74eCKHtw_OGG3w C7wZUCn4hcN3snTPgqFfD9HrYqft0SoyZ1bN2cGFFEjGoGEU_GV3go90LahMLyCGrPFGVRaniC-ngxMgNiQoGM_03aln2kTqIDGUqXPMZcgH1-W/w640-h360/ image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;神经科学家一直在努力几十年来绘制越来越大、越来越复杂的连接体。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;生物学最大的数据集之一&lt;/h2>; &lt;p>; 在这个连接组学项目中，我们将绘制 &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978 -3-642-76447-9_1&quot;>;小鼠大脑的海马结构&lt;/a>;，它将短期记忆转化为长期记忆，并帮助小鼠在空间中导航。小鼠海马结构是我们试图通过这种方式了解的大脑中最大的区域。通过绘制小鼠大脑这一区域的图谱，我们将创建生物学中最大的数据集之一，其中包含约 25,000 TB（即 25 PB 的大脑数据）。作为参考，我们的银河系大约有 2500 亿颗恒星。如果这些恒星中的每一颗都是一个字节，则需要 100,000 个银河系才能匹配该项目在绘制小鼠大脑的一小部分区域时将收集的 25 PB 数据。 &lt;/p>; &lt;p>; 为了说明海马项目的规模，我们计算了存储已完成的绘制蛔虫和果蝇大脑的连接组项目的图像数据所需的 Pixel 手机数量（如下所示的像素堆栈），如下所示以及小鼠海马区和整个小鼠大脑项目，这些项目才刚刚开始。 &lt;/p>; &lt;p>; 然后，我们将每个像素堆栈的高度与熟悉的物体和地标进行比较。需要一堆 100 个像素（相当于一个四岁女孩那么高）来存储果蝇大脑的图像数据，这是迄今为止已完成的最大项目。相比之下，小鼠海马连接组工作将需要相当于超过 48,800 个像素的存储空间，达到帝国大厦的高度。下面的动画展示了小鼠海马体项目将如何超越之前的连接组项目的规模。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitPA5h7cSox988b0Xr2cEjrzf0word6JAtcNjtU1S7QF4vsULGgdqSR1KjCh0jibAR-keO1kIkIX0ooyGgZaC9wU 61NDijkDpPgnz2eqswz_J_alrpb2FColxo17BnxOVytsuq9gIwrWmzmz52tzsE4qUu8sVnPoobeIMBToilH58YpWtNwxCLi5Bi_ASV/s1600/image1.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEitPA5h7cSox988b0Xr2cEjrzf0word6JAtcNjtU1S7QF4vsULGgdqSR1KjCh0jibAR-keO1kIkIX0ooyGgZaC9wU61NDijkDpPgnz2eqswz_J_alrpb2FColxo17 BnxOVytsuq9gIwrWmzmz52tzsE4qUu8sVnPoobeIMBToilH58YpWtNwxCLi5Bi_ASV/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;我们正在与几位合作者合作，为小鼠大脑的海马区构建连接组（脑细胞之间的连接图）。该项目将创建有史以来最大的连接组数据集，超过之前绘制较小蛔虫和果蝇大脑的项目的规模。我们希望这项努力能够促进新方法的开发，使我们能够在以后绘制整个小鼠大脑的图谱。该动画展示了连接组学领域如何通过计算存储各种项目数据所需的 Pixel 手机数量来扩大规模。只需要两个像素（橄榄的高度）来存储蛔虫连接组数据，而需要一堆珠穆朗玛峰大小的像素来存储整个小鼠连接组的数据。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 了解小鼠海马结构的连接组有助于阐明我们大脑的工作方式。例如，我们可能会发现小鼠大脑和人类大脑中的这种电路之间的共同特征，这些特征解释了我们如何知道自己在哪里，我们的大脑如何将记忆与特定位置相关联，以及无法正确形成新空间的人会出现什么问题。回忆。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;开放 PB 管道&lt;/h2>; &lt;p>; 在过去的十年中，我们的团队致力于开发用于管理大量连接组数据集并从中提取科学价值的工具。但小鼠大脑的神经元数量是果蝇大脑的 1000 倍，果蝇是一种生物体，&lt;a href=&quot;https://blog.research.google/2020/01/releasing- drosophila-hemibrain.html&quot;>;我们帮助为大脑的很大一部分构建了连接组&lt;/a>;。启动小鼠大脑连接组将挑战我们改进现有技术，使我们能够比以往更快地绘制更多数据。 &lt;/p>; &lt;p>; 我们将继续完善我们的&lt;a href=&quot;https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html&quot;>;洪水填充网络&lt;/a>;，它使用深度学习来追踪或“分段”每个神经元在由电子显微镜数据制成的三维大脑体积中的路径。我们还将扩展自我监督学习技术的功能，&lt;a href=&quot;https://blog.research.google/2022/11/multi-layered-mapping-of-brain-tissue.html&quot;>;SegCLR &lt;/a>;，它允许我们自动从分段体积中提取关键见解，例如识别细胞类型（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Pyramidal_cell#:~:text=Pyramidal %20cells%2C%20or%20pyramidal%20neurons，cortex%20and%20the%20corticospinal%20tract。&quot;>;锥体神经元&lt;/a>;，&lt;a href=&quot;https://en.wikipedia.org/wiki/Basket_cell&quot;>;篮子神经元&lt;/a>;等）和每个神经元的部分（例如轴突、树突等）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiBHvnYysuO3m0CehlpRcx_XgxxTDUhKsr4BPEW0ypOVs_q2uh8RvJQAu323SXu9IbWVmxPGTsCEqtpaOvbdzAn Uv2TLlqFnPEbfOKE1XOIMSyS9OEtlcM3KCIsWOdZgDQ1AqkG2kX6YbrFAMrn8g8FTiLlxA6ASLPz_f2AvlkqcfYzLwBEVkNA7R3OWRD/s720/image2.gif&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEiiBHvnYysuO3m0CehlpRcx_XgxxTDUhKsr4BPEW0ypOVs_q2uh8RvJQAu323SXu9IbWVmxPGTsCEqtpaOvbdzAnUv2TLlqFnPEbfOKE1XOIMSyS9OEtlcM3KCIsWOd ZgDQ1AqkG2kX6YbrFAMrn8g8FTiLlxA6ASLPz_f2AvlkqcfYzLwBEVkNA7R3OWRD/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;洪水填充网络通过三维大脑空间追踪神经元。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们还将继续增强核心连接组学基础设施的可扩展性和性能，例如&lt;a href=&quot;https://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html&quot;>;用于存储的TensorStore&lt;/a>;和&lt;a href=&quot;https://github .com/google/neuroglancer&quot;>;用于可视化的 Neuroglancer&lt;/a>;，以便使我们所有的计算管道和人工分析工作流程能够在这些新的数据规模上运行。我们渴望通过研究老鼠的大脑来了解我们自己的大脑。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本博文中描述的鼠标连接组学项目将得到 NIH BRAIN Initiative 的部分支持，奖项编号为 1UM1NS132250。谷歌研究中心正在为小鼠连接组项目贡献计算和分析资源，并且不会获得美国国立卫生研究院的资助。许多人参与了使该项目成为可能的技术开发。我们感谢 Lichtman 实验室（哈佛大学）、HHMI Janelia 和 Denk 实验室（马克斯普朗克生物智能研究所）的长期学术合作者，并感谢 Google 连接组学团队的核心贡献。我们还感谢 John Guilyard 在这篇文章中创建了说明性动画，并感谢 Elise Kleeman 和 Erika Check Hayden 的支持。感谢 Lizzie Dorfman、Michael Brenner、Jay Yagnik 和 Jeff Dean 的支持、协调和领导。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog .research.google/feeds/109886617977245321/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2023/09/google-research-embarks-on-effort-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http:// /www.blogger.com/feeds/8474926331452026626/posts/default/109886617977245321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/ 8474926331452026626/posts/default/109886617977245321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/google-research-embarks-on -effort-to.html&quot; rel=&quot;alternate&quot; title=&quot;Google 研究开始绘制小鼠大脑图谱&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>; http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com /g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height= “72”网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFf6_wflZxOT_rmYJu0VCRvigHGMvE2QXwYJeh6F7GHmxnEtg7_bEraidqQ8uii_-N5EtBr2LfOGV_ccWS2g1Qcjssq2HmmGCqbaL_ij-Uro aO6JtJVhyNbxK5PN57OfVlmkfXGSg3DMFawUlj1btghs5Nb9tmvQVSZH3pDoLK0SoKlhMh028164YArWcc/s72-c/connectome%20600x580.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http: //search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog- 8474926331452026626.POST-27620836766649407668 &lt;/id>; &lt;/id>; &lt;/id>; &lt;出版>; 2023-09-21T14：25：00.000-07：00：00：00 &lt;/00类别方案 =“http://www.blogger.com/atom/ns#”术语 =“ACL”>;&lt;/类别>;&lt;类别方案 =“http://www.blogger.com/atom/ns#”术语=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;逐步提炼：用更少的训练数据和更小的模型尺寸超越更大的语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Cheng-Yu Hsieh，学生研究员和云 AI 团队研究科学家 Chen-Yu Lee &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7i39GKWT7_noICLVOMuf2x7v7wM21MXu9deGmnVlsieFv9m_rJOb5ytwiI_A9T3N4N1vZCVprlgb s7dePmQ1qjkcz-mT0nkeyLq-LmkF4oJB-ofCmrv81jqXMHPHe8Tl1dQSbDAPS9gADUUByZHBygkr-jlwbfIx3FM14n5cAbE7ZFVm84K6UDQLfeArm/s320/英雄.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 大型语言模型（LLM）启用了一种新的数据高效学习范式，其中它们可用于通过&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;零解决看不见的新任务- 射击或几次射击提示&lt;/a>;。然而，由于法学硕士规模庞大，在实际应用中部署法学硕士具有挑战性。例如，为单个 1750 亿个 LLM 提供服务需要使用&lt;a href=&quot;https://arxiv.org/abs/2201.12023&quot;>;专用基础设施&lt;/a>;至少 350GB GPU 内存，更不用说当今的状态了-最先进的 LLM 由超过&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;5000 亿个参数&lt;/a>;组成。对于许多研究团队来说，这样的计算要求是无法满足的，特别是对于需要低延迟性能的应用程序。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了规避这些部署挑战，从业者通常选择部署较小的专用模型。这些较小的模型使用两种常见范例之一进行训练：&lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;微调&lt;/a>;或&lt;a href=&quot;https://arxiv.org /abs/1503.02531&quot;>;蒸馏&lt;/a>;。微调更新预先训练的较小模型（例如，&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/ abs/1910.10683&quot;>;T5&lt;/a>;) 使用下游手动注释数据。蒸馏法使用较大的法学硕士生成的标签来训练相同的较小模型。不幸的是，为了达到与法学硕士相当的性能，微调方法需要人工生成的标签，而获取这些标签既昂贵又繁琐，而蒸馏则需要大量未标记的数据，这些数据也很难收集。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2305.02301&quot;>;逐步蒸馏！用更少的训练数据和更小的模型大小超越更大的语言模型&lt;/a>;”，在 &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL2023&lt;/a>; 上提出，我们着手解决这一问题-模型大小和训练数据收集成本之间的差距。我们引入了逐步蒸馏，这是一种新的简单机制，使我们能够使用比标准微调或蒸馏方法少得多的训练数据来训练更小的特定于任务的模型，这些方法的性能优于少数样本提示的法学硕士的性能。我们证明，逐步蒸馏机制使 770M 参数 T5 模型能够优于仅使用基准数据集中 80% 的示例的小样本提示的 540B PaLM 模型，这表明模型大小减少了 700 倍以上，而所需的资源却少得多。标准方法所需的培训数据。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQK qwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s1570/image3.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width=&quot;1570&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR 5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;虽然法学硕士提供了强大的零样本和少样本表现，但它们在实践中具有挑战性。另一方面，训练小型特定任务模型的传统方法需要大量的训练数据。逐步提炼提供了一种新的范例，可以减少部署的模型大小以及训练所需的数据数量。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;逐步提炼&lt;/h2>; &lt;p>; 逐步提炼的关键思想是提取信息来自法学硕士的&lt;em>;自然语言&lt;/em>; &lt;em>;基本原理（即&lt;/em>;中间推理步骤）&lt;em>; &lt;/em>;来自法学硕士，这反过来又可用于以更高效的数据方式训练小型模型方式。具体来说，自然语言原理解释了输入问题与其相应输出之间的联系。例如，当被问及“&lt;em>;杰西的房间长 11 英尺，宽 15 英尺。如果她已经有​​ 16 平方英尺的地毯，她还需要多少地毯才能覆盖整个地板？&lt;/em>;”，少数镜头&lt;a href=&quot;https://blog. Research.google/2022/05/language-models-perform-reasoning-via.html&quot;>;思想链&lt;/a>; (CoT) 提示技术提供中间原理，例如“面积 = 长度” * 宽度。 Jesse 的房间有 11 * 15 平方英尺。&lt;/em>;”这更好地解释了从输入到最终答案“&lt;em>;(11 * 15 ) - 16&lt;/em>;”的联系。这些基本原理可以包含相关的任务知识，例如“面积=长度*宽度”，这些知识最初可能需要许多数据来供小模型学习。除了标准任务标签之外，我们还利用这些提取的基本原理作为额外的、更丰富的监督来训练小型模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1Cq KtMZG7gyRiw4YOQcUn1EUj386PgYZ1PP-Wq9vDS​​er0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI /s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;932&quot; data-original-width=&quot;1999&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EU j386PgYZ1PP-Wq9vDS​​er0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;逐步提炼概述：首先，我们利用 CoT 提示从 LLM 中提取基本原理。然后，我们使用生成的基本原理在多任务学习框架中训练小型特定于任务的模型，在该框架中，我们将任务前缀添加到输入示例中，并训练模型根据给定的任务前缀进行不同的输出。&lt;/td>;&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 逐步蒸馏由两个主要阶段组成。在第一阶段，我们利用小样本 CoT 提示从法学硕士中提取基本原理。具体来说，给定一个任务，我们在 LLM 输入提示中准备一些样本，其中每个示例由一个三元组组成，其中包含：（1）输入、（2）基本原理和（3）输出。根据提示，法学硕士能够模仿三元组演示，为任何新输入生成基本原理。例如，在&lt;a href=&quot;https://arxiv.org/abs/1811.00937&quot;>;常识性问答任务&lt;/a>;中，给定输入问题“Sammy想去人们所在的地方。他可能会去哪里？答案选择：(a) 人口稠密地区，(b) 赛道，(c) 沙漠，(d) 公寓，(e) 路障”，逐步提炼出问题的正确答案，“(a) 人口稠密”区域”，并结合提供问题与答案之间更好联系的基本原理，“答案必须是一个有很多人的地方。以上选择中，只有人口稠密的地区人才多。”通过在提示中提供与基本原理配对的 Co​​T 示例，&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;上下文学习能力&lt;/a>;允许法学硕士为未来未见的输入输出相应的基本原理。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc 4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s1999 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1175&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwK x2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们使用少镜头 CoT 提示，其中包含示例理由（&lt;strong>;以绿色突出显示&lt;/strong>;）和标签（&lt;strong>;以蓝色突出显示&lt;/strong>;），以从法学硕士中获取新输入示例的基本原理。 The example is from a commonsense question answering task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; After the rationales are extracted, in the second stage, we incorporate the rationales in training small models by framing the training process as a multi-task problem. Specifically, we train the small model with a novel &lt;em>;rationale generation task&lt;/em>; in addition to the standard &lt;em>;&lt;a href=&quot;https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html?m=1&quot;>;label prediction task&lt;/a>;&lt;/em>;. The rationale generation task enables the model to learn to generate the intermediate reasoning steps for the prediction, and guides the model to better predict the resultant label. We prepend &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;task prefixes&lt;/a>; (ie, [label] and [rationale] for label prediction and rationale generation, respectively) to the input examples for the model to differentiate the two tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental setup&lt;/h2>; &lt;p>; In the experiments, we consider a &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;540B PaLM&lt;/a>; model as the LLM. For task-specific downstream models, we use &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5 models&lt;/a>;. For CoT prompting, we use the &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;original CoT prompts&lt;/a>; when available and curate our own examples for new datasets. We conduct the experiments on four benchmark datasets across three different NLP tasks: &lt;a href=&quot;https://arxiv.org/abs/1812.01193&quot;>;e-SNLI&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1910.14599&quot;>;ANLI&lt;/a>; for&lt;a href=&quot;https://arxiv.org/abs/1508.05326&quot;>; natural language inference&lt;/a>;; &lt;a href=&quot;https://arxiv.org/abs/1811.00937&quot;>;CQA&lt;/a>; for commonsense question answering; and &lt;a href=&quot;https://arxiv.org/abs/2103.07191&quot;>;SVAMP&lt;/a>; for &lt;a href=&quot;https://aclanthology.org/N16-1136/&quot;>;arithmetic math word problems&lt;/a>;. We include two sets of baseline methods. For comparison to &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;few-shot prompted LLMs&lt;/a>;, we compare to &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;few-shot CoT prompting&lt;/a>; with a &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;540B PaLM&lt;/a>; model. In the &lt;a href=&quot;https://arxiv.org/abs/2305.02301&quot;>;paper&lt;/a>;, we also compare standard task-specific model training to both &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;standard fine-tuning&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;standard distillation&lt;/a>;. In this blogpost, we will focus on the comparisons to standard fine-tuning for illustration purposes. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Less training data&lt;/h3>; &lt;p>; Compared to &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;standard fine-tuning&lt;/a>;, the distilling step-by-step method achieves better performance using much less training data. For instance, on the e-SNLI dataset, we achieve better performance than standard fine-tuning when using only 12.5% of the full dataset (shown in the upper left quadrant below). Similarly, we achieve a dataset size reduction of 75%, 25% and 20% on ANLI, CQA, and SVAMP. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1477&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Distilling step-by-step compared to standard fine-tuning using 220M T5 models on varying sizes of human-labeled datasets. On all datasets, distilling step-by-step is able to outperform standard fine-tuning, trained on the full dataset, by using much less training examples.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Smaller deployed model size&lt;/h3>; &lt;p>; Compared to &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;few-shot CoT prompted LLMs&lt;/a>;, distilling step-by-step achieves better performance using much smaller model sizes. For instance, on the e-SNLI dataset, we achieve better performance than 540B PaLM by using a 220M T5 model. On ANLI, we achieve better performance than 540B PaLM by using a 770M T5 model, which is over 700X smaller. Note that on ANLI, the same 770M T5 model struggles to match PaLM&#39;s performance using standard fine-tuning. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1384&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We perform distilling step-by-step and standard fine-tuning on varying sizes of T5 models and compare their performance to LLM baselines, ie, Few-shot CoT and PINTO Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, eg, over 700× smaller models on ANLI. Standard fine-tuning fails to match LLM&#39;s performance using the same model size.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Distilling step-by-step outperforms few-shot LLMs with smaller models using less data&lt;/h3>; &lt;p>; Finally, we explore the smallest model sizes and the least amount of data for distilling step-by-step to outperform PaLM&#39;s few-shot performance. For instance, on ANLI, we surpass the performance of the 540B PaLM using a 770M T5 model. This smaller model only uses 80% of the full dataset. Meanwhile, we observe that standard fine-tuning cannot catch up with PaLM&#39;s performance even using 100% of the full dataset. This suggests that distilling step-by-step simultaneously reduces the model size as well as the amount of data required to outperform LLMs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1400&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show the minimum size of T5 models and the least amount of human-labeled examples required for distilling step-by-step to outperform LLM&#39;s few-shot CoT by a coarse-grained search. Distilling step-by-step is able to outperform few-shot CoT using not only much smaller models, but it also achieves so with much less training examples compared to standard fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We propose distilling step-by-step, a novel mechanism that extracts rationales from LLMs as informative supervision in training small, task-specific models. We show that distilling step-by-step reduces both the training dataset required to curate task-specific smaller models and the model size required to achieve, and even surpass, a few-shot prompted LLM&#39;s performance. Overall, distilling step-by-step presents a resource-efficient paradigm that tackles the trade-off between model size and training data required. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Availability on Google Cloud Platform&lt;/h2>; &lt;p>; Distilling step-by-step is available for private preview on &lt;a href=&quot;https://cloud.google.com/vertex-ai&quot;>;Vertex AI&lt;/a>;. If you are interested in trying it out, please contact &lt;a href=&quot;mailto:vertex-llm-tuning-preview@google.com&quot;>;vertex-llm-tuning-preview@google.com&lt;/a>; with your Google Cloud Project number and a summary of your use case. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Thanks to Xiang Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2762083676649407668/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2762083676649407668&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2762083676649407668&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html&quot; rel=&quot;alternate&quot; title=&quot;Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7i39GKWT7_noICLVOMuf2x7v7wM21MXu9deGmnVlsieFv9m_rJOb5ytwiI_A9T3N4N1vZCVprlgbs7dePmQ1qjkcz-mT0nkeyLq-LmkF4oJB-ofCmrv81jqXMHPHe8Tl1dQSbDAPS9gADUUByZHBygkr-jlwbfIx3FM14n5cAbE7ZFVm84K6UDQLfeArm/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7378908661087185403&lt;/id>;&lt;published>;2023-09-15T10:39:00.002-07:00&lt;/published>;&lt;updated>;2023-09-15T10:43:29.705-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MediaPipe FaceStylizer: On-device real-time few-shot face stylization&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Haolin Jia, Software Engineer, and Qifei Wang, Senior Software Engineer, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihspgvRDlnUmToZlZzCcRWCFIt9TRnLH9lepBq6ojBLxZ0nFEPBS7bQgOHY0klCfkewtZy5KSGnKYialzlh4xNSFP5Th4mvWJWcJ8XvNkCAJK9T7f_xHSK-H0uPW0paxcTG3nhQtP7eY7iKSVjX-Oaca182KHKiuGzj2C4yWT_kenY-Ys7LtS7i93RCtcP/s828/FaceStylizer-Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, we have witnessed rising interest across consumers and researchers in integrated augmented reality (AR) experiences using real-time face feature generation and editing functions in mobile applications, including short videos, virtual reality, and gaming. As a result, there is a growing demand for lightweight, yet high-quality face generation and editing models, which are often based on &lt;a href=&quot;https://arxiv.org/pdf/1406.2661.pdf&quot;>;generative adversarial network&lt;/a>; (GAN) techniques. However, the majority of GAN models suffer from high computational complexity and the need for a large training dataset. In addition, it is also important to employ GAN models responsibly. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this post, we introduce &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe FaceStylizer&lt;/a>;, an efficient design for few-shot face stylization that addresses the aforementioned model complexity and data efficiency challenges while being guided by Google&#39;s responsible &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. The model consists of a face generator and a face encoder used as &lt;a href=&quot;https://arxiv.org/pdf/2101.05278.pdf&quot;>;GAN inversion&lt;/a>; to map the image into latent code for the generator. We introduce a mobile-friendly synthesis network for the face generator with an auxiliary head that converts features to &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; at each level of the generator to generate high quality images from coarse to fine granularities. We also carefully designed the &lt;a href=&quot;https://developers.google.com/machine-learning/gan/loss&quot;>;loss functions&lt;/a>; for the aforementioned auxiliary heads and combined them with the common GAN loss functions to distill the student generator from the teacher &lt;a href=&quot;https://github.com/NVlabs/stylegan&quot;>;StyleGAN&lt;/a>; model, resulting in a lightweight model that maintains high generation quality. The proposed solution is available in open source through &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>;. Users can fine-tune the generator to learn a style from one or a few images using MediaPipe Model Maker, and deploy to on-device face stylization applications with the customized model using &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe FaceStylizer&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Few-shot on-device face stylization&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;An end-to-end pipeline&lt;/h3>; &lt;p>; Our goal is to build a pipeline to support users to adapt the MediaPipe FaceStylizer to different styles by fine-tuning the model with a few examples. To enable such a face stylization pipeline, we built the pipeline with a GAN inversion encoder and efficient face generator model (see below). The encoder and generator pipeline can then be adapted to different styles via a few-shot learning process. The user first sends a single or a few similar samples of the style images to MediaPipe ModelMaker to fine-tune the model. The fine-tuning process freezes the encoder module and only fine-tunes the generator. The training process samples multiple latent codes close to the encoding output of the input style images as the input to the generator. The generator is then trained to reconstruct an image of a person&#39;s face in the style of the input style image by optimizing a joint adversarial loss function that also accounts for style and content. With such a fine-tuning process, the MediaPipe FaceStylizer can adapt to the customized style, which approximates the user&#39;s input. It can then be applied to stylize test images of real human faces. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Generator: BlazeStyleGAN&lt;/h3>; &lt;p>; The &lt;a href=&quot;https://arxiv.org/abs/1812.04948&quot;>;StyleGAN&lt;/a>; model family has been widely adopted for face generation and various face editing tasks. To support efficient on-device face generation, we based the design of our generator on StyleGAN. This generator, which we call &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.html&quot;>;BlazeStyleGAN&lt;/a>;, is similar to StyleGAN in that it also contains a mapping network and synthesis network. However, since the synthesis network of StyleGAN is the major contributor to the model&#39;s high computation complexity, we designed and employed a more efficient synthesis network. The improved efficiency and generation quality is achieved by: &lt;/p>; &lt;ol>; &lt;li>;Reducing the latent feature dimension in the synthesis network to a quarter of the resolution of the counterpart layers in the teacher StyleGAN, &lt;/li>; &lt;li>; Designing multiple auxiliary heads to transform the downscaled feature to the image domain to form a coarse-to-fine image pyramid to evaluate the perceptual quality of the reconstruction, and &lt;/li>; &lt;li>; Skipping all but the final auxiliary head at inference time. &lt;/li>; &lt;/ol>; &lt;p>; With the newly designed architecture, we train the BlazeStyleGAN model by distilling it from a teacher StyleGAN model. We use a multi-scale perceptual loss and adversarial loss in the distillation to transfer the high fidelity generation capability from the teacher model to the student BlazeStyleGAN model and also to mitigate the artifacts from the teacher model. &lt;/p>; &lt;p>; More details of the model architecture and training scheme can be found in &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.pdf&quot;>;our paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3TpxAGbwALgLPFX9ocKbOTjYkRKvx28IY1jWHzM5mdgc3xTBFpn2MfbW1WpJPgjewSY2-zxRyo9FJEjxcx0gzaJxGf2bJGSuVkUUUTM0Eob60tjUuwaF-RUsGERGzl0o5LYkn7G0oFXB1UORri9RhnZ0FIvfbh3F5PFdt7IbVeeNw-yO6Ua8MB0Y2V5A/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;488&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3TpxAGbwALgLPFX9ocKbOTjYkRKvx28IY1jWHzM5mdgc3xTBFpn2MfbW1WpJPgjewSY2-zxRyo9FJEjxcx0gzaJxGf2bJGSuVkUUUTM0Eob60tjUuwaF-RUsGERGzl0o5LYkn7G0oFXB1UORri9RhnZ0FIvfbh3F5PFdt7IbVeeNw-yO6Ua8MB0Y2V5A/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual comparison between face samples generated by StyleGAN and BlazeStyleGAN. The images on the first row are generated by the teacher StyleGAN. The images on the second row are generated by the student BlazeStyleGAN. The face generated by BlazeStyleGAN has similar visual quality to the image generated by the teacher model. Some results demonstrate the student BlazeStyleGAN suppresses the artifacts from the teacher model in the distillation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In the above figure, we demonstrate some sample results of our BlazeStyleGAN. By comparing with the face image generated by the teacher StyleGAN model (top row), the images generated by the student BlazeStyleGAN (bottom row) maintain high visual quality and further reduce artifacts produced by the teacher due to the loss function design in our distillation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;An encoder for efficient GAN inversion&lt;/h3>; &lt;p>; To support image-to-image stylization, we also introduced an efficient GAN inversion as the encoder to map input images to the latent space of the generator. The encoder is defined by a &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;>;MobileNet V2&lt;/a>; backbone and trained with natural face images. The loss is defined as a combination of image perceptual quality loss, which measures the content difference, style similarity and embedding distance, as well as the &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization&quot;>;L1 loss&lt;/a>; between the input images and reconstructed images. &lt;/p>; &lt;br />; &lt;h2>;On-device performance&lt;/h2>; &lt;p>; We documented model complexities in terms of parameter numbers and computing &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPs&lt;/a>; in the following table. Compared to the teacher StyleGAN (33.2M parameters), BlazeStyleGAN (generator) significantly reduces the model complexity, with only 2.01M parameters and 1.28G FLOPs for output resolution 256x256. Compared to StyleGAN-1024 (generating image size of 1024x1024), the BlazeStyleGAN-1024 can reduce both model size and computation complexity by 95% with no notable quality difference and can even suppress the artifacts from the teacher StyleGAN model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Model&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Image Size&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;#Params (M)&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;FLOPs (G)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;StyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1024 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;33.17 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;74.3 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1024 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.07 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4.70 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;512 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.05 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.57 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;256 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.01 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.28 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Encoder&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;256 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.44 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.60 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model complexity measured by parameter numbers and FLOPs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We benchmarked the inference time of the MediaPipe FaceStylizer on various high-end mobile devices and demonstrated the results in the table below. From the results, both BlazeStyleGAN-256 and BlazeStyleGAN-512 achieved real-time performance on all GPU devices. It can run in less than 10 ms runtime on a high-end phone&#39;s GPU. BlazeStyleGAN-256 can also achieve real-time performance on the iOS devices&#39; CPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;strong>;型号&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align : center;&quot;>;&lt;strong>;BlazeStyleGAN-256（毫秒）&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong >;Encoder-256（毫秒）&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 11&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;12.14 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.48 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 12&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.25 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 13专业版&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.22 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt; /td>; &lt;td style=&quot;text-align: center;&quot;>;5.41 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Pixel 6&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.24 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot; >;11.23 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;三星 Galaxy S10&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-对齐：中心;&quot;>;17.01 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.70 &lt;/td>; &lt;/tr>; &lt;tr>; &lt; td>;&lt;em>;三星 Galaxy S20&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;8.95 &lt;/td>;&lt;td>; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;8.20 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;BlazeStyleGAN、面部编码器和端到端的延迟基准各种移动设备上的管道。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;公平性评估&lt; /h2>; &lt;p>; 该模型经过高多样性人脸数据集的训练，期望该模型能够对不同的人脸做到公平。公平性评估表明该模型在人脸性别、肤色等方面表现良好且均衡。肤色和年龄。&lt;/p>; &lt;br />; &lt;h2>;人脸风格化可视化&lt;/h2>; &lt;p>;下图展示了一些人脸风格化结果。顶行中的图像（橙色框中）代表用于微调模型的样式图像。左列中的图像（绿色框中）是用于测试的自然人脸图像。图像的 2x4 矩阵表示 MediaPipe FaceStylizer 的输出，该输出混合自然人脸图像之间的输出。最左列上的面孔和顶行上相应的面孔样式。结果表明，我们的解决方案可以为几种流行风格实现高质量的面部风格化。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpKZWwOnXcr79JiDtlbXO4sckZWVIpWtneSu3J4OgtIs6o6JtNpF8nbPkPpZqphMOP73LxX3MYS08OEEMQn3dAvmIG BFYBgQiL-bpMHWRKDl8WF2rO-aBca1uWr4y6p5sl8tvH1FgXMfXUyV-1c4UhuhPQ6QfTodu_tUdfkF-U257joD7tkkAAzv6AAoY/s940/image3 .jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;940&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpKZWwOnXcr79JiDtlbXO4sckZWVIpWtneSu3J4OgtIs6o6JtNpF8nbPkPpZqphMOP73LxX3MYS08OEEMQn3dAvmIGBFYBgQiL-bpMHWRKDl8WF2rO- aBca1uWr4y6p5sl8tvH1FgXMfXUyV-1c4UhuhPQ6QfTodu_tUdfkF-U257joD7tkkAAzv6AAoY/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MediaPipe FaceStylizer 的示例结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MediaPipe 解决方案&lt;/h2>; &lt;p>; MediaPipe FaceStylizer 将于 &lt;a href=&quot;https://developers.google.com 向公众用户发布/mediapipe/solutions&quot;>;MediaPipe 解决方案&lt;/a>;。用户可以利用 &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe Model Maker&lt;/a>; 使用自己的风格图像训练自定义的面部风格化模型。训练后，导出的 &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TFLite&lt;/a>; 模型文件包可以跨平台（Android、iOS、Web、Python 等）部署到应用程序中。 ）只需几行代码即可使用 &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe Tasks FaceStylizer API&lt;/a>;。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是通过 Google 多个团队的协作完成的。我们要感谢 Omer Tov、Yang Zhu、Andrey Vakunov、Fei Deng、Ariel Ephrat、Inbar Mosseri、Lu Wang、Chuo-Ling Chang、Tingbo Hou 和 Matthias Grundmann 的贡献。&lt;/em>; &lt;/p>;&lt; /content>;&lt;link href=&quot;http://blog.research.google/feeds/7378908661087185403/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/mediapipe-facestylizer-on-device-real.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7378908661087185403&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/7378908661087185403&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2023/09/mediapipe-facestylizer-on-device-real.html&quot; rel=&quot;alternate&quot; title=&quot;MediaPipe FaceStylizer：设备上实时少镜头脸部风格化&quot; type=&quot;text/html&quot;/>;&lt;author >;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16 “rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>;&lt;/gd :image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihspgvRDlnUmToZlZzCcRWCFIt9TRnLH9lepBq6ojBLxZ0nFEPBS7bQgOHY0klCfkewtZy5KSGnKYialzlh4xNSFP5Th4 mvWJWcJ8XvNkCAJK9T7f_xHSK-H0uPW0paxcTG3nhQtP7eY7iKSVjX-Oaca182KHKiuGzj2C4yWT_kenY-Ys7LtS7i93RCtcP/s72-c/FaceStylizer-Hero .png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;条目>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-118669777610020383&lt;/id>;&lt;发布>;2023-09-14T12:39:00.035-07:00&lt;/发布>;&lt;更新>;2023-09 -14T18：23：00.311-07：00 &lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“可访问性”>;&lt;/类别>;&lt;类别方案=“http” ://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;使用图神经网络进行设备上内容蒸馏&lt;/stitle>;&lt;content type= &quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院研究工程师 Gabriel Barcik 和 Duc-Hieu Tran&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEjAbKikU6UWOjYuGg9JWDI3s8QKhXHtUEl2STZt_TNmwR4Y_B65GkYs--uMmYUYVVBdbTrtAVLRmlEGc1fTgeMS2E1kaNLmUJk6xWoj0qm0axNj2OcMzzPTZ68ygM0f7 ZOo_8qoUXjTGGTO74-LZ9gvt9eK8lZjRDE0HWJNMJWYM_A2ppRGl5v8QVrxBEg7/s971/ScreenGNN.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 在当今的数字时代，智能手机和桌面网络浏览器是访问新闻和信息的主要工具。然而，网站混乱的激增——包括复杂的布局、导航元素和无关的链接——严重损害了阅读体验和文章导航。对于有无障碍要求的个人来说，这个问题尤其严重。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了改善用户体验并使阅读更方便，&lt;a href=&quot;https://www.android.com/google-features-on -android/december-2022/#accessibility&quot;>;Android&lt;/a>; 和 &lt;a href=&quot;https://blog.google/outreach-initiatives/education/chromebook-updates-2023/&quot;>;Chrome&lt;/a>; 用户可以利用阅读模式功能，该功能通过处理网页来增强可访问性，以允许可定制的对比度、可调整的文本大小、更清晰的字体以及启用文本到语音的实用程序。此外，Android 的阅读模式还可以从应用程序中提取内容。扩展阅读模式以涵盖广泛的内容并提高其性能，同时仍然在用户设备上本地运行而不向外部传输数据，提出了独特的挑战。 &lt;/p>; &lt;p>; 为了在不损害隐私的情况下扩大阅读模式功能，我们开发了一种新颖的设备上内容蒸馏模型。与早期尝试使用 &lt;a href=&quot;https://github.com/chromium/dom-distiller&quot;>;DOM Distiller&lt;/a>;（一种仅限于新闻文章的启发式方法）不同，我们的模型在各种类型的质量和多功能性方面都表现出色的内容。我们确保文章内容不会脱离当地环境的限制。我们的设备上内容蒸馏模型可以将长篇内容顺利地转换为简单且可定制的布局，以提供更愉快的阅读之旅，同时也优于领先的替代方法。在这里，我们探讨了这项研究的细节，重点介绍了我们的方法、方法和结果。 &lt;/p>; &lt;br />; &lt;h2>;图神经网络&lt;/h2>; &lt;p>; 我们不依赖于难以维护和扩展到各种文章布局的复杂启发式方法，而是将此任务视为完全&lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;>;监督学习&lt;/a>;问题。这种数据驱动的方法使模型能够更好地泛化不同的布局，而不受启发式的约束和脆弱性。以前优化阅读体验的工作依赖于 HTML 或文档的解析、过滤和建模对象模型&lt;/a>; (DOM)，一种由用户的 Web 浏览器从站点 HTML 自动生成的编程接口，它表示文档的结构并允许对其进行操作。 &lt;/p>; &lt;p>; 新的阅读模式模型依赖于&lt;a href=&quot;https://developer.chrome.com/blog/full-accessibility-tree/#what-is-the-accessibility-tree&quot;>;可访问性树&lt;/a>;，它提供了 DOM 的简化且更易于访问的表示。辅助功能树是从 DOM 树自动生成的，并由辅助技术利用，以允许残障人士与 Web 内容进行交互。这些可以在 Chrome Web 浏览器和 Android 上通过为 WebView 提供的 &lt;a href=&quot;https://developer.android.com/reference/android/view/accessibility/AccessibilityNodeInfo&quot;>;AccessibilityNodeInfo&lt;/a>; 对象来使用和本机应用程序内容。 &lt;/p>; &lt;p>; 我们首先手动收集和注释可访问性树。该项目使用的 Android 数据集包含大约 10k 个标记示例，而 Chrome 数据集包含大约 100k 个标记示例。我们开发了一种新颖的工具，它使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_neural_network&quot;>;图神经网络&lt;/a>;（GNN）使用多类从可访问性树中提取基本内容监督学习方法。数据集由从网络上采样的长篇文章组成，并标有标题、段落、图像、发布日期等类别。 &lt;/p>; &lt;p>; GNN 是处理树状数据结构的自然选择，因为传统模型通常需要详细的、手工制作的特征来理解这些树中的布局和链接，而 GNN 可以自然地学习这些连接。为了说明这一点，请考虑家谱的类比。在这样的树中，每个节点代表一个家庭成员，连接表示家庭关系。如果要使用传统模型预测某些特征，可能需要“具有某种特征的直系亲属数量”等特征。然而，对于 GNN，这种手动特征设计就变得多余了。通过直接将树结构输入模型，GNN 利用消息传递机制，每个节点与其邻居进行通信。随着时间的推移，信息在网络上共享和积累，使模型能够自然地辨别复杂的关系。 &lt;/p>; &lt;p>; 回到可访问性树的背景，这意味着 GNN 可以通过理解和利用树内的固有结构和关系来有效地提取内容。此功能使他们能够根据树内的信息流识别并可能省略非必要部分，从而确保更准确的内容提炼。 &lt;/p>; &lt;p>; 我们的架构很大程度上遵循使用 &lt;a href=&quot;https:/ 的&lt;a href=&quot;https://arxiv.org/abs/1806.01261&quot;>;编码-处理-解码&lt;/a>;范例/arxiv.org/abs/1704.01212&quot;>;消息传递神经网络&lt;/a>;用于对文本节点进行分类。整体设计如下图所示。文章的树表示是模型的输入。我们根据边界框信息、文本信息和可访问性角色计算轻量级特征。然后，GNN 使用消息传递神经网络通过树的边缘传播每个节点的潜在表示。此传播过程允许附近的节点、容器和文本元素相互共享上下文信息，从而增强模型对页面结构和内容的理解。然后，每个节点根据收到的消息更新其当前状态，为节点分类提供更明智的基础。经过固定数量的消息传递步骤后，现在上下文化的节点潜在表示被解码为基本或非基本类。这种方法使模型能够利用树中的固有关系和代表每个节点的手工特征，从而丰富最终的分类。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2RnSYjrCvHDVNxzOHlR3duUISGpF-xRz_Xv3fGUn-Fg0FLfPrkBBnYgVq8kMpbCQ525gHfFWd5L2vMwHw BSNBQiYUY7Q9ZAG-CJBR8SPU9-SQDuJ9k49Rd9Vg9b9TTenlIIgcALirLbBa6mMHyl5enrhystjwi0xOxy8gWedhGgB5SyQHi-e8YgSYjR/s1600 /image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2RnSYjrCvHDVNxzOHlR3duUISGpF-xRz_Xv3fGUn-Fg0FLfPrkBBnYgVq8kMpbCQ525gHfFWd5L2vMwHwbSlNBQiYUY7Q9ZAg-cJbR 8SpU9-SQDuJ9k49Rd9Vg9b9TTenlIIgcALirLbBa6mMHyl5enrhystjwi0xOxy8gWedhGgB5SyQHi-e8YgSYjR/s16000/image1.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在移动设备上处理文章的算法的视觉演示。图神经网络 (GNN) 用于从文章中提取基本内容。 1. 从应用程序中提取文章的树表示。 2. 为每个节点计算轻量级特征，表示为向量。 3. 消息传递神经网络通过树的边缘传播信息并更新每个节点表示。 4.包含文本内容的叶节点被分类为必要内容或非必要内容。 5. 基于 GNN 输出构建了应用程序的整洁版本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们刻意限制模型使用的特征集以增加其广泛的泛化性跨语言并加快用户设备上的推理延迟。这是一个独特的挑战，因为我们需要创建一个可以保护隐私的设备上轻量级模型。 &lt;/p>; &lt;p>; 我们最终的轻量级 Android 模型有 64k 个参数，大小为 334kB，中位延迟为 800ms，而 Chrome 模型有 241k 个参数，大小为 928kB，中位延迟为 378ms。通过采用此类设备内处理，我们确保用户数据永远不会离开设备，从而强化我们的&lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;负责任的做法&lt;/a>;以及对用户隐私的承诺。模型中使用的特征可以分为中间节点特征、叶节点文本特征和元素位置特征。我们执行了&lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_engineering&quot;>;功能工程&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_selection&quot;>;功能选择&lt;/a>;来优化模型性能和模型大小的特征集。最终模型被转换为 &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; 格式，以作为 Android 或 Chrome 上的设备上模型进行部署。 &lt;/p>; &lt;br />; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们在单个 GPU 中对 GNN 进行了大约 50 个周期的训练。 Android模型在网页和原生应用测试集上的表现如下：&lt;/p>;&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEiuC7wLsxGXfTVYqPvr7WMVSHtkC64DQwTlnbMrjZG-_EtJ7UPQO0O3vAiPPStbFESxgc4yronwPMS4MOK1o7b_qiun_zQnXzx6pdHp8mvGXzIlOf2508bsfyaRAaWo DA0th3vFLL_IQuyETrsbM3jNAunGaKkuwjCNn7CX0TfrZZdO21zlrj6XxsPEAhJz/s1339/AndroidQuality.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot; 487“数据原始宽度=“1339”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuC7wLsxGXfTVYqPvr7WMVSHtkC64DQwTlnbMrjZG-_EtJ7UPQO0O3vAiPPStbFESxgc4yronwPMS4MOK1o 7b_qiun_zQnXzx6pdHp8mvGXzIlOf2508bsfyaRAaWoDA0th3vFLL_IQuyETrsbM3jNAunGaKkuwjCNn7CX0TfrZZdO21zlrj6XxsPEAhJz/s16000/AndroidQuality.png&quot;/>;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;该表显示了 Android 中网页和本机应用的内容蒸馏指标。我们报告&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;精确度、召回率&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/F-score三个类别的“F1 分数”&lt;/a>;：非必要内容、标题和正文，包括宏观平均值和每个类别中实例数的加权平均值。节点度量以可访问性树节点的粒度评估分类性能，这类似于段落级别。相比之下，单词度量在单个单词级别评估分类，这意味着节点中的每个单词都获得相同的分类。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;tablealign=&quot;center &quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;13&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;Android 质量 &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;font-size: large; text-align: center;&quot;>;&lt;strong>;网页&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;font-size: large; text-align: center;&quot;>;&lt;strong>;原生应用&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#e8eaed&quot;>; &lt;td style=&quot;font-size: normal;&quot;>;&lt;strong>;&lt;em>;节点指标&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt; /td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;精度&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;回忆&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/ em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;精度&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;回忆&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot; text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;非必要&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp ;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9842 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9842 &lt;/td>;&lt;td>; ;&quot;>;0.9846 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9844 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/ td>; &lt;td style=&quot;text-align: center;&quot;>;0.9744 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9350 &lt;/td >;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9543 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;标题&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9187 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9784 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9476 &lt;/td>;&lt;td>; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9183 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align:中心;&quot;>;0.8568 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8865 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>; &lt;em>;主文本&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9223 &lt;/td>;&lt;td>;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9172 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9172 &lt;/td>;&lt;td>; &quot;>;0.9197 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8443 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;0.9424 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8907 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;宏观平均&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>; 0.9417 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9600 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td style=&quot;text-align: center;&quot;>;0.9506 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9124 &lt;/td>;&lt;td >;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9114 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align : center;&quot;>;0.9105 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;加权平均值&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style= &quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>; &quot;>;0.9392 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9353 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;0.9363 &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#e8eaed&quot;>; &lt;td>;&lt;strong>;&lt;em>;字词指标&lt;/em>;&lt;/strong >; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;精度&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;回想&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text -align: center;&quot;>;&lt;em>;F1 分数&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;精度&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;召回率&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1 分数&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;标题 + 主要文本&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9510&lt;/strong>; &lt;/td >;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9683&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/ td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9595&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center ;&quot;>;&lt;strong>;0.9473&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9507&lt;/strong>; &lt; /td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9490&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt; /table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container “样式=”margin-left：自动; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;该表显示了 Android 中网页和本机应用程序的内容蒸馏指标。我们报告&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;精确度、召回率&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot; >;F1-score&lt;/a>; 三个类别：非必要内容、标题和正文，包括宏观平均和每个类别中实例数的加权平均。节点指标以可访问性的粒度评估分类性能树节点，类似于段落级别。相比之下，单词度量在单个单词级别评估分类，这意味着节点中的每个单词都会获得相同的分类。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; -->; &lt;p>; 在评估经常访问的网页文章的结果质量时，主要文本（本质上是段落）的 F1 分数超过 0.9 对应于 88% 的这些文章正在处理而没有遗漏任何段落。此外，在超过 95% 的情况下，蒸馏对读者来说是有价值的。 Put simply, the vast majority of readers will perceive the distilled content as both pertinent and precise, with errors or omissions being an infrequent occurrence. &lt;/p>; &lt;p>; Chrome 内容蒸馏与其他模型（例如 &lt;a href=&quot;https://github.com/chromium/dom-distiller&quot;>;DOM Distiller&lt;/a>; 或 &lt;a href=&quot; https://github.com/mozilla/readability&quot;>;一组英语页面上的 Mozilla 可读性&lt;/a>;如下表所示。我们重用机器翻译的指标来比较这些模型的质量。参考文本来自真实的主要内容，来自模型的文本作为假设文本。结果表明，与其他基于 DOM 的方法相比，我们的模型具有出色的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM3va2Pt2otiRp6UesiiUqxZygl24Pxv3EpTYAKcS_fIJEDq5I6DMq3Lcc_DzOEg0VsE41SvIMg9ISgrdAP27yVT9 StyysA89xcqSLyj7QWYi-Hc4eye-3c3WNLA9SWIWqtt-cbcMJsPX13UJrPDoMuqTUTJdyp-sW1GqgxmAADmrsze4kCyXHV8dPnEZj/s1339/ChromeQuality .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM3va2Pt2otiRp6UesiiUqxZygl24Pxv3EpTYAKcS_fIJEDq5I6DMq3Lcc_DzOEg0VsE41SvIMg9ISgrdAP27yVT9StyysA89xcqSLyj7Q WYi-Hc4eye-3c3WNlA9SWIWqtt-cbcMJsPX13UJrPDoMuqTUTJdyp-sW1GqgxmAADmrsze4kCyXHV8dPnEZj/s16000/ChromeQuality.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;该表展示了 DOM-Distiller、Mozilla Readability 和新 Chrome 模型之间的比较。我们报告基于文本的指标，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLUE&lt;/a>;、&lt;a href=&quot;https://aclanthology.org/W15- 3049.pdf&quot;>;CHRF&lt;/a>; 和 &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE&lt;/a>;，通过比较从每个模型中提取的主体文本评分者使用我们的注释政策手动标记的真实文本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot; 0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;7&quot; style=&quot;font-size: x- large; text-align: center;&quot;>;网页上的 Chrome 模型比较 &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td style=&quot;font-size: normal; text-align: center;&quot; >;&lt;strong>;&lt;em>;公制/型号&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em >;DOM Distiller&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Mozilla 可读性&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;我们的 Chrome 模型&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td 样式=&quot;text-align: center;&quot;>;&lt;em>;BLEU&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;78.97 &lt; /td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;79.16 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td 样式=&quot;text-align: center;&quot;>;94.59 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;CHRF&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.92 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align:中心;&quot;>;0.92 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td 样式=&quot;text-align: center;&quot;>;&lt;em>;ROUGE1&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.10 &lt; /td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.62 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td 样式=&quot;text-align: center;&quot;>;95.13 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGE2&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;81.84 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align:中心;&quot;>;82.66 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;94.81 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td 样式=&quot;text-align: center;&quot;>;&lt;em>;ROUGE3&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;80.21 &lt; /td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;81.45 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td 样式=&quot;text-align: center;&quot;>;94.60 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGEL&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;83.58 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align:中心;&quot;>;84.02 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;95.04 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td 样式=&quot;text-align: center;&quot;>;&lt;em>;ROUGEL-SUM&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>; 83.46 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.03 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td style=&quot;text-align: center;&quot;>;95.04 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div >; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;表格呈现了 DOM-Distiller、Mozilla Readability 和新版 Chrome 之间的比较模型。我们报告基于文本的指标，例如 &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLUE&lt;/a>;、&lt;a href=&quot;https://aclanthology.org/ W15-3049.pdf&quot;>;CHRF&lt;/a>; 和 &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE&lt;/a>;，通过比较从中提取的主体文本每个模型都由评估者使用我们的注释策略手动标记为真实文本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;p>; Chrome 内容蒸馏模型的 F1 分数不同广泛使用的语言的测试集上的标题和主要文本内容表明，Chrome 模型尤其能够支持多种语言。&lt;/p>;&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing =&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6Z1lggR6AGKL0sxRWLmKEN50R4- OC4_JFrieWniKARBKxSSSo1jlry-Q7yRkysHBX1JpLOy0lEGhWNf3UlmEErjJoT_BixDzAUDqHwuKb-XngC0d_6-ynB5cXlRtNUjvneK1M8fzxH1va8jggc5cfROR8DwaTEx-hW0taoeGPtNML3qQ8Trxee m_J7UTM/s1339/ChromeLanguages.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;195&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEg6Z1lggR6AGKL0sxRWLmKEN50R4-OC4_JFrieWniKArBKxSSSo1jlry-Q7yRkysHBX1JpLOy0lEGhWNf3UlmEErjJoT_BixDzAUDqHwuKb-XngC0d_6-ynB5cXlRtNUjvneK1M8 fzxH1va8jggc5cfROR8DwaTEx-hW0taoeGPtNML3qQ8Trxeem_J7UTM/s16000/ChromeLanguages.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;该表显示了 Chrome 模型的标题和主要文本类别的 F1 分数的每种语言。语言代码对应于以下语言：德语、英语、西班牙语、法语、意大利语、波斯语、日语、韩语、葡萄牙语、越南语、简体中文和繁体中文。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot;类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;27&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;不同语言的 Chrome 模型&lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td style=&quot;font-size: normal; text-align: left;&quot;>;&lt;strong>;&lt;em>;F1 分数&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align : center;&quot;>;&lt;em>;德&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;en&lt;/em >; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;es&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;fr&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text -align: center;&quot;>;&lt;em>;它&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;fa&lt; /em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ja&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ko&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style= &quot;text-align: center;&quot;>;&lt;em>;pt&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>; vi&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;zh-Hans&lt;/em>; &lt;/td>;&lt; td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;zh-Hant&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/ td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;平均&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;标题&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt; /td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.99 &lt;/ td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style= &quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.89 &lt;/td>;&lt;td>;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>; &quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text -align: center;&quot;>;0.96 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;主要文本&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.84 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.84 &lt;/td>;&lt;td>; &quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.87 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text -align: center;&quot;>;0.88 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>; 0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>; &lt;/ tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; 类=&quot;tr-caption-container&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;该表显示了标题的 Chrome 型号的每种语言的 F1 分数语言代码对应以下语言：德语、英语、西班牙语、法语、意大利语、波斯语、日语、韩语、葡萄牙语、越南语、简体中文和繁体中文。&lt;/td>;&lt;/tr>;&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; -->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 数字时代需要简化的内容呈现以及对用户隐私的坚定承诺。我们的研究强调了 Android 和 Chrome 等平台中阅读模式的有效性，通过图神经网络提供了一种创新的、数据驱动的内容解析方法。至关重要的是，我们的轻量级设备上模型确保了内容蒸馏过程不会泄露用户数据，所有流程均在本地执行，这不仅增强了阅读体验，也加强了我们对用户隐私的重视。当我们探索不断变化的数字内容消费格局时，我们的发现强调了在体验和安全方面优先考虑用户的至关重要性。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;该项目是与 Manuel Tragut、Mihai Popa、Abodunrinwa Toki、Abhanshu Sharma、Matt Sharifi、David Petrou 和 Blaise 共同工作的结果阿格拉和阿卡斯。我们衷心感谢我们的合作者李刚和李阳。我们非常感谢 Tom Small 协助我们准备这篇文章。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/118669777610020383/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/on-device-content-distillation-with .html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 118669777610020383&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/118669777610020383&quot; rel=&quot;self&quot; type= “application/atom+xml”/>;&lt;link href=&quot;http://blog.research.google/2023/09/on-device-content-distillation-with.html&quot; rel=&quot;alternate&quot; title=&quot;On-使用图神经网络进行设备内容蒸馏” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog. com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEjAbKikU6UWOjYuGg9JWDI3s8QKhXHtUEl2STZt_TNmwR4Y_B65GkYs--uMmYUYVVBdbTrtAVLRmlEGc1fTgeMS2E1kaNLmUJk6xWoj0qm0axNj2OcMzzPTZ68ygM0f7 ZOo_8qoUXjTGGTO74-LZ9gvt9eK8lZjRDE0HWJNMJWYM_A2ppRGl5v8QVrxBEg7/s72-c/ScreenGNN.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6191878611070084392&lt;/id>;&lt;已发布>;2023-09-12T14:22 :00.000-07:00&lt;/已发布>;&lt;更新>;2023-09-12T14:22:55.668-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;Google 地图&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;Publications&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;强化学习&quot;>;&lt; /category>;&lt;title type=&quot;text&quot;>;Google 地图中的世界规模逆强化学习&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 软件工程师 Matt Barnes研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgA5ZpGidOzCqTYTLV8bj62lAG7yfVa0cson-09oo7hqGA9ayl7h6koU96mxkBOqP_NyqiKamaoFrSAHtBlY7UH7XMnoq5Hn1H 0hoiC5Uk0mMOkunNi6-j08iSEmUXTYEmp1YuFEgWLRJtieseqhQseMpy6e8KY5wElwNKDcy99GjCO-j04G0TVaJb0Pcr/s320/hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google 地图中的路线选择仍然是我们最有用和最常用的功能之一。确定从 A 到 B 的最佳路线需要在各种因素之间进行复杂的权衡，包括 &lt;a href=&quot;https://www.deepmind.com/blog/traffic-prediction-with-advanced-graph-neural-networks&quot;>;预计到达时间 (ETA)、通行费、直达度、路面状况（例如，已铺砌的道路、未铺砌的道路）以及用户偏好，这些因素因运输方式和当地地理位置而异。通常，我们对旅行者偏好最自然的了解是通过分析现实世界的旅行模式。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 从观察到的顺序决策行为中学习偏好是&lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning 的经典应用#Inverse_reinforcement_learning&quot;>;逆向强化学习&lt;/a>;（IRL）。给定一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;>;马尔可夫决策过程&lt;/a>;（MDP）——道路网络的形式化——和一组演示轨迹（行驶路线），IRL的目标是恢复用户的潜在奖励函数。尽管&lt;a href=&quot;https://link.springer.com/article/10.1007/s10462-021-10108-x&quot;>;过去的研究&lt;/a>;已经创建了越来越通用的IRL解决方案，但这些解决方案尚未成功扩展到世界各地大小的 MDP。扩展 IRL 算法具有挑战性，因为它们通常需要在每个更新步骤求解 RL 子例程。乍一看，由于大量路段和有限的高带宽内存，即使尝试将世界范围的 MDP 放入内存中来计算单个梯度步骤似乎也是不可行的。当将IRL应用于路由时，需要考虑每个演示的起点和目的地之间的所有合理路线。这意味着任何将世界规模的 MDP 分解为更小的组成部分的尝试都不能考虑小于大都市区的组成部分。 &lt;/p>; &lt;p>; 为此，在“&lt;a href=&quot;https://arxiv.org/abs/2305.11290&quot;>;Google 地图中的大规模可扩展逆强化学习&lt;/a>;”中，我们分享了以下结果： Google Research、Maps 和 Google DeepMind 之间的多年合作，旨在突破这种 IRL 可扩展性限制。我们重新审视该领域的经典算法，并介绍图压缩和并行化方面的进步，以及一种名为“后退地平线逆规划”(RHIP) 的新 IRL 算法，该算法提供对性能权衡的细粒度控制。最终的 RHIP 策略在全局路线匹配率（即与 Google 地图中建议路线完全匹配的去标识化行驶路线的百分比）方面实现了 16-24% 的相对改进。据我们所知，这是迄今为止现实世界中最大的 IRL 实例。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIjGlEhwPBRCKI-_LLSIF_TRSB7FGohWSwbvTh0K-3wovXgGmWKBYFXBIKhdVOkqHPLq5F4ZLO0tZVXyyprnXDJ2UqzW mDLQV4RwXDqiMjAmbCTkcey7JauVHhhcakVEveWA4U4qnzokEli9E_PblSodxtsRO7hCRnmJBRvU-zRcxNCP_9pG4-Kuff2twW/s1999/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1040&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIjGlEhwPBRCKI-_LLSIF_TRSB7FGohWSwbvTh0K-3wovXgGmWKBYFXBIKhdVOkqHPLq5F4ZLO0tZVXyyprnXDJ2UqzWmDLQV4RwXDqiMjAmbCTkcey7J auVHhhcakVEveWA4U4qnzokEli9E_PblSodxtsRO7hCRnmJBRvU-zRcxNCP_9pG4-Kuff2twW/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用 RHIP 反向强化学习策略时，Google 地图的路线匹配率相对于现有基线有所提高。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;IRL 的好处&lt;/h2>; &lt;p>; 一个微妙但至关重要的细节关于路由问题的一点是，它是&lt;em>;目标条件&lt;/em>;的，这意味着每个目的地状态都会引发略有不同的 MDP（具体来说，目的地是终端、零奖励状态）。 IRL 方法非常适合此类问题，因为学习到的奖励函数可以跨 MDP 转移，并且仅修改目标状态。这与直接学习策略的方法形成对比，后者通常需要额外的 &lt;em>;S&lt;/em>; 参数因子，其中 &lt;em>;S&lt;/em>; 是 MDP 状态的数量。 &lt;/p>; &lt;p>; 一旦通过 IRL 学习了奖励函数，我们就可以利用强大的推理时间技巧。首先，我们在&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/static-vs-dynamic-inference/video-lecture&quot;>;离线批量设置中评估一次整个图表的奖励&lt;/a>;。该计算完全在服务器上执行，无法访问各个行程，并且仅对图中的批量路段进行操作。然后，我们将结果保存到内存数据库中，并使用快速在线&lt;a href=&quot;https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm&quot;>;图搜索算法&lt;/a>;来找到最高的用于在任何起点和目的地之间路由请求的奖励路径。这避免了对深度参数化模型或策略执行在线推理的需要，并极大地改善了服务成本和延迟。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHgaS1N-fsKIzQTMZEelXDZ8TVZboZvSZ3Z9ZLPlYR48NtGRabWXer_1r38dLm4cCIF8lBuapSVBPWzkVdBkTG7fdFuOw bry0Shdg7_sR19FdVOichywmraMPVUvBLl3XYS2B0rY1JdroZxIqlWB0rjl-mIGV3gASY6IuxhEQgjVdAqVvZJL1IYJ3HU3zc/s791/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;406&quot; data-original-width=&quot;791&quot; height=&quot;328&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHgaS1N-fsKIzQTMZEelXDZ8TVZboZvSZ3Z9ZLPlYR48NtGRabWXer_1r38dLm4cCIF8lBuapSVBPWzkVdBkTG7fdFuOwbry0Shdg7_sR19FdVOichywm ramMPVUvBLl3XYS2B0rY1JdroZxIqlWB0rjl-mIGV3gASY6IuxhEQgjVdAqVvZJL1IYJ3HU3zc/w640-h328/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用批量推理和快速在线规划器奖励模型部署。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;后退地平线逆向规划&lt;/h2>; &lt;p>; 为了将 IRL 扩展到世界 MDP，我们压缩了图并使用基于地理区域的稀疏&lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;专家混合&lt;/a>; (MoE) 对全局 MDP 进行分片。然后，我们应用经典的 IRL 算法来求解本地 MDP，估计损失，并将梯度发送回 MoE。全球奖励图是通过解压最终的 MoE 奖励模型来计算的。为了更好地控制性能特征，我们引入了一种新的广义 IRL 算法，称为后退地平线逆规划 (RHIP)。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHUdaeXJURVEV2f--mytRfbx5k3yftkMKBpuu43GKhyMaAa-fJC3O4QfTxZjz3hkGjUsCBiO6LLcYRsGeyrpOgIUz DDPW1lOwlr47YXKSLUwJuXxMELxRV9SKcr4BDoy5_2HzcFZX-Wb7m3PcQhWvx1efvG5gkXG_HfrhqrqfZ_xrNyBmSado-Wf_si3wH/s1617/ image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;526&quot; data-original-width=&quot;1617&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHUdaeXJURVEV2f--mytRfbx5k3yftkMKBpuu43GKhyMaAa-fJC3O4QfTxZjz3hkGjUsCBiO6LLcYRsGeyrpOgIUzDDPW1lOwlr47YXKSLUwJuXxM ELxRV9SKcr4BDoy5_2HzcFZX-Wb7m3PcQhWvx1efvG5gkXG_HfrhqrqfZ_xrNyBmSado-Wf_si3wH/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用 MoE 并行化、图形压缩和 RHIP 进行 IRL 奖励模型训练。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; RHIP 的灵感来自于人们倾向于进行广泛的本地规划（“接下来的一小时我要做什么？”）和近似的长期规划（“5 年后我的生活会是什么样子？”）。为了利用这种洞察力，RHIP 在演示路径周围的局部区域使用稳健但昂贵的随机策略，并在超出某个范围时改用更便宜的确定性规划器。调整范围&lt;em>;H&lt;/em>;可以控制计算成本，并且通常可以发现性能最佳点。有趣的是，RHIP 概括了许多经典的 IRL 算法，并提供了新颖的见解，即可以沿着随机与确定性谱来观察它们（具体来说，对于 &lt;em>;H&lt;/em>;=∞，它简化为 &lt;a href=&quot;https:/ /cdn.aaai.org/AAAI/2008/AAAI08-227.pdf&quot;>;MaxEnt&lt;/a>;，对于 &lt;em>;H&lt;/em>;=1，它减少为 &lt;a href=&quot;https://www.ijcai .org/Proceedings/07/Papers/416.pdf&quot;>;BIRL&lt;/a>;，对于 &lt;em>;H&lt;/em>;=0，它减少为 &lt;a href=&quot;https://dl.acm.org/ doi/10.1145/1143844.1143936&quot;>;MMP&lt;/a>;）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVip8f74bfUV6t4yy3-smAgmUday76QLz1AyzFRIzsudZ-MA7vfQACQbm2_6OhbqQ_wA8ZY-aGtwlEb2bZiJs -Ltp98wTlxA92ndfccuowYt5lUa6Ve7ZIoANAx2HWMF45mVXQDuQcNHfqZRLebAgOYBdUSlHi-7P6TMTszN5Pwe0jYvVUEHIjYPrigDTm/s1039 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;422&quot; data-original-width=&quot;1039&quot; height=&quot; 260&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVip8f74bfUV6t4yy3-smAgmUday76QLz1AyzFRIzsudZ-MA7vfQACQbm2_6OhbqQ_wA8ZY-aGtwlEb2bZiJs-Ltp98wTlx A92ndfccuowYt5lUa6Ve7ZIoANAx2HWMF45mVXQDuQcNHfqZRLebAgOYBdUSlHi-7P6TMTszN5Pwe0jYvVUEHIjYPrigDTm/w640-h260/image5.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;演示了从 s&lt;sub>;o&lt;/sub>; 到 s&lt;sub >;d&lt;/sub>;, (1) RHIP 在演示周围的局部区域（&lt;strong>;蓝色区域&lt;/strong>;）遵循稳健但昂贵的随机策略。 (2) 超出某个地平线 H，RHIP 转而遵循更便宜的确定性规划器（&lt;strong>;红线&lt;/strong>;）。调整范围可以对性能和计算成本进行细粒度控制。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;路线获胜&lt;/h2>; &lt;p>; RHIP 政策使驾驶和两轮车（例如踏板车、摩托车、轻便摩托车）的全球路线匹配率相对提高了 15.9% 和 24.1%分别到经过良好调整的地图基线。我们对更可持续的交通方式所带来的好处感到特别兴奋，在这种方式中，行程时间以外的因素也发挥着重要作用。通过调整 RHIP 的水平&lt;em>;H&lt;/em>;，我们能够实现比所有其他 IRL 策略更准确且比 MaxEnt 快 70% 的策略。 &lt;/p>; &lt;p>; 我们的 360M 参数奖励模型为 Google 地图用户在实时&lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;>;A/B 实验&lt;/a中提供直观的胜利>;。检查学习奖励和基线奖励之间绝对差异较大的路段可以帮助改进某些 Google 地图路线。例如： &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhghO43O0wC_DVBWbvclvbXruodtRbalSFRwKCmiNI1ws5NmlvDI6MwPhtLUkPCR5gfjiQ72lFRyI9WUFFX2VwPBNMC2K JxJjX49yrAhfNvU9_YFGQ2Z27K8VROnu9B4K7YNCmptiuoezWZZViqMMYonhmiYlgRnttMbpP2T44XLZE1kvm1bhSxfwgR-zNJ/s1245/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;426&quot; data-original-width=&quot;1245&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEhghO43O0wC_DVBWbvclvbXruodtRbalSFRwKCmiNI1ws5NmlvDI6MwPhtLUkPCR5gfjiQ72lFRyI9WUFFX2VwPBNMC2KJxJjX49yrAhfNvU9_YFGQ2Z27K8 VROnu9B4K7YNCmptiuoezWZZViqMMYonhmiYlgRnttMbpP2T44XLZE1kvm1bhSxfwgR-zNJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot; text-align: center;&quot;>;英国诺丁汉。由于存在大门，首选路线（&lt;strong>;蓝色&lt;/strong>;）之前被标记为私有财产，这向我们的系统表明道路有时可能会关闭，对于驾驶员来说并不理想。结果，Google 地图改为让司机绕道更长的替代路线（&lt;strong>;红色&lt;/strong>;）。然而，由于现实世界的驾驶模式表明，用户经常毫无问题地选择首选路线（因为大门几乎从不关闭），IRL 现在学会通过在该路段上设置大量正奖励来引导驾驶员沿着首选路线行驶。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 通过增加规模来提高性能（无论是在数据集大小还是模型复杂性方面）已被证明是机器学习的持续趋势。逆强化学习问题的类似收益历来仍然难以捉摸，这主要是由于处理实际规模的 MDP 的挑战。通过将可扩展性改进引入经典 IRL 算法，我们现在能够分别针对具有数亿个状态、演示轨迹和模型参数的问题训练奖励模型。据我们所知，这是迄今为止现实世界中最大的现实生活实例。请参阅&lt;a href=&quot;https://arxiv.org/abs/2305.11290&quot;>;论文&lt;/a>;了解有关这项工作的更多信息。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是 Google 多个团队的协作成果。该项目的贡献者包括 Matthew Abueg、Oliver Lange、Matt Deeds、Jason Trader、Denali Molitor、Markus Wulfmeier、Shawn O&#39;Banion、Ryan Epp、Renaud Hartert、Rui Song、Thomas Sharp、Rémi Robert、Zoltan Szego、Beth Luan、Brit Larabee 和 Agnieszka Madurska。&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;我们还要感谢 Arno Eigenwillig、Jacob Moorman、Jonathan Spencer、Remi Munos、Michael Bloesch 和 Arun Ahuja 进行了宝贵的讨论和建议。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6191878611070084392/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type= “application/atom+xml”/>;&lt;link href=&quot;http://blog.research.google/2023/09/world-scale-inverse-reinforcement.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6191878611070084392&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6191878611070084392&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/09/world-scale-inverse-reinforcement.html&quot; rel=&quot;alternate&quot; title=&quot;Google 地图中的世界规模逆强化学习&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot; 16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/ gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgA5ZpGidOzCqTYTLV8bj62lAG7yfVa0cson-09oo7hqGA9ayl7h6koU96mxkBOqP_NyqiKamaoFrSAHtBlY7U H7XMnoq5Hn1H0hoiC5Uk0mMOkunNi6-j08iSEmUXTYEmp1YuFEgWLRJtieseqhQseMpy6e8KY5wElwNKDcy99GjCO-j04G0TVaJb0Pcr/s72-c/英雄。 jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-237108111388574068&lt;/id>;&lt;发布>;2023-09-08T15:59:00.002-07:00&lt;/发布>;&lt;更新>;2023-09- 11T10:00:14.121-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;安全和隐私&quot;>;&lt;/category>;&lt;title type=&quot; text&quot;>;差异化私有中位数及更多信息&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院研究科学家 Edith Cohen 和 Uri Stemmer &lt;/span>; &lt;img src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiY4hQkG4Ofm6plcTF56kw-L1RI613kh8ztTMZvJgmn2VpFF84K1TIfBymU6p_xVE0q-SRafCvZtrzdCgNz4qqsyqVTevfEffmxqhQ_Jg6rJQ go5vm3zoOkQ6JnxeMNQ4Y7LrOufGJB8lTnJoKSYZYke2qZi0rvO9PUB5POaYdou4O7lE2-IO7hD0__aJcy/s1554/dpmedian.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;差分隐私&lt;/a>; (DP) 是隐私的严格数学定义。 DP 算法是随机的，通过确保添加或删除数据点时任何特定输出的概率几乎不变来保护用户数据。因此，DP 算法的输出不会透露任何一个数据点的存在。通过 &lt;a href=&quot;https://privacysandbox.com/&quot;>;隐私沙箱&lt;/a>; 和 &lt;a href=&quot;https:// opensource.googleblog.com/2020/06/expanding-our- Differential-privacy.html&quot;>;Google 开源库&lt;/a>;。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 机器学习和数据分析算法通常可以描述为在同一数据集上执行多个基本计算步骤。当每个这样的步骤都是不同的隐私时，输出也是不同的，但如果有多个步骤，整体隐私保证就会恶化，这种现象称为组合成本。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy#Composability&quot;>;组合定理&lt;/a>;将隐私损失的增加与计算次数&lt;em>;k&lt;/em>;联系起来：一般情况下，隐私损失随着&lt;em>;k&lt;/em>;的平方根而增加。这意味着我们需要为每一步提供更严格的隐私保证，以实现我们总体的隐私保证目标。但在这种情况下，我们就失去了效用。改善隐私与实用性权衡的一种方法是确定用例何时允许比组合定理得出的更严格的隐私分析。 &lt;/p>; &lt;p>; 当每个步骤应用于数据集的不相交部分（切片）时，这种改进的良好候选者。当以数据无关的方式选择切片时，每个点仅影响 &lt;em>;k&lt;/em>; 个输出中的一个，并且隐私保证不会随着 &lt;em>;k&lt;/em>; 的变化而恶化。然而，在某些应用程序中，我们需要自适应地选择切片（即，以取决于先前步骤的输出的方式）。在这些情况下，单个数据点的更改可能会级联 - 更改多个切片，从而增加合成成本。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;阈值和拟凹优化的最佳差分隐私学习&lt;/a>;”中，介绍于 &lt;a href= “http://acm-stoc.org/stoc2023/&quot;>;STOC 2023&lt;/a>;，我们描述了一种新的范例，允许自适应地选择切片，同时避免组合成本。我们表明，用于多个基本聚合和学习任务的 DP 算法可以在这种重新排序切片计算（RSC）范式中表达，从而在实用性方面获得显着改进。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;重新排序切片计算 (RSC) 范例&lt;/h2>; &lt;p>; 一种算法 &lt;em >;A&lt;/em>; 如果可以用以下一般形式表示（请参见下面的可视化），则属于 RSC 范式。输入是一组敏感的数据点&lt;em>;D&lt;/em>;。然后，该算法执行一系列 &lt;em>;k&lt;/em>; 个步骤，如下所示：&lt;/p>; &lt;ol>; &lt;li>;选择数据点的排序、切片大小 &lt;em>;m&lt;/em>; 和DP 算法&lt;em>;M&lt;/em>;。该选择可能取决于先前步骤中&lt;em>;A&lt;/em>;的输出（因此是自适应的）。 &lt;/li>;&lt;li>;根据数据集&lt;em>;D&lt;/em>;中的顺序切出（大约）前&lt;em>;m&lt;/em>;个数据点，将&lt;em>;M&lt;/em>;应用于切片，并输出结果。 &lt;/li>; &lt;/ol>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto；margin-right：auto；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj63itU6U--NeU1FGMjUslYsM63Ut85OAGE0bilRt8hy0-bxugE6CvLlNmtf1OtRjnJRsxXcOAPBeoOS2eEI b74whQCRn4ayBbacPR1j4I46TQsZ1Au0b3NTgEwh-Gtls1pGyxItUHx_vMHg- tq2XyyrbZ6SUKi1gDvX9udLoRi98eW5peyOJGpnu6Euzb8/s666/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;478&quot; data-original-width=&quot;666&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj63itU6U--NeU1FGMjUslYsM63Ut85OAGE0bilRt8hy0-bxugE6CvLlNmtf1OtRjnJRsxXcOAPBeoOS2eEIb74whQCRn4ayBbacPR1j4I46T QsZ1Au0b3NTgEwh-GtLs1pGyxItUHx_vMHg-tq2XyyrbZ6SUKi1gDvX9udLoRi98eW5peyOJGpnu6Euzb8/s16000/image2.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;三个重新排序切片计算 (RSC) 步骤的可视化。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt; /table>; &lt;p>; 如果我们使用 DP 组合定理分析 RSC 算法的整体隐私损失，则隐私保证会受到预期组合成本的影响，即，它会随着步数的平方根而恶化 &lt;em>;k&lt; /em>;.为了消除这种组合成本，我们提供了一种新颖的分析，完全消除了对&lt;em>;k&lt;/em>;的依赖：整体隐私保证接近于单个步骤！我们更严格分析背后的想法是一种新颖的技术，该技术可以限制修改单个数据点时受影响步骤的潜在级联（详细信息&lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;论文中&lt;/一个>;）。 &lt;/p>; &lt;p>; 更严格的隐私分析意味着更好的实用性。 DP 算法的有效性通常用足以发布满足隐私要求的正确结果的最小输入大小（数据点数量）来表示。我们描述了可以用 RSC 范式表达的算法的几个问题，并且我们更严格的分析提高了实用性。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;私人间隔点&lt;/h2>; &lt;p>; 我们从以下基本聚合任务开始。输入是来自有序域 &lt;em>;X&lt;/em>; 的 &lt;em>;n&lt;/em>; 个点的数据集 &lt;em>;D&lt;/em>;（将域视为 &lt;em>;1&lt; 之间的自然数） /em>; 和&lt;em>; |X|&lt;/em>;）。目标是返回 &lt;em>;X&lt;/em>; 中的一个点 &lt;em>;y&lt;/em>;，该点位于 &lt;em>;D&lt;/em>; 的 &lt;em>;interval&lt;/em>; 中，即在&lt;em>;D&lt;/em>; 中的最小值和最大值点。 &lt;/p>; &lt;p>; 在没有隐私要求的情况下，间隔点问题的解决方案很简单：只需返回数据集中的任何点&lt;em>;D&lt;/em>;。但该解决方案不保护隐私，因为它公开了输入中特定数据点的存在。我们还可以看到，如果数据集中只有一个点，则不可能提供隐私保护解决方案，因为它必须返回该点。因此，我们可以问以下基本问题：我们可以解决私有区间点问题的最小输入大小&lt;em>;N&lt;/em>;是多少？ &lt;/p>; &lt;p>; 众所周知，&lt;em>;N&lt;/em>; 必须随着域大小 &lt;em>;|X|&lt;/em>; 的增加而增加，并且这种依赖性至少为 &lt;a href=&quot;https: //en.wikipedia.org/wiki/Iterated_logarithm&quot;>;迭代日志函数&lt;/a>;&lt;em>; &lt;/em>;log&lt;em>;* |X|&lt;/em>; [&lt;a href=&quot;https://ieeexplore .ieee.org/document/7354419&quot;>;1&lt;/a>;，&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3313276.3316312&quot;>;2&lt;/a>;]。另一方面，&lt;a href=&quot;https://proceedings.mlr.press/v125/kaplan20a.html&quot;>;最佳先验&lt;/a>; DP 算法要求输入大小至少为 (log* &lt;em>; |X|&lt;/em>;)&lt;sup>;1.5&lt;/sup>;。为了弥补这一差距，我们设计了一种 RSC 算法，该算法仅需要 log*&lt;em>; |X|&lt;/em>; 点的顺序。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Iterated_logarithm&quot;>;迭代日志函数&lt;/a>;增长极其缓慢：它是我们需要采取的次数在达到等于或小于 &lt;em>;1&lt;/em>; 之前的值的对数。这个函数是如何在分析中自然而然出来的呢？ RSC 算法的每一步都会将域重新映射为其先前大小的对数。因此总共有 log* &lt;em>;|X|&lt;/em>; 步。更严格的 RSC 分析消除了所需输入大小中步数的平方根。 &lt;/p>; &lt;p>; 尽管区间点任务看起来非常基础，但它抓住了常见聚合任务的私有解决方案的困难本质。接下来我们描述其中两个任务，并用 &lt;em>;N&lt;/em>; 表示这些任务所需的输入大小。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;私有近似中位数&lt;/h2>; &lt;p>;这些常见聚合任务之一是&lt;em>;近似中位数&lt;/em>;：输入是来自有序域&lt;em>;X&lt;/em>;的&lt;em>;n&lt;/em>;个点的数据集&lt;em>;D&lt;/em>;。目标是返回位于 &lt;em>;D&lt;/em>; 的 ⅓ 和 ⅔ 分位数之间的点 &lt;em>;y&lt;/em>;。也就是说，&lt;em>;D&lt;/em>; 中至少有三分之一的点小于或等于 &lt;em>;y&lt;/em>;，并且至少有三分之一的点大于或等于 &lt;em>;y&lt; /em>;.请注意，使用差分隐私不可能返回精确的中值，因为它公开了数据点的存在。因此，我们考虑近似中位数的宽松要求（如下所示）。 &lt;/p>; &lt;p>; 我们可以通过找到一个区间点来计算近似中位数：我们切出&lt;em>;N&lt;/em>;个最小点和&lt;em>;N&lt;/em>;个最大点，然后计算一个区间剩余点的点。后者必须是近似中位数。当数据集大小至少为 &lt;em>;3N&lt;/em>; 时，此方法有效。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf26PttXtoA75NoqKmNacVcsoHkCQFld1jOhqiOuml8CHIMbBWbcWlHXn0r6hC-697XA8ey-GvVDshwQsvaQRYb8 DwD6wRMnR0Nuo2rxMqszE8OW06pZ184dCz4s9NdKBfHF0mPhqjoA2l7lepeZ88Xru1Q3Zrxvm85W5onykpmwLeU5Gm3v2fYNYfQEWN/s1999/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;545&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf26PttXtoA75NoqKmNacVcsoHkCQFld1jOhqiOuml8CHIMbBWbcWlHXn0r6hC-697XA8ey-GvVDshwQsvaQRYb8DwD6wRMnR0Nuo2rxMqszE8OW06 pZ184dCz4s9NdKBfHF0mPhqjoA2l7lepeZ88Xru1Q3Zrxvm85W5onykpmwLeU5Gm3v2fYNYfQEWN/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;域 X 上的数据 D、间隔点集和近似中位数集的示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;轴对齐矩形的私人学习&lt;/h2>; &lt;p>; 对于下一个任务，输入是一组&lt;em >;n&lt;/em>; 个标记数据点，其中每个点 &lt;em>;x = (x&lt;sub>;1&lt;/sub>;,....,x&lt;sub>;d&lt;/sub>;)&lt;/em>; 是一个&lt;em>;X&lt;/em>; 域上的 &lt;em>;d&lt;/em>; 维向量。如下所示，目标是学习轴 &lt;em>;i=1,...,d 的值 &lt;em>;a&lt;sub>;i &lt;/sub>;、b&lt;sub>;i&lt;/sub>;&lt;/em>; &lt;/em>; 定义一个 &lt;em>;d&lt;/em>; 维矩形，因此对于每个示例 &lt;em>;x&lt;/em>; &lt;/p>; &lt;ul>; &lt;li>;If &lt;em>;x&lt;/em>; >; 被正向标记（如下图红色加号所示），则它位于矩形内，即对于所有轴 &lt;em>;i&lt;/em>;，&lt;em>;x&lt;sub>;i&lt;/sub>;&lt;/em>;位于区间 &lt;em>;[a&lt;sub>;i &lt;/sub>;,b&lt;sub>;i&lt;/sub>;]&lt;/em>; 内，并且 &lt;/li>;&lt;li>;如果 &lt;em>;x&lt;/em>;带有负标签（如下图蓝色减号所示），则它位于矩形之外，即，对于至少一个轴 &lt;em>;i&lt;/em>;，&lt;em>;x&lt;sub>;i&lt;/sub>;&lt;/em >; 位于区间 &lt;em>;[a&lt;sub>;i &lt;/sub>;,b&lt;sub>;i&lt;/sub>;]&lt;/em>; 之外。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNdbuXJ043M9Z94-l2x2Df--GPDu3CCvfru3EBHSig4GxylqwV-uiEf84bK0oLmGIaC5h0zSyimqMQR3B JPKqbm3fmWTrOVoKA3MJusfKfEHVM0UlYpki5rWfx0z2niP0pckX3FX_0cyV- nTeP5V9J5_PPRLMe13gQM1FZAXlLQ_WBM4nySEXpAO1rQBHS/s1656/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1160&quot; data-original-width=&quot;1656&quot;高度=“280”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNdbuXJ043M9Z94-l2x2Df--GPDu3CCvfru3EBHSig4GxylqwV-uiEf84bK0oLmGIaC5h0zSyimqMQR3BJPKqbm3fmWTrOVoKA3 MJusfKfEHVM0UlYpki5rWfx0z2niP0pckX3FX_0cyV-nTeP5V9J5_PPRLMe13gQM1FZAXlLQ_WBM4nySEXpAO1rQBHS/w400-h280/image4.png&quot; width=&quot;400&quot; />; &lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;一组二维标记点和相应的矩形。&lt;/a>;&lt;/td>;&lt;/tr>; td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 此问题的任何 DP 解决方案都必须是近似的，因为必须允许学习的矩形错误标记某些数据点，其中一些正标记点位于矩形之外或负标记点里面有标记的点。这是因为精确的解决方案可能对特定数据点的存在非常敏感，并且不会是私有的。我们的目标是一种 DP 解决方案，使错误标记点的必要数量保持在较小的范围内。 &lt;/p>; &lt;p>; 我们首先考虑一维情况 (&lt;em>;d = 1)&lt;/em>;。我们正在寻找一个覆盖所有正点且不覆盖任何负点的区间&lt;em>;[a,b]&lt;/em>;。我们证明，我们最多可以使用 &lt;em>;2N&lt;/em>; 个错误标记点来做到这一点。我们专注于积极标记的点。在第一个 RSC 步骤中，我们切出 &lt;em>;N&lt;/em>; 个最小点，并将私有间隔点计算为 &lt;em>;a&lt;/em>;。然后，我们切出 &lt;em>;N&lt;/em>; 个最大的点，并计算一个私有区间点为 &lt;em>;b&lt;/em>;。该解决方案&lt;em>;[a,b]&lt;/em>;正确标记了所有负标记点，最多错误标记了 &lt;em>;2N&lt;/em>; 正标记点。因此，总共最多 ~&lt;em>;2N&lt;/em>; 个点被错误标记。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0lIq2EkGGyGn15GSI5mEEAya4zPNNoAjoCBOkGj4Qljsk7M02T8s55E3FI9r_0Vfgx14N7PRpXFQN07bdqB gslKAxkkIbjhuV5LNdoMySNWRvvIwzWB9XyTn8ASLvvVQI0f1BWuFtOEbiG7ydKIbvkP3o3R1TgP9lDuXCFFT4wr8zEst5PNvbnlLlCg4z/s1999/image3.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;441&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEj0lIq2EkGGyGn15GSI5mEEAya4zPNNoAjoCBOkGj4Qljsk7M02T8s55E3FI9r_0Vfgx14N7PRpXFQN07bdqBgslKAxkkIbjhuV5LNdoMySNWRvvIwzWB9XyTn8A SLvvVQI0f1BWuFtOEbiG7ydKIbvkP3o3R1TgP9lDuXCFFT4wr8zEst5PNvbnlLlCg4z/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;以&lt;em>;d = 1&lt;/em>;为例，我们切出&lt;em>;N&lt;/em>;个左侧正点并计算间隔点&lt;em>;a&lt;/em>;，切出&lt;em>;N&lt;/em>; em>;右正点并计算区间点&lt;em>;b&lt;/em>;。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;其中&lt;em>;d >; 1&lt;/em>;，我们迭代轴&lt;em>; i = 1,....,d&lt;/em>; 并将上述内容应用于第 &lt;em>;i&lt;sup>;th &lt;/sup>;&lt;/em>;输入点的坐标以获得值&lt;em>;a&lt;sub>;i &lt;/sub>;，b&lt;sub>;i&lt;/sub>; &lt;/em>;。在每次迭代中，我们执行两个 RSC 步骤并切出 &lt;em>;2N&lt;/em>; 个正标记点。总的来说，我们切出了 &lt;em>;2dN&lt;/em>; 个点，并且所有剩余的点都被正确标记。也就是说，所有负标记点都位于最终 &lt;em>;d&lt;/em>; 维矩形之外，而所有正标记点（除了 ~&lt;em>;2dN&lt;/em>;）都位于矩形内部。请注意，该算法充分利用了 RSC 的灵活性，因为每个轴对点的排序不同。由于我们执行了 &lt;em>;d&lt;/em>; 个步骤，RSC 分析从错误标记点的数量中削减了 &lt;em>;d&lt;/em>; 的平方根因子。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;通过自适应选择训练示例来训练 ML 模型&lt;/h2>; &lt;p>; 训练效率或性能有时可以通过根据模型当前状态选择训练示例来改进 ML 模型的性能，例如自定进度 &lt;a href=&quot;https://www.frontiersin.org/research-topics/36658/课程学习和相关应用程序&quot;>;课程学习&lt;/a>;或&lt;a href=&quot;https://en.wikipedia.org/wiki/Active_learning_(machine_learning)&quot;>;主动学习&lt;/a>;。 &lt;/p>; &lt;p>; ML 模型的私人训练最常见的方法是 &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>;，其中将噪声添加到每个小批量训练示例的梯度更新。使用 DP-SGD 进行隐私分析通常假设训练示例被&lt;em>;随机&lt;/em>;划分为小批量。但是，如果我们对训练样本施加依赖于数据的选择顺序，并在训练期间进一步修改选择标准&lt;em>;k&lt;/em>;次，那么通过DP组合进行分析会导致隐私保证的恶化，其幅度等于&lt;em>;k&lt;/em>; 的平方根。 &lt;/p>; &lt;p>; 幸运的是，DP-SGD 的示例选择可以在 RSC 范式中自然地表达：每个选择标准对训练示例进行重新排序，每个小批量是一个切片（我们为其计算噪声梯度）。通过 RSC 分析，&lt;em>;k&lt;/em>; 不会造成隐私恶化，这将带有示例选择的 DP-SGD 训练带入了实际领域。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 引入 RSC 范式是为了解决一个开放问题：主要具有理论意义，但事实证明它是一种多功能工具，有可能提高生产环境中的数据效率。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;&lt;a href=&quot;https:// arxiv.org/abs/2211.06387&quot;>;此处描述的工作&lt;/a>;是与 &lt;a href=&quot;https://people.eecs.berkeley.edu/~xinlyu/&quot;>;Xin Lyu&lt;/a>;, Jelani 共同完成的尼尔森和塔马斯·萨洛斯。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/237108111388574068/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/ Differentially-private-median-and-more.html#comment-form&quot; rel= “回复” title =“0条评论”type =“text/html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/237108111388574068”rel =“编辑”type = “application/atom+xml”/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/237108111388574068&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href=&quot;http://blog.research.google/2023/09/ Differentially-private-median-and-more.html&quot; rel=&quot;alternate&quot; title=&quot;差异私有中位数及更多&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiY4hQkG4Ofm6plcTF56kw-L1RI613kh8ztTMZvJgmn2VpFF84K1TIfBymU6p_xVE0q-SRafCvZtrz dCgNz4qqsyqVTevfEffmxqhQ_Jg6rJQgo5vm3zoOkQ6JnxeMNQ4Y7LrOufGJB8lTnJoKSYZYke2qZi0rvO9PUB5POaYdou4O7lE2-IO7hD0__aJcy/s72 -c/dpmedian.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-5967099570177693866&lt;/id>;&lt;已发布>;2023-09-07T15:03:00.000-07:00&lt;/已发布>;&lt;已更新>;2023-09-07T15:03:10.510-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category方案=“http://www.blogger.com/atom/ns#”术语=“机器学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=&quot;TPU&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;用于湍流研究的新型计算流体动力学框架&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;已发布作者：Athena 团队软件工程师 Shantanu Shahane 和研究科学家 Matthias Ihme&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bAp bGhaYWLST4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E /s990/image1.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 湍流在环境和工程流体流动中普遍存在，并且在日常生活中经常遇到。更好地了解这些湍流过程可以为各个研究领域提供有价值的见解——改善对大气输送引起的云形成和湍流能量交换引起的野火蔓延的预测，了解河流中沉积物的沉积，以及提高燃烧效率例如，在飞机发动机中减少排放。然而，尽管它很重要，但我们目前的理解和可靠预测此类流量的能力仍然有限。这主要归因于高度混沌的性质以及这些流体流占据的巨大的空间和时间尺度，范围从高端的几米量级的高能大规模运动，其中能量被注入流体流，在低端一直降至微米 (μm)，湍流通过粘性摩擦耗散成热量。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 理解这些湍流的一个强大工具是&lt;a href=&quot;https://en.wikipedia.org/wiki/Direct_numerical_simulation&quot;>;直接数值模拟（DNS），它提供了非定常三维流场的详细表示，而不进行任何近似或简化。更具体地说，这种方法利用具有足够小的网格间距的离散网格来捕获控制系统动力学的基础连续方程（在这种情况下，可变密度&lt;a href=&quot;https://en.wikipedia.org/ wiki/Navier%E2%80%93Stokes_equations&quot;>;纳维-斯托克斯方程&lt;/a>;，它控制所有流体流动动力学）。当网格间距足够小时，离散网格点足以表示真实（连续）方程，而不会损失精度。虽然这很有吸引力，但此类模拟需要大量的计算资源才能捕获如此广泛的空间尺度上的正确流体流动行为。 &lt;/p>; &lt;p>; 必须应用直接数值计算的空间分辨率的实际跨度取决于任务，并由 &lt;a href=&quot;https://en.wikipedia.org/wiki/Reynolds_number&quot;>; 确定雷诺数&lt;/a>;，将惯性力与粘性力进行比较。通常，雷诺数的范围为 10&lt;sup>;2&lt;/sup>; 到 10&lt;sup>;7 &lt;/sup>;（对于大气或星际问题甚至更大）。在 3D 中，所需分辨率的网格大小大致与雷诺数的 4.5 次方成正比！由于这种强烈的缩放依赖性，模拟此类流动通常仅限于具有中等雷诺数的流动状态，并且通常需要访问 &lt;a href=&quot;https://www.top500.org/lists/top500/2023/06/&quot; >;具有数百万个 CPU/GPU 核心的高性能计算系统&lt;/a>;。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0010465522000108?via%3Dihub&quot;>;用于科学计算流体流动的 TensorFlow 模拟框架张量处理单元&lt;/a>;”，我们引入了一种新的模拟框架，可以使用 TPU 计算流体流动。通过利用 &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>; 软件和 TPU 硬件架构的最新进展，该软件工具可以以前所未有的规模对湍流进行详细的大规模模拟，突破科学发现和湍流分析的界限。我们证明了该框架可以有效地扩展以适应问题的规模，或者改进运行时间，这是值得注意的，因为大多数大规模分布式计算框架都会因扩展而降低效率。该软件作为 &lt;a href=&quot;https://github.com/google-research/swirl-lm&quot;>;GitHub&lt;/a>; 上的开源项目提供。 &lt;/p>; &lt;br />; &lt;h2>;使用加速器进行大规模科学计算&lt;/h2>; &lt;p>; 该软件使用 TensorFlow 框架在 TPU 架构上求解变密度纳维-斯托克斯方程。采用&lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;>;单指令、多数据&lt;/a>; (SIMD) 方法来并行化 TPU 求解器实现。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite_difference_method&quot;>;有限差分&lt;/a>;运算符利用 TPU 的矩阵乘法单元 (MXU)，将共置&lt;/a>;结构化网格用作 TensorFlow 卷积函数的过滤器。该框架利用了 TPU 加速器之间的低延迟高带宽芯片间互连 (ICI)。此外，通过加速线性代数 (XLA) 编译器利用单精度浮点计算和高度优化的可执行文件，可以在 TPU 硬件架构上执行具有出色扩展性的大规模模拟。 &lt;/p>; &lt;p>; 这项研究工作表明，基于&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;>;图&lt;/a>;的 TensorFlow 与新型 ML 特殊用途相结合硬件，可以用作编程范例来求解表示多物理场流的偏微分方程。后者是通过用物理模型增强纳维-斯托克斯方程来实现的，以考虑化学反应、传热和密度变化，从而能够模拟&lt;a href=&quot;https://arxiv.org/abs/ 2301.04698&quot;>;云层形成&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;野火&lt;/a>;。 &lt;/p>; &lt;p>; 值得注意的是，该框架是第一个开源&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_fluid_dynamics&quot;>;计算流体动力学&lt;/a>; (CFD) 框架用于高性能、大规模模拟，以充分利用近年来随着机器学习 (ML) 的进步而变得常见（并成为商品）的云加速器。虽然我们的工作重点是使用 TPU 加速器，但可以轻松地针对其他加速器（例如 GPU 集群）调整代码。 &lt;/p>; &lt;p>; 该框架展示了一种大大降低与运行大规模科学 CFD 模拟相关的成本和周转时间的方法，并在气候和天气研究等领域实现更高的迭代速度。由于该框架是使用 TensorFlow（一种 ML 语言）实现的，因此它还可以与 ML 方法轻松集成，并允许探索 CFD 问题的 ML 方法。由于 TPU 和 GPU 硬件的普遍可访问性，这种方法降低了研究人员为我们理解大规模湍流系统做出贡献的障碍。 &lt;/p>; &lt;br />; &lt;h2>;框架验证和均匀各向同性湍流&lt;/h2>; &lt;p>; 除了展示性能和扩展能力之外，验证该框架的正确性也很重要，以确保当它是用于 CFD 问题，我们得到了合理的结果。为此，研究人员通常在 CFD 求解器开发过程中使用理想化基准问题，我们在工作中采用了其中许多问题（更多详细信息，请参见 &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii /S0010465522000108？via%3Dihub&quot;>;论文&lt;/a>;）。 &lt;/p>; &lt;p>; 湍流分析的基准之一是&lt;a href=&quot;https://en.wikipedia.org/wiki/Homogeneous_isotropic_turbulence&quot;>;均匀各向同性湍流&lt;/a>;&lt;em>; &lt;/em>;(HIT ），这是一个经过充分研究的规范流，其中动能等统计特性在坐标轴的平移和旋转下保持不变。通过将分辨率提升到当前技术水平的极限，我们能够执行超过 80 亿度自由度的直接数值模拟，相当于沿三个方向各有 2,048 个网格点的三维网格。我们使用 512 个 TPU-v4 核心，将沿 x、y 和 z 轴的网格点的计算分别分布到 [2,2,128] 个核心上，针对 TPU 上的性能进行了优化。每个时间步长的挂钟时间约为 425 毫秒，总共模拟了 400,000 个时间步长的流程。 50 TB 数据（包括速度和密度字段）存储 400 个时间步长（每 1,000 个步长）。据我们所知，这是迄今为止进行的同类最大的湍流模拟之一。 &lt;/p>; &lt;p>; 由于湍流流场的复杂性、混沌性，其分辨率跨越多个数量级，因此有必要以高分辨率模拟系统。因为我们采用了具有 80 亿个点的高分辨率网格，所以我们能够准确地解析场。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLST4r1oj5zaGDi3Jy gwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s990/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;894&quot; data-original-width=&quot;990&quot; height=&quot;361&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ 5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/w400-h361/image1.png“宽度=”400“/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;速度沿 &lt;em>;z&lt; 方向的 &lt;em>;x&lt;/em>; 分量的轮廓/em>; 中板。模拟的高分辨率对于准确表示湍流场至关重要。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;湍流动能和耗散率是常用于分析湍流场的两个统计量。湍流。这些属性在没有额外能量注入的湍流场中的时间衰减是由于粘性耗散造成的，并且衰减渐近线遵循预期的分析&lt;a href=&quot;https://en.wikipedia.org/wiki/Energy_cascade&quot;>;幂律&lt; /a>;.这与文献中报告的理论渐近线和观察结果一致，从而验证了我们的框架。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7 fkt8YvPwlGZcapjmEL3eQgZo1o1CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s562/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;562&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7fkt8YvPwlGZcapjmEL3eQgZo1o1 CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;实线：湍流动能 (&lt;em>;k&lt;/em>;) 的时间演化。虚线：衰减均匀各向同性湍流的解析幂律 (&lt;em>;n&lt;/em>;=1.3) (&lt;em>;Ⲧ&lt;sub>;l&lt;/sub>;&lt;/em>;: &lt;a href=&quot;https://物理.stackexchange.com/questions/79184/what-are-the-length-and-time-scales-in-turbulence&quot;>;涡流周转时间&lt;/a>;）。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4Ds Muf2OR-sIgJeAGmleM2lY2kAijlsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s565/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;565&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4DsMuf2OR-sIgJeAGmleM2lY2kAij lsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;实线：耗散率 (&lt;em>;ε&lt;/em>;) 的时间演化。虚线：衰减均匀各向同性湍流的解析幂律 (n=1.3)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 湍流的能谱表示 &lt;a 上的能量含量href=&quot;https://en.wikipedia.org/wiki/Wavenumber&quot;>;波数&lt;/a>;，其中波数&lt;i>;k&lt;/i>;与波长&lt;i>;λ&lt;/i>;成正比（即，&lt;i>;k&lt;/i>; ∝ 1/&lt;i>;λ&lt;/i>;）。一般来说，光谱可以定性地分为三个范围：源范围、惯性范围和粘性耗散范围（波数轴上从左到右，如下）。源范围内的最低波数对应于最大的湍流涡流，其能量含量最高。这些大涡流将能量传递给中间波数（惯性范围）内的湍流，该湍流在统计上是各向同性的（即在所有方向上基本均匀）。最小的涡流对应于最大的波数，通过流体的粘度耗散成热能。凭借在三个空间方向上每个方向都有 2,048 个点的精细网格，我们能够将流场解析到发生粘性耗散的长度尺度。这种直接数值模拟方法是最准确的，因为它不需要任何闭合模型来近似网格尺寸以下的能量级联。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW 8Epx5sVTF0DnW-5NYdIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N -k6DcBbh912IOG/s569/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;569 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW8Epx5sVTF0DnW-5NY dIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N-k6DcBbh912IOG/s16000/image2.png&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;不同时间实例的湍流动能谱。谱图通过瞬时积分长度 (&lt;em>;l&lt;/em>;) 和湍流动能 (&lt;em>;k&lt;/em>;) 进行归一化。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;湍流研究的新时代&lt;/h2>; &lt;p>; 最近，我们扩展了这个框架来预测&lt; a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;野火&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2301.04698&quot;>;大气流动&lt;/a>;，这是与气候风险评估相关。除了实现复杂湍流的高保真模拟之外，该模拟框架还提供&lt;a href=&quot;https://www.osti.gov/servlets/purl/1478744&quot;>;科学机器学习&lt;/a>; (SciML ） - 例如，从精细网格向下采样到粗网格（&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_order_reduction&quot;>;模型缩减&lt;/a>;）或构建以较低分辨率运行的模型仍然捕获正确的动态行为。它还可以为进一步的科学发现提供途径，例如构建基于机器学习的模型，以更好地参数化湍流的微观物理，包括温度、压力、蒸汽分数等之间的物理关系，并且可以改进各种控制任务，例如减少建筑物的能源消耗或找到更高效的螺旋桨形状。尽管很有吸引力，但 SciML 的一个主要瓶颈是训练数据的可用性。为了探索这一点，我们一直与斯坦福大学和 Kaggle 的团队合作，通过社区托管的网络平台 &lt;a href=&quot;https://blastnet.github.io&quot; 提供来自高分辨率 HIT 模拟的数据。 >;BLASTNet&lt;/a>;，通过数据集网络方法向研究界提供对高保真数据的广泛访问。我们希望这些新兴的高保真模拟工具与社区驱动的数据集的结合将导致流体力学各个领域的重大进步。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢 Qing Wang、Yi-Fan Chen 和 John Anderson 提供的咨询和建议，感谢 Tyler Russell 和 Carla Bromberg 提供的项目管理。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5967099570177693866/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html&quot; rel=&quot;alternate&quot; title=&quot;一种用于湍流研究的新型计算流体动力学框架&quot; type=&quot;text/html &quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLST4r1oj 5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/ s72-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8763480087701216145&lt;/id>;&lt;已发布>;2023-09-06T12:47:00.001-07:00&lt;/已发布>;&lt;更新>;2023-09-12T16:01:52.707-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>; &lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;TSMixer：用于时间序列预测的全 MLP 架构&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：云人工智能团队学生研究员陈思安和云人工智能团队研究科学家李春亮&lt;/ span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4 BgCqd24e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s320/hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;span style=&quot;font-size:small;&quot;>;&lt;i>;&lt;b>;更新 — 2023/09/12：&lt;/b>;本文已更新，因为所描述的工作现已发布于 &lt; a href=&quot;https://www.jmlr.org/tmlr/&quot;>;&lt;em>;机器学习研究交易&lt;/em>;&lt;/a>;。&lt;/i>;&lt;/span>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;时间序列&lt;/a>;预测对于各种现实世界的应用程序至关重要，来自&lt;a href=&quot;https://www.kaggle。 com/competitions/m5-forecasting-accuracy&quot;>;需求预测&lt;/a>;到&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/29606177/&quot;>;大流行病传播预测&lt;/a>;。在&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;多变量时间序列预测&lt;/a>;（同时预测多个变量）中，可以将现有方法分为两类：单变量模型和多变量模型楷模。单变量模型侧重于序列间相互作用或时间模式，其中包含具有单个变量的时间序列上的趋势和季节性模式。这种趋势和季节性模式的例子可能是抵押贷款利率因通货膨胀而增加的方式，以及高峰时段交通高峰的方式。除了系列间模式之外，多元模型还处理系列内特征，称为跨变量信息，当一个系列是另一个系列的高级指标时，这特别有用。例如，体重增加可能导致血压升高，产品价格上涨可能导致销量下降。多变量模型&lt;a href=&quot;https://research.google/pubs/pub49697/&quot;>;最近成为多变量预测的流行解决方案&lt;/a>;，因为从业者相信他们处理跨变量信息的能力可能会带来更好的性能。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 近年来，基于深度学习 Transformer 的架构因其在序列任务上的卓越性能而成为多元预测模型的热门选择。然而，高级多变量模型&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;在常用的&lt;a href=&quot;https://github.com/2205.13504&quot;>;上比简单的单变量线性模型表现出奇差&lt;/a>;。 com/thuml/Autoformer&quot;>;长期预测基准&lt;/a>;，例如&lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;电力变压器温度&lt;/a>; (ETT)、&lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+conspiration&quot;>;电力&lt;/a>;，&lt;a href=&quot;https://pems.dot.ca .gov/&quot;>;交通&lt;/a>;和&lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;天气&lt;/a>;。这些结果提出了两个问题：&lt;/p>; &lt;ul>; &lt;li>;跨变量信息是否有利于时间序列预测？ &lt;/li>;&lt;li>;当跨变量信息没有好处时，多元模型仍然可以像单变量模型一样表现吗？ &lt;/li>; &lt;/ul>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2303.06053&quot;>;TSMixer：用于时间序列预测的全 MLP 架构&lt;/a>;”中，发表于在&lt;em>;&lt;a href=&quot;https://www.jmlr.org/tmlr/&quot;>;机器学习研究交易&lt;/a>;&lt;/em>;（TMLR）中，我们分析了单变量线性模型的优势并揭示了他们的有效性。根据此分析的见解，我们开发了时间序列混合器 (TSMixer)，这是一种先进的多元模型，它利用线性模型特征并在长期预测基准上表现良好。据我们所知，TSMixer 是第一个在长期预测基准上与最先进的单变量模型表现一样好的多变量模型，我们表明跨变量信息的益处较小。为了证明跨变量信息的重要性，我们评估了一个更具挑战性的现实应用程序，&lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy&quot;>;M5&lt;/a>;。最后，实证结果表明 TSMixer 优于最先进的模型，例如 &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;、&lt;a href=&quot;https: //arxiv.org/abs/2201.12740&quot;>;Fedformer&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;、&lt;a href=&quot;https:// arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TSMixer 架构&lt;/h2>; &lt;p>; 线性模型和 Transformer 之间的主要区别在于它们捕获的方式时间模式。一方面，线性模型应用固定且与时间步长相关的权重来捕获静态时间模式，并且无法处理跨变量信息。另一方面，Transformers 使用&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;注意力机制&lt;/a>;，在每个时间步应用动态和数据相关的权重，捕获动态时间模式并启用他们来处理跨变量信息。 &lt;/p>; &lt;p>; 在我们的分析中，我们表明，在时间模式的常见假设下，线性模型具有简单的解决方案来完美地恢复时间序列或对误差设置界限，这意味着它们是学习静态时间模式的绝佳解决方案更有效地分析单变量时间序列。相比之下，为注意力机制找到类似的解决方案并非易事，因为应用于每个时间步的权重是动态的。因此，我们通过用线性层替换 Transformer 注意力层来开发一种新的架构。由此产生的 TSMixer 模型类似于计算机视觉 &lt;a href=&quot;https://arxiv.org/abs/2105.01601&quot;>;MLP-Mixer&lt;/a>; 方法，在不同的多层感知器应用之间交替。方向，我们分别称之为&lt;em>;时间混合&lt;/em>;和&lt;em>;特征混合&lt;/em>;。 TSMixer 架构有效地捕获时间模式和跨变量信息，如下图所示。残差设计确保 TSMixer 保留时间线性模型的能力，同时仍然能够利用跨变量信息。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY 3lV1paiioZ2nw4n3mY4JJx8Lo74kiLsL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s1999/ image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY3lV1paiioZ2nw4n3mY4JJx8Lo74kiL sL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption &quot; style=&quot;text-align: center;&quot;>;Transformer 块和 TSMixer 块架构。 TSMixer 用时间混合代替了多头注意力层，时间混合是一种应用于时间维度的线性模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQizn bfrqB0cRopGe4GCKupIQZnbNt8XutFC5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/s1646/image4.png&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1646&quot; height=&quot;195&quot; src=&quot;https://blogger.googleusercontent.com/img/b /R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQiznbfrqB0cRopGe4GCKupIQZnbNt8XutFC 5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/w400-h195/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;数据相关（注意力机制）和时间步相关（线性模型）之间的比较。这是通过学习前三个时间步的权重来预测下一个时间步的示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;长期预测基准评估&lt;/h2>; &lt;p>; 我们使用七个流行的&lt;a href=&quot;https://github.com/thuml /Autoformer&quot;>;长期预测数据集&lt;/a>; (&lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTm1&lt;/a>;, &lt;a href=&quot;https://github.com /zhouhaoyi/ETDataset&quot;>;ETTm2&lt;/a>;、&lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTh1&lt;/a>;、&lt;a href=&quot;https://github.com/zhouhaoyi /ETDataset&quot;>;ETTh2&lt;/a>;，&lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+conspiration&quot;>;电力&lt;/a>;，&lt;a href=&quot;https://pems.dot.ca.gov/&quot;>;交通&lt;/a>;和&lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;天气&lt;/a>; a>;），其中&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;最近的研究&lt;/a>;表明，单变量线性模型的性能优于具有较大裕度的高级多元模型。我们将 TSMixer 与最先进的多变量模型进行比较（&lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;、&lt;a href=&quot;https://arxiv.org /abs/2201.12740&quot;>;FEDformer&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs /2012.07436&quot;>;Informer&lt;/a>;）和单变量模型，包括&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;线性模型&lt;/a>;和&lt;a href=&quot;https:// arxiv.org/abs/2211.14730&quot;>;补丁TST&lt;/a>;。下图显示了 TSMixer 与其他方法相比的&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;均方误差&lt;/a>; (MSE) 的平均改进。平均值是跨数据集和多个预测范围计算的。我们证明 TSMixer 的性能显着优于其他多变量模型，并且与最先进的单变量模型相当。这些结果表明，多变量模型的性能与单变量模型一样好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec 3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQKHbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s1200/image5.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQK HbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;与其他基线相比，TSMixer 的平均 MSE 改进。红色条显示多变量方法，蓝色条显示单变量方法。 TSMixer 比其他多变量模型取得了显着改进，并取得了与单变量模型相当的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;消融研究&lt;/h2>; &lt;p>; 我们进行了消融研究，以将 TSMixer 与 TMix-Only 进行比较，TMix-Only 是仅由时间混合层组成的 TSMixer 变体。结果表明，TMix-Only 的性能几乎与 TSMixer 相同，这意味着额外的特征混合层不会提高性能，并证实跨变量信息在流行的基准测试中不太有利。结果验证了&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;之前的研究&lt;/a>;中显示的卓越的单变量模型性能。然而，现有的长期预测基准不能很好地代表一些现实世界应用中对跨变量信息的需求，在这些应用中，时间序列可能是间歇性或稀疏的，因此时间模式可能不足以进行预测。因此，仅根据这些基准来评估多元预测模型可能是不合适的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;M5评估：跨变量信息的有效性&lt;/h2>; &lt;p>;进一步论证为了利用多变量模型的优势，我们在具有挑战性的 M5 基准上评估 TSMixer，这是一个包含关键跨变量交互的大规模零售数据集。 M5包含5年来收集的30,490个产品的信息。每个产品描述都包含时间序列数据，例如每日销售额、售价、促销活动信息和静态（非时间序列）特征，例如商店位置和产品类别。目标是预测未来 28 天每种产品的每日销售额，使用&lt;a href=&quot;https://mofc.unic.ac.cy/m5-competition/&quot;>;加权均方根缩放误差&lt; /a>; (WRMSSE) 来自 M5 竞赛。零售业的复杂性使得仅使用专注于时间模式的单变量模型进行预测变得更具挑战性，因此具有跨变量信息甚至辅助特征的多变量模型更加重要。 &lt;/p>; &lt;p>; 首先，我们将 TSMixer 与其他方法进行比较，仅考虑历史数据，例如每日销售额和历史销售价格。结果表明，多变量模型显着优于单变量模型，表明跨变量信息的有用性。在所有比较的方法中，TSMixer 有效地利用了跨变量信息并实现了最佳性能。 &lt;/p>; &lt;p>; 此外，为了利用 M5 中提供的更多信息，例如静态特征（例如，商店位置、产品类别）和未来时间序列（例如，未来几天安排的促销活动），我们提出了一个原则设计扩展 TSMixer。扩展的TSMixer将不同类型的特征对齐到相同的长度，然后将多个混合层应用于连接的特征以进行预测。扩展的 TSMixer 架构优于工业应用中流行的模型，包括 &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs /1912.09363&quot;>;TFT&lt;/a>;，展示了其对现实世界影响的强大潜力。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQ gaVscfd-K5hQniB7N3iM6tc22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/s1950/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1950&quot; data-original-width=&quot;1760&quot; height=&quot;640&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQgaVscfd-K5hQniB7N3iM6t c22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/w578-h640/image2.png&quot; width=&quot;578&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;扩展 TSMixer 的架构。在第一阶段（对齐阶段），它将不同类型的特征对齐到相同的长度，然后再将它们连接起来。在第二阶段（混合阶段），它应用以静态特征为条件的多个混合层。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKPJQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbK ZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_hA46iTaDBpogHn7iVJ615O/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKP JQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbKZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_ha46iTaDBpogHn7iVJ615O/ s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;M5 上的 WRMSSE。前三种方法（&lt;strong>;蓝色&lt;/strong>;）是单变量模型。中间三种方法（&lt;strong>;橙色&lt;/strong>;）是仅考虑历史特征的多元模型。最后三种方法（&lt;strong>;红色&lt;/strong>;）是考虑历史、未来和静态特征的多元模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了 TSMixer，这是一种先进的多元模型，它利用线性模型特征，并且性能与状态一样好。长期预测基准的最先进的单变量模型。 TSMixer 通过深入了解跨变量和辅助信息在现实场景中的重要性，为时间序列预测架构的开发创造了新的可能性。实证结果强调，在未来的研究中需要考虑更现实的多元预测模型基准。我们希望这项工作能够激发时间序列预测领域的进一步探索，并导致开发更强大、更有效的模型，以应用于实际应用。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Si-An Chen 进行，李春亮、Nate Yoder、Sercan O. Arik 和 Tomas Pfister。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google /feeds/8763480087701216145/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/ tsmixer-all-mlp-architecture-for-time.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger .com/feeds/8474926331452026626/posts/default/8763480087701216145&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/默认/8763480087701216145&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time .html&quot; rel=&quot;alternate&quot; title=&quot;TSMixer：用于时间序列预测的全 MLP 架构&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http:// /www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/ 2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4BgCqd2 4e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http:// /search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626 .post-6223088894812086013&lt;/id>;&lt;发布>;2023-08-31T10:14:00.000-07:00&lt;/发布>;&lt;更新>;2023-08-31T10:14:27.378-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“数据集”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= &quot;ML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;WeatherBench 2：下一代数据驱动天气模型的基准&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-作者&quot;>;发布者：Google 研究部研究科学家 Stephan Rasp 和项目负责人 Carla Bromberg&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFd jdsi2hvZyKcB0ODholetAjBMQ43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s800 /weatherbenchgif.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 1950 年，当研究人员使用第一台可编程通用计算机时，天气预报开始了数字革命&lt;a href=&quot;https://en.wikipedia.org/wiki/ENIAC#:~:text=ENIAC %20(%2F%CB%88%C9%9Bni,of%20them%20in%20one%20computer.&quot;>;ENIAC&lt;/a>; 用于求解描述天气如何演变的数学方程。在此后的 70 多年里，不断进步计算能力的进步和模型公式的改进导致了天气预报技能的稳步提高：今天的 7 天预报大约与 2000 年的 5 天预报和 1980 年的 3 天预报一样准确。同时提高预报准确性以大约每十年一天的速度似乎没什么大不了的，每天的改进对于影响深远的用例都很重要，例如物流规划、灾害管理、农业和能源生产。这个&lt;a href=&quot;https: //www.nature.com/articles/nature14956&quot;>;“安静”革命&lt;/a>;对社会具有巨大价值，拯救了生命并为许多部门提供了经济价值。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 现在，我们看到天气预报领域的另一场革命正在开始，这一次是由机器学习 (ML) 的进步推动的。我们的想法不是对物理方程的近似值进行硬编码，而是让算法通过查看大量过去的天气数据来了解天气是如何演变的。这样做的早期尝试可以追溯到 &lt;a href=&quot;https://gmd.copernicus.org/articles/11/3999/2018/&quot;>;2018&lt;/a>;，但在过去两年中，步伐大大加快几个大型机器学习模型展示了与最好的基于物理的模型相媲美的天气预报能力。 Google 的 MetNet [&lt;a href=&quot;https://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;1&lt;/a>;、&lt;a href=&quot;例如，https://arxiv.org/abs/2306.06079&quot;>;2&lt;/a>;] 展示了预测未来一天区域天气的最先进功能。对于全局预测，Google DeepMind 创建了 &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast&lt;/a>;，这是一个图神经网络，可以在 25 公里的水平分辨率下进行 10 天的预测，与许多技能指标中最好的基于物理的模型。 &lt;/p>; &lt;p>; 除了可能提供更准确的预测之外，此类机器学习方法的一个关键优势是，一旦经过训练，它们就可以在廉价的硬件上在几分钟内创建预测。相比之下，传统天气预报需要每天运行数小时的大型超级计算机。显然，机器学习为天气预报界带来了巨大的机遇。这也得到了领先的天气预报中心的认可，例如&lt;a href=&quot;https://www.ecmwf.int/&quot;>;欧洲中期天气预报中心&lt;/a>; (ECMWF) &lt;a href =&quot;https://www.ecmwf.int/en/elibrary/81207-machine-learning-ecmwf-roadmap-next-10-years&quot;>;机器学习路线图&lt;/a>;或&lt;a href=&quot;https:// /www.noaa.gov/&quot;>;美国国家海洋和大气管理局&lt;/a>; (NOAA) &lt;a href=&quot;https://sciencecouncil.noaa.gov/wp-content/uploads/2023/04/2020-AI- Strategy.pdf&quot;>;人工智能战略&lt;/a>;。 &lt;/p>; &lt;p>; 为了确保机器学习模型受到信任并针对正确的目标进行优化，预测评估至关重要。然而，评估天气预报并不简单，因为天气是一个令人难以置信的多方面问题。不同的最终用户对预测的不同属性感兴趣，例如，可再生能源生产商关心风速和太阳辐射，而危机应对团队则关心潜在气旋或即将到来的热浪的轨迹。换句话说，没有单一的指标来确定什么是“好的”天气预报，评估必须反映天气及其下游应用的多方面性质。此外，精确评估设置的差异（例如，使用哪种分辨率和地面实况数据）可能会导致模型比较变得困难。有一种方法以公平且可重复的方式比较新颖的方法和已建立的方法对于衡量该领域的进展至关重要。 &lt;/p>; &lt;p>; 为此，我们宣布推出 &lt;a href=&quot;https://arxiv.org/abs/2308.15560&quot;>;WeatherBench 2&lt;/a>; (WB2)，这是下一代数据的基准驱动的全球天气模型。 WB2 是对 2020 年发布的&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020MS002203&quot;>;原始基准&lt;/a>;的更新，该基准基于初始的较低分辨率的机器学习楷模。 WB2 的目标是通过提供一个可信的、可重复的框架来评估和比较不同的方法，从而加速数据驱动的天气模型的进展。 &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;官方网站&lt;/a>;包含来自多个最先进模型的分数（在撰写本文时，这些是&lt;a href= &quot;https://arxiv.org/abs/2202.07575&quot;>;Keisler (2022)&lt;/a>;，一种早期图神经网络，Google DeepMind 的 &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast &lt;/a>; 和华为的&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06185-3&quot;>;盘古天气&lt;/a>;（基于 Transformer 的 ML 模型）。此外，还包括 ECMWF 高分辨率和集合预报系统的预报，它们代表了一些最好的传统天气预报模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3h PB82GvFv2luh4j_BkavTXp9I0P61xnFiySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s1960/image3.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;372&quot; data-original-width=&quot;1960&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3hPB82GvFv2luh4j_BkavTXp9I0P61xnFi ySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;让评估更容易&lt; /h2>; &lt;p>; WB2 的关键组件是一个&lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;开源评估框架&lt;/a>;，它允许用户评估他们的预测与其他基线相同的方式。高分辨率的天气预报数据可能非常大，甚至使评估成为计算挑战。因此，我们在 &lt;a href=&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>; 上构建了评估代码，它允许用户将计算分割成更小的块并以分布式方式评估它们，例如在 Google Cloud 上使用 &lt;a href=&quot;https://cloud.google.com/dataflow&quot;>;DataFlow&lt;/a>;。该代码附带&lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/evaluation.html&quot;>;快速入门指南&lt;/a>;，可帮助人们快速上手。 &lt;/p>; &lt;p>; 此外，我们&lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/data-guide.html&quot;>;提供&lt;/a>;大部分真实数据和基线数据在 Google Cloud Storage 上以不同分辨率的云优化 &lt;a href=&quot;https://zarr.dev/&quot;>;Zarr&lt;/a>; 格式存储，例如 &lt;a href=&quot;https:// rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.3803&quot;>;ERA5&lt;/a>; 数据集用于训练大多数机器学习模型。这是 Google 更大努力的一部分，旨在为研究提供&lt;a href=&quot;https://github.com/google-research/arco-era5&quot;>;分析就绪、云优化的天气和气候数据集&lt;/a>;社区以及&lt;a href=&quot;https://github.com/google-research/arco-era5#roadmap&quot;>;超越&lt;/a>;。由于从各自的档案中下载这些数据并进行转换可能非常耗时且需要大量计算，因此我们希望这能够大大降低社区的进入门槛。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;评估预测技能&lt;/h2>; &lt;p>; 与我们的合作者一起 &lt;a href=&quot;https ://www.ecmwf.int/&quot;>;ECMWF&lt;/a>;，我们定义了一组最能反映全球天气预报质量的标题分数。如下图所示，一些基于 ML 的预测的误差低于 &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts&quot;>;关于确定性指标的最先进的物理模型&lt;/a>;。这适用于一系列变量和区域，并强调了基于机器学习的方法的竞争力和前景。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9Xtx fFzMCqVOuSFzlg-7pAfYMQYN7YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s1999/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9XtxfFzMCqVOuSFzlg-7pAfYMQYN7 YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;此记分卡显示了与 ECMWF 相比的不同模型的技能&lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and -support/changes-ecmwf-model&quot;>;综合预报系统&lt;/a>; (IFS)，针对多个变量的最佳基于物理的天气预报之一。 IFS 预测是根据 IFS 分析进行评估的。所有其他模型均根据 ERA5 进行评估。 ML 模型的顺序反映了发布日期。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实现可靠的概率预测&lt;/h2>; &lt;p>; 然而，单一预测通常是不够的。由于&lt;a href=&quot;https://en.wikipedia.org/wiki/Butterfly_effect&quot;>;蝴蝶效应&lt;/a>;，天气本质上是混乱的。出于这个原因，运营气象中心现在运行大约 50 个稍微扰动的模型实现（称为集合），以估计各种情景下的预报概率分布。例如，如果人们想知道极端天气发生的可能性，这一点很重要。 &lt;/p>; &lt;p>; 创建可靠的概率预测将是全球机器学习模型的下一个关键挑战之一。区域机器学习模型，例如 Google 的 &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;MetNet&lt;/a>; 已经估算了概率。为了预测下一代全球模型，WB2 已经提供了概率指标和基线，其中&lt;a href=&quot;https://www.ecmwf.int/en/about/media-centre/focus/2017/fact-sheet- ensemble-weather-forecasting&quot;>;ECMWF 的 IFS 集合&lt;/a>;，以加速这一方向的研究。 &lt;/p>; &lt;p>; 如上所述，天气预报有很多方面，虽然标题指标试图捕捉预报技能最重要的方面，但它们还远远不够。一个例子是预测现实主义。目前，面对大气的内在不确定性，许多机器学习预测模型倾向于“两面下注”。换句话说，他们倾向于预测平均误差较低的平滑场，但并不代表真实的、物理一致的大气状态。下面的动画就是一个例子。 Pangu-Weather 和 GraphCast（下）这两个数据驱动模型可以很好地预测大气的大规模演化。然而，与地面实况或物理预测模型 IFS HRES（上）相比，它们的小规模结构也较少。在 WB2 中，我们包含了一系列此类案例研究以及量化此类模糊的光谱指标。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-Zkx F_5hdQslK1PKIgB0HwKeCdYUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MSCBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s1200/image2 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1020&quot; data-original-width=&quot;1200&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-ZkxF_5hdQslK1PKIgB0HwKeCd YUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MscBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;2020 年 1 月 3 日启动的锋线穿过美国大陆的预报。地图显示气压为 850 度 &lt;a href=&quot;https: //sites.research.google/weatherbench/faq/&quot;>;hPa&lt;/a>;（大约相当于 1.5 公里的海拔）和 &lt;a href=&quot;https://sites.research.google/weatherbench/faq/&quot;等值线中压力水平为 500 hPa（约 5.5 公里）的 >;位势&lt;/a>;。 ERA5是相应的ground-truth分析，IFS HRES是ECMWF基于物理的预测模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; WeatherBench 2 将继续随着 ML 模型的开发而发展。 &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;官方网站&lt;/a>;将更新最新的最先进模型。 （要提交模型，请按照&lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/submit.html&quot;>;这些说明&lt;/a>;操作）。我们还邀请社区通过 &lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;WB2 GitHub 页面&lt;/a>;上的问题和拉取请求提供反馈和改进建议。 &lt;/p>; &lt;p>; 精心设计评估并瞄准正确的指标对于确保机器学习天气模型尽快造福社会至关重要。现在的 WeatherBench 2 只是一个起点。我们计划在未来扩展它，以解决基于机器学习的天气预报未来的关键问题。具体来说，我们希望添加站点观测和更好的降水数据集。此外，我们将探索将临近预报和次季节到季节预测纳入基准。 &lt;/p>; &lt;p>; 随着天气预报的不断发展，我们希望 WeatherBench 2 能够为研究人员和最终用户提供帮助。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;WeatherBench 2 是多个不同领域合作的成果Google 的团队和 ECMWF 的外部合作者。我们要感谢 ECMWF 的 Matthew Chantry、Zied Ben Bouallegue 和 Peter Dueben。我们谨向 Google 致谢该项目的核心贡献者：Stephan Rasp、Stephan Hoyer、Peter Battaglia、Alex Merose、Ian Langmore、Tyler Russell、Alvaro Sanchez、Antonio Lobato、Laurence Chiu、Rob Carver、Vivian Yang、Shreya Agrawal 、托马斯·特恩布尔、杰森·希基、卡拉·布隆伯格、贾里德·西斯克、​​卢克·巴林顿、亚伦·贝尔和沙飞。我们还要感谢 Kunal Shah、Rahul Mahrsee、Aniket Rawat 和 Satish Kumar。感谢 John Anderson 对 WeatherBench 2 的赞助。此外，我们还要感谢盘古天气团队的毕凯峰和 Ryan Keisler 将模型添加到 WeatherBench 2 的帮助。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href=&quot;http://blog.research.google/feeds/6223088894812086013/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08 /weatherbench-2-benchmark-for-next.html&quot; rel=&quot;alternate&quot; title=&quot;WeatherBench 2：下一代数据驱动天气模型的基准&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel =&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image >;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFdjdsi2hvZyKcB0ODholetAjBMQ 43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s72-c/weatherbenchgif.gif&quot;宽度=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger .com，1999：blog-8474926331452026626.post-1342410539466537463&lt;/id>;&lt;发布>;2023-08-30T12:34:00.002-07:00&lt;/发布>;&lt;更新>;2023-08-30T12:39:33.664- 07 ：00 &lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“HCI”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com” /atom/ns#&quot; term=&quot;自然语言理解&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;建模并提高实时字幕中的文本稳定性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class= &quot;byline-author&quot;>;发布者：Google 增强现实研究科学家 Vikas Bahirwani 和软件工程师 Susan Xu&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45 -muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U5-8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s23 00/stabilizedcaptions.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 自动语音识别 (ASR) 技术通过远程会议软件、移动应用程序和 &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3379337.3415817&quot; 中的实时字幕使对话变得更加容易>;头戴式​​显示器&lt;/a>;。然而，为了保持实时响应能力，实时字幕系统通常会显示临时预测，这些预测会随着收到新话语而更新。这可能会导致文本不稳定（之前显示的文本被更新时出现“闪烁”，如下面视频左侧的字幕所示），这可能会因分心、疲劳、以及跟不上谈话的困难。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://research.google/pubs/pub52450/&quot;>;建模并提高实时字幕中的文本稳定性&lt;/” a>;”，在 &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/98883&quot;>;ACM CHI 2023&lt;/a>; 上提出，我们通过一些方法将文本稳定性问题形式化关键贡献。首先，我们通过采用基于视觉的闪烁指标来量化文本不稳定性，该指标使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Luminance&quot;>;亮度&lt;/a>;对比度和&lt;a href=&quot;https ://en.wikipedia.org/wiki/Discrete_Fourier_transform&quot;>;离散傅里叶变换&lt;/a>;。其次，我们还引入了一种稳定性算法，通过标记对齐、语义合并和平滑动画来稳定实时字幕的渲染。最后，我们进行了一项用户研究 (N=123)，以了解观众对实时字幕的体验&lt;strong>;。 &lt;/strong>;我们的统计分析表明，我们提出的闪烁指标与观众体验之间存在很强的相关性。此外，它表明我们提出的稳定技术显着改善了观看者的体验（例如，上面视频中右侧的字幕）。 &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/-hiAnT6QkmU?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;-hiAnT6QkmU&quot;>;&lt;/iframe>;&lt;/div>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr- Caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;原始 ASR 字幕与稳定字幕&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;公制&lt;/h2>; &lt;p>; 受到 &lt;a href=&quot;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/5203/0000/Toward-perceptual-metrics-for-video-watermark-evaluation/10.1117/12.512550 的启发。 full&quot;>;之前的工作&lt;/a>;，我们提出了一种基于闪烁的指标来量化文本稳定性并客观评估实时字幕系统的性能。具体来说，我们的目标是量化灰度实时字幕视频中的闪烁。我们通过比较构成视频的各个帧（下图中的帧）之间的亮度差异来实现这一点。亮度的大的视觉变化是显而易见的（例如，在底部的图中添加“明亮”一词），但是细微的变化（例如，从“...这个金色。很好..”更新为“...这个” .金子很好”）对于读者来说可能很难辨别。然而，将亮度变化转换为其构成频率会暴露出明显的和微妙的变化。 &lt;/p>; &lt;p>; 因此，对于每对连续帧，我们使用离散傅里叶变换将亮度差异转换为其构成频率。然后，我们对每个低频和高频进行求和，以量化该对中的闪烁。最后，我们对所有帧对进行平均以获得每个视频的闪烁。 &lt;/p>; &lt;p>; 例如，我们可以在下面看到两个相同的帧（顶部）产生 0 闪烁，而两个不同的帧（底部）产生非零闪烁。值得注意的是，较高的度量值表示视频中的高闪烁，因此与较低的度量值相比，用户体验更差。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBf LLlGqieWGTJT4qRV_ZeUkUvQ3aYHkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s1399/noflicker.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;268&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBfLLlGqieWGTJT4qRV_ZeUkUvQ3aY HkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s16000/noflicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;两个相同帧之间的闪烁指标说明。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot; 0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNe HPRxE1XIotPM7DZE- LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11-7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s1399/flicker.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;263&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNeHPRxE1XIotPM7DZE-LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11- 7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s16000/flicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;两个不相同的帧之间的闪烁说明。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;稳定性算法&lt;/h2>; &lt;p>;为了提高实时字幕的稳定性，我们提出了一种算法，该算法将已渲染的标记序列（例如下图中的“上一个”）和新序列作为输入ASR 预测，并输出更新的稳定文本（例如，下面的“更新文本（具有稳定功能）”）。它既考虑了自然语言理解 (NLU) 方面，也考虑了人体工程学方面（显示、布局等）用户在决定何时以及如何生成稳定的更新文本时的体验。具体来说，我们的算法执行标记化对齐、语义合并和平滑动画来实现这一目标。在下文中，标记被定义为由 ASR 生成的单词或标点符号。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45 w4_GiltlNjsU3dz82vn9qhoTJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s1995/image3.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;1995&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45w4_GiltlNjsU3dz82vn9qho TJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;我们显示 (a) 之前已经渲染的文本，(b) 没有我们的合并算法的更新文本的基线布局，以及 (c) 由我们的合并算法生成的更新文本稳定算法。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />;&lt;p>;&lt;br />;&lt;/p>;&lt;p>;&lt;br />;&lt;/p>; &lt;p>; 我们的算法通过首先识别三类更改（下面以红色、绿色和蓝色突出显示）来解决生成稳定的更新文本的挑战： &lt;/p>; &lt;ol type=&quot;A&quot;>; &lt;li>;红色：在末尾添加标记先前呈现的标题（例如，“怎么样”）。 &lt;/li>; &lt;li>;绿色：在已渲染的字幕中间添加/删除标记。 &lt;ul>; &lt;li>;B1：添加标记（例如“我”和“朋友”）。这些可能会也可能不会影响字幕的整体理解，但可能会导致布局变化。在实时字幕中不需要此类布局更改，因为它们会导致显着的抖动和较差的用户体验。这里“我”并不能增加理解力，但“朋友”却可以。因此，平衡更新和稳定性非常重要，特别是对于 B1 类型代币。 &lt;/li>; &lt;li>;B2：删除标记，例如，在更新的句子中删除“in”。&lt;/li>; &lt;/ul>; &lt;/li>; &lt;li>;蓝色：重新标记标记：这包括可能会或可能不会影响字幕的整体理解的记号编辑。&lt;/li>;&lt;ul>; &lt;li>;C1：诸如“disney land”之类的专有名词已更新为“Disneyland”。&lt;/li >; &lt;li>;C2：“it&#39;s”等语法简写更新为“It was”。&lt;/li>; &lt;/ul>; &lt;/ol>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0 “ 类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0wEYXbx4wXxWiW- r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-04qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLh C8Db2aLqq/s1999/image2.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0weEYXbx4wXxWiW-r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-0 4qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLhC8Db2aLqq/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;之前显示和更新的文本之间的变化类别。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;对齐、合并和平滑 &lt;/h2>; &lt;p>; 为了最大限度地提高文本稳定性，我们的目标是使用更新将旧序列与新序列对齐，从而对现有布局进行最小的更改，同时确保准确且有意义的标题。为了实现这一点，我们利用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm#:~:text=The%20Needleman%E2 的变体%80%93Wunsch%20algorithm%20is%20still%20widely%20used%20for%20optimal,alignments%20having%20the%20highest%20score。&quot;>;Needleman-Wunsch 算法&lt;/a>; 使用动态规划来根据上面定义的标记类别： &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;案例 A 标记：&lt;/b>; 我们直接添加案例 A 标记，并根据需要添加换行符以适应更新的标题。 &lt;/li>;&lt;li>;&lt;b>;案例 B 标记：&lt;/b>;我们的初步研究表明，用户更喜欢先前显示的字幕的稳定性而不是准确性。因此，如果更新不会破坏现有的线路布局，我们仅更新情况 B 标记。 &lt;/li>;&lt;li>;&lt;b>;案例 C 标记：&lt;/b>;我们通过将原始句子和更新后的句子转换为句子嵌入、测量它们的点积并仅在它们满足时才更新它们来比较案例 C 标记的语义相似性。语义上不同（相似度&lt;0.85）并且更新不会导致新的换行符。 &lt;/li>; &lt;/ul>; &lt;p>; 最后，我们利用动画来减少视觉抖动。我们对新添加的标记实现平滑滚动和淡入淡出，以进一步稳定实时字幕的整体布局。 &lt;/p>; &lt;h2>;用户评估&lt;/h2>; &lt;p>; 我们对 123 名参与者进行了一项用户研究，目的是 (1) 检查我们提出的闪烁指标与观看者对实时字幕的体验的相关性，以及 (2) 评估我们的稳定技术的有效性。 &lt;/p>; &lt;p>; 我们在 YouTube 中手动选择了 20 个视频，以获得广泛的主题覆盖，包括视频会议、纪录片、学术讲座、教程、新闻、喜剧等。对于每个视频，我们选择了一个 30 秒的剪辑，其中至少包含 90% 的语音。 &lt;/p>; &lt;p>; 我们准备了四种类型的实时字幕渲染来进行比较： &lt;/p>; &lt;ol>; &lt;li>;原始 ASR：来自语音转文本 API 的原始语音转文本结果。 &lt;/li>;&lt;li>;原始 ASR + 阈值：仅在置信度得分高于 0.85 时才显示临时语音转文本结果。 &lt;/li>;&lt;li>;稳定字幕：使用我们上述算法进行对齐和合并的字幕。 &lt;/li>;&lt;li>;稳定且平滑的字幕：具有平滑动画（滚动+淡入淡出）的稳定字幕，以评估柔和的显示体验是否有助于改善用户体验。 &lt;/li>; &lt;/ol>; &lt;p>; 我们通过要求参与者观看录制的实时字幕来收集用户评分，并对他们对舒适度、分心度、阅读难易度、跟随视频的难易度、疲劳程度以及字幕是否正常的评估进行评分。损害了他们的经验。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;闪烁指标与用户体验之间的相关性&lt;/h3>; &lt;p>;我们计算了&lt;a href=&quot; https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot;>;闪烁指标与每个行为测量之间的斯皮尔曼系数（值范围从 -1 到 1，其中负值表示之间的负关系）两个变量，正值表示正相关，零表示没有关系）。如下所示，我们的研究表明我们的闪烁指标和用户评分之间存在统计显着性 (𝑝 &lt; 0.001) 相关性。系数的绝对值在0.3左右，表明关系中等。 &lt;/p>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;>;&lt;tbody>;&lt;tr>;&lt;td>;&lt;b>;行为测量&lt; /b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;&lt;b>;与闪烁指标的相关性*&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;舒适度 &lt;/td>; &lt;tdalign=&quot;center&quot;>; -0.29 &lt;/ td>; &lt;/tr>; &lt;tr>; &lt;td>;分散注意力 &lt;/td>; &lt;tdalign=&quot;center&quot;>; 0.33 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;易于阅读&lt;/td>; &lt;tdalign =&quot;center&quot;>; -0.31 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;易于观看的视频 &lt;/td>; &lt;tdalign=&quot;center&quot;>; -0.29 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;疲劳&lt;/td>; &lt;tdalign=&quot;center&quot;>;0.36&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td>;体验受损&lt;/td>;&lt;tdalign=&quot;center&quot;>;0.31&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto” ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们提出的闪烁指标的 Spearman 相关性测试。 *&lt;em>;p&lt;/em>; &lt; 0.001.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;实时字幕稳定性&lt;/h3 >; &lt;p>; 我们提出的技术（稳定的平滑字幕）始终获得更好的评级，根据 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&quot;>;Mann- Whitney U 检验&lt;/a>;（下图中&lt;em>;p&lt;/em>; &lt; 0.01），在前述六份调查陈述中的五份中。也就是说，用户认为经过平滑处理的稳定字幕更舒适、更容易阅读，同时与其他类型的渲染相比，他们感觉更少的分心、疲劳和体验受损。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7y YjqjaNsJ-5K9TsJBMyKDEmy8kS92ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s960/graph.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;387&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7yYjqjaNsJ-5K9TsJBMyKDEmy8kS9 2ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s16000/graph.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;用户对调查陈述的评分从 1（强烈不同意）到 7（强烈同意）。 (**: p&lt;0.01, ***: p&lt;0.001; ****: p&lt;0.0001; ns: 不显着)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div 样式=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和未来方向&lt;/h2>; &lt;p>;实时字幕中的文本不稳定会严重影响用户的阅读体验。这项工作提出了一种基于视觉的指标来对字幕稳定性进行建模，该指标在统计​​上与用户体验显着相关，并提出了一种稳定实时字幕渲染的算法。我们提出的解决方案可以集成到现有的 ASR 系统中，以增强实时字幕对各种用户的可用性，包括那些有翻译需求或有听力无障碍需求的用户。 &lt;/p>; &lt;p>; 我们的工作代表了朝着测量和提高文本稳定性迈出的实质性一步。这可以演变为包括基于语言的指标，重点关注实时字幕中使用的单词和短语随时间的一致性。这些指标可以反映用户的不适感，因为它与现实场景中的语言理解和理解有关。我们还有兴趣进行&lt;a href=&quot;https://en.wikipedia.org/wiki/Eye_tracking&quot;>;眼球追踪&lt;/a>;研究（例如，下面显示的视频）来跟踪观看者的注视模式，例如眼睛注视和扫视，使我们能够更好地了解最容易分散注意力的错误类型以及如何提高这些错误的文本稳定性。 &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowedfullscreen=&quot;&quot; youtube-src-id =&quot;1E2jGUscWyc&quot;宽度=&quot;640&quot;高度=&quot;360&quot; src=&quot;https://www.youtube.com/embed/1E2jGUscWyc?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;阅读原始 ASR 字幕时跟踪观看者视线的图示。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;div class=&quot;separator&quot; style=&quot;clear: 两者; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowedfullscreen=&quot;&quot; youtube-src-id=&quot;YfotwPIznL8&quot; width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com /embed/YfotwPIznL8?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;阅读稳定和平滑的字幕时跟踪观看者视线的图示。&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;p>; 通过提高实时字幕中的文本稳定性，我们可以创建更有效的沟通工具，并改善人们在日常对话中使用熟悉的语言或通过翻译使用不熟悉的语言进行联系的方式。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是 Google 多个团队的协作成果。主要贡献者包括 Xingyu “Bruce” Liu、Jun Zhang、Leonardo Ferrer、Susan Xu、Vikas Bahirwani、Boris Smus、Alex Olwal 和 Ruofei Du。我们衷心感谢提供帮助的同事，包括 Nishtha Bhatia、Max Spear 和 Darcy Philippon。我们还要感谢 Lin Li、Evan Parker 和 CHI 2023 审稿人。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1342410539466537463/comments/默认&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text -stability.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/默认/1342410539466537463&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1342410539466537463&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text-stability.html&quot; rel=&quot;alternate&quot; title=&quot;建模并提高实时字幕中的文本稳定性” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog” .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45-muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U5- 8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s72-c/stabilizedcaptions.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media :thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签:blogger.com,1999:blog-8474926331452026626.post-8352967842060136321&lt;/id>;&lt;发布>;2023-08 -29T12:57:00.001-07:00&lt;/已发布>;&lt;更新>;2023-08-30T17:14:40.708-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot; http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SayTap：四足运动语言&lt;/stitle>;&lt;content type=&quot;html&quot; >;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Yujin Tang 和 Wenhao Yu&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17- mXVFjbz-EOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQqM01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK- FZnvm-tRfXtxOHWHaGqcHaMI/s320/SayTap%20hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人类和四足机器人之间简单有效的交互为创造智能且有能力的辅助机器人铺平了道路，创造了一个技术以超乎我们想象的方式改善我们生活的未来。这种人机交互系统的关键是使四足机器人能够响应自然语言指令。 &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;大型语言模型&lt;/a>; (LLM) 的最新发展证明了执行&lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;高层规划&lt;/a>;的潜力。然而，对于法学硕士来说，理解低级命令（例如关节角度目标或电机扭矩）仍然是一个挑战，特别是对于本质上不稳定的有腿机器人，需要高频控制信号。因此，大多数&lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;现有&lt;/a>;&lt;a href=&quot;https://code-as-policies.github.io/&quot;>;工作&lt;/a>; 假设为法学硕士提供高级 API 来规定机器人行为，这从本质上限制了系统的表达能力。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://saytap.github.io&quot;>;SayTap：四足运动语言&lt;/a>;”中，在 &lt;a href=&quot;https://www.corl2023.org/&quot;>;CoRL 2023&lt;/a>; 上提出，我们提出了一种使用脚接触模式的方法（指的是四足动物接触脚的顺序和方式）智能体在移动时将脚放在地面上）作为连接自然语言人类命令和输出低级命令的运动控制器的接口。这产生了交互式四足机器人系统，该系统允许用户灵活地制定不同的运动行为（例如，用户可以使用简单的语言要求机器人行走、跑步、跳跃或进行其他动作）。我们贡献了 LLM 提示设计、奖励函数以及将 SayTap 控制器暴露给接触模式的可行分布的方法。我们证明 SayTap 是一个能够实现多种运动模式的控制器，这些模式可以转移到真实的机器人硬件上。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;SayTap 方法&lt;/h2>; &lt;p>; SayTap 方法使用接触模式模板，该模板是4 X &lt;i>;T&lt;/i>; 0 和 1 矩阵，其中 0 代表代理的脚在空中，1 代表脚在地面上。从上到下，矩阵中的每一行给出左前 (FL)、右前 (FR)、左后 (RL) 和右后 (RR) 脚的脚接触模式。 SayTap 的控制频率为 50 Hz，因此每个 0 或 1 持续 0.02 秒。在这项工作中，所需的脚接触模式由大小为 L&lt;sub>;w&lt;/sub>; 和形状为 4 X L&lt;sub>;w&lt;/sub>; 的循环滑动窗口定义。滑动窗口从接触模式模板中提取四个脚接地标志，这些标志指示脚是在地面上还是在 &lt;i>;t&lt;/i>; + 1 和 &lt;i>;t&lt;/i>; + L 之间的空中&lt;子>;w&lt;/子>;。下图概述了 SayTap 方法。 &lt;/p>; &lt;p>; SayTap 引入了这些所需的脚部接触模式，作为自然语言用户命令和运动控制器之间的新接口。运动控制器用于完成主要任务（例如，遵循指定的速度）并在指定的时间将机器人的脚放在地面上，使得实现的脚接触模式尽可能接近期望的接触模式。为了实现这一点，除了机器人的本体感觉数据（例如，关节位置和速度）和任务相关输入（例如，用户指定的速度命令）之外，运动控制器还将每个时间步所需的脚部接触模式作为其输入。 。我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_reinforcement_learning&quot;>;深度强化学习&lt;/a>;来训练运动控制器并将其表示为深度神经网络。在控制器训练期间，随机生成器对所需的脚部接触模式进行采样，然后优化策略以输出低级机器人动作，以实现所需的脚部接触模式。然后在测试时，法学硕士将用户命令转换为脚部接触模式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2​​aLrGlJ9CTOIkfI-G8jVZrYE083 _U84CCOq2pGuewb4szwCvoRc8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/s935/image13.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;935&quot; height=&quot;297&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2​​aLrGlJ9CTOIkfI-G8jVZrYE083_U84CCOq2pGuewb4szwCvoRc 8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/w640-h297/image13.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap 方法概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;br />; &lt;video autoplay=&quot; “循环=””静音=””playsinline=””样式==左边缘：10%；右边距：10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/saytap_blog.mp4&quot; type= &quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left” ： 汽车;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap 使用脚接触模式（例如，每只脚的 0 和 1 序列插图（其中 0 表示脚在空中，1 表示脚在地面）作为桥接自然语言用户命令和低级控制命令的接口。使用基于强化学习的运动控制器，经过训练可实现所需的接触模式，SayTap 允许四足机器人接受简单直接的指令（例如，“慢慢向前小跑。”）以及模糊的用户命令（例如，“好消息，我们这个周末要去野餐！”）并做出反应&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 我们证明，当给出正确设计的提示时，法学硕士能够准确地将用户命令映射到指定格式的足部接触模式模板中，即使在命令是非结构化或模糊的情况下。在训练中，我们使用随机模式生成器来生成具有各种模式长度 &lt;i>;T&lt;/i>; 的接触模式模板，基于循环内的脚与地面的接触比给定的步态类型&lt;i>;G&lt;/i>;，以便运动控制器能够学习广泛的运动分布，从而获得更好的泛化能力。有关更多详细信息，请参阅&lt;a href=&quot;https://arxiv.org/abs/2306.07580&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 一个简单的提示，仅包含三个上下文示例常见的脚部接触模式，法学硕士可以将各种人类命令准确地转换为接触模式，甚至可以推广到那些没有明确指定机器人应如何反应的指令。 &lt;/p>; &lt;p>; SayTap 提示简洁明了，由四个部分组成： (1) 一般说明，描述 LLM 应完成的任务； (2) 步态定义，提醒法学硕士有关四足步态的基本知识以及它们如何与情绪相关； (3)输出格式定义； (4) 为法学硕士提供在上下文中学习的机会的示例。我们还指定了五种速度，允许机器人向前或向后、快速或缓慢移动或保持静止。 &lt;/p>; &lt;span style=&quot;font-size:small;&quot;>; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px;white-space: pre-wrap;&quot;>;&lt; font color=&quot;#0000ff&quot;>;一般说明块&lt;/font>; 您是狗脚接触模式专家。您的工作是根据输入给出速度和脚接触模式。无论输入是什么，您始终会以正确的格式给出输出。 &lt;font color=&quot;#0000ff&quot;>;步态定义块&lt;/font>; 下面是关于步态的描述： 1. 小跑是两条对角相对的腿同时着地的步态。 2. 踱步是身体左右两侧的腿同时着地的步态。 3. 跳跃是两前/后腿同时着地的步态。它具有较长的暂停阶段，例如，在至少 25% 的周期长度内，所有脚都离开地面。这样的步态也给人一种幸福的感觉。 &lt;font color=&quot;#0000ff&quot;>;输出格式定义块&lt;/font>; 以下是描述速度和脚接触模式的规则： 1. 您应该首先输出速度，然后输出脚接触模式。 2. 有五种速度可供选择：[-1.0、-0.5、0.0、0.5、1.0]。 3. 一个图案有 4 条线，每条线代表一条腿的脚接触图案。 4. 每行都有一个标签。 “FL”是左前腿，“FR”是右前腿，“RL”是左后腿，“RR”是右后腿。 5. 每行中，“0”代表脚在空中，“1”代表脚在地上。 &lt;font color=&quot;#0000ff&quot;>;示例块&lt;/font>; 输入：Trot Slow 输出：0.5 FL: 11111111111111111000000000 FR: 00000000011111111111111111 RL: 00000000011111111111111111 RR: 11111111111111111000000000 输入: 绑定到位 输出: 0.0 FL: 11111111111100000000000000 FR: 11111111111100000000000000 RL: 00000011111111111100000000 RR: 00000011111111111100000000 输入: 快退 输出: -1.0 FL: 11111111100001111111110000 FR: 0000111111111000011 1111111 RL: 11111111100001111111110000 RR: 00001111111110000111111111 输入: &lt;/pre>;&lt;/span>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0 &quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;SayTap 提示 LLM。蓝色文字用于说明，不输入LLM。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;遵循简单直接的命令&lt;/h3>; &lt;p>; 我们在下面的视频中演示了 SayTap 系统可以成功执行命令直接且清晰的任务。尽管三个上下文示例中未涵盖某些命令，但我们能够引导 LLM 通过“步态定义块”（请参阅​​上面提示中的第二个块）表达其预训练阶段的内部知识提示。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;源src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/ source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;遵循非结构化或模糊命令&lt;/h3>; &lt;p >; 但更有趣的是 SayTap 处理非结构化和模糊指令的能力。只需稍加提示即可将某些步态与一般情绪印象联系起来，机器人在听到令人兴奋的信息（例如“我们要去野餐！”）时就会上下跳跃。此外，它还准确地呈现场景（例如，当被告知地面非常热时，它的脚几乎不接触地面，快速移动）。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/ source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和未来工作&lt;/h2>; &lt;p>;我们推出 SayTap，这是一种四足机器人交互系统，允许用户灵活地制定不同的运动行为。 SayTap 引入了所需的脚接触模式作为自然语言和低级控制器之间的新接口。这个新界面简单灵活，此外，它允许机器人遵循直接指令和没有明确说明机器人应如何反应的命令。 &lt;/p>; &lt;p>; 未来工作的一个有趣方向是测试暗示特定感觉的命令是否允许法学硕士输出所需的步态。在上面结果部分显示的步态定义块中，我们提供了一个将快乐心情与跳跃步态联系起来的句子。我们相信，提供更多信息可以增强法学硕士的解释（例如，隐含的感受）。在我们的评估中，快乐的感觉和跳跃步态之间的联系使机器人在遵循模糊的人类命令时表现得生动。未来工作的另一个有趣的方向是引入多模式输入，例如视频和音频。理论上，从这些信号转换而来的脚部接触模式仍然适用于我们的管道，并将解锁更多有趣的用例。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;Yujin Tang、Wenhao Yu、Jie Tan、Heiga Zen、Aleksandra Faust 和 Tatsuya Harada 进行了这项研究。这项工作是在团队在 Google Research 期间构思和执行的，并将在 Google DeepMind 继续进行。作者衷心感谢张廷南、Linda Luu、Kuang-Huei Lee、Vincent Vanhoucke 和 Douglas Eck 在实验中提供的宝贵讨论和技术支持。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/内容>;&lt;link href=&quot;http://blog.research.google/feeds/8352967842060136321/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08 /saytap-language-to-quadrupedal.html&quot; rel=&quot;alternate&quot; title=&quot;SayTap：语言到四足运动&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>; http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com /g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height= “72”网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQq M01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s72-c/SayTap%20hero.jpg&quot; width=&quot;72 &quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签： Blogger.com，1999：Blog-847492631452026626.POST-781660063956467509 &lt;/ID>; &lt;/ID>; &lt;/id>; &lt;Ebtumed>; 2023-08-28-28T09：59：009：00.00.003-07：00：003-07：00：00 07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;RO-ViT：使用视觉转换器进行开放词汇对象检测的区域感知预训练&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者Dahun Kim 和 Wei Cheng Kuo，研究科学家，Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa 98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVuIj5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s1100/RO-VIT -hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 检测视觉世界中的物体的能力对于计算机视觉和机器智能至关重要，从而实现自适应自主代理和多功能购物系统等应用。然而，现代物体检测器受到训练数据的手动注释的限制，导致词汇量明显小于现实中遇到的大量物体。为了克服这个问题，出现了&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;开放词汇检测任务&lt;/a>; (OVD)，它利用图像-文本对进行训练并合并新的类别名称在测试时将它们与图像内容相关联。通过将类别视为文本嵌入，开放词汇检测器可以预测各种看不见的对象。各种技术，例如&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;图像文本预训练&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot; >;知识蒸馏&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2112.09106&quot;>;伪标签&lt;/a>;和冻结模型，通常采用&lt;a href=&quot;https://en. wikipedia.org/wiki/Convolutional_neural_network&quot;>;卷积神经网络&lt;/a>;（CNN）主干已经被提出。随着&lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;>;视觉转换器&lt;/a>; (ViT) 的日益普及，这一点非常重要探索它们构建熟练的开放词汇检测器的潜力。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 现有方法假设可以使用预训练的&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;视觉语言模型&lt;/a>;（VLM），并专注于从这些模型中进行微调或&lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;蒸馏&lt;/a>;，以解决图像之间的差异级预训练和对象级微调。然而，由于 VLM 主要是为分类和检索等图像级任务而设计的，因此它们在预训练阶段没有充分利用对象或区域的概念。因此，如果我们将局部性信息构建到图像文本预训练中，这可能有利于开放词汇检测。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2305.07011&quot;>;RO-ViT：使用视觉变压器进行开放词汇对象检测的区域感知预训练&lt;/a>;”中，在 &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>; 上提出，我们引入了一种简单的方法，以区域感知的方式预训练视觉变换器，以改进开放词汇检测。在视觉转换器中，位置嵌入被添加到图像块中，以对图像中每个块的空间位置信息进行编码。标准预训练通常使用全图像位置嵌入，这不能很好地推广到检测任务。因此，我们提出了一种新的位置嵌入方案，称为“裁剪位置嵌入”，它可以更好地与检测微调中区域裁剪的使用保持一致。此外，我们将 &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function#Neural_networks&quot;>;softmax 交叉熵损失&lt;/a>; 替换为 &lt;a href=&quot;https://arxiv.org/ abs/1708.02002&quot;>;对比图像文本学习中的焦点损失&lt;/a>;，使我们能够从更具挑战性和信息量更大的示例中学习。最后，我们利用新对象提议的最新进展来增强开放词汇检测微调，这是由于观察到现有方法经常由于对前景类别的过度拟合而在提议阶段错过新对象。我们还在&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm/rovit&quot;>;此处&lt;/a>;发布了代码。 &lt;/p>; &lt;br />; &lt;h2>;区域感知图像文本预训练&lt;/h2>; &lt;p>; 现有的 VLM 经过训练，可以将整个图像与文本描述进行匹配。然而，我们观察到现有对比预训练方法和开放词汇检测中使用位置嵌入的方式之间存在不匹配。位置嵌入对于转换器很重要，因为它们提供了集合中每个元素来自何处的信息。此信息通常对下游识别和定位任务有用。预训练方法通常在训练期间应用全图像位置嵌入，并对下游任务（例如零样本识别）使用相同的位置嵌入。然而，识别发生在开放词汇检测微调的区域级别，这需要全图像位置嵌入泛化到他们在预训练期间从未见过的区域。 &lt;/p>; &lt;p>; 为了解决这个问题，我们提出了&lt;em>;裁剪位置嵌入&lt;/em>;（CPE）。通过 CPE，我们将位置嵌入从预训练的典型图像尺寸（例如 224x224 像素）上采样到检测任务的典型图像尺寸（例如 1024x1024 像素）。然后我们随机裁剪一个区域并调整其大小，并在预训练期间将其用作图像级位置嵌入。作物的位置、比例和纵横比是随机采样的。直观上，这导致模型将图像本身视为完整图像，而不是一些较大的未知图像的区域裁剪。这更好地匹配下游检测用例，其中识别发生在区域级别而不是图像级别。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZ qTJq4heQruWCeaQsM5uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s952 /RO-ViT-img6.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;952 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZqTJq4heQruWCeaQsM5 uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s16000/RO-ViT-img6.jpg&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;对于预训练，我们提出&lt;em>;裁剪位置嵌入&lt;/em>;&amp;nbsp; （CPE）随机裁剪位置嵌入区域并调整其大小，而不是使用整个图像位置嵌入（PE）。此外，我们使用焦点损失而不是常见的softmax交叉熵损失来进行对比学习。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们还发现从困难的例子中学习是有益的焦点损失。与 softmax 交叉熵损失相比，焦点损失可以更好地控制样本的权重。我们在图像到文本和文本到图像的损失中采用焦点损失并将其替换为 softmax 交叉熵损失。 CPE 和焦点损失都没有引入额外的参数并且计算成本最小。 &lt;/p>; &lt;br />; &lt;h2>;开放词汇检测器微调&lt;/h2>; &lt;p>;开放词汇检测器使用“基本”类别的检测标签进行训练，但需要检测“测试时的“基础”和“新颖”（未标记）类别。尽管主干特征是根据大量开放词汇数据预先训练的，但添加的检测器层（颈部和头部）是使用下游检测数据集进行新训练的。现有的方法经常在对象提议阶段错过新颖/未标记的对象，因为提议倾向于将它们分类为背景。为了解决这个问题，我们利用新颖的对象提议方法的最新进展，并采用基于本地化质量的对象性（即&lt;a href=&quot;https://arxiv.org/abs/2108.06753&quot;>;centerness&lt;/a>;分数）而不是物体或非物体的二元分类得分，它与检测得分相结合。在训练过程中，我们将每个检测到的区域的检测分数计算为该区域嵌入之间的&lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;余弦相似度&lt;/a>;（通过&lt;a href计算） =&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks&quot;>;RoI-Align&lt;/a>; 操作）和基本类别的文本嵌入。在测试时，我们附加新颖类别的文本嵌入，并且现在通过基本类别和新颖类别的联合来计算检测分数。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw 7MCZE0IsOhdkt8ny6ipDfFi4BxNOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s682/RO-ViT -img4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;682&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw7MCZE0IsOhdkt8ny6ipDfFi4Bx NOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s16000/RO-ViT-img4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过用检测器头替换全局平均池化，将预训练的 ViT 主干转移到下游开放词汇检测。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks#:~:text=new%20method%20叫-,ROIAlign,-%2C%20which%20can%20represent&quot; style=&quot;text- align: left;&quot;>;RoI-Align&lt;/a>;&amp;nbsp;嵌入与缓存的类别嵌入匹配以获得VLM分数，该分数与检测分数组合成开放词汇检测分数。&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们评估 RO-ViT &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; 开放词汇检测基准。在系统级别，我们的最佳模型在稀有类别上实现了 33.6 个框&lt;a href=&quot;https://blog.paperspace.com/mean-average- precision/&quot;>;平均精度&lt;/a>;（&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;AP&lt;sub>;r&lt;/sub>;&lt;/a>;) 和 32.1 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;掩码AP&lt;sub>;r&lt;/sub>;&lt;/a>;，其性能优于现有的基于 ViT 的最佳方法 OWL-ViT 8.0 AP&lt;sub>;r&lt;/sub>; 和最佳的基于 CNN 的方法 ViLD-Ens 5.8 掩模 AP&lt;sub>;r&lt;/sub>;。它还超过了许多其他基于知识蒸馏、预训练或弱监督联合训练的方法的性能。&lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr -caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel-0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQH rll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height =“426”data-original-width =“1200”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel- 0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQHrll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s16000/image2.png &quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RO-ViT 的性能优于当前状态艺术 (SOTA)&lt;a href=&quot;https://arxiv.org/abs/2205.06230&quot; style=&quot;text-align: left;&quot;>;基于 ViT&lt;/a>; 和&lt;a href=&quot; https://arxiv.org/abs/2104.13921&quot; style=&quot;text-align: left;&quot;>;基于 CNN 的 LVIS 开放词汇检测基准的方法。我们在稀有类别 (AP&lt;sub>;r&lt;/sub>;) 上显示掩模 AP，但基于 SOTA ViT 的 (OwL-ViT) 则显示框 AP。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/表>; &lt;p>; 除了通过开放词汇检测评估区域级表示之外，我们还通过 &lt;a href=&quot;https://arxiv.org/abs 评估 RO-ViT 在图像文本检索中的图像级表示/1504.00325&quot;>;MS-COCO&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/1505.04870&quot;>;Flickr30K&lt;/a>; 基准测试。我们的具有 303M ViT 的模型在 MS COCO 上优于具有 1B ViT 的最先进的 CoCa 模型，并且在 Flickr30K 上处于同等水平。这表明我们的预训练方法不仅提高了区域级表示，还提高了检索的全局图像级表示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKV rwp3_wXzf8KGqaY50rK3fLcWtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s1692/image5.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;940&quot; data-original-width=&quot;1692&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKVrwp3_wXzf8KGqaY50rK3fLc WtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;我们在 MS COCO 和 Flickr30K 基准测试上展示了零样本图像文本检索，并与双编码器方法进行了比较。我们报告图像到文本 (I2T) 和文本到图像 (T2I) 检索任务的recall@1（top-1 召回率）。 RO-ViT 的性能优于最先进的&lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot; style=&quot;text-align: left;&quot;>;CoCa&lt;/a>; &amp;nbsp;相同的主干。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFfI7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT- 03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s1476/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;362&quot; 数据-original-width=&quot;1476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFf I7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT-03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td >;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LVIS 上的 RO-ViT 开放词汇检测。为了清楚起见，我们仅显示新颖的类别。 RO-ViT detects many novel categories that it has never seen during detection training: “fishbowl”, “sombrero”, “persimmon”, “gargoyle”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Visualization of positional embeddings&lt;/h2>; &lt;p>; We visualize and compare the learned positional embeddings of RO-ViT with the baseline. Each tile is the cosine similarity between positional embeddings of one patch and all other patches. For example, the tile in the top-left corner (marked in red) visualizes the similarity between the positional embedding of the location (row=1, column=1) and those positional embeddings of all other locations in 2D.补丁的亮度表示不同位置的学习位置嵌入的接近程度。 RO-ViT 在不同的斑块位置形成更明显的簇，在中心斑块周围显示对称的全局模式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwS twANDQkmi6f2w62aXNhkEu3daexyXV5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s1686/RO-ViT-img1.jpg&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1686&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwStwANDQkmi6f2w62aXNHkEu3daexyX V5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s16000/RO-ViT-img1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;每个图块显示补丁的位置嵌入（在指示的行列位置）与所有其他补丁的位置嵌入之间的余弦相似度。使用 ViT-B/16 主干。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了 RO-ViT，一种对比图像文本预训练框架，以弥合图像级预训练和开放词汇检测微调之间的差距。我们的方法简单、可扩展，并且易于应用于任何对比骨干网，计算开销最小，并且不增加参数。 RO-ViT 在 LVIS 开放词汇检测基准和图像文本检索基准上实现了最先进的水平，表明学习的表示不仅在区域级别有益，而且在图像级别也非常有效。我们希望这项研究能够帮助从图像文本预训练的角度进行开放词汇检测的研究，这对区域级和图像级任务都有好处。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;Dahun Kim、Anelia Angelova 和 Wei Cheng Kuo 进行了这项工作，目前在 Google DeepMind 工作。我们要感谢 Google Research 的同事提供的建议和有益的讨论。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/781660063956467509/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html#comment-form&quot; rel=&quot;replies &quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;edit&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;RO-ViT：区域感知预训练使用视觉转换器进行开放词汇对象检测训练” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/ uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1 .blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVu Ij5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s72-c/RO-ViT-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss /&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-3964459643854458124&lt;/id>; &lt;发布>;2023-08-25T10:38:00.003-07:00&lt;/发布>;&lt;更新>;2023-08-25T10:46:59.205-07:00&lt;/更新>;&lt;category schema=&quot;http://www .blogger.com/atom/ns#&quot; term=&quot;机器感知&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT 亮点&quot;>; &lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 研究的负责任的人工智能：感知公平性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部感知公平性团队联合负责人 Susanna Ricco 和 Utsav Prabhu&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MOHdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJ e8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1-inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s320/hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google 的&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;负责任的人工智能研究&lt;/a>;建立在具有不同背景和专业知识的团队之间的协作基础上，研究人员和产品开发人员之间，并最终与整个社区之间。感知公平团队将计算机视觉和机器学习 (ML) 公平性方面的深厚主题专业知识与构建感知系统的研究人员直接联系相结合，推动 Google 及其他领域产品的进步。我们正在共同努力，在 &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google 人工智能原则&lt;/a>;的指导下，从头开始有意设计具有包容性的系统。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf -_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkypL7MqBpdeqIJm 9H5sani/s948/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;214&quot; data-original-width=&quot;948 “高度=“144”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf-_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ 7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkYpL7MqBpdeqIJm9H5sani/w640-h144/image1.gif&quot;宽度=“640”/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;感知公平研究涵盖先进多模式模型的设计、开发和部署，包括最新的基础和生成模型为 Google 产品提供支持的模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 我们团队的使命是推进多模式 ML 系统的公平性和包容性前沿，特别是与&lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;>;基础&lt;/a>;模型和&lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;生成人工智能&lt;/a >;。这涵盖了核心技术组件，包括分类、本地化、字幕、检索、视觉问答、文本到图像或文本到视频生成以及生成图像和视频编辑。我们相信，公平和包容可以而且应该成为这些应用程序的首要性能目标。我们的研究重点是解锁新颖的分析和缓解措施，使我们能够在整个开发周期中主动设计这些目标。我们回答核心问题，例如：我们如何使用机器学习负责任地、忠实地模拟人类对人口、文化和社会身份的看法，以促进公平和包容？我们可以测量哪些类型的系统偏差（例如，在某些肤色的人的图像上表现不佳）以及如何使用这些指标来设计更好的算法？如何构建更具包容性的算法和系统，并在发生故障时快速反应？ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;衡量媒体中人物的代表性&lt;/h2>; &lt;p>; 可以编辑、策划或创建图像或视频可以影响任何接触其输出的人，塑造或强化世界各地观众的信念。减少代表性伤害（例如强化刻板印象或诋毁或消除人群）的研究需要对内容和&lt;a href=&quot;https://ai.googleblog.com/2023/07/using-societal -context-knowledge-to.html&quot;>;社会背景&lt;/a>;。它取决于不同的观察者如何看待自己、他们的社区或他人的代表方式。关于哪些社会类别应该使用计算工具进行研究以及如何负责任地进行研究，该领域存在相当多的争论。我们的研究重点是致力于寻求可扩展的解决方案，这些解决方案以社会学和社会心理学为基础，与人类的感知相一致，拥抱问题的主观本质，并实现细致入微的测量和缓解。一个例子是我们对&lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;人类感知和肤色注释差异的研究使用&lt;a href=&quot;skintone.google&quot;>;僧侣肤色等级&lt;/a>;的图像&lt;/a>;。 &lt;/p>; &lt;p>; 我们的工具还用于研究大规模内容集合中的表示。通过我们的社交探索媒体理解 (MUSE) 项目，我们与学术研究人员、非营利组织和主要消费品牌合作，了解主流媒体和广告内容的模式。我们于 2017 年首次发表了这项研究成果，其中一项共同撰写的研究分析了&lt;a href=&quot;https://about.google/intl/ALL_au/main/gender-equality-films/&quot;>;好莱坞电影中的性别平等&lt;/a >;。从那时起，我们增加了分析的规模和深度。 2019 年，我们发布了基于&lt;a href=&quot;https://www.thinkwithgoogle.com/feature/diversity-inclusion/&quot;>;超过 270 万条 YouTube 广告&lt;/a>;的调查结果。在&lt;a href=&quot;https://blog.google/technology/ai/using-ai-to-study-12-years-of-representation-in-tv/&quot;>;最新研究&lt;/a>;中，我们研究了在超过十二年的美国流行电视节目中，感知性别表现、感知年龄和肤色的交叉表现。这些研究为内容创作者和广告商提供了见解，并进一步为我们自己的研究提供信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFuL3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPt S8ONz02Nrgn4If7YOukbw0txEfy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/s800/image3.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;800&quot; height=&quot;360&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFul3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPtS8ONz02Nrgn4If7YOukbw0tx Efy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/w640-h360/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;计算信号的图示（非实际数据），可以大规模分析以揭示媒体集合中的代表性模式。 [视频集/Getty Images]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;展望未来，我们正在扩展我们关注的机器学习公平性概念以及涉及的领域它们得到负责任的应用。除了逼真的人物图像之外，我们正在努力开发工具，以在插图、人形角色的抽象描述，甚至根本没有人的图像中对社区和文化的表现进行建模。最后，我们不仅需要推理描绘的是谁，还需要推理他们是如何被描绘的——通过周围的图像内容、随附的文本和更广泛的文化背景传达了什么样的叙事。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;分析感知系统的偏差属性&lt;/h2>; &lt;p>;构建高级机器学习系统非常复杂，多个利益相关者告知决定产品行为的各种标准。历史上，总体质量是使用测试数据集的汇总统计数据（例如总体准确性）作为用户体验的代理来定义和测量的。但并非所有用户都以相同的方式体验产品。 &lt;br />; &lt;/p>; &lt;p>; 感知公平性能够对汇总统计之外的细微系统行为进行实际测量，并使这些指标成为系统质量的核心，直接通知产品行为和发布决策。这通常比看起来要困难得多。将复杂的偏见问题（例如，交叉亚组之间的表现差异或刻板印象强化实例）提炼为少量指标而不丢失重要的细微差别是极具挑战性的。另一个挑战是平衡公平性指标和其他产品指标（例如用户满意度、准确性、延迟）之间的相互作用，尽管它们是兼容的，但通常被认为是冲突的。研究人员通常将他们的工作描述为优化“准确性-公平性”权衡，而实际上广泛的用户满意度与满足公平性和包容性目标是一致的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu 2yPn_wK23Z8PdYkeEmycLRJ0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s640/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu2yPn_wK23Z8PdYkeEmycLRJ 0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们构建并发布了&lt;a href=&quot;https://storage.googleapis.com/openimages/web/extended.html#miap&quot;>;MIAP 数据集&lt;/a>;作为&lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;>;开放图像&lt;/a>;的一部分，利用我们对社交相关概念的感知和检测的研究复杂系统中的偏见行为，以创建进一步推进计算机视觉中 ML 公平性研究的资源。原始照片来源 - 左：&lt;a href=&quot;https://www.flickr.com/photos/boston_public_library/8242010414/&quot;>;波士顿公共图书馆&lt;/a>;；中间：&lt;a href=&quot;https://www.flickr.com/photos/jenrobinson/20183915655/&quot;>;jen robinson&lt;/a>;；右：&lt;a href=&quot;https://www.flickr.com/photos/mrgarin/2484859086/&quot;>;加林·丰斯&lt;/a>;；所有使用均已获得 &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;>;CC-BY 2.0 许可证&lt;/a>;许可。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt; /table>; &lt;br />; &lt;p>; 为此，我们的团队专注于两个广泛的研究方向。首先，民主化对易于理解且广泛适用的公平分析工具的访问，让合作伙伴组织将其采用到产品工作流程中，并通知整个公司的领导层解释结果。这项工作包括制定广泛的基准、&lt;a href=&quot;https://ai.googleblog.com/2021/06/a-step-toward-more-inclusive-people.html&quot;>;整理广泛使用的高质量测试数据集&lt;/a>; 以及以切片分析和反事实测试等技术为中心的工具——通常建立在前面描述的核心表示信号工作的基础上。其次，&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3461702.3462557&quot;>;推进公平分析的新颖方法&lt;/a>; - 包括与&lt;a href=&quot;https://ai 合作.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;可能带来突破性发现的产品努力&lt;/a>;或&lt;a href=&quot;https://services.google.com/ fh/files/blogs/bsr-google-cr-api-hria-executive-summary.pdf&quot;>;告知启动策略&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;负责任地推进人工智能&lt;/h2>; &lt;p>;我们的工作并不止于分析模型行为。相反，我们将此作为与产品团队的其他研究人员和工程师合作确定算法改进的起点。在过去的一年里，我们推出了升级的组件，为 Google 相册中的搜索和&lt;a href=&quot;https://blog.google/products/photos/google-photos-memories-view/&quot;>;回忆&lt;/a>;功能提供支持，通过添加层来防止错误在系统中级联，从而实现更一致的性能并大幅提高稳健性。我们正在努力改进 Google 图片中的排名算法，以实现多样化。我们更新了可能强化历史刻板印象的算法，负责任地使用额外信号，以便&lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;每个人都更有可能看到他们自己反映在搜索结果中并找到他们正在寻找的内容&lt;/a>;。 &lt;/p>; &lt;p>; 这项工作自然地延续到了生成人工智能的世界，其中 &lt;a href=&quot;https://blog.google/technology/research/how-ai-creates-photorealistic-images-from-text /&quot;>;模型可以根据图像和文本提示创建图像或视频集合&lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning -in.html&quot;>;可以回答有关图像和视频的问题&lt;/a>;。我们对这些技术&lt;a href=&quot;https://blog.google/products/shopping/ai-virtual-try-on-google-shopping/&quot;>;为用户提供新体验&lt;/a的潜力感到兴奋>; 并作为进一步我们自己研究的工具。为了实现这一目标，我们正在与研究和负责任的人工智能社区合作，开发减轻故障模式的护栏。我们正在利用我们的工具来理解表示，以支持可与人类反馈相结合的可扩展基准，并投资于从预训练到部署的研究，以引导模型生成更高质量、更具包容性和更可控的输出。我们希望这些模型能够激励人们，产生多样化的输出，在不依赖比喻或刻板印象的情况下翻译概念，并在提示的反事实变化中提供一致的行为和响应。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;机遇和正在进行的工作&lt;/h2>; &lt;p>; 尽管经过十多年的专注工作，该领域感知公平技术似乎仍然是一个新兴且快速发展的领域，充满了突破性技术的机会。我们继续看到在跨学科奖学金支持下贡献技术进步的机会。我们可以在图像中测量的内容与人类身份和表达的基本方面之间的差距很大 - 缩小这一差距将需要越来越复杂的媒体分析解决方案。表明真实代表性的数据指标，位于适当的背景下并考虑多种观点，对我们来说仍然是一个公开的挑战。我们是否能够可靠地识别对细微刻板印象的描述，不断更新它们以反映不断变化的社会，并辨别它们可能令人反感的情况？由人类反馈驱动的算法进步指明了一条充满希望的前进道路。 &lt;/p>; &lt;p>; 最近对现代大型模型开发背景下的人工智能安全和道德的关注激发了衡量系统偏差的新思维方式。我们正在探索使用这些模型的多种途径 - 以及基于概念的可解释性方法、因果推理方法和尖端用户体验研究的最新发展 - 来量化和最大程度地减少不良偏见行为。我们期待着应对未来的挑战并开发适合每个人的技术。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢 Perception 的每一位成员公平团队以及我们所有的合作者。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3964459643854458124/comments/default&quot; rel=&quot;replies&quot; title =&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html#comment-form &quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124&quot; rel=&quot;edit “ type =“application/atom+xml”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124”rel =“self”type =“application/atom+xml” &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Google 研究的负责任的人工智能：感知公平&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com &lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded .gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MO HdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJe8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1 -inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr ：总计>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6350776943545232437&lt;/id>;&lt;发布>;2023-08-24T15:10:00.000-07:00&lt;/已发布>;&lt;更新>;2023-08-24T15:10:57.268-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;量子计算&quot;>;&lt; /category>;&lt;title type=&quot;text&quot;>;如何将嘈杂的量子处理器与经典计算机进行比较&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Sergio Boixo 和 Vadim 发布Smelyanskiy，Google 量子 AI 团队首席科学家&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qs EcLuEDPn​​WZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s320/hero.jpg&quot;样式= “显示：无；” />; &lt;p>; 一台全面的纠错量子计算机将能够解决一些经典计算机无法解决的问题，但建造这样的设备是一项艰巨的任务。我们为实现完全错误所取得的&lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;里程碑&lt;/a>;感到自豪-修正了量子计算机，但是大型计算机还需要几年的时间。与此同时，我们正在使用当前的噪声量子处理器作为量子实验的灵活平台。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 与纠错量子&lt;em>;计算机&lt;/em>;相比，目前在噪声量子&lt;em>;处理器&lt;/em>;中进行的实验在噪声降低量子态之前，仅限于几千个量子操作或门。 2019 年，我们在量子处理器&lt;a href= “https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;并首次表明&lt;/a>;它的性能优于最先进的经典超级计算。 &lt;/p>; &lt;p>; 虽然它们尚未达到超越经典的能力，但我们也使用我们的处理器来观察新的物理现象，例如&lt;a href=&quot;https://www.nature.com/articles/s41586 -021-04257-w&quot;>;时间晶体&lt;/a>;和&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;马约拉纳边缘模式&lt;/a>;，并制作了新的实验发现，例如相互作用光子的强大&lt;a href=&quot;https://ai.googleblog.com/2022/12/formation-of-robust-bound-states-of.html&quot;>;束缚态&lt;/a>;以及 Floquet 演化的马约拉纳边缘模式的&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;噪声弹性&lt;/a>;。 &lt;/p>; &lt;p>; 我们预计，即使在这种中间的、嘈杂的状态下，我们也会找到量子处理器的应用，其中有用的量子实验可以比经典超级计算机上的计算速度快得多——我们称之为“计算应用” “量子处理器。目前还没有人展示出这样一种超越经典的计算应用。因此，当我们的目标是实现这一里程碑时，问题是：将在此类量子处理器上运行的量子实验与经典应用程序的计算成本进行比较的最佳方法是什么？ &lt;/p>; &lt;p>; 我们已经知道如何将纠错量子算法与经典算法进行比较。在这种情况下，&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_complexity&quot;>;计算复杂度&lt;/a>;领域告诉我们，我们可以比较它们各自的计算成本——即完成任务所需的操作。但对于我们目前的实验性量子处理器来说，情况还没有那么明确。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;噪声量子处理实验的有效量子体积、保真度和计算成本&lt;/a>;”中，我们提供了一个框架为了测量量子实验的计算成本，引入了实验的“有效量子体积”，即有助于测量结果的量子操作或门的数量。我们应用这个框架来评估最近三个实验的计算成本：我们的&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;随机电路采样&lt;/a>; &lt;a href=&quot;https://arxiv .org/abs/2304.11119&quot;>;实验&lt;/a>;，我们的&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>;实验测量被称为“失序相关器”的量”（OTOC）&lt;/a>;，以及&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;最近关于 Floquet 进化的实验&lt;/a>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Ising_model&quot;>;伊辛模型&lt;/a>;。我们对 OTOC 感到特别兴奋，因为它们提供了一种直接的方法来通过实验测量电路的有效量子体积（一系列量子门或操作），这本身对于经典计算机来说是一项计算上困难的任务，要精确估计。 OTOC 在&lt;a href=&quot;https://en.wikipedia.org/wiki/Nuclear_Magnetic_resonance&quot;>;核磁共振&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/ Electron_paraMagnetic_resonance&quot;>;电子自旋共振光谱&lt;/a>;。因此，我们相信 OTOC 实验是量子处理器首次计算应用的有希望的候选者。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd 6VsOOMzDaexufiJs_8oyo1G37h2lr4Y1Y28zD7lZ3OIB_EL_LSY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/s1280/image3.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;1280&quot; height=&quot;297&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd6VsOOMzDaexufiJs_8oyo1G 37h2lr4Y1Y28zD7lZ3OIB_EL_LsY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/w640-h297/image3.png&quot;宽度=“640”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;计算成本和最近一些量子实验的影响图。虽然有些（例如，&lt;a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;QC-QMC 2022&lt;/a>;）产生了很大影响，而另一些（例如，&lt; a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;RCS 2023&lt;/a>;）的计算成本很高，但还没有一个既有用又困难，值得考虑“计算应用程序”。我们假设我们未来的 OTOC 实验可能是第一个突破这个门槛的实验。文中引用了绘制的其他实验。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div >; &lt;h2>;随机电路采样：评估噪声电路的计算成本&lt;/h2>; &lt;p>; 当谈到在噪声量子处理器上运行量子电路时，有两个相互竞争的考虑因素。一方面，我们的目标是做一些经典难以实现的事情。计算成本（在经典计算机上完成任务所需的操作数量）取决于量子电路的&lt;em>;有效量子体积&lt;/em>;：体积越大，计算成本越高，量子越多处理器的性能可以超越经典处理器。 &lt;/p>; &lt;p>; 但另一方面，在嘈杂的处理器上，每个量子门都会给计算带来错误。操作越多，误差就越大，并且量子电路在测量感兴趣的量时的保真度就越低。考虑到这一点，我们可能更喜欢更简单、有效体积更小的电路，但这些电路很容易被经典计算机模拟。我们希望最大化这些相互竞争的考虑因素的平衡，称为“计算资源”，如下所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1w OEPS4TwK55cMxJmg19mQD7ZCtuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/s973/image1.png&quot; 样式 = &quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;653&quot; data-original-width=&quot;973&quot; height=&quot;430&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1wOEPS4TwK55cMxJmg19mQD7Z Ctuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/w640-h430/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;量子电路中量子体积和噪声之间的权衡图，以称为“计算资源”的量捕获。对于有噪声的量子电路，这最初会随着计算成本的增加而增加，但最终，噪声会超出电路并导致其减少。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; 我们可以通过一个简单的 &lt;a href=&quot;https://ai.googleblog.com/2019&quot; 看到这些相互竞争的考虑因素如何发挥作用/10/quantum-supremacy-using-programmable.html&quot;>;量子处理器的“hello world”程序&lt;/a>;，称为随机电路采样（RCS），这是量子处理器超越经典计算机的首次演示。任何一个门的任何错误都可能使这个实验失败。不可避免的是，这是一个很难以高保真度实现的实验，因此它也可以作为系统保真度的基准。但它也对应于量子处理器可实现的最高已知计算成本。我们最近报告了迄今为止进行的&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;最强大的 RCS&lt;/a>; 实验，测得的实验保真度较低，为 1.7x10&lt;sup>;-3&lt;/ support>;，理论计算成本高达 ~10&lt;sup>;23&lt;/sup>;。这些量子电路有 700 个二量子位门。我们估计这个实验需要约 47 年的时间才能在世界上最大的超级计算机上进行模拟。虽然这检查了计算应用程序所需的两个框之一——它的性能优于传统的超级计算机——但它本身并不是一个特别有用的应用程序。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;OTOC 和 Floquet 演化：局部可观测量的有效量子体积&lt;/h2>; &lt;p>; 有量子&lt;a href=&quot;https://en.wikipedia.org/wiki/Many-body_problem&quot;>;多体物理学&lt;/a>;中存在许多经典的棘手问题，因此在我们的量子上运行其中一些实验处理器潜力巨大。我们通常认为这些实验与 RCS 实验有些不同。我们通常关心的是更具体的局部物理可观测值，而不是在实验结束时测量所有量子位的量子态。由于并非电路中的每个操作都必然影响可观测量，因此局部可观测量的有效量子体积可能小于运行实验所需的完整电路的有效量子体积。 &lt;/p>; &lt;p>; 我们可以通过应用&lt;a href=&quot;https://en.wikipedia.org/wiki/Special_relativity&quot;>;相对论&lt;/a>;中的光锥概念来理解这一点，它决定了哪些事件时空之间可以存在因果关系：某些事件不可能相互影响，因为信息需要时间在它们之间传播。我们说两个这样的事件都在它们各自的光锥之外。在量子实验中，我们用一种叫做“蝴蝶锥”的东西取代了光锥，其中锥体的生长由蝴蝶速度决定，即信息在整个系统中传播的速度。 （这个速度的特征是测量 OTOC，稍后讨论。）局部可观测量的有效量子体积本质上是蝴蝶锥体的体积，仅包括与可观测量有因果关系的量子运算。因此，信息在系统中传播得越快，有效体积就越大，因此经典模拟就越困难。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY -WpDN34wUfig-rtH1NC7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/s1392/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1392&quot; data-original-width=&quot;1092&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY-WpDN34wUfig-rtH1NC 7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/w502-h640/image2.png&quot; width=&quot;502&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;描述门的有效体积 V&lt;sub>;eff&lt;/sub>;局部可观测量 B。称为有效面积 A&lt;sub>;eff&lt;/sub>; 的相关量由平面和圆锥体的横截面表示。底座的周长对应于以蝴蝶速度v&lt;sub>;B&lt;/sub>;移动的信息传播的前端。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>;我们将此框架应用于最近的&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;实验&lt;/a>;，实现所谓的 Floquet Ising 模型（一种物理模型）与时间晶体和马约拉纳实验有关。根据该实验的数据，可以直接估计最大电路的有效保真度为 0.37。由于测得的门错误率为 ~1%，因此估计的有效体积为 ~100。这比光锥小得多，光锥包括 127 个量子位上的 2000 个门。所以，本次实验的蝴蝶速度相当小。事实上，我们认为，使用获得比实验更高精度的数值模拟，有效体积仅覆盖约 28 个量子位，而不是 127 个。这种小的有效体积也已通过 OTOC 技术&lt;a href=&quot;https://arxiv.org/abs/2306.17839&quot;>;证实&lt;/a>;。尽管这是一个深度电路，但估计的计算成本为 5x10&lt;sup>;11&lt;/sup>;，几乎比最近的 RCS 实验少了一万亿倍。相应地，该实验可以在单个 A100 GPU 上&lt;a href=&quot;https://arxiv.org/abs/2306.15970&quot;>;模拟&lt;/a>;每个数据点不到一秒的时间。因此，虽然这确实是一个有用的应用程序，但它并没有满足计算应用程序的第二个要求：大大优于经典模拟。 &lt;/p>; &lt;p>; OTOC 的信息置乱实验是计算应用的一个有前途的途径。 OTOC 可以告诉我们有关系统的重要物理信息，例如蝶形速度，这对于精确测量电路的有效量子体积至关重要。使用快速纠缠门的 OTOC 实验为首次超越经典的量子处理器计算应用演示提供了潜在的途径。事实上，在 2021 年的&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>;实验&lt;/a>;中，我们实现了 F&lt;sub>;eff &lt;/sub>; 的有效保真度~ 0.06，实验信噪比约为 1，对应于 ~250 个门的有效体积和 2x10&lt;sup>;12&lt;/sup>; 的计算成本。 &lt;/p>; &lt;p>; 虽然这些早期的 OTOC 实验不够复杂，无法超越经典模拟，但 OTOC 实验成为计算应用首次演示的良好候选者有其深刻的物理原因。近期量子处理器可以实现的大多数有趣的量子现象很难经典地模拟，这与探索许多量子能级的量子电路相对应。这种演化通常是混乱的，并且标准时间顺序相关器（TOC）在这种情况下很快衰减到纯随机平均值。没有留下任何实验信号。 &lt;a href=&quot;https://arxiv.org/abs/2101.08870&quot;>;OTOC 测量&lt;/a>;不会发生这种情况，它允许我们随意增加复杂性，仅受每个门的错误的限制。我们预计，将错误率降低一半将使计算成本加倍，从而将该实验推向超越经典的阶段。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;使用我们开发的有效量子体积框架，我们确定我们的 RCS 和 OTOC 实验以及最近的 Floquet 进化实验的计算成本。虽然这些都无法满足计算应用的要求，但我们预计，随着错误率的提高，OTOC 实验将成为量子处理器的第一个超越经典的有用应用。 &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6350776943545232437/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html&quot; rel=&quot;alternate&quot; title=&quot;How to compare a noisy quantum processor to a classical computer&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qsEcLuEDPnWZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3589347607411181063&lt;/id>;&lt;published>;2023-08-24T12:33:00.002-07:00&lt;/published>;&lt;updated>;2023-08-24T12:33:37.720-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Teaching language models to reason algorithmically&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hattie Zhou, Graduate Student at MILA, Hanie Sedghi, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Large language models (LLMs), such as &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, have shown impressive progress in recent years, which have been driven by &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;scaling up models&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;>;training data sizes&lt;/a>;. Nonetheless, a long standing debate has been whether LLMs can reason symbolically (ie, manipulating symbols based on logical rules). For example, LLMs are able to perform simple arithmetic operations when numbers are small, but struggle to perform with large numbers. This suggests that LLMs have not learned the underlying rules needed to perform these arithmetic operations. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While neural networks have powerful &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&quot;>;pattern matching capabilities&lt;/a>;, they are prone to overfitting to spurious statistical patterns in the data. This does not hinder good performance when the training data is large and diverse and the evaluation is in-distribution. However, for tasks that require rule-based reasoning (such as addition), LLMs struggle with out-of-distribution generalization as spurious correlations in the training data are often much easier to exploit than the true rule-based solution. As a result, despite significant progress in a variety of natural language processing tasks, performance on simple arithmetic tasks like addition has remained a challenge. Even with modest improvement of &lt;a href=&quot;https://openai.com/research/gpt-4&quot;>;GPT-4&lt;/a>; on the &lt;a href=&quot;https://arxiv.org/abs/2103.03874&quot;>;MATH&lt;/a>; dataset, &lt;a href=&quot;https://arxiv.org/abs/2303.12712&quot;>;errors are still largely due to arithmetic and calculation mistakes&lt;/a>;. Thus, an important question is whether LLMs are capable of algorithmic reasoning, which involves solving a task by applying a set of abstract rules that define the algorithm. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;Teaching Algorithmic Reasoning via In-Context Learning&lt;/a>;”, we describe an approach that leverages &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; to enable algorithmic reasoning capabilities in LLMs. In-context learning refers to a model&#39;s ability to perform a task after seeing a few examples of it within the context of the model. The task is specified to the model using a prompt, without the need for weight updates. We also present a novel algorithmic prompting technique that enables general purpose language models to achieve strong &lt;a href=&quot;https://arxiv.org/abs/2207.04901&quot;>;generalization&lt;/a>; on arithmetic problems that are more difficult than those seen in the prompt. Finally, we demonstrate that a model can reliably execute algorithms on out-of-distribution examples with an appropriate choice of prompting strategy. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/s1200/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;1200&quot; height=&quot;89&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/w640-h89/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;By providing algorithmic prompts, we can teach a model the rules of arithmetic via in-context learning. In this example, the LLM (word predictor) outputs the correct answer when prompted with an easy addition question (eg, 267+197), but fails when asked a similar addition question with longer digits. However, when the more difficult question is appended with an algorithmic prompt for addition (blue box with white +&lt;strong>; &lt;/strong>;shown below the word predictor), the model is able to answer correctly. Moreover, the model is capable of simulating the multiplication algorithm (&lt;strong>;X&lt;/strong>;) by composing a series of addition calculations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Teaching an algorithm as a skill&lt;/h2>; &lt;p>; In order to teach a model an algorithm as a skill, we develop algorithmic prompting, which builds upon other rationale-augmented approaches (eg, &lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpad&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>;). Algorithmic prompting extracts algorithmic reasoning abilities from LLMs, and has two notable distinctions compared to other prompting approaches: (1) it solves tasks by outputting the steps needed for an algorithmic solution, and (2) it explains each algorithmic step with sufficient detail so there is no room for misinterpretation by the LLM. &lt;/p>; &lt;p>; To gain intuition for algorithmic prompting, let&#39;s consider the task of two-number addition. In a scratchpad-style prompt, we process each digit from right to left and keep track of the carry value (ie, we add a 1 to the next digit if the current digit is greater than 9) at each step. However, the rule of carry is ambiguous after seeing only a few examples of carry values. We find that including explicit equations to describe the rule of carry helps the model focus on the relevant details and interpret the prompt more accurately. We use this insight to develop an algorithmic prompt for two-number addition, where we provide explicit equations for each step of computation and describe various indexing operations in non-ambiguous formats. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/s1584/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1250&quot; data-original-width=&quot;1584&quot; height=&quot;505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/w640-h505/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of various prompt strategies for addition.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Using only three prompt examples of addition with answer length up to five digits, we evaluate performance on additions of up to 19 digits. Accuracy is measured over 2,000 total examples sampled uniformly over the length of the answer. As shown below, the use of algorithmic prompts maintains high accuracy for questions significantly longer than what&#39;s seen in the prompt, which demonstrates that the model is indeed solving the task by executing an input-agnostic algorithm. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s1600/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;548&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Test accuracy on addition questions of increasing length for different prompting methods. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Leveraging algorithmic skills as tool use&lt;/h2>; &lt;p>; To evaluate if the model can leverage algorithmic reasoning in a broader reasoning process, we evaluate performance using grade school math word problems (&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;GSM8k&lt;/a>;). We specifically attempt to replace addition calculations from GSM8k with an algorithmic solution. &lt;/p>; &lt;p>; Motivated by context length limitations and possible interference between different algorithms, we explore a strategy where differently-prompted models interact with one another to solve complex tasks. In the context of GSM8k, we have one model that specializes in informal mathematical reasoning using &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;chain-of-thought prompting&lt;/a>;, and a second model that specializes in addition using &lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;algorithmic prompting&lt;/a>;. The informal mathematical reasoning model is prompted to output specialized tokens in order to call on the addition-prompted model to perform the arithmetic steps. We extract the queries between tokens, send them to the addition-model and return the answer to the first model, after which the first model continues its output. We evaluate our approach using a difficult problem from the GSM8k (GSM8k-Hard), where we randomly select 50 addition-only questions and increase the numerical values in the questions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/s1268/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;330&quot; data-original-width=&quot;1268&quot; height=&quot;167&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/w640-h167/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example from the GSM8k-Hard dataset. The chain-of-thought prompt is augmented with brackets to indicate when an algorithmic call should be performed.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We find that using separate contexts and models with specialized prompts is an effective way to tackle GSM8k-Hard. Below, we observe that the performance of the model with algorithmic call for addition is 2.3x the chain-of-thought baseline. Finally, this strategy presents an example of solving complex tasks by facilitating interactions between LLMs specialized to different skills via in-context learning. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/s716/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;192&quot; data-original-width=&quot;716&quot; height=&quot;108&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/w400-h108/image2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Chain-of-thought (CoT) performance on GSM8k-Hard with or without algorithmic call.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present an approach that leverages &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; and a novel algorithmic prompting technique to unlock algorithmic reasoning abilities in LLMs. Our results suggest that it may be possible to transform longer context into better reasoning performance by providing more detailed explanations. Thus, these findings point to the ability of using or otherwise simulating long contexts and generating more informative rationales as promising research directions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our co-authors Behnam Neyshabur, Azade Nova, Hugo Larochelle and Aaron Courville for their valuable contributions to the paper and great feedback on the blog. We thank Tom Small for creating the animations in this post. This work was done during Hattie Zhou&#39;s internship at Google Research.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3589347607411181063/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html&quot; rel=&quot;alternate&quot; title=&quot;Teaching language models to reason algorithmically&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6641308922483151364&lt;/id>;&lt;published>;2023-08-22T11:47:00.004-07:00&lt;/published>;&lt;updated>;2023-08-24T10:34:33.888-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Language to rewards for robotic skill synthesis&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Wenhao Yu and Fei Xia, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s320/Language%20to%20reward.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Empowering end-users to interactively teach robots to perform novel tasks is a crucial capability for their successful integration into real-world applications. For example, a user may want to teach a robot dog to perform a new trick, or teach a manipulator robot how to organize a lunch box based on user preferences. The recent advancements in &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;large language models&lt;/a>; (LLMs) pre-trained on extensive internet data have shown a promising path towards achieving this goal. Indeed, researchers have explored diverse ways of leveraging LLMs for robotics, from &lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;step-by-step planning&lt;/a>; and &lt;a href=&quot;https://interactive-language.github.io/&quot;>;goal-oriented dialogue&lt;/a>; to &lt;a href=&quot;https://ai.googleblog.com/2022/11/robots-that-write-their-own-code.html&quot;>;robot-code-writing agents&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While these methods impart new modes of compositional generalization, they focus on using language to link together new behaviors from an &lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;existing library of control primitives&lt;/a>; that are either manually engineered or learned &lt;em>;a priori&lt;/em>;. Despite having internal knowledge about robot motions, LLMs struggle to directly output low-level robot commands due to the limited availability of relevant training data. As a result, the expression of these methods are bottlenecked by the breadth of the available primitives, the design of which often requires extensive expert knowledge or massive data collection. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2306.08647&quot;>;Language to Rewards for Robotic Skill Synthesis&lt;/a>;”, we propose an approach to enable users to teach robots novel actions through natural language input. To do so, we leverage reward functions as an interface that bridges the gap between language and low-level robot actions. We posit that reward functions provide an ideal interface for such tasks given their richness in semantics, modularity, and interpretability. They also provide a direct connection to low-level policies through black-box optimization or reinforcement learning (RL). We developed a language-to-reward system that leverages LLMs to translate natural language user instructions into reward-specifying code and then applies &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; to find optimal low-level robot actions that maximize the generated reward function. We demonstrate our language-to-reward system on a variety of robotic control tasks in simulation using a quadruped robot and a dexterous manipulator robot. We further validate our method on a physical robot manipulator. &lt;/p>; &lt;p>; The language-to-reward system consists of two core components: (1) a Reward Translator, and (2) a Motion Controller&lt;em>;. &lt;/em>;The&lt;em>; &lt;/em>;Reward Translator maps natural language instruction from users to reward functions represented as &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;python code&lt;/a>;. The Motion Controller optimizes the given reward function using &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;receding horizon optimization&lt;/a>; to find the optimal low-level robot actions, such as the amount of torque that should be applied to each robot motor. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s720/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLMs cannot directly generate low-level robotic actions due to lack of data in pre-training dataset. We propose to use reward functions to bridge the gap between language and low-level robot actions, and enable novel complex robot motions from natural language instructions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Reward Translator: Translating user instructions to reward functions&lt;/h2>; &lt;p>; The Reward Translator module was built with the goal of mapping natural language user instructions to reward functions. Reward tuning is highly domain-specific and requires expert knowledge, so it was not surprising to us when we found that LLMs trained on generic language datasets are unable to directly generate a reward function for a specific hardware. To address this, we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;in-context learning&lt;/a>; ability of LLMs. Furthermore, we split the Reward Translator into two sub-modules: &lt;em>;Motion Descriptor&lt;/em>; and &lt;em>;Reward Coder&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Motion Descriptor&lt;/h3>; &lt;p>; First, we design a Motion Descriptor that interprets input from a user and expands it into a natural language description of the desired robot motion following a predefined template. This Motion Descriptor turns potentially ambiguous or vague user instructions into more specific and descriptive robot motions, making the reward coding task more stable. Moreover, users interact with the system through the motion description field, so this also provides a more interpretable interface for users compared to directly showing the reward function. &lt;/p>; &lt;p>; To create the Motion Descriptor, we use an LLM to translate the user input into a detailed description of the desired robot motion. We design &lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_motion_descriptor.txt&quot;>;prompts&lt;/a>; that guide the LLMs to output the motion description with the right amount of details and format. By translating a vague user instruction into a more detailed description, we are able to more reliably generate the reward function with our system. This idea can also be potentially applied more generally beyond robotics tasks, and is relevant to &lt;a href=&quot;https://innermonologue.github.io/&quot;>;Inner-Monologue&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought prompting&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Reward Coder &lt;/h3>; &lt;p>; In the second stage, we use the same LLM from Motion Descriptor for Reward Coder, which translates generated motion description into the reward function. Reward functions are represented using python code to benefit from the LLMs&#39; knowledge of reward, coding, and code structure. &lt;/p>; &lt;p>; Ideally, we would like to use an LLM to directly generate a reward function &lt;i>;R&lt;/i>; (&lt;i>;s&lt;/i>;, &lt;i>;t&lt;/i>;) that maps the robot state &lt;i>;s&lt;/i>; and time &lt;i>;t&lt;/i>; into a scalar reward value. However, generating the correct reward function from scratch is still a challenging problem for LLMs and correcting the errors requires the user to understand the generated code to provide the right feedback. As such, we pre-define a set of reward terms that are commonly used for the robot of interest and allow LLMs to composite different reward terms to formulate the final reward function. To achieve this, we design a &lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_reward_coder.txt&quot;>;prompt&lt;/a>; that specifies the reward terms and guide the LLM to generate the correct reward function for the task.&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s1293/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;1293&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The internal structure of the Reward Translator, which is tasked to map user inputs to reward functions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Motion Controller: Translating reward functions to robot actions&lt;/h2>; &lt;p>; The Motion Controller takes the reward function generated by the Reward Translator and synthesizes a controller that maps robot observation to low-level robot actions. To do this, we formulate the controller synthesis problem as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;>;Markov decision process&lt;/a>; (MDP), which can be solved using different strategies, including RL, &lt;a href=&quot;https://en.wikipedia.org/wiki/Trajectory_optimization&quot;>;offline trajectory optimization&lt;/a>;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;model predictive control&lt;/a>; (MPC). Specifically, we use an open-source implementation based on the &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; (MJPC). &lt;/p>; &lt;p>; MJPC has demonstrated the interactive creation of diverse behaviors, such as legged locomotion, grasping, and finger-gaiting, while supporting multiple planning algorithms, such as &lt;a href=&quot;https://homes.cs.washington.edu/~todorov/papers/TodorovACC05.pdf&quot;>;iterative linear–quadratic–Gaussian&lt;/a>; (iLQG) and &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;predictive sampling&lt;/a>;. More importantly, the frequent re-planning in MJPC empowers its robustness to uncertainties in the system and enables an interactive motion synthesis and correction system when combined with LLMs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Examples &lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Robot dog&lt;/h3>; &lt;p>; In the first example, we apply the language-to-reward system to a simulated quadruped robot and teach it to perform various skills. For each skill, the user will provide a concise instruction to the system, which will then synthesize the robot motion by using reward functions as an intermediate interface. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_quadruped_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Dexterous manipulator&lt;/h3>; &lt;p>; We then apply the language-to-reward system to a dexterous manipulator robot to perform a variety of manipulation tasks. The dexterous manipulator has 27 degrees of freedom, which is very challenging to control. Many of these tasks require manipulation skills beyond grasping, making it difficult for pre-designed primitives to work. We also include an example where the user can interactively instruct the robot to place an apple inside a drawer. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_manipulation_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Validation on real robots&lt;/h2>; &lt;p>; We also validate the language-to-reward method using a real-world manipulation robot to perform tasks such as picking up objects and opening a drawer. To perform the optimization in Motion Controller, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/ARTag&quot;>;AprilTag&lt;/a>;, a fiducial marker system, and &lt;a href=&quot;https://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot;>;F-VLM&lt;/a>;, an open-vocabulary object detection tool, to identify the position of the table and objects being manipulated. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;yes&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/real/l2r_demo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we describe a new paradigm for interfacing an LLM with a robot through reward functions, powered by a low-level model predictive control tool, MuJoCo MPC. Using reward functions as the interface enables LLMs to work in a semantic-rich space that plays to the strengths of LLMs, while ensuring the expressiveness of the resulting controller. To further improve the performance of the system, we propose to use a structured motion description template to better extract internal knowledge about robot motions from LLMs. We demonstrate our proposed system on two simulated robot platforms and one real robot for both locomotion and manipulation tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our co-authors Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, and Yuval Tassa for their help and support in various aspects of the project. We would also like to acknowledge Ken Caluwaerts, Kristian Hartikainen, Steven Bohez, Carolina Parada, Marc Toussaint, and the teams at Google DeepMind for their feedback and contributions.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6641308922483151364/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html&quot; rel=&quot;alternate&quot; title=&quot;Language to rewards for robotic skill synthesis&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s72-c/Language%20to%20reward.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6990000750711940626&lt;/id>;&lt;published>;2023-08-21T00:33:00.002-07:00&lt;/published>;&lt;updated>;2023-08-21T00:35:57.579-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Interspeech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at Interspeech 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJVLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s1075/Interspeech2023-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the 24th Annual &lt;a href=&quot;https://interspeech2023.org/&quot;>;Conference of the International Speech Communication Association&lt;/a>; (INTERSPEECH 2023) is being held in Dublin, Ireland, representing one of the world&#39;s most extensive conferences on research and technology of spoken language understanding and processing. Experts in speech-related research fields gather to take part in oral presentations and poster sessions and to build collaborations across the globe. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to be a &lt;a href=&quot;https://interspeech2023.org/sponsorship-exhibition/sponsors/&quot;>;Platinum Sponsor&lt;/a>; of INTERSPEECH 2023, where we will be showcasing more than 20 research publications and supporting a number of workshops and special sessions. We welcome in-person attendees to drop by the Google Research booth to meet our researchers and participate in Q&amp;amp;As and demonstrations of some of our latest speech technologies, which help to improve accessibility and provide convenience in communication for billions of users. In addition, online attendees are encouraged to visit our &lt;a href=&quot;http://topia.io/google-research&quot;>;virtual booth in Topia&lt;/a>; where you can get up-to-date information on research and opportunities at Google. Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions). You can also learn more about the Google research being presented at INTERSPEECH 2023 below (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; ISCA Board, Technical Committee Chair: &lt;strong>;&lt;em>;Bhuvana Ramabhadran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Area Chairs include: &lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Analysis of Speech and Audio Signals: &lt;strong>;&lt;em>;Richard Rose&lt;/em>;&lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Speech Synthesis and Spoken Language Generation: &lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Special Areas: &lt;strong>;&lt;em>;Tara Sainath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Satellite events&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/competition2023.html&quot;>;VoxCeleb Speaker Recognition Challenge 2023&lt;/a>; (VoxSRC-23)&lt;br />; Organizers include: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ssw2023.org/&quot;>;ISCA Speech Synthesis Workshop&lt;/a>; (SSW12)&lt;br />; Speakers include: &lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Keynote talk – ISCA Medalist&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/narayanan23_interspeech.pdf&quot;>;Bridging Speech Science and Technology — Now and Into the Future&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Shrikanth Narayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Survey Talk&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Speech Compression in the AI Era&lt;br />; Speaker: &lt;strong>;&lt;em>;Jan Skoglund&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Special session papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rose23_interspeech.pdf&quot;>;Cascaded Encoders for Fine-Tuning ASR Models on Overlapped Speech&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Richard Rose&lt;/b>;, &lt;b>;Oscar Chang&lt;/b>;, &lt;b>;Olivier Siohan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/erdogan23_interspeech.pdf&quot;>;TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Hakan Erdogan&lt;/b>;, &lt;b>;Scott Wisdom&lt;/b>;, Xuankai Chang*, &lt;b>;Zalán Borsos&lt;/b>;, &lt;b>;Marco Tagliasacchi&lt;/b>;, &lt;b>;Neil Zeghidour&lt;/b>;, &lt;b>;John R. Hershey&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/liang23d_interspeech.pdf&quot;>;DeePMOS: Deep Posterior Mean-Opinion-Score of Speech&lt;/a>;&lt;br />; &lt;em>;Xinyu Liang, Fredrik Cumlin,&lt;strong>; Christian Schüldt&lt;/strong>;, Saikat Chatterjee&lt;strong>;&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/baskar23_interspeech.pdf&quot;>;O-1: Self-Training with Oracle and 1-Best Hypothesis&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Murali Karthick Baskar&lt;/b>;, &lt;b>;Andrew Rosenberg&lt;/b>;, &lt;b>;Bhuvana Ramabhadran&lt;/b>;, &lt;b>;Kartik Audhkhasi&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/huo23b_interspeech.pdf&quot;>;Re-investigating the Efficient Transfer Learning of Speech Foundation Model Using Feature Fusion Methods&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Zhouyuan Huo&lt;/b>;, &lt;b>;Khe Chai Sim&lt;/b>;, &lt;b>;Dongseong Hwang&lt;/b>;, &lt;b>;Tsendsuren Munkhdalai&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;,&lt;b>; Pedro Moreno&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/camp23_interspeech.pdf&quot;>;MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Joshua Camp&lt;/b>;, &lt;b>;Tom Kenter&lt;/b>;, &lt;b>;Lev Finkelstein&lt;/b>;, &lt;b>;Rob Clark&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/gong23c_interspeech.pdf&quot;>;LanSER: Language-Model Supported Speech Emotion Recognition&lt;/a>;&lt;br />; &lt;em>;Taesik Gong,&lt;strong>; Josh Belanich&lt;/strong>;, &lt;strong>;Krishna Somandepalli&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Brian Eoff&lt;/strong>;, &lt;strong>;Brendan Jou&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23fa_interspeech.pdf&quot;>;Modular Domain Adaptation for Conformer-Based Streaming ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Qiujia Li&lt;/b>;, &lt;b>;Bo Li&lt;/b>;, &lt;b>;Dongseong Hwang&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;, &lt;b>;Pedro M. Mengibar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/panchapagesan23_interspeech.pdf&quot;>;On Training a Neural Residual Acoustic Echo Suppressor for Improved ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Sankaran Panchapagesan&lt;/b>;, &lt;b>;Turaj Zakizadeh Shabestary&lt;/b>;, &lt;b>;Arun Narayanan&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/eisenstein23_interspeech.pdf&quot;>;MD3: The Multi-dialect Dataset of Dialogues&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Jacob Eisenstein&lt;/b>;, &lt;b>;Vinodkumar Prabhakaran&lt;/b>;, &lt;b>;Clara Rivera&lt;/b>;, Dorottya Demszky, Devyani Sharma&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/wu23e_interspeech.pdf&quot;>;Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Zelin Wu&lt;/b>;, &lt;b>;Tsendsuren Munkhdalai&lt;/b>;, &lt;b>;Pat Rondon&lt;/b>;, &lt;b>;Golan Pundak&lt;/b>;, &lt;b>;Khe Chai Sim&lt;/b>;, &lt;b>;Christopher Li&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/blau23_interspeech.pdf&quot;>;Using Text Injection to Improve Recognition of Personal Identifiers in Speech&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yochai Blau&lt;/b>;, &lt;b>;Rohan Agrawal&lt;/b>;, &lt;b>;Lior Madmony&lt;/b>;, &lt;b>;Gary Wang&lt;/b>;, &lt;b>;Andrew Rosenberg&lt;/b>;, &lt;b>;Zhehuai Chen&lt;/b>;, &lt;b>;Zorik Gekhman&lt;/b>;, &lt;b>;Genady Beryozkin&lt;/b>;, &lt;b>;Parisa Haghani&lt;/b>;, &lt;b>;Bhuvana Ramabhadran&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23j_interspeech.pdf&quot;>;How to Estimate Model Transferability of Pre-trained Speech Models?&lt;/a>;&lt;br />; &lt;em>;Zih-Ching Chen, Chao-Han Huck Yang*, &lt;strong>;Bo Li&lt;/strong>;, &lt;strong>;Yu Zhang&lt;/strong>;, &lt;strong>;Nanxin Chen&lt;/strong>;, &lt;strong>;Shuo-yiin Chang&lt;/strong>;, &lt;strong>;Rohit Prabhavalkar, &lt;/strong>;Hung-yi Lee, &lt;strong>;Tara N. Sainath&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/peyser23_interspeech.pdf&quot;>;Improving Joint Speech-Text Representations Without Alignment&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Cal Peyser&lt;/b>;, &lt;b>;Zhong Meng&lt;/b>;, &lt;b>;Ke Hu&lt;/b>;, &lt;b>;Rohit Prabhavalkar&lt;/b>;, &lt;b>;Andrew Rosenberg&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;, Michael Picheny, Kyunghyun Cho &lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/bijwadia23_interspeech.pdf&quot;>;Text Injection for Capitalization and Turn-Taking Prediction in Speech Models&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shaan Bijwadia&lt;/b>;, &lt;b>;Shuo-yiin Chang&lt;/b>;, &lt;b>;Weiran Wang&lt;/b>;, &lt;b>;Zhong Meng&lt;/b>;, &lt;b>;Hao Zhang&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23_interspeech.pdf&quot;>;Streaming Parrotron for On-Device Speech-to-Speech Conversion&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;, &lt;b>;Fadi Biadsy&lt;/b>;, &lt;b>;Xia Zhang&lt;/b>;, &lt;b>;Liyang Jiang&lt;/b>;, &lt;b>;Phoenix Meadowlark&lt;/b>;, &lt;b>;Shivani Agrawal&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/huang23b_interspeech.pdf&quot;>;Semantic Segmentation with Bidirectional Language Models Improves Long-Form ASR&lt;/a>;&lt;br />; &lt;strong>;W. Ronny Huang&lt;/strong>;, &lt;strong>;Hao Zhang&lt;/strong>;, &lt;strong>;Shankar Kumar&lt;/strong>;, &lt;strong>;Shuo-yiin Chang&lt;/strong>;, &lt;strong>;Tara N. Sainath&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/taguchi23_interspeech.pdf&quot;>;Universal Automatic Phonetic Transcription into the International Phonetic Alphabet&lt;/a>;&lt;br />; &lt;em>;Chihiro Taguchi, Yusuke Sakai,&lt;strong>; Parisa Haghani&lt;/strong>;, David Chiang&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/hu23c_interspeech.pdf&quot;>;Mixture-of-Expert Conformer for Streaming Multilingual ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Ke Hu&lt;/b>;, &lt;b>;Bo Li&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;, &lt;b>;Yu Zhang&lt;/b>;, &lt;b>;Francoise Beaufays&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23c_interspeech.pdf&quot;>;Real Time Spectrogram Inversion on Mobile Phone&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;, &lt;b>;Marco Tagliasacchi&lt;/b>;, &lt;b>;Yunpeng Li&lt;/b>;, &lt;b>;Liyang Jiang&lt;/b>;, &lt;b>;Xia Zhang&lt;/b>;, &lt;b>;Fadi Biadsy&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23b_interspeech.pdf&quot;>;2-Bit Conformer Quantization for Automatic Speech Recognition&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;, &lt;b>;Phoenix Meadowlark&lt;/b>;, &lt;b>;Shaojin Ding&lt;/b>;, &lt;b>;David Qiu&lt;/b>;, &lt;b>;Jian Li&lt;/b>;, &lt;b>;David Rim&lt;/b>;, &lt;b>;Yanzhang He&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/koizumi23_interspeech.pdf&quot;>;LibriTTS-R: A Restored Multi-speaker Text-to-Speech Corpus&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yuma Koizumi&lt;/b>;, &lt;b>;Heiga Zen&lt;/b>;, &lt;b>;Shigeki Karita&lt;/b>;, &lt;b>;Yifan Ding&lt;/b>;, Kohei Yatabe, &lt;b>;Nobuyuki Morioka&lt;/b>;, &lt;b>;Michiel Bacchiani&lt;/b>;, &lt;b>;Yu Zhang&lt;/b>;, &lt;b>;Wei Han&lt;/b>;, &lt;b>;Ankur Bapna&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/yu23_interspeech.pdf&quot;>;PronScribe: Highly Accurate Multimodal Phonemic Transcription from Speech and Text&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yang Yu&lt;/b>;, Matthew Perez*, &lt;b>;Ankur Bapna&lt;/b>;, &lt;b>;Fadi Haik&lt;/b>;, &lt;b>;Siamak Tazari&lt;/b>;, &lt;b>;Yu Zhang&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/vashishth23_interspeech.pdf&quot;>;Label Aware Speech Representation Learning for Language Identification&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shikhar Vashishth&lt;/b>;, &lt;b>;Shikhar Bharadwaj&lt;/b>;, &lt;b>;Sriram Ganapathy&lt;/b>;, &lt;b>;Ankur Bapna&lt;/b>;, &lt;b>;Min Ma&lt;/b>;, &lt;b>;Wei Han&lt;/b>;, &lt;b>;Vera Axelrod&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6990000750711940626/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/google-at-interspeech-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/google-at-interspeech-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at Interspeech 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJVLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s72-c/Interspeech2023-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-362086873015740792&lt;/id>;&lt;published>;2023-08-18T11:28:00.004-07:00&lt;/published>;&lt;updated>;2023-08-18T11:30:51.167-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Autonomous visual information seeking with large language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ziniu Hu, Student Researcher, and Alireza Fathi, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s2500/AVIS.png&quot; style=&quot;display: none;&quot; />; &lt;p>; There has been great progress towards adapting large language models (LLMs) to accommodate multimodal inputs for tasks including &lt;a href=&quot;https://huggingface.co/docs/transformers/main/tasks/image_captioning#:~:text=Image%20captioning%20is%20the%20task,by%20describing%20images%20to%20them.&quot;>;image captioning&lt;/a>;, &lt;a href=&quot;https://huggingface.co/tasks/visual-question-answering&quot;>;visual question answering (VQA)&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open vocabulary recognition&lt;/a>;. Despite such achievements, current state-of-the-art visual language models (VLMs) perform inadequately on visual information seeking datasets, such as &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; and &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>;, where external knowledge is required to answer the questions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of visual information seeking queries where external knowledge is required to answer the question. Images are taken from the OK-VQA dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs//2306.08129&quot;>;AVIS: Autonomous Visual Information Seeking with Large Language Models&lt;/a>;”, we introduce a novel method that achieves state-of-the-art results on visual information seeking tasks. Our method integrates LLMs with three types of tools: (i) computer vision tools for extracting visual information from images, (ii) a web search tool for retrieving open world knowledge and facts, and (iii) an image search tool to glean relevant information from metadata associated with visually similar images. AVIS employs an LLM-powered planner to choose tools and queries at each step. It also uses an LLM-powered reasoner to analyze tool outputs and extract key information. A working memory component retains information throughout the process. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoSFcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s1999/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1758&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoSFcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of AVIS&#39;s generated workflow for answering a challenging visual information seeking question. The input image is taken from the Infoseek dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Comparison to previous work&lt;/h2>; &lt;p>; Recent studies (eg, &lt;a href=&quot;https://arxiv.org/abs/2304.09842&quot;>;Chameleon&lt;/a>;, &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>; and &lt;a href=&quot;https://multimodal-react.github.io/&quot;>;MM-ReAct&lt;/a>;) explored adding tools to LLMs for multimodal inputs. These systems follow a two-stage process: planning (breaking down questions into structured programs or instructions) and execution (using tools to gather information). Despite success in basic tasks, this approach often falters in complex real-world scenarios. &lt;/p>; &lt;p>; There has also been a surge of interest in applying LLMs as autonomous agents (eg, &lt;a href=&quot;https://openai.com/research/webgpt&quot;>;WebGPT&lt;/a>; and &lt;a href=&quot;https://react-lm.github.io/&quot;>;ReAct&lt;/a>;). These agents interact with their environment, adapt based on real-time feedback, and achieve goals. However, these methods do not restrict the tools that can be invoked at each stage, leading to an immense search space. Consequently, even the most advanced LLMs today can fall into infinite loops or propagate errors. AVIS tackles this via guided LLM use, influenced by human decisions from a user study. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Informing LLM decision making with a user study&lt;/h2>; &lt;p>; Many of the visual questions in datasets such as &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; and &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; pose a challenge even for humans, often requiring the assistance of various tools and APIs. An example question from the OK-VQA dataset is shown below. We conducted a user study to understand human decision-making when using external tools. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;889&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We conducted a user study to understand human decision-making when using external tools. Image is taken from the OK-VQA dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The users were equipped with an identical set of tools as our method, including &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PALI&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Search_engine&quot;>;web search&lt;/a>;. They received input images, questions, detected object crops, and buttons linked to image search results. These buttons offered diverse information about the detected object crops, such as knowledge graph entities, similar image captions, related product titles, and identical image captions. &lt;/p>; &lt;p>; We record user actions and outputs and use it as a guide for our system in two key ways. First, we construct a transition graph (shown below) by analyzing the sequence of decisions made by users. This graph defines distinct states and restricts the available set of actions at each state. For example, at the start state, the system can take only one of these three actions: PALI caption, PALI VQA, or object detection. Second, we use the examples of human decision-making to guide our planner and reasoner with relevant contextual instances to enhance the performance and effectiveness of our system. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/s1146/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;845&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/w472-h640/image7.png&quot; width=&quot;472&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS transition graph.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;General framework&lt;/h2>; &lt;p>; Our approach employs a dynamic decision-making strategy designed to respond to visual information-seeking queries. Our system has three primary components. First, we have a &lt;em>;planner&lt;/em>; to determine the subsequent action, including the appropriate API call and the query it needs to process. Second, we have a &lt;em>;working memory&lt;/em>; that retains information about the results obtained from API executions. Last, we have a &lt;em>;reasoner&lt;/em>;, whose role is to process the outputs from the API calls. It determines whether the obtained information is sufficient to produce the final response, or if additional data retrieval is required. &lt;/p>; &lt;p>; The planner undertakes a series of steps each time a decision is required regarding which tool to employ and what query to send to it. Based on the present state, the planner provides a range of potential subsequent actions. The potential action space may be so large that it makes the search space intractable. To address this issue, the planner refers to the transition graph to eliminate irrelevant actions. The planner also excludes the actions that have already been taken before and are stored in the working memory. &lt;/p>; &lt;p>; Next, the planner collects a set of relevant in-context examples that are assembled from the decisions previously made by humans during the user study. With these examples and the working memory that holds data collected from past tool interactions, the planner formulates a prompt. The prompt is then sent to the LLM, which returns a structured answer, determining the next tool to be activated and the query to be dispatched to it. This design allows the planner to be invoked multiple times throughout the process, thereby facilitating dynamic decision-making that gradually leads to answering the input query. &lt;/p>; &lt;p>; We employ a reasoner to analyze the output of the tool execution, extract the useful information and decide into which category the tool output falls: informative, uninformative, or final answer. Our method utilizes the LLM with appropriate prompting and in-context examples to perform the reasoning. If the reasoner concludes that it&#39;s ready to provide an answer, it will output the final response, thus concluding the task. If it determines that the tool output is uninformative, it will revert back to the planner to select another action based on the current state. If it finds the tool output to be useful, it will modify the state and transfer control back to the planner to make a new decision at the new state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1469&quot; data-original-width=&quot;1999&quot; height=&quot;470&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/w640-h470/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS employs a dynamic decision-making strategy to respond to visual information-seeking queries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate AVIS on &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; and &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; datasets. As shown below, even robust visual-language models, such as &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;, fail to yield high accuracy when fine-tuned on Infoseek. Our approach (AVIS), without fine-tuning, achieves 50.7% accuracy on the unseen entity split of this dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS visual question answering results on Infoseek dataset. AVIS achieves higher accuracy in comparison to previous baselines based on &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results on the OK-VQA dataset are shown below. AVIS with few-shot in-context examples achieves an accuracy of 60.2%, higher than most of the previous works. AVIS achieves lower but comparable accuracy in comparison to the PALI model fine-tuned on OK-VQA. This difference, compared to Infoseek where AVIS outperforms fine-tuned PALI, is due to the fact that most question-answer examples in OK-VQA rely on common sense knowledge rather than on fine-grained knowledge. Therefore, PaLI is able to encode such generic knowledge in the model parameters and doesn&#39;t require external knowledge. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual question answering results on A-OKVQA. AVIS achieves higher accuracy in comparison to previous works that use few-shot or zero-shot learning, including &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; and &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>;. AVIS also achieves higher accuracy than most of the previous works that are fine-tuned on OK-VQA dataset, including &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;REVEAL&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2206.01201&quot;>;ReVIVE&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.08614&quot;>;KAT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>;, and achieves results that are close to the fine-tuned &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present a novel approach that equips LLMs with the ability to use a variety of tools for answering knowledge-intensive visual questions. Our methodology, anchored in human decision-making data collected from a user study, employs a structured framework that uses an LLM-powered planner to dynamically decide on tool selection and query formation. An LLM-powered reasoner is tasked with processing and extracting key information from the output of the selected tool. Our method iteratively employs the planner and reasoner to leverage different tools until all necessary information required to answer the visual question is amassed. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A. Ross, Cordelia Schmid and Alireza Fathi.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/362086873015740792/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous-visual-information-seeking.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous-visual-information-seeking.html&quot; rel=&quot;alternate&quot; title=&quot;Autonomous visual information seeking with large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s72-c/AVIS.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4779594044050650231&lt;/id>;&lt;published>;2023-08-17T11:08:00.000-07:00&lt;/published>;&lt;updated>;2023-08-17T11:08:24.346-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Neural network pruning with combinatorial optimization&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hussein Hazimeh, Research Scientist, Athena Team, and Riade Benbaki, Graduate Student at MIT&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s320/CHITA%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Modern neural networks have achieved impressive performance across a variety of applications, such as &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;language, mathematical reasoning&lt;/a>;, and &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;vision&lt;/a>;. However, these networks often use large architectures that require lots of computational resources. This can make it impractical to serve such models to users, especially in resource-constrained environments like wearables and smartphones. A &lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;widely used approach&lt;/a>; to mitigate the inference costs of pre-trained networks is to prune them by removing some of their weights, in a way that doesn&#39;t significantly affect utility. In standard neural networks, each weight defines a connection between two neurons. So after weights are pruned, the input will propagate through a smaller set of connections and thus requires less computational resources. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s1600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Original network vs. a pruned network.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Pruning methods can be applied at different stages of the network&#39;s training process: post, during, or before training (ie, immediately after weight initialization). In this post, we focus on the post-training setting: given a pre-trained network, how can we determine which weights should be pruned? One popular method is &lt;a href=&quot;https://jmlr.org/papers/volume22/21-0366/21-0366.pdf&quot;>;magnitude pruning&lt;/a>;, which removes weights with the smallest magnitude. While efficient, this method doesn&#39;t directly consider the effect of removing weights on the network&#39;s performance. Another popular paradigm is &lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;optimization-based pruning&lt;/a>;, which removes weights based on how much their removal impacts the loss function. Although conceptually appealing, most existing optimization-based approaches seem to face a serious tradeoff between performance and computational requirements. Methods that make crude approximations (eg, assuming a diagonal &lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;>;Hessian matrix&lt;/a>;) can scale well, but have relatively low performance. On the other hand, while methods that make fewer approximations tend to perform better, they appear to be much less scalable. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2302.14623&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>;”, presented at &lt;a href=&quot;https://icml.cc/Conferences/2023/&quot;>;ICML 2023&lt;/a>;, we describe how we developed an optimization-based approach for pruning pre-trained neural networks at scale. CHITA (which stands for “Combinatorial Hessian-free Iterative Thresholding Algorithm”) outperforms existing pruning methods in terms of scalability and performance tradeoffs, and it does so by leveraging advances from several fields, including &lt;a href=&quot;https://en.wikipedia.org/wiki/High-dimensional_statistics&quot;>;high-dimensional statistics&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;combinatorial optimization&lt;/a>;, and neural network pruning. For example, CHITA can be 20x to 1000x faster than state-of-the-art methods for pruning &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; and improves accuracy by over 10% in many settings. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overview of contributions&lt;/h2>; &lt;p>; CHITA has two notable technical improvements over popular methods: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Efficient use of second-order information&lt;/strong>;: Pruning methods that use second-order information (ie, relating to &lt;a href=&quot;https://en.wikipedia.org/wiki/Second_derivative&quot;>;second derivatives&lt;/a>;) achieve the state of the art in many settings. In the literature, this information is typically used by computing the Hessian matrix or its inverse, an operation that is very difficult to scale because the Hessian size is quadratic with respect to the number of weights. Through careful reformulation, CHITA uses second-order information without having to compute or store the Hessian matrix explicitly, thus allowing for more scalability. &lt;/li>;&lt;li>;&lt;strong>;Combinatorial optimization&lt;/strong>;: Popular optimization-based methods use a simple optimization technique that prunes weights in isolation, ie, when deciding to prune a certain weight they don&#39;t take into account whether other weights have been pruned. This could lead to pruning important weights because weights deemed unimportant in isolation may become important when other weights are pruned. CHITA avoids this issue by using a more advanced, combinatorial optimization algorithm that takes into account how pruning one weight impacts others. &lt;/li>; &lt;/ul>; &lt;p>; In the sections below, we discuss CHITA&#39;s pruning formulation and algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A computation-friendly pruning formulation&lt;/h2>; &lt;p>; There are many possible pruning candidates, which are obtained by retaining only a subset of the weights from the original network. Let &lt;em>;k&lt;/em>; be a user-specified parameter that denotes the number of weights to retain. Pruning can be naturally formulated as a &lt;a href=&quot;https://arxiv.org/abs/1803.01454&quot;>;best-subset selection&lt;/a>; (BSS) problem: among all possible pruning candidates (ie, subsets of weights) with only &lt;em>;k&lt;/em>; weights retained, the candidate that has the smallest loss is selected. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s1600/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Pruning as a BSS problem: among all possible pruning candidates with the same total number of weights, the best candidate is defined as the one with the least loss. This illustration shows four candidates, but this number is generally much larger.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Solving the pruning BSS problem on the original loss function is generally computationally intractable. Thus, similar to previous work, such as &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf&quot;>;OBD&lt;/a>; and &lt;a href=&quot;https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&quot;>;OBS&lt;/a>;, we approximate the loss with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Quadratic_form&quot;>;quadratic function&lt;/a>; by using a second-order &lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor_series&quot;>;Taylor series&lt;/a>;, where the Hessian is estimated with the empirical &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;>;Fisher information matrix&lt;/a>;. While gradients can be typically computed efficiently, computing and storing the Hessian matrix is prohibitively expensive due to its sheer size. In the literature, it is common to deal with this challenge by making restrictive assumptions on the Hessian (eg, diagonal matrix) and also on the algorithm (eg, pruning weights in isolation). &lt;/p>; &lt;p>; CHITA uses an efficient reformulation of the pruning problem (BSS using the quadratic loss) that avoids explicitly computing the Hessian matrix, while still using all the information from this matrix. This is made possible by exploiting the low-&lt;a href=&quot;https://en.wikipedia.org/wiki/Rank_(linear_algebra)&quot;>;rank&lt;/a>; structure of the empirical Fisher information matrix. This reformulation can be viewed as a sparse &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;>;linear regression&lt;/a>; problem, where each regression coefficient corresponds to a certain weight in the neural network. After obtaining a solution to this regression problem, coefficients set to zero will correspond to weights that should be pruned. Our regression data matrix is (&lt;em>;n&lt;/em>; x &lt;em>;p&lt;/em>;), where &lt;em>;n&lt;/em>; is the batch (sub-sample) size and &lt;em>;p&lt;/em>; is the number of weights in the original network. Typically &lt;em>;n&lt;/em>; &amp;lt;&amp;lt; &lt;em>;p&lt;/em>;, so storing and operating with this data matrix is much more scalable than common pruning approaches that operate with the (&lt;em>;p&lt;/em>; x &lt;em>;p&lt;/em>;) Hessian. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s1601/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;CHITA reformulates the quadratic loss approximation, which requires an expensive Hessian matrix, as a linear regression (LR) problem. The LR&#39;s data matrix is linear in &lt;em>;p&lt;/em>;, which makes the reformulation more scalable than the original quadratic approximation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scalable optimization algorithms&lt;/h2>; &lt;p>; CHITA reduces pruning to a linear regression problem under the following sparsity constraint: at most &lt;em>;k&lt;/em>; regression coefficients can be nonzero. To obtain a solution to this problem, we consider a modification of the well-known &lt;a href=&quot;https://arxiv.org/pdf/0805.0510.pdf&quot;>;iterative hard thresholding&lt;/a>; (IHT) algorithm. IHT performs &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;>;gradient descent&lt;/a>; where after each update the following post-processing step is performed: all regression coefficients outside the Top-&lt;em>;k&lt;/em>; (ie, the &lt;em>;k&lt;/em>; coefficients with the largest magnitude) are set to zero. IHT typically delivers a good solution to the problem, and it does so iteratively exploring different pruning candidates and jointly optimizing over the weights. &lt;/p>; &lt;p>; Due to the scale of the problem, standard IHT with constant &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;>;learning rate&lt;/a>; can suffer from very slow convergence. For faster convergence, we developed a new &lt;a href=&quot;https://en.wikipedia.org/wiki/Line_search&quot;>;line-search&lt;/a>; method that exploits the problem structure to find a suitable learning rate, ie, one that leads to a sufficiently large decrease in the loss. We also employed several computational schemes to improve CHITA&#39;s efficiency and the quality of the second-order approximation, leading to an improved version that we call CHITA++. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We compare CHITA&#39;s run time and accuracy with several state-of-the-art pruning methods using different architectures, including &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;>;MobileNet&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Run time&lt;/strong>;: CHITA is much more scalable than comparable methods that perform joint optimization (as opposed to pruning weights in isolation). For example, CHITA&#39;s speed-up can reach over 1000x when pruning ResNet. &lt;/p>; &lt;p>; &lt;strong>;Post-pruning accuracy&lt;/strong>;:&lt;strong>; &lt;/strong>;Below, we compare the performance of CHITA and CHITA++ with magnitude pruning (MP), &lt;a href=&quot;https://arxiv.org/abs/2004.14340&quot;>;Woodfisher&lt;/a>; (WF), and &lt;a href=&quot;https://arxiv.org/abs/2203.04466&quot;>;Combinatorial Brain Surgeon&lt;/a>; (CBS), for pruning 70% of the model weights. Overall, we see good improvements from CHITA and CHITA++. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Post-pruning accuracy of various methods on ResNet20. Results are reported for pruning 70% of the model weights.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Post-pruning accuracy of various methods on MobileNet. Results are reported for pruning 70% of the model weights.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Next, we report results for pruning a larger network: ResNet50 (on this network, some of the methods listed in the ResNet20 figure couldn&#39;t scale). Here we compare with magnitude pruning and &lt;a href=&quot;https://arxiv.org/abs/2107.03356&quot;>;M-FAC&lt;/a>;. The figure below shows that CHITA achieves better test accuracy for a wide range of sparsity levels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/s1050/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;784&quot; data-original-width=&quot;1050&quot; height=&quot;478&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/w640-h478/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Test accuracy of pruned networks, obtained using different methods.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion, limitations, and future work&lt;/h2>; &lt;p>; We presented CHITA, an optimization-based approach for pruning pre-trained neural networks. CHITA offers scalability and competitive performance by efficiently using second-order information and drawing on ideas from combinatorial optimization and high-dimensional statistics. &lt;/p>; &lt;p>; CHITA is designed for &lt;em>;unstructured pruning&lt;/em>; in which any weight can be removed. In theory, unstructured pruning can significantly reduce computational requirements. However, realizing these reductions in practice requires special software (and possibly hardware) that support sparse computations. In contrast, &lt;em>;structured pruning&lt;/em>;, which removes whole structures like neurons, may offer improvements that are easier to attain on general-purpose software and hardware. It would be interesting to extend CHITA to structured pruning. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is part of a research collaboration between Google and MIT. Thanks to Rahul Mazumder, Natalia Ponomareva, Wenyu Chen, Xiang Meng, Zhe Zhao, and Sergei Vassilvitskii for their help in preparing this post and the paper. Also thanks to John Guilyard for creating the graphics in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4779594044050650231/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html&quot; rel=&quot;alternate&quot; title=&quot;Neural network pruning with combinatorial optimization&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s72-c/CHITA%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8689679451447865270&lt;/id>;&lt;published>;2023-08-15T12:59:00.001-07:00&lt;/published>;&lt;updated>;2023-08-15T13:00:54.887-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Recommender Systems&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Social Networks&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;STUDY: Socially aware temporally causal decoder recommender systems&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eltayeb Ahmed, Research Engineer, and Subhrajit Roy, Senior Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s837/study-hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Reading has many benefits for young students, such as &lt;a href=&quot;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/284286/reading_for_pleasure.pdf&quot;>;better linguistic and life skills&lt;/a>;, and reading for pleasure has been shown to correlate with &lt;a href=&quot;https://files.eric.ed.gov/fulltext/ED541404.pdf&quot;>;academic success&lt;/a>;. Furthermore students have reported &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/1361454042000312284&quot;>;improved emotional wellbeing&lt;/a>; from reading, as well as &lt;a href=&quot;https://eric.ed.gov/?id=ED496343&quot;>;better general knowledge and better understanding of other cultures&lt;/a>;. With the vast amount of reading material both online and off, finding age-appropriate, relevant and engaging content can be a challenging task, but helping students do so is a necessary step to engage them in reading. Effective recommendations that present students with relevant reading material helps keep students reading, and this is where machine learning (ML) can help. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ML has been widely used in building &lt;a href=&quot;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592-5&quot;>;recommender systems&lt;/a>; for various types of digital content, ranging from videos to books to e-commerce items. Recommender systems are used across a range of digital platforms to help surface relevant and engaging content to users. In these systems, ML models are trained to suggest items to each user individually based on user preferences, user engagement, and the items under recommendation. These data provide a strong learning signal for models to be able to recommend items that are likely to be of interest, thereby improving user experience. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2306.07946&quot;>;STUDY: Socially Aware Temporally Causal Decoder Recommender Systems&lt;/a>;”, we present a content recommender system for audiobooks in an educational setting taking into account the social nature of reading. We developed the STUDY algorithm in partnership with &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>;, an educational nonprofit, aimed at promoting reading in dyslexic students, that provides audiobooks to students through a school-wide subscription program. Leveraging the wide range of audiobooks in the Learning Ally library, our goal is to help students find the right content to help boost their reading experience and engagement. Motivated by the fact that what a person&#39;s peers are currently reading has significant effects on what they would find interesting to read, we jointly process the reading engagement history of students who are in the same classroom. This allows our model to benefit from live information about what is currently trending within the student&#39;s localized social group, in this case, their classroom. &lt;/p>; &lt;br />; &lt;h2>;Data&lt;/h2>; &lt;p>; &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; has a large digital library of curated audiobooks targeted at students, making it well-suited for building a social recommendation model to help improve student learning outcomes. We received two years of anonymized audiobook consumption data. All students, schools and groupings in the data were anonymized, only identified by a randomly generated ID not traceable back to real entities by Google. Furthermore all potentially identifiable metadata was only shared in an aggregated form, to protect students and institutions from being re-identified. The data consisted of time-stamped records of student&#39;s interactions with audiobooks. For each interaction we have an anonymized student ID (which includes the student&#39;s grade level and anonymized school ID), an audiobook identifier and a date. While many schools distribute students in a single grade across several classrooms, we leverage this metadata to make the simplifying assumption that all students in the same school and in the same grade level are in the same classroom. While this provides the foundation needed to build a better social recommender model, it&#39;s important to note that this does not enable us to re-identify individuals, class groups or schools. &lt;/p>; &lt;br />; &lt;h2>;The STUDY algorithm&lt;/h2>; &lt;p>; We framed the recommendation problem as a &lt;a href=&quot;https://arxiv.org/abs/2104.10584&quot;>;click-through rate&lt;/a>; prediction problem, where we model the conditional probability of a user interacting with each specific item conditioned on both 1) user and item characteristics and 2) the item interaction history sequence for the user at hand. &lt;a href=&quot;https://arxiv.org/abs/1905.06874&quot;>;Previous work&lt;/a>; suggests &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer&lt;/a>;-based models, a widely used model class developed by Google Research, are well suited for modeling this problem. When each user is processed individually this becomes an &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive sequence modeling problem&lt;/a>;. We use this conceptual framework to model our data and then extend this framework to create the STUDY approach. &lt;/p>; &lt;p>; While this approach for click-through rate prediction can model dependencies between past and future item preferences for an individual user and can learn patterns of similarity across users at train time, it cannot model dependencies across different users at inference time. To recognise the social nature of reading and remediate this shortcoming we developed the STUDY model, which concatenates multiple sequences of books read by each student into a single sequence that collects data from multiple students in a single classroom. &lt;/p>; &lt;p>; However, this data representation requires careful diligence if it is to be modeled by transformers. In transformers, the attention mask is the matrix that controls which inputs can be used to inform the predictions of which outputs. The pattern of using all prior tokens in a sequence to inform the prediction of an output leads to the upper triangular attention matrix traditionally found in causal decoders. However, since the sequence fed into the STUDY model is not temporally ordered, even though each of its constituent subsequences is, a standard &lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;>;causal decoder&lt;/a>; is no longer a good fit for this sequence. When trying to predict each token, the model is not allowed to attend to every token that precedes it in the sequence; some of these tokens might have timestamps that are later and contain information that would not be available at deployment time. &lt;br />; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tqRd-ZWDXOCRpHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/s1787/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1651&quot; data-original-width=&quot;1787&quot; height=&quot;370&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tqRd-ZWDXOCRpHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/w400-h370/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In this figure we show the attention mask typically used in causal decoders. Each column represents an output and each column represents an output. A value of 1 (shown as blue) for a matrix entry at a particular position denotes that the model can observe the input of that row when predicting the output of the corresponding column, whereas a value of 0 (shown as white) denotes the opposite.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The STUDY model builds on causal transformers by replacing the triangular matrix attention mask with a flexible attention mask with values based on timestamps to allow attention across different subsequences. Compared to a regular transformer, which would not allow attention across different subsequences and would have a triangular matrix mask within sequence, STUDY maintains a causal triangular attention matrix within a sequence and has flexible values across sequences with values that depend on timestamps. Hence, predictions at any output point in the sequence are informed by all input points that occurred in the past relative to the current time point, regardless of whether they appear before or after the current input in the sequence. This causal constraint is important because if it is not enforced at train time, the model could potentially learn to make predictions using information from the future, which would not be available for a real world deployment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s1104/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In (a) we show a sequential autoregressive transformer with causal attention that processes each user individually; in (b) we show an equivalent joint forward pass that results in the same computation as (a); and finally, in (c) we show that by introducing new nonzero values (shown in purple) to the attention mask we allow information to flow across users. We do this by allowing a prediction to condition on all interactions with an earlier timestamp, irrespective of whether the interaction came from the same user or not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s1035/Study.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1035&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s16000/Study.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In (a) we show a sequential autoregressive transformer with causal attention that processes each user individually; in (b) we show an equivalent joint forward pass that results in the same computation as (a); and finally, in (c) we show that by introducing new nonzero values (shown in purple) to the attention mask we allow information to flow across users. We do this by allowing a prediction to condition on all interactions with an earlier timestamp, irrespective of whether the interaction came from the same user or not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/s1104/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;548&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/w318-h640/image3.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In (a) we show a sequential autoregressive transformer with causal attention that processes each user individually; in (b) we show an equivalent joint forward pass that results in the same computation as (a); and finally, in (c) we show that by introducing new nonzero values (shown in purple) to the attention mask we allow information to flow across users. We do this by allowing a prediction to condition on all interactions with an earlier timestamp, irrespective of whether the interaction came from the same user or not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We used the Learning Ally dataset to train the STUDY model along with multiple baselines for comparison. We implemented an autoregressive click-through rate transformer decoder, which we refer to as “Individual”, a &lt;em>;k&lt;/em>;-nearest neighbor baseline (KNN), and a comparable social baseline, social attention memory network (SAMN). We used the data from the first school year for training and we used the data from the second school year for validation and testing. &lt;/p>; &lt;p>; We evaluated these models by measuring the percentage of the time the next item the user actually interacted with was in the model&#39;s top &lt;em>;n&lt;/em>; recommendations, ie, hits@&lt;em>;n,&lt;/em>; for different values of &lt;em>;n&lt;/em>;. In addition to evaluating the models on the entire test set we also report the models&#39; scores on two subsets of the test set that are more challenging than the whole data set. We observed that students will typically interact with an audiobook over multiple sessions, so simply recommending the last book read by the user would be a strong trivial recommendation. Hence, the first test subset, which we refer to as “non-continuation”, is where we only look at each model&#39;s performance on recommendations when the students interact with books that are different from the previous interaction. We also observe that students revisit books they have read in the past, so strong performance on the test set can be achieved by restricting the recommendations made for each student to only the books they have read in the past. Although there might be value in recommending old favorites to students, much value from recommender systems comes from surfacing content that is new and unknown to the user. To measure this we evaluate the models on the subset of the test set where the students interact with a title for the first time. We name this evaluation subset “novel”. &lt;/p>; &lt;p>; We find that STUDY outperforms all other tested models across almost every single slice we evaluated against. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/s1526/Study-img2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1486&quot; data-original-width=&quot;1526&quot; height=&quot;390&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/w400-h390/Study-img2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In this figure we compare the performance of four models, Study, Individual, KNN and SAMN. We measure the performance with hits@5, ie, how likely the model is to suggest the next title the user read within the model&#39;s top 5 recommendations. We evaluate the model on the entire test set (all) as well as the novel and non-continuation splits. We see STUDY consistently outperforms the other three models presented across all splits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Importance of appropriate grouping&lt;/h2>; &lt;p>; At the heart of the STUDY algorithm is organizing users into groups and doing joint inference over multiple users who are in the same group in a single forward pass of the model. We conducted an ablation study where we looked at the importance of the actual groupings used on the performance of the model. In our presented model we group together all students who are in the same grade level and school. We then experiment with groups defined by all students in the same grade level and district and also place all students in a single group with a random subset used for each forward pass. We also compare these models against the Individual model for reference. &lt;/p>; &lt;p>; We found that using groups that were more localized was more effective, with the school and grade level grouping outperforming the district and grade level grouping. This supports the hypothesis that the STUDY model is successful because of the social nature of activities such as reading — people&#39;s reading choices are likely to correlate with the reading choices of those around them. Both of these models outperformed the other two models (single group and Individual) where grade level is not used to group students. This suggests that data from users with similar reading levels and interests is beneficial for performance. &lt;/p>; &lt;br />; &lt;h2>;Future work&lt;/h2>; &lt;p>; This work is limited to modeling recommendations for user populations where the social connections are assumed to be homogenous. In the future it would be beneficial to model a user population where relationships are not homogeneous, ie, where categorically different types of relationships exist or where the relative strength or influence of different relationships is known. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work involved collaborative efforts from a multidisciplinary team of researchers, software engineers and educational subject matter experts. We thank our co-authors: Diana Mincu, Lauren Harrell, and Katherine Heller from Google. We also thank our colleagues at Learning Ally, Jeff Ho, Akshat Shah, Erin Walker, and Tyler Bastian, and our collaborators at Google, Marc Repnyek, Aki Estrella, Fernando Diaz, Scott Sanner, Emily Salkey and Lev Proleev.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8689679451447865270/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/study-socially-aware-temporally-causal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/study-socially-aware-temporally-causal.html&quot; rel=&quot;alternate&quot; title=&quot;STUDY: Socially aware temporally causal decoder recommender systems&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s72-c/study-hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3240052415427459327&lt;/id>;&lt;published>;2023-08-09T11:32:00.001-07:00&lt;/published>;&lt;updated>;2023-08-09T12:26:53.831-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Advances in document understanding&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sandeep Tata, Software Engineer, Google Research, Athena Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s320/Document%20understanding%20hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The last few years have seen rapid progress in systems that can automatically process complex business documents and turn them into structured objects. A system that can &lt;a href=&quot;https://ai.googleblog.com/2020/06/extracting-structured-data-from.html&quot;>;automatically extract data&lt;/a>; from documents, eg, receipts, insurance quotes, and financial statements, has the potential to dramatically improve the efficiency of business workflows by avoiding error-prone, manual work. Recent models, based on the &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; architecture, have shown &lt;a href=&quot;https://ai.googleblog.com/2022/04/formnet-beyond-sequential-modeling-for.html&quot;>;impressive gains in accuracy&lt;/a>;. Larger models, such as &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>;, are also being leveraged to further streamline these business workflows. However, the datasets used in academic literature fail to capture the challenges seen in real-world use cases. Consequently, academic benchmarks report strong model accuracy, but these same models do poorly when used for complex real-world applications. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;VRDU: A Benchmark for Visually-rich Document Understanding&lt;/a>;”, presented at &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>;, we announce the release of the new &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;Visually Rich Document Understanding&lt;/a>; (VRDU) dataset that aims to bridge this gap and help researchers better track progress on document understanding tasks. We list five requirements for a good document understanding benchmark, based on the kinds of real-world documents for which document understanding models are frequently used. Then, we describe how most datasets currently used by the research community fail to meet one or more of these requirements, while VRDU meets all of them. We are excited to announce the public release of the VRDU &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;dataset&lt;/a>; and &lt;a href=&quot;https://github.com/google-research-datasets/vrdu&quot;>;evaluation code&lt;/a>; under a &lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;Creative Commons license&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Benchmark requirements&lt;/h2>; &lt;p>; First, we compared state-of-the-art model accuracy (eg, with &lt;a href=&quot;https://arxiv.org/abs/2203.08411&quot;>;FormNet&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2012.14740&quot;>;LayoutLMv2&lt;/a>;) on real-world use cases to academic benchmarks (eg, &lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;, &lt;a href=&quot;https://openreview.net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;, &lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;). We observed that state-of-the-art models did not match academic benchmark results and delivered much lower accuracy in the real world. Next, we compared typical datasets for which document understanding models are frequently used with academic benchmarks and identified five dataset requirements that allow a dataset to better capture the complexity of real-world applications: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Rich Schema:&lt;/strong>; In practice, we see a wide variety of rich schemas for structured extraction. Entities have different data types (numeric, strings, dates, etc.) that may be required, optional, or repeated in a single document or may even be nested. Extraction tasks over simple flat schemas like (header, question, answer) do not reflect typical problems encountered in practice. &lt;/li>;&lt;li>;&lt;strong>;Layout-Rich Documents:&lt;/strong>; The documents should have complex layout elements. Challenges in practical settings come from the fact that documents may contain tables, key-value pairs, switch between single-column and double-column layout, have varying font-sizes for different sections, include pictures with captions and even footnotes. Contrast this with datasets where most documents are organized in sentences, paragraphs, and chapters with section headers — the kinds of documents that are typically the focus of classic natural language processing literature on &lt;a href=&quot;https://arxiv.org/abs/2007.14062&quot;>;long&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2004.08483&quot;>;inputs&lt;/a>;. &lt;/li>;&lt;li>;&lt;strong>;Diverse Templates:&lt;/strong>; A benchmark should include different structural layouts or templates. It is trivial for a high-capacity model to extract from a particular template by memorizing the structure. However, in practice, one needs to be able to generalize to new templates/layouts, an ability that the train-test split in a benchmark should measure. &lt;/li>;&lt;li>;&lt;strong>;High-Quality OCR&lt;/strong>;: Documents should have high-quality &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;Optical Character Recognition&lt;/a>; (OCR) results. Our aim with this benchmark is to focus on the VRDU task itself and to exclude the variability brought on by the choice of OCR engine. &lt;/li>;&lt;li>;&lt;strong>;Token-Level Annotation&lt;/strong>;: Documents should contain ground-truth annotations that can be mapped back to corresponding input text, so that each token can be annotated as part of the corresponding entity. This is in contrast with simply providing the text of the value to be extracted for the entity. This is key to generating clean training data where we do not have to worry about incidental matches to the given value. For instance, in some receipts, the &#39;total-before-tax&#39; field may have the same value as the &#39;total&#39; field if the tax amount is zero. Having token level annotations prevents us from generating training data where both instances of the matching value are marked as ground-truth for the &#39;total&#39; field, thus producing noisy examples. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s1600/Benchmark%20GIF.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s16000/Benchmark%20GIF.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;VRDU datasets and tasks&lt;/h2>; &lt;p>; The VRDU dataset is a combination of two publicly available datasets, &lt;a href=&quot;https://efile.fara.gov/ords/fara/f?p=1235:10&quot;>;Registration Forms&lt;/a>; and &lt;a href=&quot;https://publicfiles.fcc.gov/&quot;>;Ad-Buy forms&lt;/a>;. These datasets provide examples that are representative of real-world use cases, and satisfy the five benchmark requirements described above. &lt;/p>; &lt;p>; The Ad-buy Forms dataset consists of 641 documents with political advertisement details. Each document is either an invoice or receipt signed by a TV station and a campaign group. The documents use tables, multi-columns, and key-value pairs to record the advertisement information, such as the product name, broadcast dates, total price, and release date and time. &lt;/p>; &lt;p>; The Registration Forms dataset consists of 1,915 documents with information about foreign agents registering with the US government. Each document records essential information about foreign agents involved in activities that require public disclosure. Contents include the name of the registrant, the address of related bureaus, the purpose of activities, and other details. &lt;/p>; &lt;p>; We gathered a random sample of documents from the public &lt;a href=&quot;https://www.fcc.gov/&quot;>;Federal Communications Commission&lt;/a>; (FCC) and &lt;a href=&quot;https://www.justice.gov/nsd-fara&quot;>;Foreign Agents Registration Act&lt;/a>; (FARA) sites, and converted the images to text using &lt;a href=&quot;https://cloud.google.com/&quot;>;Google Cloud&#39;s&lt;/a>; &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;OCR&lt;/a>;. We discarded a small number of documents that were several pages long and the processing did not complete in under two minutes. This also allowed us to avoid sending very long documents for manual annotation — a task that can take over an hour for a single document. Then, we defined the schema and corresponding labeling instructions for a team of annotators experienced with document-labeling tasks. &lt;/p>; &lt;p>; The annotators were also provided with a few sample labeled documents that we labeled ourselves. The task required annotators to examine each document, draw a bounding box around every occurrence of an entity from the schema for each document, and associate that bounding box with the target entity. After the first round of labeling, a pool of experts were assigned to review the results. The corrected results are included in the published VRDU dataset. Please see the &lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;paper&lt;/a>; for more details on the labeling protocol and the schema for each dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s1600/Chart.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s16000/Chart.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Existing academic benchmarks (&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;, &lt;a href=&quot;https://openreview.net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;, &lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-NDA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-Charity&lt;/a>;, &lt;a href=&quot;https://wandb.ai/stacey/deepform_v1/reports/DeepForm-Understand-Structured-Documents-at-Scale--VmlldzoyODQ3Njg&quot;>;DeepForm&lt;/a>;) fall-short on one or more of the five requirements we identified for a good document understanding benchmark. VRDU satisfies all of them. See our &lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;paper&lt;/a>; for background on each of these datasets and a discussion on how they fail to meet one or more of the requirements.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We built four different model training sets with 10, 50, 100, and 200 samples respectively. Then, we evaluated the VRDU datasets using three tasks (described below): (1) Single Template Learning, (2) Mixed Template Learning, and (3) Unseen Template Learning. For each of these tasks, we included 300 documents in the testing set. We evaluate models using the &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1 score&lt;/a>; on the testing set. &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Single Template Learning&lt;/em>; (STL): This is the simplest scenario where the training, testing, and validation sets only contain a single template. This simple task is designed to evaluate a model&#39;s ability to deal with a fixed template. Naturally, we expect very high F1 scores (0.90+) for this task. &lt;/li>;&lt;li>;&lt;em>;Mixed Template Learning&lt;/em>; (MTL): This task is similar to the task that most related papers use: the training, testing, and validation sets all contain documents belonging to the same set of templates. We randomly sample documents from the datasets and construct the splits to make sure the distribution of each template is not changed during sampling. &lt;/li>;&lt;li>;&lt;em>;Unseen Template Learning&lt;/em>; (UTL): This is the most challenging setting, where we evaluate if the model can generalize to unseen templates. For example, in the Registration Forms dataset, we train the model with two of the three templates and test the model with the remaining one. The documents in the training, testing, and validation sets are drawn from disjoint sets of templates. To our knowledge, previous benchmarks and datasets do not explicitly provide such a task designed to evaluate the model&#39;s ability to generalize to templates not seen during training. &lt;/li>; &lt;/ul>; &lt;p>; The objective is to be able to evaluate models on their data efficiency. In our &lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;paper&lt;/a>;, we compared two recent models using the STL, MTL, and UTL tasks and made three observations. First, unlike with other benchmarks, VRDU is challenging and shows that models have plenty of room for improvements. Second, we show that few-shot performance for even state-of-the-art models is surprisingly low with even the best models resulting in less than an F1 score of 0.60. Third, we show that models struggle to deal with structured repeated fields and perform particularly poorly on them. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We release the new &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;Visually Rich Document Understanding&lt;/a>; (VRDU) dataset that helps researchers better track progress on document understanding tasks. We describe why VRDU better reflects practical challenges in this domain. We also present experiments showing that VRDU tasks are challenging, and recent models have substantial headroom for improvements compared to the datasets typically used in the literature with F1 scores of 0.90+ being typical. We hope the release of the VRDU dataset and evaluation code helps research teams advance the state of the art in document understanding. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Many thanks to Zilong Wang, Yichao Zhou, Wei Wei, and Chen-Yu Lee, who co-authored the paper along with Sandeep Tata. Thanks to Marc Najork, Riham Mansour and numerous partners across Google Research and the Cloud AI team for providing valuable insights. Thanks to John Guilyard for creating the animations in this post. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3240052415427459327/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/advances-in-document-understanding.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/advances-in-document-understanding.html&quot; rel=&quot;alternate&quot; title=&quot;Advances in document understanding&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s72-c/Document%20understanding%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;