<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>æ ‡ç­¾ï¼šblogger.comï¼Œ1999ï¼šblog-8474926331452026626</id><updated> 2023-08-30T17:33:17.744-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="HCI"></category><category term="MOOC"></category><category term="Security and Privacy"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="accessibility"></category><category term="optimization"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Android"></category><category term="Awards"></category><category term="ICML"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Responsible AI"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Maps"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="RAI-HCT Highlights"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI åšå®¢&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;æ¥è‡ª Google AI çš„æœ€æ–°æ–°é—»ã€‚&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1274&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;æ ‡ç­¾ï¼šblogger.comï¼Œ1999ï¼šblog-8474926331452026626.post-1342410539466537463&lt;/id>;&lt;å‘å¸ƒ>;2023-08-30T12:34:00.002-07:00&lt;/å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08- 30T12ï¼š39ï¼š33.664-07ï¼š00 &lt;/æ›´æ–°>; &lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atom/ns#â€æœ¯è¯­=â€œHCIâ€>;&lt;/ç±»åˆ«>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttpï¼š //www.blogger.com/atom/ns#&quot; term=&quot;è‡ªç„¶è¯­è¨€ç†è§£&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;å»ºæ¨¡å¹¶æé«˜å®žæ—¶å­—å¹•ä¸­çš„æ–‡æœ¬ç¨³å®šæ€§&lt;/stitle>;&lt;content type=&quot; html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šGoogle å¢žå¼ºçŽ°å®žç ”ç©¶ç§‘å­¦å®¶ Vikas Bahirwani å’Œè½¯ä»¶å·¥ç¨‹å¸ˆ Susan Xu&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45-muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U 5-8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s2300/stabilizedcaptions.png&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; è‡ªåŠ¨è¯­éŸ³è¯†åˆ« (ASR) æŠ€æœ¯é€šè¿‡è¿œç¨‹ä¼šè®®è½¯ä»¶ã€ç§»åŠ¨åº”ç”¨ç¨‹åºå’Œ &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3379337.3415817&quot; ä¸­çš„å®žæ—¶å­—å¹•ä½¿å¯¹è¯å˜å¾—æ›´åŠ å®¹æ˜“>;å¤´æˆ´å¼â€‹â€‹æ˜¾ç¤ºå™¨&lt;/a>;ã€‚ç„¶è€Œï¼Œä¸ºäº†ä¿æŒå®žæ—¶å“åº”èƒ½åŠ›ï¼Œå®žæ—¶å­—å¹•ç³»ç»Ÿé€šå¸¸ä¼šæ˜¾ç¤ºä¸´æ—¶é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹ä¼šéšç€æ”¶åˆ°æ–°è¯è¯­è€Œæ›´æ–°ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´æ–‡æœ¬ä¸ç¨³å®šï¼ˆä¹‹å‰æ˜¾ç¤ºçš„æ–‡æœ¬è¢«æ›´æ–°æ—¶å‡ºçŽ°â€œé—ªçƒâ€ï¼Œå¦‚ä¸‹é¢è§†é¢‘å·¦ä¾§çš„å­—å¹•æ‰€ç¤ºï¼‰ï¼Œè¿™å¯èƒ½ä¼šå› åˆ†å¿ƒã€ç–²åŠ³ã€ä»¥åŠè·Ÿä¸ä¸Šè°ˆè¯çš„å›°éš¾ã€‚ &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://research.google/pubs/pub52450/&quot;>;å»ºæ¨¡å¹¶æé«˜å®žæ—¶å­—å¹•ä¸­çš„æ–‡æœ¬ç¨³å®šæ€§&lt;/â€ a>;â€ï¼Œåœ¨ &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/98883&quot;>;ACM CHI 2023&lt;/a>; ä¸Šæå‡ºï¼Œæˆ‘ä»¬é€šè¿‡ä¸€äº›æ–¹æ³•å°†æ–‡æœ¬ç¨³å®šæ€§é—®é¢˜å½¢å¼åŒ–å…³é”®è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨åŸºäºŽè§†è§‰çš„é—ªçƒæŒ‡æ ‡æ¥é‡åŒ–æ–‡æœ¬ä¸ç¨³å®šæ€§ï¼Œè¯¥æŒ‡æ ‡ä½¿ç”¨&lt;a href=&quot;https://en.wikipedia.org/wiki/Luminance&quot;>;äº®åº¦&lt;/a>;å¯¹æ¯”åº¦å’Œ&lt;a href=&quot;https ://en.wikipedia.org/wiki/Discrete_Fourier_transform&quot;>;ç¦»æ•£å‚…é‡Œå¶å˜æ¢&lt;/a>;ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ç¨³å®šæ€§ç®—æ³•ï¼Œé€šè¿‡æ ‡è®°å¯¹é½ã€è¯­ä¹‰åˆå¹¶å’Œå¹³æ»‘åŠ¨ç”»æ¥ç¨³å®šå®žæ—¶å­—å¹•çš„æ¸²æŸ“ã€‚æœ€åŽï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ (N=123)ï¼Œä»¥äº†è§£è§‚ä¼—å¯¹å®žæ—¶å­—å¹•çš„ä½“éªŒ&lt;strong>;ã€‚ &lt;/strong>;æˆ‘ä»¬çš„ç»Ÿè®¡åˆ†æžè¡¨æ˜Žï¼Œæˆ‘ä»¬æå‡ºçš„é—ªçƒæŒ‡æ ‡ä¸Žè§‚ä¼—ä½“éªŒä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œå®ƒè¡¨æ˜Žæˆ‘ä»¬æå‡ºçš„ç¨³å®šæŠ€æœ¯æ˜¾ç€æ”¹å–„äº†è§‚çœ‹è€…çš„ä½“éªŒï¼ˆä¾‹å¦‚ï¼Œä¸Šé¢è§†é¢‘ä¸­å³ä¾§çš„å­—å¹•ï¼‰ã€‚ &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/-hiAnT6QkmU?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;-hiAnT6QkmU&quot;>;&lt;/iframe>;&lt;/div>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr- Caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;åŽŸå§‹ ASR å­—å¹•ä¸Žç¨³å®šå­—å¹•&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;å…¬åˆ¶&lt;/h2>; &lt;p>; å—åˆ° &lt;a href=&quot;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/5203/0000/Toward-perceptual-metrics-for-video-watermark-evaluation/10.1117/12.512550 çš„å¯å‘ã€‚ full&quot;>;ä¹‹å‰çš„å·¥ä½œ&lt;/a>;ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºŽé—ªçƒçš„æŒ‡æ ‡æ¥é‡åŒ–æ–‡æœ¬ç¨³å®šæ€§å¹¶å®¢è§‚è¯„ä¼°å®žæ—¶å­—å¹•ç³»ç»Ÿçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é‡åŒ–ç°åº¦å®žæ—¶å­—å¹•è§†é¢‘ä¸­çš„é—ªçƒã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒæž„æˆè§†é¢‘çš„å„ä¸ªå¸§ï¼ˆä¸‹å›¾ä¸­çš„å¸§ï¼‰ä¹‹é—´çš„äº®åº¦å·®å¼‚æ¥å®žçŽ°è¿™ä¸€ç‚¹ã€‚äº®åº¦çš„å¤§çš„è§†è§‰å˜åŒ–æ˜¯æ˜¾è€Œæ˜“è§çš„ï¼ˆä¾‹å¦‚ï¼Œåœ¨åº•éƒ¨çš„å›¾ä¸­æ·»åŠ â€œæ˜Žäº®â€ä¸€è¯ï¼‰ï¼Œä½†æ˜¯ç»†å¾®çš„å˜åŒ–ï¼ˆä¾‹å¦‚ï¼Œä»Žâ€œ...è¿™ä¸ªé‡‘è‰²ã€‚å¾ˆå¥½..â€æ›´æ–°ä¸ºâ€œ...è¿™ä¸ªâ€ .é‡‘å­å¾ˆå¥½â€ï¼‰å¯¹äºŽè¯»è€…æ¥è¯´å¯èƒ½å¾ˆéš¾è¾¨åˆ«ã€‚ç„¶è€Œï¼Œå°†äº®åº¦å˜åŒ–è½¬æ¢ä¸ºå…¶æž„æˆé¢‘çŽ‡ä¼šæš´éœ²å‡ºæ˜Žæ˜¾çš„å’Œå¾®å¦™çš„å˜åŒ–ã€‚ &lt;/p>; &lt;p>; å› æ­¤ï¼Œå¯¹äºŽæ¯å¯¹è¿žç»­å¸§ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¦»æ•£å‚…é‡Œå¶å˜æ¢å°†äº®åº¦å·®å¼‚è½¬æ¢ä¸ºå…¶æž„æˆé¢‘çŽ‡ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªä½Žé¢‘å’Œé«˜é¢‘è¿›è¡Œæ±‚å’Œï¼Œä»¥é‡åŒ–è¯¥å¯¹ä¸­çš„é—ªçƒã€‚æœ€åŽï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰å¸§å¯¹è¿›è¡Œå¹³å‡ä»¥èŽ·å¾—æ¯ä¸ªè§†é¢‘çš„é—ªçƒã€‚ &lt;/p>; &lt;p>; ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°ä¸¤ä¸ªç›¸åŒçš„å¸§ï¼ˆé¡¶éƒ¨ï¼‰äº§ç”Ÿ 0 é—ªçƒï¼Œè€Œä¸¤ä¸ªä¸åŒçš„å¸§ï¼ˆåº•éƒ¨ï¼‰äº§ç”Ÿéžé›¶é—ªçƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¾ƒé«˜çš„åº¦é‡å€¼è¡¨ç¤ºè§†é¢‘ä¸­çš„é«˜é—ªçƒï¼Œå› æ­¤ä¸Žè¾ƒä½Žçš„åº¦é‡å€¼ç›¸æ¯”ï¼Œç”¨æˆ·ä½“éªŒæ›´å·®ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBf LLlGqieWGTJT4qRV_ZeUkUvQ3aYHkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s1399/noflicker.png&quot;æ ·å¼=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;268&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBfLLlGqieWGTJT4qRV_ZeUkUvQ3aY HkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s16000/noflicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;ä¸¤ä¸ªç›¸åŒå¸§ä¹‹é—´çš„é—ªçƒæŒ‡æ ‡è¯´æ˜Žã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot; 0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNe HPRxE1XIotPM7DZE- LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11-7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s1399/flicker.png&quot; style=&quot;margin-left: è‡ªåŠ¨; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;263&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNeHPRxE1XIotPM7DZE-LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11- 7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s16000/flicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;ä¸¤ä¸ªä¸ç›¸åŒçš„å¸§ä¹‹é—´çš„é—ªçƒè¯´æ˜Žã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;ç¨³å®šæ€§ç®—æ³•&lt;/h2>; &lt;p>;ä¸ºäº†æé«˜å®žæ—¶å­—å¹•çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†å·²æ¸²æŸ“çš„æ ‡è®°åºåˆ—ï¼ˆä¾‹å¦‚ä¸‹å›¾ä¸­çš„â€œä¸Šä¸€ä¸ªâ€ï¼‰å’Œæ–°åºåˆ—ä½œä¸ºè¾“å…¥ASR é¢„æµ‹ï¼Œå¹¶è¾“å‡ºæ›´æ–°çš„ç¨³å®šæ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œä¸‹é¢çš„â€œæ›´æ–°æ–‡æœ¬ï¼ˆå…·æœ‰ç¨³å®šåŠŸèƒ½ï¼‰â€ï¼‰ã€‚å®ƒæ—¢è€ƒè™‘äº†è‡ªç„¶è¯­è¨€ç†è§£ (NLU) æ–¹é¢ï¼Œä¹Ÿè€ƒè™‘äº†äººä½“å·¥ç¨‹å­¦æ–¹é¢ï¼ˆæ˜¾ç¤ºã€å¸ƒå±€ç­‰ï¼‰ç”¨æˆ·åœ¨å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•ç”Ÿæˆç¨³å®šçš„æ›´æ–°æ–‡æœ¬æ—¶çš„ä½“éªŒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç®—æ³•æ‰§è¡Œæ ‡è®°åŒ–å¯¹é½ã€è¯­ä¹‰åˆå¹¶å’Œå¹³æ»‘åŠ¨ç”»æ¥å®žçŽ°è¿™ä¸€ç›®æ ‡ã€‚åœ¨ä¸‹æ–‡ä¸­ï¼Œæ ‡è®°è¢«å®šä¹‰ä¸ºç”± ASR ç”Ÿæˆçš„å•è¯æˆ–æ ‡ç‚¹ç¬¦å·ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45 w4_GiltlNjsU3dz82vn9qhoTJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s1995/image3.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;1995&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45w4_GiltlNjsU3dz82vn9qho TJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;æˆ‘ä»¬æ˜¾ç¤º (a) ä¹‹å‰å·²ç»æ¸²æŸ“çš„æ–‡æœ¬ï¼Œ(b) æ²¡æœ‰æˆ‘ä»¬çš„åˆå¹¶ç®—æ³•çš„æ›´æ–°æ–‡æœ¬çš„åŸºçº¿å¸ƒå±€ï¼Œä»¥åŠ (c) ç”±æˆ‘ä»¬çš„åˆå¹¶ç®—æ³•ç”Ÿæˆçš„æ›´æ–°æ–‡æœ¬ç¨³å®šç®—æ³•ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />;&lt;p>;&lt;br />;&lt;/p>;&lt;p>;&lt;br />;&lt;/p>; &lt;p>; æˆ‘ä»¬çš„ç®—æ³•é€šè¿‡é¦–å…ˆè¯†åˆ«ä¸‰ç±»æ›´æ”¹ï¼ˆä¸‹é¢ä»¥çº¢è‰²ã€ç»¿è‰²å’Œè“è‰²çªå‡ºæ˜¾ç¤ºï¼‰æ¥è§£å†³ç”Ÿæˆç¨³å®šçš„æ›´æ–°æ–‡æœ¬çš„æŒ‘æˆ˜ï¼š &lt;/p>; &lt;ol type=&quot;A&quot;>; &lt;li>;çº¢è‰²ï¼šåœ¨æœ«å°¾æ·»åŠ æ ‡è®°å…ˆå‰å‘ˆçŽ°çš„æ ‡é¢˜ï¼ˆä¾‹å¦‚ï¼Œâ€œæ€Žä¹ˆæ ·â€ï¼‰ã€‚ &lt;/li>; &lt;li>;ç»¿è‰²ï¼šåœ¨å·²æ¸²æŸ“çš„å­—å¹•ä¸­é—´æ·»åŠ /åˆ é™¤æ ‡è®°ã€‚ &lt;ul>; &lt;li>;B1ï¼šæ·»åŠ æ ‡è®°ï¼ˆä¾‹å¦‚â€œæˆ‘â€å’Œâ€œæœ‹å‹â€ï¼‰ã€‚è¿™äº›å¯èƒ½ä¼šä¹Ÿå¯èƒ½ä¸ä¼šå½±å“å­—å¹•çš„æ•´ä½“ç†è§£ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´å¸ƒå±€å˜åŒ–ã€‚åœ¨å®žæ—¶å­—å¹•ä¸­ä¸éœ€è¦æ­¤ç±»å¸ƒå±€æ›´æ”¹ï¼Œå› ä¸ºå®ƒä»¬ä¼šå¯¼è‡´æ˜¾ç€çš„æŠ–åŠ¨å’Œè¾ƒå·®çš„ç”¨æˆ·ä½“éªŒã€‚è¿™é‡Œâ€œæˆ‘â€å¹¶ä¸èƒ½å¢žåŠ ç†è§£åŠ›ï¼Œä½†â€œæœ‹å‹â€å´å¯ä»¥ã€‚å› æ­¤ï¼Œå¹³è¡¡æ›´æ–°å’Œç¨³å®šæ€§éžå¸¸é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºŽ B1 ç±»åž‹ä»£å¸ã€‚ &lt;/li>; &lt;li>;B2ï¼šåˆ é™¤æ ‡è®°ï¼Œä¾‹å¦‚ï¼Œåœ¨æ›´æ–°çš„å¥å­ä¸­åˆ é™¤â€œinâ€ã€‚&lt;/li>; &lt;/ul>; &lt;/li>; &lt;li>;è“è‰²ï¼šé‡æ–°æ ‡è®°æ ‡è®°ï¼šè¿™åŒ…æ‹¬å¯èƒ½ä¼šæˆ–å¯èƒ½ä¸ä¼šå½±å“å­—å¹•çš„æ•´ä½“ç†è§£çš„è®°å·ç¼–è¾‘ã€‚&lt;/li>;&lt;ul>; &lt;li>;C1ï¼šè¯¸å¦‚â€œdisney landâ€ä¹‹ç±»çš„ä¸“æœ‰åè¯å·²æ›´æ–°ä¸ºâ€œDisneylandâ€ã€‚&lt;/li >; &lt;li>;C2ï¼šâ€œit&#39;sâ€ç­‰è¯­æ³•ç®€å†™æ›´æ–°ä¸ºâ€œIt wasâ€ã€‚&lt;/li>; &lt;/ul>; &lt;/ol>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0 â€œ ç±»=â€œtr-caption-containerâ€æ ·å¼=â€œmargin-leftï¼šè‡ªåŠ¨ï¼› margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0wEYXbx4wXxWiW- r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-04qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLh C8Db2aLqq/s1999/image2.png&quot; style=&quot;margin-left: è‡ªåŠ¨; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0weEYXbx4wXxWiW-r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-0 4qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLhC8Db2aLqq/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;ä¹‹å‰æ˜¾ç¤ºå’Œæ›´æ–°çš„æ–‡æœ¬ä¹‹é—´çš„å˜åŒ–ç±»åˆ«ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;å¯¹é½ã€åˆå¹¶å’Œå¹³æ»‘ &lt;/h2>; &lt;p>; ä¸ºäº†æœ€å¤§é™åº¦åœ°æé«˜æ–‡æœ¬ç¨³å®šæ€§ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿ç”¨æ›´æ–°å°†æ—§åºåˆ—ä¸Žæ–°åºåˆ—å¯¹é½ï¼Œä»Žè€Œå¯¹çŽ°æœ‰å¸ƒå±€è¿›è¡Œæœ€å°çš„æ›´æ”¹ï¼ŒåŒæ—¶ç¡®ä¿å‡†ç¡®ä¸”æœ‰æ„ä¹‰çš„æ ‡é¢˜ã€‚ä¸ºäº†å®žçŽ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ©ç”¨ &lt;a href=&quot;https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm#:~:text=The%20Needleman%E2 çš„å˜ä½“%80%93Wunsch%20algorithm%20is%20still%20widely%20used%20for%20optimal,alignments%20having%20the%20highest%20scoreã€‚&quot;>;Needleman-Wunsch ç®—æ³•&lt;/a>; ä½¿ç”¨åŠ¨æ€è§„åˆ’æ¥æ ¹æ®ä¸Šé¢å®šä¹‰çš„æ ‡è®°ç±»åˆ«ï¼š &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;æ¡ˆä¾‹ A æ ‡è®°ï¼š&lt;/b>; æˆ‘ä»¬ç›´æŽ¥æ·»åŠ æ¡ˆä¾‹ A æ ‡è®°ï¼Œå¹¶æ ¹æ®éœ€è¦æ·»åŠ æ¢è¡Œç¬¦ä»¥é€‚åº”æ›´æ–°çš„æ ‡é¢˜ã€‚ &lt;/li>;&lt;li>;&lt;b>;æ¡ˆä¾‹ B æ ‡è®°ï¼š&lt;/b>;æˆ‘ä»¬çš„åˆæ­¥ç ”ç©¶è¡¨æ˜Žï¼Œç”¨æˆ·æ›´å–œæ¬¢å…ˆå‰æ˜¾ç¤ºçš„å­—å¹•çš„ç¨³å®šæ€§è€Œä¸æ˜¯å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œå¦‚æžœæ›´æ–°ä¸ä¼šç ´åçŽ°æœ‰çš„çº¿è·¯å¸ƒå±€ï¼Œæˆ‘ä»¬ä»…æ›´æ–°æƒ…å†µ B æ ‡è®°ã€‚ &lt;/li>;&lt;li>;&lt;b>;æ¡ˆä¾‹ C æ ‡è®°ï¼š&lt;/b>;æˆ‘ä»¬é€šè¿‡å°†åŽŸå§‹å¥å­å’Œæ›´æ–°åŽçš„å¥å­è½¬æ¢ä¸ºå¥å­åµŒå…¥ã€æµ‹é‡å®ƒä»¬çš„ç‚¹ç§¯å¹¶ä»…åœ¨å®ƒä»¬æ»¡è¶³æ—¶æ‰æ›´æ–°å®ƒä»¬æ¥æ¯”è¾ƒæ¡ˆä¾‹ C æ ‡è®°çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚è¯­ä¹‰ä¸Šä¸åŒï¼ˆç›¸ä¼¼åº¦&lt;0.85ï¼‰å¹¶ä¸”æ›´æ–°ä¸ä¼šå¯¼è‡´æ–°çš„æ¢è¡Œç¬¦ã€‚ &lt;/li>; &lt;/ul>; &lt;p>; æœ€åŽï¼Œæˆ‘ä»¬åˆ©ç”¨åŠ¨ç”»æ¥å‡å°‘è§†è§‰æŠ–åŠ¨ã€‚æˆ‘ä»¬å¯¹æ–°æ·»åŠ çš„æ ‡è®°å®žçŽ°å¹³æ»‘æ»šåŠ¨å’Œæ·¡å…¥æ·¡å‡ºï¼Œä»¥è¿›ä¸€æ­¥ç¨³å®šå®žæ—¶å­—å¹•çš„æ•´ä½“å¸ƒå±€ã€‚ &lt;/p>; &lt;h2>;ç”¨æˆ·è¯„ä¼°&lt;/h2>; &lt;p>; æˆ‘ä»¬å¯¹ 123 åå‚ä¸Žè€…è¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œç›®çš„æ˜¯ (1) æ£€æŸ¥æˆ‘ä»¬æå‡ºçš„é—ªçƒæŒ‡æ ‡ä¸Žè§‚çœ‹è€…å¯¹å®žæ—¶å­—å¹•çš„ä½“éªŒçš„ç›¸å…³æ€§ï¼Œä»¥åŠ (2) è¯„ä¼°æˆ‘ä»¬çš„ç¨³å®šæŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬åœ¨ YouTube ä¸­æ‰‹åŠ¨é€‰æ‹©äº† 20 ä¸ªè§†é¢‘ï¼Œä»¥èŽ·å¾—å¹¿æ³›çš„ä¸»é¢˜è¦†ç›–ï¼ŒåŒ…æ‹¬è§†é¢‘ä¼šè®®ã€çºªå½•ç‰‡ã€å­¦æœ¯è®²åº§ã€æ•™ç¨‹ã€æ–°é—»ã€å–œå‰§ç­‰ã€‚å¯¹äºŽæ¯ä¸ªè§†é¢‘ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ä¸ª 30 ç§’çš„å‰ªè¾‘ï¼Œå…¶ä¸­è‡³å°‘åŒ…å« 90% çš„è¯­éŸ³ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬å‡†å¤‡äº†å››ç§ç±»åž‹çš„å®žæ—¶å­—å¹•æ¸²æŸ“æ¥è¿›è¡Œæ¯”è¾ƒï¼š &lt;/p>; &lt;ol>; &lt;li>;åŽŸå§‹ ASRï¼šæ¥è‡ªè¯­éŸ³è½¬æ–‡æœ¬ API çš„åŽŸå§‹è¯­éŸ³è½¬æ–‡æœ¬ç»“æžœã€‚ &lt;/li>;&lt;li>;åŽŸå§‹ ASR + é˜ˆå€¼ï¼šä»…åœ¨ç½®ä¿¡åº¦å¾—åˆ†é«˜äºŽ 0.85 æ—¶æ‰æ˜¾ç¤ºä¸´æ—¶è¯­éŸ³è½¬æ–‡æœ¬ç»“æžœã€‚ &lt;/li>;&lt;li>;ç¨³å®šå­—å¹•ï¼šä½¿ç”¨æˆ‘ä»¬ä¸Šè¿°ç®—æ³•è¿›è¡Œå¯¹é½å’Œåˆå¹¶çš„å­—å¹•ã€‚ &lt;/li>;&lt;li>;ç¨³å®šä¸”å¹³æ»‘çš„å­—å¹•ï¼šå…·æœ‰å¹³æ»‘åŠ¨ç”»ï¼ˆæ»šåŠ¨+æ·¡å…¥æ·¡å‡ºï¼‰çš„ç¨³å®šå­—å¹•ï¼Œä»¥è¯„ä¼°æŸ”å’Œçš„æ˜¾ç¤ºä½“éªŒæ˜¯å¦æœ‰åŠ©äºŽæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚ &lt;/li>; &lt;/ol>; &lt;p>; æˆ‘ä»¬é€šè¿‡è¦æ±‚å‚ä¸Žè€…è§‚çœ‹å½•åˆ¶çš„å®žæ—¶å­—å¹•æ¥æ”¶é›†ç”¨æˆ·è¯„åˆ†ï¼Œå¹¶å¯¹ä»–ä»¬å¯¹èˆ’é€‚åº¦ã€åˆ†å¿ƒåº¦ã€é˜…è¯»éš¾æ˜“åº¦ã€è·Ÿéšè§†é¢‘çš„éš¾æ˜“åº¦ã€ç–²åŠ³ç¨‹åº¦ä»¥åŠå­—å¹•æ˜¯å¦æ­£å¸¸çš„è¯„ä¼°è¿›è¡Œè¯„åˆ†ã€‚æŸå®³äº†ä»–ä»¬çš„ç»éªŒã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;é—ªçƒæŒ‡æ ‡ä¸Žç”¨æˆ·ä½“éªŒä¹‹é—´çš„ç›¸å…³æ€§&lt;/h3>; &lt;p>;æˆ‘ä»¬è®¡ç®—äº†&lt;a href=&quot; https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot;>;é—ªçƒæŒ‡æ ‡ä¸Žæ¯ä¸ªè¡Œä¸ºæµ‹é‡ä¹‹é—´çš„æ–¯çš®å°”æ›¼ç³»æ•°ï¼ˆå€¼èŒƒå›´ä»Ž -1 åˆ° 1ï¼Œå…¶ä¸­è´Ÿå€¼è¡¨ç¤ºä¹‹é—´çš„è´Ÿå…³ç³»ï¼‰ä¸¤ä¸ªå˜é‡ï¼Œæ­£å€¼è¡¨ç¤ºæ­£ç›¸å…³ï¼Œé›¶è¡¨ç¤ºæ²¡æœ‰å…³ç³»ï¼‰ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜Žæˆ‘ä»¬çš„é—ªçƒæŒ‡æ ‡å’Œç”¨æˆ·è¯„åˆ†ä¹‹é—´å­˜åœ¨ç»Ÿè®¡æ˜¾ç€æ€§ (ð‘ &lt; 0.001) ç›¸å…³æ€§ã€‚ç³»æ•°çš„ç»å¯¹å€¼åœ¨0.3å·¦å³ï¼Œè¡¨æ˜Žå…³ç³»ä¸­ç­‰ã€‚ &lt;/p>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;>;&lt;tbody>;&lt;tr>;&lt;td>;&lt;b>;è¡Œä¸ºæµ‹é‡&lt; /b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;&lt;b>;ä¸Žé—ªçƒæŒ‡æ ‡çš„ç›¸å…³æ€§*&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;èˆ’é€‚åº¦ &lt;/td>; &lt;tdalign=&quot;center&quot;>; -0.29 &lt;/ td>; &lt;/tr>; &lt;tr>; &lt;td>;åˆ†æ•£æ³¨æ„åŠ› &lt;/td>; &lt;tdalign=&quot;center&quot;>; 0.33 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;æ˜“äºŽé˜…è¯»&lt;/td>; &lt;tdalign =&quot;center&quot;>; -0.31 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;æ˜“äºŽè§‚çœ‹çš„è§†é¢‘ &lt;/td>; &lt;tdalign=&quot;center&quot;>; -0.29 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ç–²åŠ³&lt;/td>; &lt;tdalign=&quot;center&quot;>;0.36&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td>;ä½“éªŒå—æŸ&lt;/td>;&lt;tdalign=&quot;center&quot;>;0.31&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=â€œcenterâ€cellpadding=â€œ0â€cellspacing=â€œ0â€class=â€œtr-caption-containerâ€style=â€œmargin-leftï¼šautoâ€ ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æˆ‘ä»¬æå‡ºçš„é—ªçƒæŒ‡æ ‡çš„ Spearman ç›¸å…³æ€§æµ‹è¯•ã€‚ *&lt;em>;p&lt;/em>; &lt; 0.001.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;å®žæ—¶å­—å¹•ç¨³å®šæ€§&lt;/h3 >; &lt;p>; æˆ‘ä»¬æå‡ºçš„æŠ€æœ¯ï¼ˆç¨³å®šçš„å¹³æ»‘å­—å¹•ï¼‰å§‹ç»ˆèŽ·å¾—æ›´å¥½çš„è¯„çº§ï¼Œæ ¹æ® &lt;a href=&quot;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&quot;>;Mann- Whitney U æ£€éªŒ&lt;/a>;ï¼ˆä¸‹å›¾ä¸­&lt;em>;p&lt;/em>; &lt; 0.01ï¼‰ï¼Œåœ¨å‰è¿°å…­ä»½è°ƒæŸ¥é™ˆè¿°ä¸­çš„äº”ä»½ä¸­ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç”¨æˆ·è®¤ä¸ºç»è¿‡å¹³æ»‘å¤„ç†çš„ç¨³å®šå­—å¹•æ›´èˆ’é€‚ã€æ›´å®¹æ˜“é˜…è¯»ï¼ŒåŒæ—¶ä¸Žå…¶ä»–ç±»åž‹çš„æ¸²æŸ“ç›¸æ¯”ï¼Œä»–ä»¬æ„Ÿè§‰æ›´å°‘çš„åˆ†å¿ƒã€ç–²åŠ³å’Œä½“éªŒå—æŸã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7y YjqjaNsJ-5K9TsJBMyKDEmy8kS92ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s960/graph.png&quot;æ ·å¼=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;387&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7yYjqjaNsJ-5K9TsJBMyKDEmy8kS9 2ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s16000/graph.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;ç”¨æˆ·å¯¹è°ƒæŸ¥é™ˆè¿°çš„è¯„åˆ†ä»Ž 1ï¼ˆå¼ºçƒˆä¸åŒæ„ï¼‰åˆ° 7ï¼ˆå¼ºçƒˆåŒæ„ï¼‰ã€‚ (**: p&lt;0.01, ***: p&lt;0.001; ****: p&lt;0.0001; ns: ä¸æ˜¾ç€)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div æ ·å¼=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®ºå’Œæœªæ¥æ–¹å‘&lt;/h2>; &lt;p>;å®žæ—¶å­—å¹•ä¸­çš„æ–‡æœ¬ä¸ç¨³å®šä¼šä¸¥é‡å½±å“ç”¨æˆ·çš„é˜…è¯»ä½“éªŒã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºŽè§†è§‰çš„æŒ‡æ ‡æ¥å¯¹å­—å¹•ç¨³å®šæ€§è¿›è¡Œå»ºæ¨¡ï¼Œè¯¥æŒ‡æ ‡åœ¨ç»Ÿè®¡â€‹â€‹ä¸Šä¸Žç”¨æˆ·ä½“éªŒæ˜¾ç€ç›¸å…³ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç¨³å®šå®žæ—¶å­—å¹•æ¸²æŸ“çš„ç®—æ³•ã€‚æˆ‘ä»¬æå‡ºçš„è§£å†³æ–¹æ¡ˆå¯ä»¥é›†æˆåˆ°çŽ°æœ‰çš„ ASR ç³»ç»Ÿä¸­ï¼Œä»¥å¢žå¼ºå®žæ—¶å­—å¹•å¯¹å„ç§ç”¨æˆ·çš„å¯ç”¨æ€§ï¼ŒåŒ…æ‹¬é‚£äº›æœ‰ç¿»è¯‘éœ€æ±‚æˆ–æœ‰å¬åŠ›æ— éšœç¢éœ€æ±‚çš„ç”¨æˆ·ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬çš„å·¥ä½œä»£è¡¨äº†æœç€æµ‹é‡å’Œæé«˜æ–‡æœ¬ç¨³å®šæ€§è¿ˆå‡ºçš„å®žè´¨æ€§ä¸€æ­¥ã€‚è¿™å¯ä»¥æ¼”å˜ä¸ºåŒ…æ‹¬åŸºäºŽè¯­è¨€çš„æŒ‡æ ‡ï¼Œé‡ç‚¹å…³æ³¨å®žæ—¶å­—å¹•ä¸­ä½¿ç”¨çš„å•è¯å’ŒçŸ­è¯­éšæ—¶é—´çš„ä¸€è‡´æ€§ã€‚è¿™äº›æŒ‡æ ‡å¯ä»¥åæ˜ ç”¨æˆ·çš„ä¸é€‚æ„Ÿï¼Œå› ä¸ºå®ƒä¸ŽçŽ°å®žåœºæ™¯ä¸­çš„è¯­è¨€ç†è§£å’Œç†è§£æœ‰å…³ã€‚æˆ‘ä»¬è¿˜æœ‰å…´è¶£è¿›è¡Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Eye_tracking&quot;>;çœ¼çƒè¿½è¸ª&lt;/a>;ç ”ç©¶ï¼ˆä¾‹å¦‚ï¼Œä¸‹é¢æ˜¾ç¤ºçš„è§†é¢‘ï¼‰æ¥è·Ÿè¸ªè§‚çœ‹è€…çš„æ³¨è§†æ¨¡å¼ï¼Œä¾‹å¦‚çœ¼ç›æ³¨è§†å’Œæ‰«è§†ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°äº†è§£æœ€å®¹æ˜“åˆ†æ•£æ³¨æ„åŠ›çš„é”™è¯¯ç±»åž‹ä»¥åŠå¦‚ä½•æé«˜è¿™äº›é”™è¯¯çš„æ–‡æœ¬ç¨³å®šæ€§ã€‚ &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowedfullscreen=&quot;&quot; youtube-src-id =&quot;1E2jGUscWyc&quot;å®½åº¦=&quot;640&quot;é«˜åº¦=&quot;360&quot; src=&quot;https://www.youtube.com/embed/1E2jGUscWyc?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;é˜…è¯»åŽŸå§‹ ASR å­—å¹•æ—¶è·Ÿè¸ªè§‚çœ‹è€…è§†çº¿çš„å›¾ç¤ºã€‚&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;div class=&quot;separator&quot; style=&quot;clear: ä¸¤è€…; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowedfullscreen=&quot;&quot; youtube-src-id=&quot;YfotwPIznL8&quot; width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com /embed/YfotwPIznL8?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;é˜…è¯»ç¨³å®šå’Œå¹³æ»‘çš„å­—å¹•æ—¶è·Ÿè¸ªè§‚çœ‹è€…è§†çº¿çš„å›¾ç¤ºã€‚&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;p>; é€šè¿‡æé«˜å®žæ—¶å­—å¹•ä¸­çš„æ–‡æœ¬ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæ›´æœ‰æ•ˆçš„æ²Ÿé€šå·¥å…·ï¼Œå¹¶æ”¹å–„äººä»¬åœ¨æ—¥å¸¸å¯¹è¯ä¸­ä½¿ç”¨ç†Ÿæ‚‰çš„è¯­è¨€æˆ–é€šè¿‡ç¿»è¯‘ä½¿ç”¨ä¸ç†Ÿæ‚‰çš„è¯­è¨€è¿›è¡Œè”ç³»çš„æ–¹å¼ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;è¿™é¡¹å·¥ä½œæ˜¯ Google å¤šä¸ªå›¢é˜Ÿçš„åä½œæˆæžœã€‚ä¸»è¦è´¡çŒ®è€…åŒ…æ‹¬ Xingyu â€œBruceâ€ Liuã€Jun Zhangã€Leonardo Ferrerã€Susan Xuã€Vikas Bahirwaniã€Boris Smusã€Alex Olwal å’Œ Ruofei Duã€‚æˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢æä¾›å¸®åŠ©çš„åŒäº‹ï¼ŒåŒ…æ‹¬ Nishtha Bhatiaã€Max Spear å’Œ Darcy Philipponã€‚æˆ‘ä»¬è¿˜è¦æ„Ÿè°¢ Lin Liã€Evan Parker å’Œ CHI 2023 å®¡ç¨¿äººã€‚&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1342410539466537463/comments/é»˜è®¤&quot; rel=&quot;replies&quot; title=&quot;å‘å¸ƒè¯„è®º&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text -stability.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/é»˜è®¤/1342410539466537463&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1342410539466537463&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text-stability.html&quot; rel=&quot;alternate&quot; title=&quot;å»ºæ¨¡å¹¶æé«˜å®žæ—¶å­—å¹•ä¸­çš„æ–‡æœ¬ç¨³å®šæ€§â€ type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;ç”µå­é‚®ä»¶>;noreply@blogger.com&lt;/ç”µå­é‚®ä»¶>;&lt;gdï¼šå›¾åƒé«˜åº¦=â€œ16â€rel=â€œhttp://schemas.google.com/g/2005#thumbnailâ€src=â€œhttps://img1.blogblogâ€ .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45-muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U5- 8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s72-c/stabilizedcaptions.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media :thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;æ ‡ç­¾:blogger.com,1999:blog-8474926331452026626.post-8352967842060136321&lt;/id>;&lt;å‘å¸ƒ>;2023-08 -29T12:57:00.001-07:00&lt;/å·²å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-30T17:14:40.708-07:00&lt;/æ›´æ–°>;&lt;category schema=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;æœºå™¨å­¦ä¹ &quot;>;&lt;/category>;&lt;category schema=&quot; http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SayTapï¼šå››è¶³è¿åŠ¨è¯­è¨€&lt;/stitle>;&lt;content type=&quot;html&quot; >;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šGoogle ç ”ç©¶ç§‘å­¦å®¶ Yujin Tang å’Œ Wenhao Yu&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17- mXVFjbz-EOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQqM01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK- FZnvm-tRfXtxOHWHaGqcHaMI/s320/SayTap%20hero.jpg&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; äººç±»å’Œå››è¶³æœºå™¨äººä¹‹é—´ç®€å•æœ‰æ•ˆçš„äº¤äº’ä¸ºåˆ›é€ æ™ºèƒ½ä¸”æœ‰èƒ½åŠ›çš„è¾…åŠ©æœºå™¨äººé“ºå¹³äº†é“è·¯ï¼Œåˆ›é€ äº†ä¸€ä¸ªæŠ€æœ¯ä»¥è¶…ä¹Žæˆ‘ä»¬æƒ³è±¡çš„æ–¹å¼æ”¹å–„æˆ‘ä»¬ç”Ÿæ´»çš„æœªæ¥ã€‚è¿™ç§äººæœºäº¤äº’ç³»ç»Ÿçš„å…³é”®æ˜¯ä½¿å››è¶³æœºå™¨äººèƒ½å¤Ÿå“åº”è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚ &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;å¤§åž‹è¯­è¨€æ¨¡åž‹&lt;/a>; (LLM) çš„æœ€æ–°å‘å±•è¯æ˜Žäº†æ‰§è¡Œ&lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;é«˜å±‚è§„åˆ’&lt;/a>;çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºŽæ³•å­¦ç¡•å£«æ¥è¯´ï¼Œç†è§£ä½Žçº§å‘½ä»¤ï¼ˆä¾‹å¦‚å…³èŠ‚è§’åº¦ç›®æ ‡æˆ–ç”µæœºæ‰­çŸ©ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºŽæœ¬è´¨ä¸Šä¸ç¨³å®šçš„æœ‰è…¿æœºå™¨äººï¼Œéœ€è¦é«˜é¢‘æŽ§åˆ¶ä¿¡å·ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°&lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;çŽ°æœ‰&lt;/a>;&lt;a href=&quot;https://code-as-policies.github.io/&quot;>;å·¥ä½œ&lt;/a>; å‡è®¾ä¸ºæ³•å­¦ç¡•å£«æä¾›é«˜çº§ API æ¥è§„å®šæœºå™¨äººè¡Œä¸ºï¼Œè¿™ä»Žæœ¬è´¨ä¸Šé™åˆ¶äº†ç³»ç»Ÿçš„è¡¨è¾¾èƒ½åŠ›ã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://saytap.github.io&quot;>;SayTapï¼šå››è¶³è¿åŠ¨è¯­è¨€&lt;/a>;â€ä¸­ï¼Œåœ¨ &lt;a href=&quot;https://www.corl2023.org/&quot;>;CoRL 2023&lt;/a>; ä¸Šæå‡ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨è„šæŽ¥è§¦æ¨¡å¼çš„æ–¹æ³•ï¼ˆæŒ‡çš„æ˜¯å››è¶³åŠ¨ç‰©æŽ¥è§¦è„šçš„é¡ºåºå’Œæ–¹å¼ï¼‰æ™ºèƒ½ä½“åœ¨ç§»åŠ¨æ—¶å°†è„šæ”¾åœ¨åœ°é¢ä¸Šï¼‰ä½œä¸ºè¿žæŽ¥è‡ªç„¶è¯­è¨€äººç±»å‘½ä»¤å’Œè¾“å‡ºä½Žçº§å‘½ä»¤çš„è¿åŠ¨æŽ§åˆ¶å™¨çš„æŽ¥å£ã€‚è¿™äº§ç”Ÿäº†äº¤äº’å¼å››è¶³æœºå™¨äººç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå…è®¸ç”¨æˆ·çµæ´»åœ°åˆ¶å®šä¸åŒçš„è¿åŠ¨è¡Œä¸ºï¼ˆä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨ç®€å•çš„è¯­è¨€è¦æ±‚æœºå™¨äººè¡Œèµ°ã€è·‘æ­¥ã€è·³è·ƒæˆ–è¿›è¡Œå…¶ä»–åŠ¨ä½œï¼‰ã€‚æˆ‘ä»¬è´¡çŒ®äº† LLM æç¤ºè®¾è®¡ã€å¥–åŠ±å‡½æ•°ä»¥åŠå°† SayTap æŽ§åˆ¶å™¨æš´éœ²ç»™æŽ¥è§¦æ¨¡å¼çš„å¯è¡Œåˆ†å¸ƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬è¯æ˜Ž SayTap æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå®žçŽ°å¤šç§è¿åŠ¨æ¨¡å¼çš„æŽ§åˆ¶å™¨ï¼Œè¿™äº›æ¨¡å¼å¯ä»¥è½¬ç§»åˆ°çœŸå®žçš„æœºå™¨äººç¡¬ä»¶ä¸Šã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;SayTap æ–¹æ³•&lt;/h2>; &lt;p>; SayTap æ–¹æ³•ä½¿ç”¨æŽ¥è§¦æ¨¡å¼æ¨¡æ¿ï¼Œè¯¥æ¨¡æ¿æ˜¯4 X &lt;i>;T&lt;/i>; 0 å’Œ 1 çŸ©é˜µï¼Œå…¶ä¸­ 0 ä»£è¡¨ä»£ç†çš„è„šåœ¨ç©ºä¸­ï¼Œ1 ä»£è¡¨è„šåœ¨åœ°é¢ä¸Šã€‚ä»Žä¸Šåˆ°ä¸‹ï¼ŒçŸ©é˜µä¸­çš„æ¯ä¸€è¡Œç»™å‡ºå·¦å‰ (FL)ã€å³å‰ (FR)ã€å·¦åŽ (RL) å’Œå³åŽ (RR) è„šçš„è„šæŽ¥è§¦æ¨¡å¼ã€‚ SayTap çš„æŽ§åˆ¶é¢‘çŽ‡ä¸º 50 Hzï¼Œå› æ­¤æ¯ä¸ª 0 æˆ– 1 æŒç»­ 0.02 ç§’ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæ‰€éœ€çš„è„šæŽ¥è§¦æ¨¡å¼ç”±å¤§å°ä¸º L&lt;sub>;w&lt;/sub>; å’Œå½¢çŠ¶ä¸º 4 X L&lt;sub>;w&lt;/sub>; çš„å¾ªçŽ¯æ»‘åŠ¨çª—å£å®šä¹‰ã€‚æ»‘åŠ¨çª—å£ä»ŽæŽ¥è§¦æ¨¡å¼æ¨¡æ¿ä¸­æå–å››ä¸ªè„šæŽ¥åœ°æ ‡å¿—ï¼Œè¿™äº›æ ‡å¿—æŒ‡ç¤ºè„šæ˜¯åœ¨åœ°é¢ä¸Šè¿˜æ˜¯åœ¨ &lt;i>;t&lt;/i>; + 1 å’Œ &lt;i>;t&lt;/i>; + L ä¹‹é—´çš„ç©ºä¸­&lt;å­>;w&lt;/å­>;ã€‚ä¸‹å›¾æ¦‚è¿°äº† SayTap æ–¹æ³•ã€‚ &lt;/p>; &lt;p>; SayTap å¼•å…¥äº†è¿™äº›æ‰€éœ€çš„è„šéƒ¨æŽ¥è§¦æ¨¡å¼ï¼Œä½œä¸ºè‡ªç„¶è¯­è¨€ç”¨æˆ·å‘½ä»¤å’Œè¿åŠ¨æŽ§åˆ¶å™¨ä¹‹é—´çš„æ–°æŽ¥å£ã€‚è¿åŠ¨æŽ§åˆ¶å™¨ç”¨äºŽå®Œæˆä¸»è¦ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œéµå¾ªæŒ‡å®šçš„é€Ÿåº¦ï¼‰å¹¶åœ¨æŒ‡å®šçš„æ—¶é—´å°†æœºå™¨äººçš„è„šæ”¾åœ¨åœ°é¢ä¸Šï¼Œä½¿å¾—å®žçŽ°çš„è„šæŽ¥è§¦æ¨¡å¼å°½å¯èƒ½æŽ¥è¿‘æœŸæœ›çš„æŽ¥è§¦æ¨¡å¼ã€‚ä¸ºäº†å®žçŽ°è¿™ä¸€ç‚¹ï¼Œé™¤äº†æœºå™¨äººçš„æœ¬ä½“æ„Ÿè§‰æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œå…³èŠ‚ä½ç½®å’Œé€Ÿåº¦ï¼‰å’Œä»»åŠ¡ç›¸å…³è¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œç”¨æˆ·æŒ‡å®šçš„é€Ÿåº¦å‘½ä»¤ï¼‰ä¹‹å¤–ï¼Œè¿åŠ¨æŽ§åˆ¶å™¨è¿˜å°†æ¯ä¸ªæ—¶é—´æ­¥æ‰€éœ€çš„è„šéƒ¨æŽ¥è§¦æ¨¡å¼ä½œä¸ºå…¶è¾“å…¥ã€‚ ã€‚æˆ‘ä»¬ä½¿ç”¨&lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_reinforcement_learning&quot;>;æ·±åº¦å¼ºåŒ–å­¦ä¹ &lt;/a>;æ¥è®­ç»ƒè¿åŠ¨æŽ§åˆ¶å™¨å¹¶å°†å…¶è¡¨ç¤ºä¸ºæ·±åº¦ç¥žç»ç½‘ç»œã€‚åœ¨æŽ§åˆ¶å™¨è®­ç»ƒæœŸé—´ï¼Œéšæœºç”Ÿæˆå™¨å¯¹æ‰€éœ€çš„è„šéƒ¨æŽ¥è§¦æ¨¡å¼è¿›è¡Œé‡‡æ ·ï¼Œç„¶åŽä¼˜åŒ–ç­–ç•¥ä»¥è¾“å‡ºä½Žçº§æœºå™¨äººåŠ¨ä½œï¼Œä»¥å®žçŽ°æ‰€éœ€çš„è„šéƒ¨æŽ¥è§¦æ¨¡å¼ã€‚ç„¶åŽåœ¨æµ‹è¯•æ—¶ï¼Œæ³•å­¦ç¡•å£«å°†ç”¨æˆ·å‘½ä»¤è½¬æ¢ä¸ºè„šéƒ¨æŽ¥è§¦æ¨¡å¼ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2â€‹â€‹aLrGlJ9CTOIkfI-G8jVZrYE083 _U84CCOq2pGuewb4szwCvoRc8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/s935/image13.png&quot;æ ·å¼=&quot;å·¦è¾¹è·ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;935&quot; height=&quot;297&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2â€‹â€‹aLrGlJ9CTOIkfI-G8jVZrYE083_U84CCOq2pGuewb4szwCvoRc 8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/w640-h297/image13.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td ç±»=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap æ–¹æ³•æ¦‚è¿°ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;br />; &lt;video autoplay=&quot; â€œå¾ªçŽ¯=â€â€é™éŸ³=â€â€playsinline=â€â€æ ·å¼==å·¦è¾¹ç¼˜ï¼š10%ï¼›å³è¾¹è·ï¼š10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/saytap_blog.mp4&quot; type= &quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;tablealign=â€œcenterâ€cellpadding=â€œ0â€cellspacing=â€œ0â€class=â€œtr-caption-containerâ€style=â€œmargin-leftâ€ ï¼š æ±½è½¦;å³è¾¹è·ï¼šè‡ªåŠ¨ï¼› text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap ä½¿ç”¨è„šæŽ¥è§¦æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼Œæ¯åªè„šçš„ 0 å’Œ 1 åºåˆ—æ’å›¾ï¼ˆå…¶ä¸­ 0 è¡¨ç¤ºè„šåœ¨ç©ºä¸­ï¼Œ1 è¡¨ç¤ºè„šåœ¨åœ°é¢ï¼‰ä½œä¸ºæ¡¥æŽ¥è‡ªç„¶è¯­è¨€ç”¨æˆ·å‘½ä»¤å’Œä½Žçº§æŽ§åˆ¶å‘½ä»¤çš„æŽ¥å£ã€‚ä½¿ç”¨åŸºäºŽå¼ºåŒ–å­¦ä¹ çš„è¿åŠ¨æŽ§åˆ¶å™¨ï¼Œç»è¿‡è®­ç»ƒå¯å®žçŽ°æ‰€éœ€çš„æŽ¥è§¦æ¨¡å¼ï¼ŒSayTap å…è®¸å››è¶³æœºå™¨äººæŽ¥å—ç®€å•ç›´æŽ¥çš„æŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼Œâ€œæ…¢æ…¢å‘å‰å°è·‘ã€‚â€ï¼‰ä»¥åŠæ¨¡ç³Šçš„ç”¨æˆ·å‘½ä»¤ï¼ˆä¾‹å¦‚ï¼Œâ€œå¥½æ¶ˆæ¯ï¼Œæˆ‘ä»¬è¿™ä¸ªå‘¨æœ«è¦åŽ»é‡Žé¤ï¼â€ï¼‰å¹¶åšå‡ºååº”&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; æˆ‘ä»¬è¯æ˜Žï¼Œå½“ç»™å‡ºæ­£ç¡®è®¾è®¡çš„æç¤ºæ—¶ï¼Œæ³•å­¦ç¡•å£«èƒ½å¤Ÿå‡†ç¡®åœ°å°†ç”¨æˆ·å‘½ä»¤æ˜ å°„åˆ°æŒ‡å®šæ ¼å¼çš„è¶³éƒ¨æŽ¥è§¦æ¨¡å¼æ¨¡æ¿ä¸­ï¼Œå³ä½¿åœ¨å‘½ä»¤æ˜¯éžç»“æž„åŒ–æˆ–æ¨¡ç³Šçš„æƒ…å†µä¸‹ã€‚åœ¨è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨éšæœºæ¨¡å¼ç”Ÿæˆå™¨æ¥ç”Ÿæˆå…·æœ‰å„ç§æ¨¡å¼é•¿åº¦ &lt;i>;T&lt;/i>; çš„æŽ¥è§¦æ¨¡å¼æ¨¡æ¿ï¼ŒåŸºäºŽå¾ªçŽ¯å†…çš„è„šä¸Žåœ°é¢çš„æŽ¥è§¦æ¯”ç»™å®šçš„æ­¥æ€ç±»åž‹&lt;i>;G&lt;/i>;ï¼Œä»¥ä¾¿è¿åŠ¨æŽ§åˆ¶å™¨èƒ½å¤Ÿå­¦ä¹ å¹¿æ³›çš„è¿åŠ¨åˆ†å¸ƒï¼Œä»Žè€ŒèŽ·å¾—æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…&lt;a href=&quot;https://arxiv.org/abs/2306.07580&quot;>;è®ºæ–‡&lt;/a>;ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“æžœ&lt;/h2>; &lt;p>; ä¸€ä¸ªç®€å•çš„æç¤ºï¼Œä»…åŒ…å«ä¸‰ä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹å¸¸è§çš„è„šéƒ¨æŽ¥è§¦æ¨¡å¼ï¼Œæ³•å­¦ç¡•å£«å¯ä»¥å°†å„ç§äººç±»å‘½ä»¤å‡†ç¡®åœ°è½¬æ¢ä¸ºæŽ¥è§¦æ¨¡å¼ï¼Œç”šè‡³å¯ä»¥æŽ¨å¹¿åˆ°é‚£äº›æ²¡æœ‰æ˜Žç¡®æŒ‡å®šæœºå™¨äººåº”å¦‚ä½•ååº”çš„æŒ‡ä»¤ã€‚ &lt;/p>; &lt;p>; SayTap æç¤ºç®€æ´æ˜Žäº†ï¼Œç”±å››ä¸ªéƒ¨åˆ†ç»„æˆï¼š (1) ä¸€èˆ¬è¯´æ˜Žï¼Œæè¿° LLM åº”å®Œæˆçš„ä»»åŠ¡ï¼› (2) æ­¥æ€å®šä¹‰ï¼Œæé†’æ³•å­¦ç¡•å£«æœ‰å…³å››è¶³æ­¥æ€çš„åŸºæœ¬çŸ¥è¯†ä»¥åŠå®ƒä»¬å¦‚ä½•ä¸Žæƒ…ç»ªç›¸å…³ï¼› (3)è¾“å‡ºæ ¼å¼å®šä¹‰ï¼› (4) ä¸ºæ³•å­¦ç¡•å£«æä¾›åœ¨ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ çš„æœºä¼šçš„ç¤ºä¾‹ã€‚æˆ‘ä»¬è¿˜æŒ‡å®šäº†äº”ç§é€Ÿåº¦ï¼Œå…è®¸æœºå™¨äººå‘å‰æˆ–å‘åŽã€å¿«é€Ÿæˆ–ç¼“æ…¢ç§»åŠ¨æˆ–ä¿æŒé™æ­¢ã€‚ &lt;/p>; &lt;span style=&quot;font-size:small;&quot;>; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px;white-space: pre-wrap;&quot;>;&lt; font color=&quot;#0000ff&quot;>;ä¸€èˆ¬è¯´æ˜Žå—&lt;/font>; æ‚¨æ˜¯ç‹—è„šæŽ¥è§¦æ¨¡å¼ä¸“å®¶ã€‚æ‚¨çš„å·¥ä½œæ˜¯æ ¹æ®è¾“å…¥ç»™å‡ºé€Ÿåº¦å’Œè„šæŽ¥è§¦æ¨¡å¼ã€‚æ— è®ºè¾“å…¥æ˜¯ä»€ä¹ˆï¼Œæ‚¨å§‹ç»ˆä¼šä»¥æ­£ç¡®çš„æ ¼å¼ç»™å‡ºè¾“å‡ºã€‚ &lt;font color=&quot;#0000ff&quot;>;æ­¥æ€å®šä¹‰å—&lt;/font>; ä¸‹é¢æ˜¯å…³äºŽæ­¥æ€çš„æè¿°ï¼š 1. å°è·‘æ˜¯ä¸¤æ¡å¯¹è§’ç›¸å¯¹çš„è…¿åŒæ—¶ç€åœ°çš„æ­¥æ€ã€‚ 2. è¸±æ­¥æ˜¯èº«ä½“å·¦å³ä¸¤ä¾§çš„è…¿åŒæ—¶ç€åœ°çš„æ­¥æ€ã€‚ 3. è·³è·ƒæ˜¯ä¸¤å‰/åŽè…¿åŒæ—¶ç€åœ°çš„æ­¥æ€ã€‚å®ƒå…·æœ‰è¾ƒé•¿çš„æš‚åœé˜¶æ®µï¼Œä¾‹å¦‚ï¼Œåœ¨è‡³å°‘ 25% çš„å‘¨æœŸé•¿åº¦å†…ï¼Œæ‰€æœ‰è„šéƒ½ç¦»å¼€åœ°é¢ã€‚è¿™æ ·çš„æ­¥æ€ä¹Ÿç»™äººä¸€ç§å¹¸ç¦çš„æ„Ÿè§‰ã€‚ &lt;font color=&quot;#0000ff&quot;>;è¾“å‡ºæ ¼å¼å®šä¹‰å—&lt;/font>; ä»¥ä¸‹æ˜¯æè¿°é€Ÿåº¦å’Œè„šæŽ¥è§¦æ¨¡å¼çš„è§„åˆ™ï¼š 1. æ‚¨åº”è¯¥é¦–å…ˆè¾“å‡ºé€Ÿåº¦ï¼Œç„¶åŽè¾“å‡ºè„šæŽ¥è§¦æ¨¡å¼ã€‚ 2. æœ‰äº”ç§é€Ÿåº¦å¯ä¾›é€‰æ‹©ï¼š[-1.0ã€-0.5ã€0.0ã€0.5ã€1.0]ã€‚ 3. ä¸€ä¸ªå›¾æ¡ˆæœ‰ 4 æ¡çº¿ï¼Œæ¯æ¡çº¿ä»£è¡¨ä¸€æ¡è…¿çš„è„šæŽ¥è§¦å›¾æ¡ˆã€‚ 4. æ¯è¡Œéƒ½æœ‰ä¸€ä¸ªæ ‡ç­¾ã€‚ â€œFLâ€æ˜¯å·¦å‰è…¿ï¼Œâ€œFRâ€æ˜¯å³å‰è…¿ï¼Œâ€œRLâ€æ˜¯å·¦åŽè…¿ï¼Œâ€œRRâ€æ˜¯å³åŽè…¿ã€‚ 5. æ¯è¡Œä¸­ï¼Œâ€œ0â€ä»£è¡¨è„šåœ¨ç©ºä¸­ï¼Œâ€œ1â€ä»£è¡¨è„šåœ¨åœ°ä¸Šã€‚ &lt;font color=&quot;#0000ff&quot;>;ç¤ºä¾‹å—&lt;/font>; è¾“å…¥ï¼šTrot Slow è¾“å‡ºï¼š0.5 FL: 11111111111111111000000000 FR: 00000000011111111111111111 RL: 00000000011111111111111111 RR: 11111111111111111000000000 è¾“å…¥: ç»‘å®šåˆ°ä½ è¾“å‡º: 0.0 FL: 11111111111100000000000000 FR: 11111111111100000000000000 RL: 00000011111111111100000000 RR: 00000011111111111100000000 è¾“å…¥: å¿«é€€ è¾“å‡º: -1.0 FL: 11111111100001111111110000 FR: 0000111111111000011 1111111 RL: 11111111100001111111110000 RR: 00001111111110000111111111 è¾“å…¥: &lt;/pre>;&lt;/span>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0 &quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;SayTap æç¤º LLMã€‚è“è‰²æ–‡å­—ç”¨äºŽè¯´æ˜Žï¼Œä¸è¾“å…¥LLMã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;éµå¾ªç®€å•ç›´æŽ¥çš„å‘½ä»¤&lt;/h3>; &lt;p>; æˆ‘ä»¬åœ¨ä¸‹é¢çš„è§†é¢‘ä¸­æ¼”ç¤ºäº† SayTap ç³»ç»Ÿå¯ä»¥æˆåŠŸæ‰§è¡Œå‘½ä»¤ç›´æŽ¥ä¸”æ¸…æ™°çš„ä»»åŠ¡ã€‚å°½ç®¡ä¸‰ä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­æœªæ¶µç›–æŸäº›å‘½ä»¤ï¼Œä½†æˆ‘ä»¬èƒ½å¤Ÿå¼•å¯¼ LLM é€šè¿‡â€œæ­¥æ€å®šä¹‰å—â€ï¼ˆè¯·å‚é˜…â€‹â€‹ä¸Šé¢æç¤ºä¸­çš„ç¬¬äºŒä¸ªå—ï¼‰è¡¨è¾¾å…¶é¢„è®­ç»ƒé˜¶æ®µçš„å†…éƒ¨çŸ¥è¯†æç¤ºã€‚ &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;æºsrc=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/æº>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/ source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;éµå¾ªéžç»“æž„åŒ–æˆ–æ¨¡ç³Šå‘½ä»¤&lt;/h3>; &lt;p >; ä½†æ›´æœ‰è¶£çš„æ˜¯ SayTap å¤„ç†éžç»“æž„åŒ–å’Œæ¨¡ç³ŠæŒ‡ä»¤çš„èƒ½åŠ›ã€‚åªéœ€ç¨åŠ æç¤ºå³å¯å°†æŸäº›æ­¥æ€ä¸Žä¸€èˆ¬æƒ…ç»ªå°è±¡è”ç³»èµ·æ¥ï¼Œæœºå™¨äººåœ¨å¬åˆ°ä»¤äººå…´å¥‹çš„ä¿¡æ¯ï¼ˆä¾‹å¦‚â€œæˆ‘ä»¬è¦åŽ»é‡Žé¤ï¼â€ï¼‰æ—¶å°±ä¼šä¸Šä¸‹è·³è·ƒã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å‡†ç¡®åœ°å‘ˆçŽ°åœºæ™¯ï¼ˆä¾‹å¦‚ï¼Œå½“è¢«å‘ŠçŸ¥åœ°é¢éžå¸¸çƒ­æ—¶ï¼Œå®ƒçš„è„šå‡ ä¹Žä¸æŽ¥è§¦åœ°é¢ï¼Œå¿«é€Ÿç§»åŠ¨ï¼‰ã€‚ &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/æº>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/æº>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/æº>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/ source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®ºå’Œæœªæ¥å·¥ä½œ&lt;/h2>; &lt;p>;æˆ‘ä»¬æŽ¨å‡º SayTapï¼Œè¿™æ˜¯ä¸€ç§å››è¶³æœºå™¨äººäº¤äº’ç³»ç»Ÿï¼Œå…è®¸ç”¨æˆ·çµæ´»åœ°åˆ¶å®šä¸åŒçš„è¿åŠ¨è¡Œä¸ºã€‚ SayTap å¼•å…¥äº†æ‰€éœ€çš„è„šæŽ¥è§¦æ¨¡å¼ä½œä¸ºè‡ªç„¶è¯­è¨€å’Œä½Žçº§æŽ§åˆ¶å™¨ä¹‹é—´çš„æ–°æŽ¥å£ã€‚è¿™ä¸ªæ–°ç•Œé¢ç®€å•çµæ´»ï¼Œæ­¤å¤–ï¼Œå®ƒå…è®¸æœºå™¨äººéµå¾ªç›´æŽ¥æŒ‡ä»¤å’Œæ²¡æœ‰æ˜Žç¡®è¯´æ˜Žæœºå™¨äººåº”å¦‚ä½•ååº”çš„å‘½ä»¤ã€‚ &lt;/p>; &lt;p>; æœªæ¥å·¥ä½œçš„ä¸€ä¸ªæœ‰è¶£æ–¹å‘æ˜¯æµ‹è¯•æš—ç¤ºç‰¹å®šæ„Ÿè§‰çš„å‘½ä»¤æ˜¯å¦å…è®¸æ³•å­¦ç¡•å£«è¾“å‡ºæ‰€éœ€çš„æ­¥æ€ã€‚åœ¨ä¸Šé¢ç»“æžœéƒ¨åˆ†æ˜¾ç¤ºçš„æ­¥æ€å®šä¹‰å—ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå°†å¿«ä¹å¿ƒæƒ…ä¸Žè·³è·ƒæ­¥æ€è”ç³»èµ·æ¥çš„å¥å­ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæä¾›æ›´å¤šä¿¡æ¯å¯ä»¥å¢žå¼ºæ³•å­¦ç¡•å£«çš„è§£é‡Šï¼ˆä¾‹å¦‚ï¼Œéšå«çš„æ„Ÿå—ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸­ï¼Œå¿«ä¹çš„æ„Ÿè§‰å’Œè·³è·ƒæ­¥æ€ä¹‹é—´çš„è”ç³»ä½¿æœºå™¨äººåœ¨éµå¾ªæ¨¡ç³Šçš„äººç±»å‘½ä»¤æ—¶è¡¨çŽ°å¾—ç”ŸåŠ¨ã€‚æœªæ¥å·¥ä½œçš„å¦ä¸€ä¸ªæœ‰è¶£çš„æ–¹å‘æ˜¯å¼•å…¥å¤šæ¨¡å¼è¾“å…¥ï¼Œä¾‹å¦‚è§†é¢‘å’ŒéŸ³é¢‘ã€‚ç†è®ºä¸Šï¼Œä»Žè¿™äº›ä¿¡å·è½¬æ¢è€Œæ¥çš„è„šéƒ¨æŽ¥è§¦æ¨¡å¼ä»ç„¶é€‚ç”¨äºŽæˆ‘ä»¬çš„ç®¡é“ï¼Œå¹¶å°†è§£é”æ›´å¤šæœ‰è¶£çš„ç”¨ä¾‹ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;Yujin Tangã€Wenhao Yuã€Jie Tanã€Heiga Zenã€Aleksandra Faust å’Œ Tatsuya Harada è¿›è¡Œäº†è¿™é¡¹ç ”ç©¶ã€‚è¿™é¡¹å·¥ä½œæ˜¯åœ¨å›¢é˜Ÿåœ¨ Google Research æœŸé—´æž„æ€å’Œæ‰§è¡Œçš„ï¼Œå¹¶å°†åœ¨ Google DeepMind ç»§ç»­è¿›è¡Œã€‚ä½œè€…è¡·å¿ƒæ„Ÿè°¢å¼ å»·å—ã€Linda Luuã€Kuang-Huei Leeã€Vincent Vanhoucke å’Œ Douglas Eck åœ¨å®žéªŒä¸­æä¾›çš„å®è´µè®¨è®ºå’ŒæŠ€æœ¯æ”¯æŒã€‚&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/å†…å®¹>;&lt;link href=&quot;http://blog.research.google/feeds/8352967842060136321/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 æ¡è¯„è®º&quot; type=&quot;text/html&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08 /saytap-language-to-quadrupedal.html&quot; rel=&quot;alternate&quot; title=&quot;SayTapï¼šè¯­è¨€åˆ°å››è¶³è¿åŠ¨&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>; http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com /g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height= â€œ72â€ç½‘å€=â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQq M01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s72-c/SayTap%20hero.jpg&quot; width=&quot;72 &quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;æ ‡ç­¾ï¼š Blogger.comï¼Œ1999ï¼šBlog-847492631452026626.POST-781660063956467509 &lt;/ID>; &lt;/ID>; &lt;/id>; &lt;Ebtumed>; 2023-08-28-28T09ï¼š59ï¼š009ï¼š00.00.003-07ï¼š00ï¼š003-07ï¼š00ï¼š00 07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;è®¡ç®—æœºè§†è§‰&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;å¤šæ¨¡å¼å­¦ä¹ &quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;RO-ViTï¼šä½¿ç”¨è§†è§‰è½¬æ¢å™¨è¿›è¡Œå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹çš„åŒºåŸŸæ„ŸçŸ¥é¢„è®­ç»ƒ&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…Dahun Kim å’Œ Wei Cheng Kuoï¼Œç ”ç©¶ç§‘å­¦å®¶ï¼ŒGoogle&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa 98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVuIj5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s1100/RO-VIT -hero.jpg&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; æ£€æµ‹è§†è§‰ä¸–ç•Œä¸­çš„ç‰©ä½“çš„èƒ½åŠ›å¯¹äºŽè®¡ç®—æœºè§†è§‰å’Œæœºå™¨æ™ºèƒ½è‡³å…³é‡è¦ï¼Œä»Žè€Œå®žçŽ°è‡ªé€‚åº”è‡ªä¸»ä»£ç†å’Œå¤šåŠŸèƒ½è´­ç‰©ç³»ç»Ÿç­‰åº”ç”¨ã€‚ç„¶è€Œï¼ŒçŽ°ä»£ç‰©ä½“æ£€æµ‹å™¨å—åˆ°è®­ç»ƒæ•°æ®çš„æ‰‹åŠ¨æ³¨é‡Šçš„é™åˆ¶ï¼Œå¯¼è‡´è¯æ±‡é‡æ˜Žæ˜¾å°äºŽçŽ°å®žä¸­é‡åˆ°çš„å¤§é‡ç‰©ä½“ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œå‡ºçŽ°äº†&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;å¼€æ”¾è¯æ±‡æ£€æµ‹ä»»åŠ¡&lt;/a>; (OVD)ï¼Œå®ƒåˆ©ç”¨å›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒå¹¶åˆå¹¶æ–°çš„ç±»åˆ«åç§°åœ¨æµ‹è¯•æ—¶å°†å®ƒä»¬ä¸Žå›¾åƒå†…å®¹ç›¸å…³è”ã€‚é€šè¿‡å°†ç±»åˆ«è§†ä¸ºæ–‡æœ¬åµŒå…¥ï¼Œå¼€æ”¾è¯æ±‡æ£€æµ‹å™¨å¯ä»¥é¢„æµ‹å„ç§çœ‹ä¸è§çš„å¯¹è±¡ã€‚å„ç§æŠ€æœ¯ï¼Œä¾‹å¦‚&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒ&lt;/a>;ã€&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot; >;çŸ¥è¯†è’¸é¦&lt;/a>;ã€&lt;a href=&quot;https://arxiv.org/abs/2112.09106&quot;>;ä¼ªæ ‡ç­¾&lt;/a>;å’Œå†»ç»“æ¨¡åž‹ï¼Œé€šå¸¸é‡‡ç”¨&lt;a href=&quot;https://en. wikipedia.org/wiki/Convolutional_neural_network&quot;>;å·ç§¯ç¥žç»ç½‘ç»œ&lt;/a>;ï¼ˆCNNï¼‰ä¸»å¹²å·²ç»è¢«æå‡ºã€‚éšç€&lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;>;è§†è§‰è½¬æ¢å™¨&lt;/a>; (ViT) çš„æ—¥ç›Šæ™®åŠï¼Œè¿™ä¸€ç‚¹éžå¸¸é‡è¦æŽ¢ç´¢å®ƒä»¬æž„å»ºç†Ÿç»ƒçš„å¼€æ”¾è¯æ±‡æ£€æµ‹å™¨çš„æ½œåŠ›ã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; çŽ°æœ‰æ–¹æ³•å‡è®¾å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;è§†è§‰è¯­è¨€æ¨¡åž‹&lt;/a>;ï¼ˆVLMï¼‰ï¼Œå¹¶ä¸“æ³¨äºŽä»Žè¿™äº›æ¨¡åž‹ä¸­è¿›è¡Œå¾®è°ƒæˆ–&lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;è’¸é¦&lt;/a>;ï¼Œä»¥è§£å†³å›¾åƒä¹‹é—´çš„å·®å¼‚çº§é¢„è®­ç»ƒå’Œå¯¹è±¡çº§å¾®è°ƒã€‚ç„¶è€Œï¼Œç”±äºŽ VLM ä¸»è¦æ˜¯ä¸ºåˆ†ç±»å’Œæ£€ç´¢ç­‰å›¾åƒçº§ä»»åŠ¡è€Œè®¾è®¡çš„ï¼Œå› æ­¤å®ƒä»¬åœ¨é¢„è®­ç»ƒé˜¶æ®µæ²¡æœ‰å……åˆ†åˆ©ç”¨å¯¹è±¡æˆ–åŒºåŸŸçš„æ¦‚å¿µã€‚å› æ­¤ï¼Œå¦‚æžœæˆ‘ä»¬å°†å±€éƒ¨æ€§ä¿¡æ¯æž„å»ºåˆ°å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒä¸­ï¼Œè¿™å¯èƒ½æœ‰åˆ©äºŽå¼€æ”¾è¯æ±‡æ£€æµ‹ã€‚ &lt;/p>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://arxiv.org/abs/2305.07011&quot;>;RO-ViTï¼šä½¿ç”¨è§†è§‰å˜åŽ‹å™¨è¿›è¡Œå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹çš„åŒºåŸŸæ„ŸçŸ¥é¢„è®­ç»ƒ&lt;/a>;â€ä¸­ï¼Œåœ¨ &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>; ä¸Šæå‡ºï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œä»¥åŒºåŸŸæ„ŸçŸ¥çš„æ–¹å¼é¢„è®­ç»ƒè§†è§‰å˜æ¢å™¨ï¼Œä»¥æ”¹è¿›å¼€æ”¾è¯æ±‡æ£€æµ‹ã€‚åœ¨è§†è§‰è½¬æ¢å™¨ä¸­ï¼Œä½ç½®åµŒå…¥è¢«æ·»åŠ åˆ°å›¾åƒå—ä¸­ï¼Œä»¥å¯¹å›¾åƒä¸­æ¯ä¸ªå—çš„ç©ºé—´ä½ç½®ä¿¡æ¯è¿›è¡Œç¼–ç ã€‚æ ‡å‡†é¢„è®­ç»ƒé€šå¸¸ä½¿ç”¨å…¨å›¾åƒä½ç½®åµŒå…¥ï¼Œè¿™ä¸èƒ½å¾ˆå¥½åœ°æŽ¨å¹¿åˆ°æ£€æµ‹ä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä½ç½®åµŒå…¥æ–¹æ¡ˆï¼Œç§°ä¸ºâ€œè£å‰ªä½ç½®åµŒå…¥â€ï¼Œå®ƒå¯ä»¥æ›´å¥½åœ°ä¸Žæ£€æµ‹å¾®è°ƒä¸­åŒºåŸŸè£å‰ªçš„ä½¿ç”¨ä¿æŒä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°† &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function#Neural_networks&quot;>;softmax äº¤å‰ç†µæŸå¤±&lt;/a>; æ›¿æ¢ä¸º &lt;a href=&quot;https://arxiv.org/ abs/1708.02002&quot;>;å¯¹æ¯”å›¾åƒæ–‡æœ¬å­¦ä¹ ä¸­çš„ç„¦ç‚¹æŸå¤±&lt;/a>;ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä»Žæ›´å…·æŒ‘æˆ˜æ€§å’Œä¿¡æ¯é‡æ›´å¤§çš„ç¤ºä¾‹ä¸­å­¦ä¹ ã€‚æœ€åŽï¼Œæˆ‘ä»¬åˆ©ç”¨æ–°å¯¹è±¡æè®®çš„æœ€æ–°è¿›å±•æ¥å¢žå¼ºå¼€æ”¾è¯æ±‡æ£€æµ‹å¾®è°ƒï¼Œè¿™æ˜¯ç”±äºŽè§‚å¯Ÿåˆ°çŽ°æœ‰æ–¹æ³•ç»å¸¸ç”±äºŽå¯¹å‰æ™¯ç±»åˆ«çš„è¿‡åº¦æ‹Ÿåˆè€Œåœ¨æè®®é˜¶æ®µé”™è¿‡æ–°å¯¹è±¡ã€‚æˆ‘ä»¬è¿˜åœ¨&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm/rovit&quot;>;æ­¤å¤„&lt;/a>;å‘å¸ƒäº†ä»£ç ã€‚ &lt;/p>; &lt;br />; &lt;h2>;åŒºåŸŸæ„ŸçŸ¥å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒ&lt;/h2>; &lt;p>; çŽ°æœ‰çš„ VLM ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥å°†æ•´ä¸ªå›¾åƒä¸Žæ–‡æœ¬æè¿°è¿›è¡ŒåŒ¹é…ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°çŽ°æœ‰å¯¹æ¯”é¢„è®­ç»ƒæ–¹æ³•å’Œå¼€æ”¾è¯æ±‡æ£€æµ‹ä¸­ä½¿ç”¨ä½ç½®åµŒå…¥çš„æ–¹å¼ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ã€‚ä½ç½®åµŒå…¥å¯¹äºŽè½¬æ¢å™¨å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒä»¬æä¾›äº†é›†åˆä¸­æ¯ä¸ªå…ƒç´ æ¥è‡ªä½•å¤„çš„ä¿¡æ¯ã€‚æ­¤ä¿¡æ¯é€šå¸¸å¯¹ä¸‹æ¸¸è¯†åˆ«å’Œå®šä½ä»»åŠ¡æœ‰ç”¨ã€‚é¢„è®­ç»ƒæ–¹æ³•é€šå¸¸åœ¨è®­ç»ƒæœŸé—´åº”ç”¨å…¨å›¾åƒä½ç½®åµŒå…¥ï¼Œå¹¶å¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼ˆä¾‹å¦‚é›¶æ ·æœ¬è¯†åˆ«ï¼‰ä½¿ç”¨ç›¸åŒçš„ä½ç½®åµŒå…¥ã€‚ç„¶è€Œï¼Œè¯†åˆ«å‘ç”Ÿåœ¨å¼€æ”¾è¯æ±‡æ£€æµ‹å¾®è°ƒçš„åŒºåŸŸçº§åˆ«ï¼Œè¿™éœ€è¦å…¨å›¾åƒä½ç½®åµŒå…¥æ³›åŒ–åˆ°ä»–ä»¬åœ¨é¢„è®­ç»ƒæœŸé—´ä»Žæœªè§è¿‡çš„åŒºåŸŸã€‚ &lt;/p>; &lt;p>; ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†&lt;em>;è£å‰ªä½ç½®åµŒå…¥&lt;/em>;ï¼ˆCPEï¼‰ã€‚é€šè¿‡ CPEï¼Œæˆ‘ä»¬å°†ä½ç½®åµŒå…¥ä»Žé¢„è®­ç»ƒçš„å…¸åž‹å›¾åƒå°ºå¯¸ï¼ˆä¾‹å¦‚ 224x224 åƒç´ ï¼‰ä¸Šé‡‡æ ·åˆ°æ£€æµ‹ä»»åŠ¡çš„å…¸åž‹å›¾åƒå°ºå¯¸ï¼ˆä¾‹å¦‚ 1024x1024 åƒç´ ï¼‰ã€‚ç„¶åŽæˆ‘ä»¬éšæœºè£å‰ªä¸€ä¸ªåŒºåŸŸå¹¶è°ƒæ•´å…¶å¤§å°ï¼Œå¹¶åœ¨é¢„è®­ç»ƒæœŸé—´å°†å…¶ç”¨ä½œå›¾åƒçº§ä½ç½®åµŒå…¥ã€‚ä½œç‰©çš„ä½ç½®ã€æ¯”ä¾‹å’Œçºµæ¨ªæ¯”æ˜¯éšæœºé‡‡æ ·çš„ã€‚ç›´è§‚ä¸Šï¼Œè¿™å¯¼è‡´æ¨¡åž‹å°†å›¾åƒæœ¬èº«è§†ä¸ºå®Œæ•´å›¾åƒï¼Œè€Œä¸æ˜¯ä¸€äº›è¾ƒå¤§çš„æœªçŸ¥å›¾åƒçš„åŒºåŸŸè£å‰ªã€‚è¿™æ›´å¥½åœ°åŒ¹é…ä¸‹æ¸¸æ£€æµ‹ç”¨ä¾‹ï¼Œå…¶ä¸­è¯†åˆ«å‘ç”Ÿåœ¨åŒºåŸŸçº§åˆ«è€Œä¸æ˜¯å›¾åƒçº§åˆ«ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZ qTJq4heQruWCeaQsM5uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s952 /RO-ViT-img6.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;952 â€œ src =â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZqTJq4heQruWCeaQsM5 uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s16000/RO-ViT-img6.jpg&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;å¯¹äºŽé¢„è®­ç»ƒï¼Œæˆ‘ä»¬æå‡º&lt;em>;è£å‰ªä½ç½®åµŒå…¥&lt;/em>;&amp;nbsp; ï¼ˆCPEï¼‰éšæœºè£å‰ªä½ç½®åµŒå…¥åŒºåŸŸå¹¶è°ƒæ•´å…¶å¤§å°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ•´ä¸ªå›¾åƒä½ç½®åµŒå…¥ï¼ˆPEï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç„¦ç‚¹æŸå¤±è€Œä¸æ˜¯å¸¸è§çš„softmaxäº¤å‰ç†µæŸå¤±æ¥è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;æˆ‘ä»¬è¿˜å‘çŽ°ä»Žå›°éš¾çš„ä¾‹å­ä¸­å­¦ä¹ æ˜¯æœ‰ç›Šçš„ç„¦ç‚¹æŸå¤±ã€‚ä¸Ž softmax äº¤å‰ç†µæŸå¤±ç›¸æ¯”ï¼Œç„¦ç‚¹æŸå¤±å¯ä»¥æ›´å¥½åœ°æŽ§åˆ¶æ ·æœ¬çš„æƒé‡ã€‚æˆ‘ä»¬åœ¨å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒçš„æŸå¤±ä¸­é‡‡ç”¨ç„¦ç‚¹æŸå¤±å¹¶å°†å…¶æ›¿æ¢ä¸º softmax äº¤å‰ç†µæŸå¤±ã€‚ CPE å’Œç„¦ç‚¹æŸå¤±éƒ½æ²¡æœ‰å¼•å…¥é¢å¤–çš„å‚æ•°å¹¶ä¸”è®¡ç®—æˆæœ¬æœ€å°ã€‚ &lt;/p>; &lt;br />; &lt;h2>;å¼€æ”¾è¯æ±‡æ£€æµ‹å™¨å¾®è°ƒ&lt;/h2>; &lt;p>;å¼€æ”¾è¯æ±‡æ£€æµ‹å™¨ä½¿ç”¨â€œåŸºæœ¬â€ç±»åˆ«çš„æ£€æµ‹æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œä½†éœ€è¦æ£€æµ‹â€œæµ‹è¯•æ—¶çš„â€œåŸºç¡€â€å’Œâ€œæ–°é¢–â€ï¼ˆæœªæ ‡è®°ï¼‰ç±»åˆ«ã€‚å°½ç®¡ä¸»å¹²ç‰¹å¾æ˜¯æ ¹æ®å¤§é‡å¼€æ”¾è¯æ±‡æ•°æ®é¢„å…ˆè®­ç»ƒçš„ï¼Œä½†æ·»åŠ çš„æ£€æµ‹å™¨å±‚ï¼ˆé¢ˆéƒ¨å’Œå¤´éƒ¨ï¼‰æ˜¯ä½¿ç”¨ä¸‹æ¸¸æ£€æµ‹æ•°æ®é›†è¿›è¡Œæ–°è®­ç»ƒçš„ã€‚çŽ°æœ‰çš„æ–¹æ³•ç»å¸¸åœ¨å¯¹è±¡æè®®é˜¶æ®µé”™è¿‡æ–°é¢–/æœªæ ‡è®°çš„å¯¹è±¡ï¼Œå› ä¸ºæè®®å€¾å‘äºŽå°†å®ƒä»¬åˆ†ç±»ä¸ºèƒŒæ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æ–°é¢–çš„å¯¹è±¡æè®®æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œå¹¶é‡‡ç”¨åŸºäºŽæœ¬åœ°åŒ–è´¨é‡çš„å¯¹è±¡æ€§ï¼ˆå³&lt;a href=&quot;https://arxiv.org/abs/2108.06753&quot;>;centerness&lt;/a>;åˆ†æ•°ï¼‰è€Œä¸æ˜¯ç‰©ä½“æˆ–éžç‰©ä½“çš„äºŒå…ƒåˆ†ç±»å¾—åˆ†ï¼Œå®ƒä¸Žæ£€æµ‹å¾—åˆ†ç›¸ç»“åˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæ£€æµ‹åˆ°çš„åŒºåŸŸçš„æ£€æµ‹åˆ†æ•°è®¡ç®—ä¸ºè¯¥åŒºåŸŸåµŒå…¥ä¹‹é—´çš„&lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;ä½™å¼¦ç›¸ä¼¼åº¦&lt;/a>;ï¼ˆé€šè¿‡&lt;a hrefè®¡ç®—ï¼‰ =&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks&quot;>;RoI-Align&lt;/a>; æ“ä½œï¼‰å’ŒåŸºæœ¬ç±»åˆ«çš„æ–‡æœ¬åµŒå…¥ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬é™„åŠ æ–°é¢–ç±»åˆ«çš„æ–‡æœ¬åµŒå…¥ï¼Œå¹¶ä¸”çŽ°åœ¨é€šè¿‡åŸºæœ¬ç±»åˆ«å’Œæ–°é¢–ç±»åˆ«çš„è”åˆæ¥è®¡ç®—æ£€æµ‹åˆ†æ•°ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw 7MCZE0IsOhdkt8ny6ipDfFi4BxNOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s682/RO-ViT -img4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;682&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw7MCZE0IsOhdkt8ny6ipDfFi4Bx NOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s16000/RO-ViT-img4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;é€šè¿‡ç”¨æ£€æµ‹å™¨å¤´æ›¿æ¢å…¨å±€å¹³å‡æ± åŒ–ï¼Œå°†é¢„è®­ç»ƒçš„ ViT ä¸»å¹²è½¬ç§»åˆ°ä¸‹æ¸¸å¼€æ”¾è¯æ±‡æ£€æµ‹ã€‚ &lt;a href=&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks#:~:text=new%20method%20å«-,ROIAlign,-%2C%20which%20can%20represent&quot; style=&quot;text- align: left;&quot;>;RoI-Align&lt;/a>;&amp;nbsp;åµŒå…¥ä¸Žç¼“å­˜çš„ç±»åˆ«åµŒå…¥åŒ¹é…ä»¥èŽ·å¾—VLMåˆ†æ•°ï¼Œè¯¥åˆ†æ•°ä¸Žæ£€æµ‹åˆ†æ•°ç»„åˆæˆå¼€æ”¾è¯æ±‡æ£€æµ‹åˆ†æ•°ã€‚&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“æžœ&lt;/h2>; &lt;p>; æˆ‘ä»¬è¯„ä¼° RO-ViT &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; å¼€æ”¾è¯æ±‡æ£€æµ‹åŸºå‡†ã€‚åœ¨ç³»ç»Ÿçº§åˆ«ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡åž‹åœ¨ç¨€æœ‰ç±»åˆ«ä¸Šå®žçŽ°äº† 33.6 ä¸ªæ¡†&lt;a href=&quot;https://blog.paperspace.com/mean-average- precision/&quot;>;å¹³å‡ç²¾åº¦&lt;/a>;ï¼ˆ&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;AP&lt;sub>;r&lt;/sub>;&lt;/a>;) å’Œ 32.1 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;æŽ©ç AP&lt;sub>;r&lt;/sub>;&lt;/a>;ï¼Œå…¶æ€§èƒ½ä¼˜äºŽçŽ°æœ‰çš„åŸºäºŽ ViT çš„æœ€ä½³æ–¹æ³• OWL-ViT 8.0 AP&lt;sub>;r&lt;/sub>; å’Œæœ€ä½³çš„åŸºäºŽ CNN çš„æ–¹æ³• ViLD-Ens 5.8 æŽ©æ¨¡ AP&lt;sub>;r&lt;/sub>;ã€‚å®ƒè¿˜è¶…è¿‡äº†è®¸å¤šå…¶ä»–åŸºäºŽçŸ¥è¯†è’¸é¦ã€é¢„è®­ç»ƒæˆ–å¼±ç›‘ç£è”åˆè®­ç»ƒçš„æ–¹æ³•çš„æ€§èƒ½ã€‚&lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr -caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel-0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQH rll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height =â€œ426â€data-original-width =â€œ1200â€src =â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel- 0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQHrll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s16000/image2.png &quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RO-ViT çš„æ€§èƒ½ä¼˜äºŽå½“å‰çŠ¶æ€è‰ºæœ¯ (SOTA)&lt;a href=&quot;https://arxiv.org/abs/2205.06230&quot; style=&quot;text-align: left;&quot;>;åŸºäºŽ ViT&lt;/a>; å’Œ&lt;a href=&quot; https://arxiv.org/abs/2104.13921&quot; style=&quot;text-align: left;&quot;>;åŸºäºŽ CNN çš„ LVIS å¼€æ”¾è¯æ±‡æ£€æµ‹åŸºå‡†çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ç¨€æœ‰ç±»åˆ« (AP&lt;sub>;r&lt;/sub>;) ä¸Šæ˜¾ç¤ºæŽ©æ¨¡ APï¼Œä½†åŸºäºŽ SOTA ViT çš„ (OwL-ViT) åˆ™æ˜¾ç¤ºæ¡† APã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/è¡¨>; &lt;p>; é™¤äº†é€šè¿‡å¼€æ”¾è¯æ±‡æ£€æµ‹è¯„ä¼°åŒºåŸŸçº§è¡¨ç¤ºä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡ &lt;a href=&quot;https://arxiv.org/abs è¯„ä¼° RO-ViT åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢ä¸­çš„å›¾åƒçº§è¡¨ç¤º/1504.00325&quot;>;MS-COCO&lt;/a>; å’Œ &lt;a href=&quot;https://arxiv.org/abs/1505.04870&quot;>;Flickr30K&lt;/a>; åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å…·æœ‰ 303M ViT çš„æ¨¡åž‹åœ¨ MS COCO ä¸Šä¼˜äºŽå…·æœ‰ 1B ViT çš„æœ€å…ˆè¿›çš„ CoCa æ¨¡åž‹ï¼Œå¹¶ä¸”åœ¨ Flickr30K ä¸Šå¤„äºŽåŒç­‰æ°´å¹³ã€‚è¿™è¡¨æ˜Žæˆ‘ä»¬çš„é¢„è®­ç»ƒæ–¹æ³•ä¸ä»…æé«˜äº†åŒºåŸŸçº§è¡¨ç¤ºï¼Œè¿˜æé«˜äº†æ£€ç´¢çš„å…¨å±€å›¾åƒçº§è¡¨ç¤ºã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKV rwp3_wXzf8KGqaY50rK3fLcWtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s1692/image5.png&quot;æ ·å¼=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;940&quot; data-original-width=&quot;1692&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKVrwp3_wXzf8KGqaY50rK3fLc WtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;æˆ‘ä»¬åœ¨ MS COCO å’Œ Flickr30K åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†é›¶æ ·æœ¬å›¾åƒæ–‡æœ¬æ£€ç´¢ï¼Œå¹¶ä¸ŽåŒç¼–ç å™¨æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬æŠ¥å‘Šå›¾åƒåˆ°æ–‡æœ¬ (I2T) å’Œæ–‡æœ¬åˆ°å›¾åƒ (T2I) æ£€ç´¢ä»»åŠ¡çš„recall@1ï¼ˆtop-1 å¬å›žçŽ‡ï¼‰ã€‚ RO-ViT çš„æ€§èƒ½ä¼˜äºŽæœ€å…ˆè¿›çš„&lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot; style=&quot;text-align: left;&quot;>;CoCa&lt;/a>; &amp;nbsp;ç›¸åŒçš„ä¸»å¹²ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-å·¦ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFfI7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT- 03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s1476/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;362&quot; æ•°æ®-original-width=&quot;1476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFf I7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT-03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td >;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LVIS ä¸Šçš„ RO-ViT å¼€æ”¾è¯æ±‡æ£€æµ‹ã€‚ä¸ºäº†æ¸…æ¥šèµ·è§ï¼Œæˆ‘ä»¬ä»…æ˜¾ç¤ºæ–°é¢–çš„ç±»åˆ«ã€‚ RO-ViT æ£€æµ‹åˆ°è®¸å¤šåœ¨æ£€æµ‹è®­ç»ƒæœŸé—´ä»Žæœªè§è¿‡çš„æ–°ç±»åˆ«ï¼šâ€œfishbowlâ€ã€â€œsombreroâ€ã€â€œpersimmonâ€ã€â€œgargoyleâ€ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ä½ç½®åµŒå…¥çš„å¯è§†åŒ–&lt;/h2>; &lt;p>; æˆ‘ä»¬å°† RO-ViT å­¦ä¹ åˆ°çš„ä½ç½®åµŒå…¥å¯è§†åŒ–å¹¶ä¸ŽåŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚æ¯ä¸ªå›¾å—æ˜¯ä¸€ä¸ªè¡¥ä¸ä¸Žæ‰€æœ‰å…¶ä»–è¡¥ä¸çš„ä½ç½®åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä¾‹å¦‚ï¼Œå·¦ä¸Šè§’çš„å›¾å—ï¼ˆä»¥çº¢è‰²æ ‡è®°ï¼‰å¯è§†åŒ–äº†ä½ç½®ï¼ˆè¡Œ=1ï¼Œåˆ—=1ï¼‰çš„ä½ç½®åµŒå…¥ä¸Ž 2D ä¸­æ‰€æœ‰å…¶ä»–ä½ç½®çš„ä½ç½®åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚è¡¥ä¸çš„äº®åº¦è¡¨ç¤ºä¸åŒä½ç½®çš„å­¦ä¹ ä½ç½®åµŒå…¥çš„æŽ¥è¿‘ç¨‹åº¦ã€‚ RO-ViT åœ¨ä¸åŒçš„æ–‘å—ä½ç½®å½¢æˆæ›´æ˜Žæ˜¾çš„ç°‡ï¼Œåœ¨ä¸­å¿ƒæ–‘å—å‘¨å›´æ˜¾ç¤ºå¯¹ç§°çš„å…¨å±€æ¨¡å¼ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwS twANDQkmi6f2w62aXNhkEu3daexyXV5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s1686/RO-ViT-img1.jpg&quot;æ ·å¼=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1686&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwStwANDQkmi6f2w62aXNHkEu3daexyX V5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s16000/RO-ViT-img1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;æ¯ä¸ªå›¾å—æ˜¾ç¤ºè¡¥ä¸çš„ä½ç½®åµŒå…¥ï¼ˆåœ¨æŒ‡ç¤ºçš„è¡Œåˆ—ä½ç½®ï¼‰ä¸Žæ‰€æœ‰å…¶ä»–è¡¥ä¸çš„ä½ç½®åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä½¿ç”¨ ViT-B/16 ä¸»å¹²ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®º&lt;/h2>; &lt;p>; æˆ‘ä»¬æå‡ºäº† RO-ViTï¼Œä¸€ç§å¯¹æ¯”å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒæ¡†æž¶ï¼Œä»¥å¼¥åˆå›¾åƒçº§é¢„è®­ç»ƒå’Œå¼€æ”¾è¯æ±‡æ£€æµ‹å¾®è°ƒä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€å•ã€å¯æ‰©å±•ï¼Œå¹¶ä¸”æ˜“äºŽåº”ç”¨äºŽä»»ä½•å¯¹æ¯”éª¨å¹²ç½‘ï¼Œè®¡ç®—å¼€é”€æœ€å°ï¼Œå¹¶ä¸”ä¸å¢žåŠ å‚æ•°ã€‚ RO-ViT åœ¨ LVIS å¼€æ”¾è¯æ±‡æ£€æµ‹åŸºå‡†å’Œå›¾åƒæ–‡æœ¬æ£€ç´¢åŸºå‡†ä¸Šå®žçŽ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œè¡¨æ˜Žå­¦ä¹ çš„è¡¨ç¤ºä¸ä»…åœ¨åŒºåŸŸçº§åˆ«æœ‰ç›Šï¼Œè€Œä¸”åœ¨å›¾åƒçº§åˆ«ä¹Ÿéžå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½å¤Ÿå¸®åŠ©ä»Žå›¾åƒæ–‡æœ¬é¢„è®­ç»ƒçš„è§’åº¦è¿›è¡Œå¼€æ”¾è¯æ±‡æ£€æµ‹çš„ç ”ç©¶ï¼Œè¿™å¯¹åŒºåŸŸçº§å’Œå›¾åƒçº§ä»»åŠ¡éƒ½æœ‰å¥½å¤„ã€‚ &lt;/p>; &lt;br />; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;Dahun Kimã€Anelia Angelova å’Œ Wei Cheng Kuo è¿›è¡Œäº†è¿™é¡¹å·¥ä½œï¼Œç›®å‰åœ¨ Google DeepMind å·¥ä½œã€‚æˆ‘ä»¬è¦æ„Ÿè°¢ Google Research çš„åŒäº‹æä¾›çš„å»ºè®®å’Œæœ‰ç›Šçš„è®¨è®ºã€‚ &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/781660063956467509/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html#comment-form&quot; rel=&quot;replies &quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;edit&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;RO-ViTï¼šåŒºåŸŸæ„ŸçŸ¥é¢„è®­ç»ƒä½¿ç”¨è§†è§‰è½¬æ¢å™¨è¿›è¡Œå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹è®­ç»ƒâ€ type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/ uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1 .blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVu Ij5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s72-c/RO-ViT-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss /&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;æ ‡ç­¾ï¼šblogger.com,1999:blog-8474926331452026626.post-3964459643854458124&lt;/id>; &lt;å‘å¸ƒ>;2023-08-25T10:38:00.003-07:00&lt;/å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-25T10:46:59.205-07:00&lt;/æ›´æ–°>;&lt;category schema=&quot;http://www .blogger.com/atom/ns#&quot; term=&quot;æœºå™¨æ„ŸçŸ¥&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT äº®ç‚¹&quot;>; &lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google ç ”ç©¶çš„è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ï¼šæ„ŸçŸ¥å…¬å¹³æ€§&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šGoogle ç ”ç©¶éƒ¨æ„ŸçŸ¥å…¬å¹³æ€§å›¢é˜Ÿè”åˆè´Ÿè´£äºº Susanna Ricco å’Œ Utsav Prabhu&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MOHdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJ e8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1-inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s320/hero.gif&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; Google çš„&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ç ”ç©¶&lt;/a>;å»ºç«‹åœ¨å…·æœ‰ä¸åŒèƒŒæ™¯å’Œä¸“ä¸šçŸ¥è¯†çš„å›¢é˜Ÿä¹‹é—´çš„åä½œåŸºç¡€ä¸Šï¼Œç ”ç©¶äººå‘˜å’Œäº§å“å¼€å‘äººå‘˜ä¹‹é—´ï¼Œå¹¶æœ€ç»ˆä¸Žæ•´ä¸ªç¤¾åŒºä¹‹é—´ã€‚æ„ŸçŸ¥å…¬å¹³å›¢é˜Ÿå°†è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹  (ML) å…¬å¹³æ€§æ–¹é¢çš„æ·±åŽšä¸»é¢˜ä¸“ä¸šçŸ¥è¯†ä¸Žæž„å»ºæ„ŸçŸ¥ç³»ç»Ÿçš„ç ”ç©¶äººå‘˜ç›´æŽ¥è”ç³»ç›¸ç»“åˆï¼ŒæŽ¨åŠ¨ Google åŠå…¶ä»–é¢†åŸŸäº§å“çš„è¿›æ­¥ã€‚æˆ‘ä»¬æ­£åœ¨å…±åŒåŠªåŠ›ï¼Œåœ¨ &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google äººå·¥æ™ºèƒ½åŽŸåˆ™&lt;/a>;çš„æŒ‡å¯¼ä¸‹ï¼Œä»Žå¤´å¼€å§‹æœ‰æ„è®¾è®¡å…·æœ‰åŒ…å®¹æ€§çš„ç³»ç»Ÿã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; å³è¾¹è·ï¼šè‡ªåŠ¨;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf -_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkypL7MqBpdeqIJm 9H5sani/s948/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;214&quot; data-original-width=&quot;948 â€œé«˜åº¦=â€œ144â€src=â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf-_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ 7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkYpL7MqBpdeqIJm9H5sani/w640-h144/image1.gif&quot;å®½åº¦=â€œ640â€/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æ„ŸçŸ¥å…¬å¹³ç ”ç©¶æ¶µç›–å…ˆè¿›å¤šæ¨¡å¼æ¨¡åž‹çš„è®¾è®¡ã€å¼€å‘å’Œéƒ¨ç½²ï¼ŒåŒ…æ‹¬æœ€æ–°çš„åŸºç¡€å’Œç”Ÿæˆæ¨¡åž‹ä¸º Google äº§å“æä¾›æ”¯æŒçš„æ¨¡åž‹ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; æˆ‘ä»¬å›¢é˜Ÿçš„ä½¿å‘½æ˜¯æŽ¨è¿›å¤šæ¨¡å¼ ML ç³»ç»Ÿçš„å…¬å¹³æ€§å’ŒåŒ…å®¹æ€§å‰æ²¿ï¼Œç‰¹åˆ«æ˜¯ä¸Ž&lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;>;åŸºç¡€&lt;/a>;æ¨¡åž‹å’Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;ç”Ÿæˆäººå·¥æ™ºèƒ½&lt;/a >;ã€‚è¿™æ¶µç›–äº†æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æœ¬åœ°åŒ–ã€å­—å¹•ã€æ£€ç´¢ã€è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒæˆ–æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»¥åŠç”Ÿæˆå›¾åƒå’Œè§†é¢‘ç¼–è¾‘ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œå…¬å¹³å’ŒåŒ…å®¹å¯ä»¥è€Œä¸”åº”è¯¥æˆä¸ºè¿™äº›åº”ç”¨ç¨‹åºçš„é¦–è¦æ€§èƒ½ç›®æ ‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶é‡ç‚¹æ˜¯è§£é”æ–°é¢–çš„åˆ†æžå’Œç¼“è§£æŽªæ–½ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ•´ä¸ªå¼€å‘å‘¨æœŸä¸­ä¸»åŠ¨è®¾è®¡è¿™äº›ç›®æ ‡ã€‚æˆ‘ä»¬å›žç­”æ ¸å¿ƒé—®é¢˜ï¼Œä¾‹å¦‚ï¼šæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨æœºå™¨å­¦ä¹ è´Ÿè´£ä»»åœ°ã€å¿ å®žåœ°æ¨¡æ‹Ÿäººç±»å¯¹äººå£ã€æ–‡åŒ–å’Œç¤¾ä¼šèº«ä»½çš„çœ‹æ³•ï¼Œä»¥ä¿ƒè¿›å…¬å¹³å’ŒåŒ…å®¹ï¼Ÿæˆ‘ä»¬å¯ä»¥æµ‹é‡å“ªäº›ç±»åž‹çš„ç³»ç»Ÿåå·®ï¼ˆä¾‹å¦‚ï¼Œåœ¨æŸäº›è‚¤è‰²çš„äººçš„å›¾åƒä¸Šè¡¨çŽ°ä¸ä½³ï¼‰ä»¥åŠå¦‚ä½•ä½¿ç”¨è¿™äº›æŒ‡æ ‡æ¥è®¾è®¡æ›´å¥½çš„ç®—æ³•ï¼Ÿå¦‚ä½•æž„å»ºæ›´å…·åŒ…å®¹æ€§çš„ç®—æ³•å’Œç³»ç»Ÿï¼Œå¹¶åœ¨å‘ç”Ÿæ•…éšœæ—¶å¿«é€Ÿååº”ï¼Ÿ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è¡¡é‡åª’ä½“ä¸­äººç‰©çš„ä»£è¡¨æ€§&lt;/h2>; &lt;p>; å¯ä»¥ç¼–è¾‘ã€ç­–åˆ’æˆ–åˆ›å»ºå›¾åƒæˆ–è§†é¢‘å¯ä»¥å½±å“ä»»ä½•æŽ¥è§¦å…¶è¾“å‡ºçš„äººï¼Œå¡‘é€ æˆ–å¼ºåŒ–ä¸–ç•Œå„åœ°è§‚ä¼—çš„ä¿¡å¿µã€‚å‡å°‘ä»£è¡¨æ€§ä¼¤å®³ï¼ˆä¾‹å¦‚å¼ºåŒ–åˆ»æ¿å°è±¡æˆ–è¯‹æ¯æˆ–æ¶ˆé™¤äººç¾¤ï¼‰çš„ç ”ç©¶éœ€è¦å¯¹å†…å®¹å’Œ&lt;a href=&quot;https://ai.googleblog.com/2023/07/using-societal -context-knowledge-to.html&quot;>;ç¤¾ä¼šèƒŒæ™¯&lt;/a>;ã€‚å®ƒå–å†³äºŽä¸åŒçš„è§‚å¯Ÿè€…å¦‚ä½•çœ‹å¾…è‡ªå·±ã€ä»–ä»¬çš„ç¤¾åŒºæˆ–ä»–äººçš„ä»£è¡¨æ–¹å¼ã€‚å…³äºŽå“ªäº›ç¤¾ä¼šç±»åˆ«åº”è¯¥ä½¿ç”¨è®¡ç®—å·¥å…·è¿›è¡Œç ”ç©¶ä»¥åŠå¦‚ä½•è´Ÿè´£ä»»åœ°è¿›è¡Œç ”ç©¶ï¼Œè¯¥é¢†åŸŸå­˜åœ¨ç›¸å½“å¤šçš„äº‰è®ºã€‚æˆ‘ä»¬çš„ç ”ç©¶é‡ç‚¹æ˜¯è‡´åŠ›äºŽå¯»æ±‚å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆä»¥ç¤¾ä¼šå­¦å’Œç¤¾ä¼šå¿ƒç†å­¦ä¸ºåŸºç¡€ï¼Œä¸Žäººç±»çš„æ„ŸçŸ¥ç›¸ä¸€è‡´ï¼Œæ‹¥æŠ±é—®é¢˜çš„ä¸»è§‚æœ¬è´¨ï¼Œå¹¶å®žçŽ°ç»†è‡´å…¥å¾®çš„æµ‹é‡å’Œç¼“è§£ã€‚ä¸€ä¸ªä¾‹å­æ˜¯æˆ‘ä»¬å¯¹&lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;äººç±»æ„ŸçŸ¥å’Œè‚¤è‰²æ³¨é‡Šå·®å¼‚çš„ç ”ç©¶ä½¿ç”¨&lt;a href=&quot;skintone.google&quot;>;åƒ§ä¾£è‚¤è‰²ç­‰çº§&lt;/a>;çš„å›¾åƒ&lt;/a>;ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬çš„å·¥å…·è¿˜ç”¨äºŽç ”ç©¶å¤§è§„æ¨¡å†…å®¹é›†åˆä¸­çš„è¡¨ç¤ºã€‚é€šè¿‡æˆ‘ä»¬çš„ç¤¾äº¤æŽ¢ç´¢åª’ä½“ç†è§£ (MUSE) é¡¹ç›®ï¼Œæˆ‘ä»¬ä¸Žå­¦æœ¯ç ”ç©¶äººå‘˜ã€éžè¥åˆ©ç»„ç»‡å’Œä¸»è¦æ¶ˆè´¹å“ç‰Œåˆä½œï¼Œäº†è§£ä¸»æµåª’ä½“å’Œå¹¿å‘Šå†…å®¹çš„æ¨¡å¼ã€‚æˆ‘ä»¬äºŽ 2017 å¹´é¦–æ¬¡å‘è¡¨äº†è¿™é¡¹ç ”ç©¶æˆæžœï¼Œå…¶ä¸­ä¸€é¡¹å…±åŒæ’°å†™çš„ç ”ç©¶åˆ†æžäº†&lt;a href=&quot;https://about.google/intl/ALL_au/main/gender-equality-films/&quot;>;å¥½èŽ±åžç”µå½±ä¸­çš„æ€§åˆ«å¹³ç­‰&lt;/a >;ã€‚ä»Žé‚£æ—¶èµ·ï¼Œæˆ‘ä»¬å¢žåŠ äº†åˆ†æžçš„è§„æ¨¡å’Œæ·±åº¦ã€‚ 2019 å¹´ï¼Œæˆ‘ä»¬å‘å¸ƒäº†åŸºäºŽ&lt;a href=&quot;https://www.thinkwithgoogle.com/feature/diversity-inclusion/&quot;>;è¶…è¿‡ 270 ä¸‡æ¡ YouTube å¹¿å‘Š&lt;/a>;çš„è°ƒæŸ¥ç»“æžœã€‚åœ¨&lt;a href=&quot;https://blog.google/technology/ai/using-ai-to-study-12-years-of-representation-in-tv/&quot;>;æœ€æ–°ç ”ç©¶&lt;/a>;ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨è¶…è¿‡åäºŒå¹´çš„ç¾Žå›½æµè¡Œç”µè§†èŠ‚ç›®ä¸­ï¼Œæ„ŸçŸ¥æ€§åˆ«è¡¨çŽ°ã€æ„ŸçŸ¥å¹´é¾„å’Œè‚¤è‰²çš„äº¤å‰è¡¨çŽ°ã€‚è¿™äº›ç ”ç©¶ä¸ºå†…å®¹åˆ›ä½œè€…å’Œå¹¿å‘Šå•†æä¾›äº†è§è§£ï¼Œå¹¶è¿›ä¸€æ­¥ä¸ºæˆ‘ä»¬è‡ªå·±çš„ç ”ç©¶æä¾›ä¿¡æ¯ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFuL3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPt S8ONz02Nrgn4If7YOukbw0txEfy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/s800/image3.gif&quot;æ ·å¼=&quot;å·¦è¾¹è·ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;800&quot; height=&quot;360&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFul3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPtS8ONz02Nrgn4If7YOukbw0tx Efy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/w640-h360/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td ç±»=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;è®¡ç®—ä¿¡å·çš„å›¾ç¤ºï¼ˆéžå®žé™…æ•°æ®ï¼‰ï¼Œå¯ä»¥å¤§è§„æ¨¡åˆ†æžä»¥æ­ç¤ºåª’ä½“é›†åˆä¸­çš„ä»£è¡¨æ€§æ¨¡å¼ã€‚ [è§†é¢‘é›†/Getty Images]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬æ­£åœ¨æ‰©å±•æˆ‘ä»¬å…³æ³¨çš„æœºå™¨å­¦ä¹ å…¬å¹³æ€§æ¦‚å¿µä»¥åŠæ¶‰åŠçš„é¢†åŸŸå®ƒä»¬å¾—åˆ°è´Ÿè´£ä»»çš„åº”ç”¨ã€‚é™¤äº†é€¼çœŸçš„äººç‰©å›¾åƒä¹‹å¤–ï¼Œæˆ‘ä»¬æ­£åœ¨åŠªåŠ›å¼€å‘å·¥å…·ï¼Œä»¥åœ¨æ’å›¾ã€äººå½¢è§’è‰²çš„æŠ½è±¡æè¿°ï¼Œç”šè‡³æ ¹æœ¬æ²¡æœ‰äººçš„å›¾åƒä¸­å¯¹ç¤¾åŒºå’Œæ–‡åŒ–çš„è¡¨çŽ°è¿›è¡Œå»ºæ¨¡ã€‚æœ€åŽï¼Œæˆ‘ä»¬ä¸ä»…éœ€è¦æŽ¨ç†æç»˜çš„æ˜¯è°ï¼Œè¿˜éœ€è¦æŽ¨ç†ä»–ä»¬æ˜¯å¦‚ä½•è¢«æç»˜çš„â€”â€”é€šè¿‡å‘¨å›´çš„å›¾åƒå†…å®¹ã€éšé™„çš„æ–‡æœ¬å’Œæ›´å¹¿æ³›çš„æ–‡åŒ–èƒŒæ™¯ä¼ è¾¾äº†ä»€ä¹ˆæ ·çš„å™äº‹ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;åˆ†æžæ„ŸçŸ¥ç³»ç»Ÿçš„åå·®å±žæ€§&lt;/h2>; &lt;p>;æž„å»ºé«˜çº§æœºå™¨å­¦ä¹ ç³»ç»Ÿéžå¸¸å¤æ‚ï¼Œå¤šä¸ªåˆ©ç›Šç›¸å…³è€…å‘ŠçŸ¥å†³å®šäº§å“è¡Œä¸ºçš„å„ç§æ ‡å‡†ã€‚åŽ†å²ä¸Šï¼Œæ€»ä½“è´¨é‡æ˜¯ä½¿ç”¨æµ‹è¯•æ•°æ®é›†çš„æ±‡æ€»ç»Ÿè®¡æ•°æ®ï¼ˆä¾‹å¦‚æ€»ä½“å‡†ç¡®æ€§ï¼‰ä½œä¸ºç”¨æˆ·ä½“éªŒçš„ä»£ç†æ¥å®šä¹‰å’Œæµ‹é‡çš„ã€‚ä½†å¹¶éžæ‰€æœ‰ç”¨æˆ·éƒ½ä»¥ç›¸åŒçš„æ–¹å¼ä½“éªŒäº§å“ã€‚ &lt;br />; &lt;/p>; &lt;p>; æ„ŸçŸ¥å…¬å¹³æ€§èƒ½å¤Ÿå¯¹æ±‡æ€»ç»Ÿè®¡ä¹‹å¤–çš„ç»†å¾®ç³»ç»Ÿè¡Œä¸ºè¿›è¡Œå®žé™…æµ‹é‡ï¼Œå¹¶ä½¿è¿™äº›æŒ‡æ ‡æˆä¸ºç³»ç»Ÿè´¨é‡çš„æ ¸å¿ƒï¼Œç›´æŽ¥é€šçŸ¥äº§å“è¡Œä¸ºå’Œå‘å¸ƒå†³ç­–ã€‚è¿™é€šå¸¸æ¯”çœ‹èµ·æ¥è¦å›°éš¾å¾—å¤šã€‚å°†å¤æ‚çš„åè§é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œäº¤å‰äºšç»„ä¹‹é—´çš„è¡¨çŽ°å·®å¼‚æˆ–åˆ»æ¿å°è±¡å¼ºåŒ–å®žä¾‹ï¼‰æç‚¼ä¸ºå°‘é‡æŒ‡æ ‡è€Œä¸ä¸¢å¤±é‡è¦çš„ç»†å¾®å·®åˆ«æ˜¯æžå…·æŒ‘æˆ˜æ€§çš„ã€‚å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å¹³è¡¡å…¬å¹³æ€§æŒ‡æ ‡å’Œå…¶ä»–äº§å“æŒ‡æ ‡ï¼ˆä¾‹å¦‚ç”¨æˆ·æ»¡æ„åº¦ã€å‡†ç¡®æ€§ã€å»¶è¿Ÿï¼‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå°½ç®¡å®ƒä»¬æ˜¯å…¼å®¹çš„ï¼Œä½†é€šå¸¸è¢«è®¤ä¸ºæ˜¯å†²çªçš„ã€‚ç ”ç©¶äººå‘˜é€šå¸¸å°†ä»–ä»¬çš„å·¥ä½œæè¿°ä¸ºä¼˜åŒ–â€œå‡†ç¡®æ€§-å…¬å¹³æ€§â€æƒè¡¡ï¼Œè€Œå®žé™…ä¸Šå¹¿æ³›çš„ç”¨æˆ·æ»¡æ„åº¦ä¸Žæ»¡è¶³å…¬å¹³æ€§å’ŒåŒ…å®¹æ€§ç›®æ ‡æ˜¯ä¸€è‡´çš„ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu 2yPn_wK23Z8PdYkeEmycLRJ0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s640/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu2yPn_wK23Z8PdYkeEmycLRJ 0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td ç±»=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æˆ‘ä»¬æž„å»ºå¹¶å‘å¸ƒäº†&lt;a href=&quot;https://storage.googleapis.com/openimages/web/extended.html#miap&quot;>;MIAP æ•°æ®é›†&lt;/a>;ä½œä¸º&lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;>;å¼€æ”¾å›¾åƒ&lt;/a>;çš„ä¸€éƒ¨åˆ†ï¼Œåˆ©ç”¨æˆ‘ä»¬å¯¹ç¤¾äº¤ç›¸å…³æ¦‚å¿µçš„æ„ŸçŸ¥å’Œæ£€æµ‹çš„ç ”ç©¶å¤æ‚ç³»ç»Ÿä¸­çš„åè§è¡Œä¸ºï¼Œä»¥åˆ›å»ºè¿›ä¸€æ­¥æŽ¨è¿›è®¡ç®—æœºè§†è§‰ä¸­ ML å…¬å¹³æ€§ç ”ç©¶çš„èµ„æºã€‚åŽŸå§‹ç…§ç‰‡æ¥æº - å·¦ï¼š&lt;a href=&quot;https://www.flickr.com/photos/boston_public_library/8242010414/&quot;>;æ³¢å£«é¡¿å…¬å…±å›¾ä¹¦é¦†&lt;/a>;ï¼›ä¸­é—´ï¼š&lt;a href=&quot;https://www.flickr.com/photos/jenrobinson/20183915655/&quot;>;jen robinson&lt;/a>;ï¼›å³ï¼š&lt;a href=&quot;https://www.flickr.com/photos/mrgarin/2484859086/&quot;>;åŠ æž—Â·ä¸°æ–¯&lt;/a>;ï¼›æ‰€æœ‰ä½¿ç”¨å‡å·²èŽ·å¾— &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;>;CC-BY 2.0 è®¸å¯è¯&lt;/a>;è®¸å¯ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt; /table>; &lt;br />; &lt;p>; ä¸ºæ­¤ï¼Œæˆ‘ä»¬çš„å›¢é˜Ÿä¸“æ³¨äºŽä¸¤ä¸ªå¹¿æ³›çš„ç ”ç©¶æ–¹å‘ã€‚é¦–å…ˆï¼Œæ°‘ä¸»åŒ–å¯¹æ˜“äºŽç†è§£ä¸”å¹¿æ³›é€‚ç”¨çš„å…¬å¹³åˆ†æžå·¥å…·çš„è®¿é—®ï¼Œè®©åˆä½œä¼™ä¼´ç»„ç»‡å°†å…¶é‡‡ç”¨åˆ°äº§å“å·¥ä½œæµç¨‹ä¸­ï¼Œå¹¶é€šçŸ¥æ•´ä¸ªå…¬å¸çš„é¢†å¯¼å±‚è§£é‡Šç»“æžœã€‚è¿™é¡¹å·¥ä½œåŒ…æ‹¬åˆ¶å®šå¹¿æ³›çš„åŸºå‡†ã€&lt;a href=&quot;https://ai.googleblog.com/2021/06/a-step-toward-more-inclusive-people.html&quot;>;æ•´ç†å¹¿æ³›ä½¿ç”¨çš„é«˜è´¨é‡æµ‹è¯•æ•°æ®é›†&lt;/a>; ä»¥åŠä»¥åˆ‡ç‰‡åˆ†æžå’Œåäº‹å®žæµ‹è¯•ç­‰æŠ€æœ¯ä¸ºä¸­å¿ƒçš„å·¥å…·â€”â€”é€šå¸¸å»ºç«‹åœ¨å‰é¢æè¿°çš„æ ¸å¿ƒè¡¨ç¤ºä¿¡å·å·¥ä½œçš„åŸºç¡€ä¸Šã€‚å…¶æ¬¡ï¼Œ&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3461702.3462557&quot;>;æŽ¨è¿›å…¬å¹³åˆ†æžçš„æ–°é¢–æ–¹æ³•&lt;/a>; - åŒ…æ‹¬ä¸Ž&lt;a href=&quot;https://ai åˆä½œ.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;å¯èƒ½å¸¦æ¥çªç ´æ€§å‘çŽ°çš„äº§å“åŠªåŠ›&lt;/a>;æˆ–&lt;a href=&quot;https://services.google.com/ fh/files/blogs/bsr-google-cr-api-hria-executive-summary.pdf&quot;>;å‘ŠçŸ¥å¯åŠ¨ç­–ç•¥&lt;/a>;ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è´Ÿè´£ä»»åœ°æŽ¨è¿›äººå·¥æ™ºèƒ½&lt;/h2>; &lt;p>;æˆ‘ä»¬çš„å·¥ä½œå¹¶ä¸æ­¢äºŽåˆ†æžæ¨¡åž‹è¡Œä¸ºã€‚ç›¸åï¼Œæˆ‘ä»¬å°†æ­¤ä½œä¸ºä¸Žäº§å“å›¢é˜Ÿçš„å…¶ä»–ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆåˆä½œç¡®å®šç®—æ³•æ”¹è¿›çš„èµ·ç‚¹ã€‚åœ¨è¿‡åŽ»çš„ä¸€å¹´é‡Œï¼Œæˆ‘ä»¬æŽ¨å‡ºäº†å‡çº§çš„ç»„ä»¶ï¼Œä¸º Google ç›¸å†Œä¸­çš„æœç´¢å’Œ&lt;a href=&quot;https://blog.google/products/photos/google-photos-memories-view/&quot;>;å›žå¿†&lt;/a>;åŠŸèƒ½æä¾›æ”¯æŒï¼Œé€šè¿‡æ·»åŠ å±‚æ¥é˜²æ­¢é”™è¯¯åœ¨ç³»ç»Ÿä¸­çº§è”ï¼Œä»Žè€Œå®žçŽ°æ›´ä¸€è‡´çš„æ€§èƒ½å¹¶å¤§å¹…æé«˜ç¨³å¥æ€§ã€‚æˆ‘ä»¬æ­£åœ¨åŠªåŠ›æ”¹è¿› Google å›¾ç‰‡ä¸­çš„æŽ’åç®—æ³•ï¼Œä»¥å®žçŽ°å¤šæ ·åŒ–ã€‚æˆ‘ä»¬æ›´æ–°äº†å¯èƒ½å¼ºåŒ–åŽ†å²åˆ»æ¿å°è±¡çš„ç®—æ³•ï¼Œè´Ÿè´£ä»»åœ°ä½¿ç”¨é¢å¤–ä¿¡å·ï¼Œä»¥ä¾¿&lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;æ¯ä¸ªäººéƒ½æ›´æœ‰å¯èƒ½çœ‹åˆ°ä»–ä»¬è‡ªå·±åæ˜ åœ¨æœç´¢ç»“æžœä¸­å¹¶æ‰¾åˆ°ä»–ä»¬æ­£åœ¨å¯»æ‰¾çš„å†…å®¹&lt;/a>;ã€‚ &lt;/p>; &lt;p>; è¿™é¡¹å·¥ä½œè‡ªç„¶åœ°å»¶ç»­åˆ°äº†ç”Ÿæˆäººå·¥æ™ºèƒ½çš„ä¸–ç•Œï¼Œå…¶ä¸­ &lt;a href=&quot;https://blog.google/technology/research/how-ai-creates-photorealistic-images-from-text /&quot;>;æ¨¡åž‹å¯ä»¥æ ¹æ®å›¾åƒå’Œæ–‡æœ¬æç¤ºåˆ›å»ºå›¾åƒæˆ–è§†é¢‘é›†åˆ&lt;/a>;å’Œ&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning -in.html&quot;>;å¯ä»¥å›žç­”æœ‰å…³å›¾åƒå’Œè§†é¢‘çš„é—®é¢˜&lt;/a>;ã€‚æˆ‘ä»¬å¯¹è¿™äº›æŠ€æœ¯&lt;a href=&quot;https://blog.google/products/shopping/ai-virtual-try-on-google-shopping/&quot;>;ä¸ºç”¨æˆ·æä¾›æ–°ä½“éªŒ&lt;/açš„æ½œåŠ›æ„Ÿåˆ°å…´å¥‹>; å¹¶ä½œä¸ºè¿›ä¸€æ­¥æˆ‘ä»¬è‡ªå·±ç ”ç©¶çš„å·¥å…·ã€‚ä¸ºäº†å®žçŽ°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æ­£åœ¨ä¸Žç ”ç©¶å’Œè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ç¤¾åŒºåˆä½œï¼Œå¼€å‘å‡è½»æ•…éšœæ¨¡å¼çš„æŠ¤æ ã€‚æˆ‘ä»¬æ­£åœ¨åˆ©ç”¨æˆ‘ä»¬çš„å·¥å…·æ¥ç†è§£è¡¨ç¤ºï¼Œä»¥æ”¯æŒå¯ä¸Žäººç±»åé¦ˆç›¸ç»“åˆçš„å¯æ‰©å±•åŸºå‡†ï¼Œå¹¶æŠ•èµ„äºŽä»Žé¢„è®­ç»ƒåˆ°éƒ¨ç½²çš„ç ”ç©¶ï¼Œä»¥å¼•å¯¼æ¨¡åž‹ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´å…·åŒ…å®¹æ€§å’Œæ›´å¯æŽ§çš„è¾“å‡ºã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›æ¨¡åž‹èƒ½å¤Ÿæ¿€åŠ±äººä»¬ï¼Œäº§ç”Ÿå¤šæ ·åŒ–çš„è¾“å‡ºï¼Œåœ¨ä¸ä¾èµ–æ¯”å–»æˆ–åˆ»æ¿å°è±¡çš„æƒ…å†µä¸‹ç¿»è¯‘æ¦‚å¿µï¼Œå¹¶åœ¨æç¤ºçš„åäº‹å®žå˜åŒ–ä¸­æä¾›ä¸€è‡´çš„è¡Œä¸ºå’Œå“åº”ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;æœºé‡å’Œæ­£åœ¨è¿›è¡Œçš„å·¥ä½œ&lt;/h2>; &lt;p>; å°½ç®¡ç»è¿‡åå¤šå¹´çš„ä¸“æ³¨å·¥ä½œï¼Œè¯¥é¢†åŸŸæ„ŸçŸ¥å…¬å¹³æŠ€æœ¯ä¼¼ä¹Žä»ç„¶æ˜¯ä¸€ä¸ªæ–°å…´ä¸”å¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œå……æ»¡äº†çªç ´æ€§æŠ€æœ¯çš„æœºä¼šã€‚æˆ‘ä»¬ç»§ç»­çœ‹åˆ°åœ¨è·¨å­¦ç§‘å¥–å­¦é‡‘æ”¯æŒä¸‹è´¡çŒ®æŠ€æœ¯è¿›æ­¥çš„æœºä¼šã€‚æˆ‘ä»¬å¯ä»¥åœ¨å›¾åƒä¸­æµ‹é‡çš„å†…å®¹ä¸Žäººç±»èº«ä»½å’Œè¡¨è¾¾çš„åŸºæœ¬æ–¹é¢ä¹‹é—´çš„å·®è·å¾ˆå¤§ - ç¼©å°è¿™ä¸€å·®è·å°†éœ€è¦è¶Šæ¥è¶Šå¤æ‚çš„åª’ä½“åˆ†æžè§£å†³æ–¹æ¡ˆã€‚è¡¨æ˜ŽçœŸå®žä»£è¡¨æ€§çš„æ•°æ®æŒ‡æ ‡ï¼Œä½äºŽé€‚å½“çš„èƒŒæ™¯ä¸‹å¹¶è€ƒè™‘å¤šç§è§‚ç‚¹ï¼Œå¯¹æˆ‘ä»¬æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå…¬å¼€çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ˜¯å¦èƒ½å¤Ÿå¯é åœ°è¯†åˆ«å¯¹ç»†å¾®åˆ»æ¿å°è±¡çš„æè¿°ï¼Œä¸æ–­æ›´æ–°å®ƒä»¬ä»¥åæ˜ ä¸æ–­å˜åŒ–çš„ç¤¾ä¼šï¼Œå¹¶è¾¨åˆ«å®ƒä»¬å¯èƒ½ä»¤äººåæ„Ÿçš„æƒ…å†µï¼Ÿç”±äººç±»åé¦ˆé©±åŠ¨çš„ç®—æ³•è¿›æ­¥æŒ‡æ˜Žäº†ä¸€æ¡å……æ»¡å¸Œæœ›çš„å‰è¿›é“è·¯ã€‚ &lt;/p>; &lt;p>; æœ€è¿‘å¯¹çŽ°ä»£å¤§åž‹æ¨¡åž‹å¼€å‘èƒŒæ™¯ä¸‹çš„äººå·¥æ™ºèƒ½å®‰å…¨å’Œé“å¾·çš„å…³æ³¨æ¿€å‘äº†è¡¡é‡ç³»ç»Ÿåå·®çš„æ–°æ€ç»´æ–¹å¼ã€‚æˆ‘ä»¬æ­£åœ¨æŽ¢ç´¢ä½¿ç”¨è¿™äº›æ¨¡åž‹çš„å¤šç§é€”å¾„ - ä»¥åŠåŸºäºŽæ¦‚å¿µçš„å¯è§£é‡Šæ€§æ–¹æ³•ã€å› æžœæŽ¨ç†æ–¹æ³•å’Œå°–ç«¯ç”¨æˆ·ä½“éªŒç ”ç©¶çš„æœ€æ–°å‘å±• - æ¥é‡åŒ–å’Œæœ€å¤§ç¨‹åº¦åœ°å‡å°‘ä¸è‰¯åè§è¡Œä¸ºã€‚æˆ‘ä»¬æœŸå¾…ç€åº”å¯¹æœªæ¥çš„æŒ‘æˆ˜å¹¶å¼€å‘é€‚åˆæ¯ä¸ªäººçš„æŠ€æœ¯ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;æˆ‘ä»¬è¦æ„Ÿè°¢ Perception çš„æ¯ä¸€ä½æˆå‘˜å…¬å¹³å›¢é˜Ÿä»¥åŠæˆ‘ä»¬æ‰€æœ‰çš„åˆä½œè€…ã€‚&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3964459643854458124/comments/default&quot; rel=&quot;replies&quot; title =&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html#comment-form &quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124&quot; rel=&quot;edit â€œ type =â€œapplication/atom+xmlâ€/>;&lt;link href =â€œhttp://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124â€rel =â€œselfâ€type =â€œapplication/atom+xmlâ€ &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Google ç ”ç©¶çš„è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ï¼šæ„ŸçŸ¥å…¬å¹³&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com &lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded .gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MO HdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJe8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1 -inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr ï¼šæ€»è®¡>;&lt;/entry>;&lt;entry>;&lt;id>;æ ‡ç­¾ï¼šblogger.com,1999:blog-8474926331452026626.post-6350776943545232437&lt;/id>;&lt;å‘å¸ƒ>;2023-08-24T15:10:00.000-07:00&lt;/å·²å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-24T15:10:57.268-07:00&lt;/æ›´æ–°>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;é‡å­è®¡ç®—&quot;>;&lt; /category>;&lt;title type=&quot;text&quot;>;å¦‚ä½•å°†å˜ˆæ‚çš„é‡å­å¤„ç†å™¨ä¸Žç»å…¸è®¡ç®—æœºè¿›è¡Œæ¯”è¾ƒ&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;ç”± Sergio Boixo å’Œ Vadim å‘å¸ƒSmelyanskiyï¼ŒGoogle é‡å­ AI å›¢é˜Ÿé¦–å¸­ç§‘å­¦å®¶&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qs EcLuEDPnâ€‹â€‹WZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s320/hero.jpg&quot;æ ·å¼= â€œæ˜¾ç¤ºï¼šæ— ï¼›â€ />; &lt;p>; ä¸€å°å…¨é¢çš„çº é”™é‡å­è®¡ç®—æœºå°†èƒ½å¤Ÿè§£å†³ä¸€äº›ç»å…¸è®¡ç®—æœºæ— æ³•è§£å†³çš„é—®é¢˜ï¼Œä½†å»ºé€ è¿™æ ·çš„è®¾å¤‡æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚æˆ‘ä»¬ä¸ºå®žçŽ°å®Œå…¨é”™è¯¯æ‰€å–å¾—çš„&lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;é‡Œç¨‹ç¢‘&lt;/a>;æ„Ÿåˆ°è‡ªè±ª-ä¿®æ­£äº†é‡å­è®¡ç®—æœºï¼Œä½†æ˜¯å¤§åž‹è®¡ç®—æœºè¿˜éœ€è¦å‡ å¹´çš„æ—¶é—´ã€‚ä¸Žæ­¤åŒæ—¶ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨å½“å‰çš„å™ªå£°é‡å­å¤„ç†å™¨ä½œä¸ºé‡å­å®žéªŒçš„çµæ´»å¹³å°ã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ä¸Žçº é”™é‡å­&lt;em>;è®¡ç®—æœº&lt;/em>;ç›¸æ¯”ï¼Œç›®å‰åœ¨å™ªå£°é‡å­&lt;em>;å¤„ç†å™¨&lt;/em>;ä¸­è¿›è¡Œçš„å®žéªŒåœ¨å™ªå£°é™ä½Žé‡å­æ€ä¹‹å‰ï¼Œä»…é™äºŽå‡ åƒä¸ªé‡å­æ“ä½œæˆ–é—¨ã€‚ 2019 å¹´ï¼Œæˆ‘ä»¬åœ¨é‡å­å¤„ç†å™¨&lt;a href= â€œhttps://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;å¹¶é¦–æ¬¡è¡¨æ˜Ž&lt;/a>;å®ƒçš„æ€§èƒ½ä¼˜äºŽæœ€å…ˆè¿›çš„ç»å…¸è¶…çº§è®¡ç®—ã€‚ &lt;/p>; &lt;p>; è™½ç„¶å®ƒä»¬å°šæœªè¾¾åˆ°è¶…è¶Šç»å…¸çš„èƒ½åŠ›ï¼Œä½†æˆ‘ä»¬ä¹Ÿä½¿ç”¨æˆ‘ä»¬çš„å¤„ç†å™¨æ¥è§‚å¯Ÿæ–°çš„ç‰©ç†çŽ°è±¡ï¼Œä¾‹å¦‚&lt;a href=&quot;https://www.nature.com/articles/s41586 -021-04257-w&quot;>;æ—¶é—´æ™¶ä½“&lt;/a>;å’Œ&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;é©¬çº¦æ‹‰çº³è¾¹ç¼˜æ¨¡å¼&lt;/a>;ï¼Œå¹¶åˆ¶ä½œäº†æ–°çš„å®žéªŒå‘çŽ°ï¼Œä¾‹å¦‚ç›¸äº’ä½œç”¨å…‰å­çš„å¼ºå¤§&lt;a href=&quot;https://ai.googleblog.com/2022/12/formation-of-robust-bound-states-of.html&quot;>;æŸç¼šæ€&lt;/a>;ä»¥åŠ Floquet æ¼”åŒ–çš„é©¬çº¦æ‹‰çº³è¾¹ç¼˜æ¨¡å¼çš„&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;å™ªå£°å¼¹æ€§&lt;/a>;ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬é¢„è®¡ï¼Œå³ä½¿åœ¨è¿™ç§ä¸­é—´çš„ã€å˜ˆæ‚çš„çŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬ä¹Ÿä¼šæ‰¾åˆ°é‡å­å¤„ç†å™¨çš„åº”ç”¨ï¼Œå…¶ä¸­æœ‰ç”¨çš„é‡å­å®žéªŒå¯ä»¥æ¯”ç»å…¸è¶…çº§è®¡ç®—æœºä¸Šçš„è®¡ç®—é€Ÿåº¦å¿«å¾—å¤šâ€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œè®¡ç®—åº”ç”¨â€ â€œé‡å­å¤„ç†å™¨ã€‚ç›®å‰è¿˜æ²¡æœ‰äººå±•ç¤ºå‡ºè¿™æ ·ä¸€ç§è¶…è¶Šç»å…¸çš„è®¡ç®—åº”ç”¨ã€‚å› æ­¤ï¼Œå½“æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å®žçŽ°è¿™ä¸€é‡Œç¨‹ç¢‘æ—¶ï¼Œé—®é¢˜æ˜¯ï¼šå°†åœ¨æ­¤ç±»é‡å­å¤„ç†å™¨ä¸Šè¿è¡Œçš„é‡å­å®žéªŒä¸Žç»å…¸åº”ç”¨ç¨‹åºçš„è®¡ç®—æˆæœ¬è¿›è¡Œæ¯”è¾ƒçš„æœ€ä½³æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ &lt;/p>; &lt;p>; æˆ‘ä»¬å·²ç»çŸ¥é“å¦‚ä½•å°†çº é”™é‡å­ç®—æ³•ä¸Žç»å…¸ç®—æ³•è¿›è¡Œæ¯”è¾ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_complexity&quot;>;è®¡ç®—å¤æ‚åº¦&lt;/a>;é¢†åŸŸå‘Šè¯‰æˆ‘ä»¬ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒå®ƒä»¬å„è‡ªçš„è®¡ç®—æˆæœ¬â€”â€”å³å®Œæˆä»»åŠ¡æ‰€éœ€çš„æ“ä½œã€‚ä½†å¯¹äºŽæˆ‘ä»¬ç›®å‰çš„å®žéªŒæ€§é‡å­å¤„ç†å™¨æ¥è¯´ï¼Œæƒ…å†µè¿˜æ²¡æœ‰é‚£ä¹ˆæ˜Žç¡®ã€‚ &lt;/p>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;å™ªå£°é‡å­å¤„ç†å®žéªŒçš„æœ‰æ•ˆé‡å­ä½“ç§¯ã€ä¿çœŸåº¦å’Œè®¡ç®—æˆæœ¬&lt;/a>;â€ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ¡†æž¶ä¸ºäº†æµ‹é‡é‡å­å®žéªŒçš„è®¡ç®—æˆæœ¬ï¼Œå¼•å…¥äº†å®žéªŒçš„â€œæœ‰æ•ˆé‡å­ä½“ç§¯â€ï¼Œå³æœ‰åŠ©äºŽæµ‹é‡ç»“æžœçš„é‡å­æ“ä½œæˆ–é—¨çš„æ•°é‡ã€‚æˆ‘ä»¬åº”ç”¨è¿™ä¸ªæ¡†æž¶æ¥è¯„ä¼°æœ€è¿‘ä¸‰ä¸ªå®žéªŒçš„è®¡ç®—æˆæœ¬ï¼šæˆ‘ä»¬çš„&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;éšæœºç”µè·¯é‡‡æ ·&lt;/a>; &lt;a href=&quot;https://arxiv .org/abs/2304.11119&quot;>;å®žéªŒ&lt;/a>;ï¼Œæˆ‘ä»¬çš„&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>;å®žéªŒæµ‹é‡è¢«ç§°ä¸ºâ€œå¤±åºç›¸å…³å™¨â€çš„é‡â€ï¼ˆOTOCï¼‰&lt;/a>;ï¼Œä»¥åŠ&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;æœ€è¿‘å…³äºŽ Floquet è¿›åŒ–çš„å®žéªŒ&lt;/a>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Ising_model&quot;>;ä¼Šè¾›æ¨¡åž‹&lt;/a>;ã€‚æˆ‘ä»¬å¯¹ OTOC æ„Ÿåˆ°ç‰¹åˆ«å…´å¥‹ï¼Œå› ä¸ºå®ƒä»¬æä¾›äº†ä¸€ç§ç›´æŽ¥çš„æ–¹æ³•æ¥é€šè¿‡å®žéªŒæµ‹é‡ç”µè·¯çš„æœ‰æ•ˆé‡å­ä½“ç§¯ï¼ˆä¸€ç³»åˆ—é‡å­é—¨æˆ–æ“ä½œï¼‰ï¼Œè¿™æœ¬èº«å¯¹äºŽç»å…¸è®¡ç®—æœºæ¥è¯´æ˜¯ä¸€é¡¹è®¡ç®—ä¸Šå›°éš¾çš„ä»»åŠ¡ï¼Œè¦ç²¾ç¡®ä¼°è®¡ã€‚ OTOC åœ¨&lt;a href=&quot;https://en.wikipedia.org/wiki/Nuclear_Magnetic_resonance&quot;>;æ ¸ç£å…±æŒ¯&lt;/a>;å’Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/ Electron_paraMagnetic_resonance&quot;>;ç”µå­è‡ªæ—‹å…±æŒ¯å…‰è°±&lt;/a>;ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç›¸ä¿¡ OTOC å®žéªŒæ˜¯é‡å­å¤„ç†å™¨é¦–æ¬¡è®¡ç®—åº”ç”¨çš„æœ‰å¸Œæœ›çš„å€™é€‰è€…ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd 6VsOOMzDaexufiJs_8oyo1G37h2lr4Y1Y28zD7lZ3OIB_EL_LSY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/s1280/image3.png â€œ style=â€margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;1280&quot; height=&quot;297&quot; src= â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd6VsOOMzDaexufiJs_8oyo1G 37h2lr4Y1Y28zD7lZ3OIB_EL_LsY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/w640-h297/image3.png&quot;å®½åº¦=â€œ640â€/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;è®¡ç®—æˆæœ¬å’Œæœ€è¿‘ä¸€äº›é‡å­å®žéªŒçš„å½±å“å›¾ã€‚è™½ç„¶æœ‰äº›ï¼ˆä¾‹å¦‚ï¼Œ&lt;a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;QC-QMC 2022&lt;/a>;ï¼‰äº§ç”Ÿäº†å¾ˆå¤§å½±å“ï¼Œè€Œå¦ä¸€äº›ï¼ˆä¾‹å¦‚ï¼Œ&lt; a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;RCS 2023&lt;/a>;ï¼‰çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ï¼Œä½†è¿˜æ²¡æœ‰ä¸€ä¸ªæ—¢æœ‰ç”¨åˆå›°éš¾ï¼Œå€¼å¾—è€ƒè™‘â€œè®¡ç®—åº”ç”¨ç¨‹åºâ€ã€‚æˆ‘ä»¬å‡è®¾æˆ‘ä»¬æœªæ¥çš„ OTOC å®žéªŒå¯èƒ½æ˜¯ç¬¬ä¸€ä¸ªçªç ´è¿™ä¸ªé—¨æ§›çš„å®žéªŒã€‚æ–‡ä¸­å¼•ç”¨äº†ç»˜åˆ¶çš„å…¶ä»–å®žéªŒã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div >; &lt;h2>;éšæœºç”µè·¯é‡‡æ ·ï¼šè¯„ä¼°å™ªå£°ç”µè·¯çš„è®¡ç®—æˆæœ¬&lt;/h2>; &lt;p>; å½“è°ˆåˆ°åœ¨å™ªå£°é‡å­å¤„ç†å™¨ä¸Šè¿è¡Œé‡å­ç”µè·¯æ—¶ï¼Œæœ‰ä¸¤ä¸ªç›¸äº’ç«žäº‰çš„è€ƒè™‘å› ç´ ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åšä¸€äº›ç»å…¸éš¾ä»¥å®žçŽ°çš„äº‹æƒ…ã€‚è®¡ç®—æˆæœ¬ï¼ˆåœ¨ç»å…¸è®¡ç®—æœºä¸Šå®Œæˆä»»åŠ¡æ‰€éœ€çš„æ“ä½œæ•°é‡ï¼‰å–å†³äºŽé‡å­ç”µè·¯çš„&lt;em>;æœ‰æ•ˆé‡å­ä½“ç§¯&lt;/em>;ï¼šä½“ç§¯è¶Šå¤§ï¼Œè®¡ç®—æˆæœ¬è¶Šé«˜ï¼Œé‡å­è¶Šå¤šå¤„ç†å™¨çš„æ€§èƒ½å¯ä»¥è¶…è¶Šç»å…¸å¤„ç†å™¨ã€‚ &lt;/p>; &lt;p>; ä½†å¦ä¸€æ–¹é¢ï¼Œåœ¨å˜ˆæ‚çš„å¤„ç†å™¨ä¸Šï¼Œæ¯ä¸ªé‡å­é—¨éƒ½ä¼šç»™è®¡ç®—å¸¦æ¥é”™è¯¯ã€‚æ“ä½œè¶Šå¤šï¼Œè¯¯å·®å°±è¶Šå¤§ï¼Œå¹¶ä¸”é‡å­ç”µè·¯åœ¨æµ‹é‡æ„Ÿå…´è¶£çš„é‡æ—¶çš„ä¿çœŸåº¦å°±è¶Šä½Žã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯èƒ½æ›´å–œæ¬¢æ›´ç®€å•ã€æœ‰æ•ˆä½“ç§¯æ›´å°çš„ç”µè·¯ï¼Œä½†è¿™äº›ç”µè·¯å¾ˆå®¹æ˜“è¢«ç»å…¸è®¡ç®—æœºæ¨¡æ‹Ÿã€‚æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è¿™äº›ç›¸äº’ç«žäº‰çš„è€ƒè™‘å› ç´ çš„å¹³è¡¡ï¼Œç§°ä¸ºâ€œè®¡ç®—èµ„æºâ€ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1w OEPS4TwK55cMxJmg19mQD7ZCtuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/s973/image1.png&quot; æ ·å¼ = &quot;å·¦è¾¹è·ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;653&quot; data-original-width=&quot;973&quot; height=&quot;430&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1wOEPS4TwK55cMxJmg19mQD7Z Ctuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/w640-h430/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td ç±»=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;é‡å­ç”µè·¯ä¸­é‡å­ä½“ç§¯å’Œå™ªå£°ä¹‹é—´çš„æƒè¡¡å›¾ï¼Œä»¥ç§°ä¸ºâ€œè®¡ç®—èµ„æºâ€çš„é‡æ•èŽ·ã€‚å¯¹äºŽæœ‰å™ªå£°çš„é‡å­ç”µè·¯ï¼Œè¿™æœ€åˆä¼šéšç€è®¡ç®—æˆæœ¬çš„å¢žåŠ è€Œå¢žåŠ ï¼Œä½†æœ€ç»ˆï¼Œå™ªå£°ä¼šè¶…å‡ºç”µè·¯å¹¶å¯¼è‡´å…¶å‡å°‘ã€‚ &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„ &lt;a href=&quot;https://ai.googleblog.com/2019&quot; çœ‹åˆ°è¿™äº›ç›¸äº’ç«žäº‰çš„è€ƒè™‘å› ç´ å¦‚ä½•å‘æŒ¥ä½œç”¨/10/quantum-supremacy-using-programmable.html&quot;>;é‡å­å¤„ç†å™¨çš„â€œhello worldâ€ç¨‹åº&lt;/a>;ï¼Œç§°ä¸ºéšæœºç”µè·¯é‡‡æ ·ï¼ˆRCSï¼‰ï¼Œè¿™æ˜¯é‡å­å¤„ç†å™¨è¶…è¶Šç»å…¸è®¡ç®—æœºçš„é¦–æ¬¡æ¼”ç¤ºã€‚ä»»ä½•ä¸€ä¸ªé—¨çš„ä»»ä½•é”™è¯¯éƒ½å¯èƒ½ä½¿è¿™ä¸ªå®žéªŒå¤±è´¥ã€‚ä¸å¯é¿å…çš„æ˜¯ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆéš¾ä»¥é«˜ä¿çœŸåº¦å®žçŽ°çš„å®žéªŒï¼Œå› æ­¤å®ƒä¹Ÿå¯ä»¥ä½œä¸ºç³»ç»Ÿä¿çœŸåº¦çš„åŸºå‡†ã€‚ä½†å®ƒä¹Ÿå¯¹åº”äºŽé‡å­å¤„ç†å™¨å¯å®žçŽ°çš„æœ€é«˜å·²çŸ¥è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æœ€è¿‘æŠ¥å‘Šäº†è¿„ä»Šä¸ºæ­¢è¿›è¡Œçš„&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;æœ€å¼ºå¤§çš„ RCS&lt;/a>; å®žéªŒï¼Œæµ‹å¾—çš„å®žéªŒä¿çœŸåº¦è¾ƒä½Žï¼Œä¸º 1.7x10&lt;sup>;-3&lt;/ support>;ï¼Œç†è®ºè®¡ç®—æˆæœ¬é«˜è¾¾ ~10&lt;sup>;23&lt;/sup>;ã€‚è¿™äº›é‡å­ç”µè·¯æœ‰ 700 ä¸ªäºŒé‡å­ä½é—¨ã€‚æˆ‘ä»¬ä¼°è®¡è¿™ä¸ªå®žéªŒéœ€è¦çº¦ 47 å¹´çš„æ—¶é—´æ‰èƒ½åœ¨ä¸–ç•Œä¸Šæœ€å¤§çš„è¶…çº§è®¡ç®—æœºä¸Šè¿›è¡Œæ¨¡æ‹Ÿã€‚è™½ç„¶è¿™æ£€æŸ¥äº†è®¡ç®—åº”ç”¨ç¨‹åºæ‰€éœ€çš„ä¸¤ä¸ªæ¡†ä¹‹ä¸€â€”â€”å®ƒçš„æ€§èƒ½ä¼˜äºŽä¼ ç»Ÿçš„è¶…çº§è®¡ç®—æœºâ€”â€”ä½†å®ƒæœ¬èº«å¹¶ä¸æ˜¯ä¸€ä¸ªç‰¹åˆ«æœ‰ç”¨çš„åº”ç”¨ç¨‹åºã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;OTOC å’Œ Floquet æ¼”åŒ–ï¼šå±€éƒ¨å¯è§‚æµ‹é‡çš„æœ‰æ•ˆé‡å­ä½“ç§¯&lt;/h2>; &lt;p>; æœ‰é‡å­&lt;a href=&quot;https://en.wikipedia.org/wiki/Many-body_problem&quot;>;å¤šä½“ç‰©ç†å­¦&lt;/a>;ä¸­å­˜åœ¨è®¸å¤šç»å…¸çš„æ£˜æ‰‹é—®é¢˜ï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„é‡å­ä¸Šè¿è¡Œå…¶ä¸­ä¸€äº›å®žéªŒå¤„ç†å™¨æ½œåŠ›å·¨å¤§ã€‚æˆ‘ä»¬é€šå¸¸è®¤ä¸ºè¿™äº›å®žéªŒä¸Ž RCS å®žéªŒæœ‰äº›ä¸åŒã€‚æˆ‘ä»¬é€šå¸¸å…³å¿ƒçš„æ˜¯æ›´å…·ä½“çš„å±€éƒ¨ç‰©ç†å¯è§‚æµ‹å€¼ï¼Œè€Œä¸æ˜¯åœ¨å®žéªŒç»“æŸæ—¶æµ‹é‡æ‰€æœ‰é‡å­ä½çš„é‡å­æ€ã€‚ç”±äºŽå¹¶éžç”µè·¯ä¸­çš„æ¯ä¸ªæ“ä½œéƒ½å¿…ç„¶å½±å“å¯è§‚æµ‹é‡ï¼Œå› æ­¤å±€éƒ¨å¯è§‚æµ‹é‡çš„æœ‰æ•ˆé‡å­ä½“ç§¯å¯èƒ½å°äºŽè¿è¡Œå®žéªŒæ‰€éœ€çš„å®Œæ•´ç”µè·¯çš„æœ‰æ•ˆé‡å­ä½“ç§¯ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬å¯ä»¥é€šè¿‡åº”ç”¨&lt;a href=&quot;https://en.wikipedia.org/wiki/Special_relativity&quot;>;ç›¸å¯¹è®º&lt;/a>;ä¸­çš„å…‰é”¥æ¦‚å¿µæ¥ç†è§£è¿™ä¸€ç‚¹ï¼Œå®ƒå†³å®šäº†å“ªäº›äº‹ä»¶æ—¶ç©ºä¹‹é—´å¯ä»¥å­˜åœ¨å› æžœå…³ç³»ï¼šæŸäº›äº‹ä»¶ä¸å¯èƒ½ç›¸äº’å½±å“ï¼Œå› ä¸ºä¿¡æ¯éœ€è¦æ—¶é—´åœ¨å®ƒä»¬ä¹‹é—´ä¼ æ’­ã€‚æˆ‘ä»¬è¯´ä¸¤ä¸ªè¿™æ ·çš„äº‹ä»¶éƒ½åœ¨å®ƒä»¬å„è‡ªçš„å…‰é”¥ä¹‹å¤–ã€‚åœ¨é‡å­å®žéªŒä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ç§å«åšâ€œè´è¶é”¥â€çš„ä¸œè¥¿å–ä»£äº†å…‰é”¥ï¼Œå…¶ä¸­é”¥ä½“çš„ç”Ÿé•¿ç”±è´è¶é€Ÿåº¦å†³å®šï¼Œå³ä¿¡æ¯åœ¨æ•´ä¸ªç³»ç»Ÿä¸­ä¼ æ’­çš„é€Ÿåº¦ã€‚ ï¼ˆè¿™ä¸ªé€Ÿåº¦çš„ç‰¹å¾æ˜¯æµ‹é‡ OTOCï¼Œç¨åŽè®¨è®ºã€‚ï¼‰å±€éƒ¨å¯è§‚æµ‹é‡çš„æœ‰æ•ˆé‡å­ä½“ç§¯æœ¬è´¨ä¸Šæ˜¯è´è¶é”¥ä½“çš„ä½“ç§¯ï¼Œä»…åŒ…æ‹¬ä¸Žå¯è§‚æµ‹é‡æœ‰å› æžœå…³ç³»çš„é‡å­è¿ç®—ã€‚å› æ­¤ï¼Œä¿¡æ¯åœ¨ç³»ç»Ÿä¸­ä¼ æ’­å¾—è¶Šå¿«ï¼Œæœ‰æ•ˆä½“ç§¯å°±è¶Šå¤§ï¼Œå› æ­¤ç»å…¸æ¨¡æ‹Ÿå°±è¶Šå›°éš¾ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY -WpDN34wUfig-rtH1NC7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/s1392/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1392&quot; data-original-width=&quot;1092&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY-WpDN34wUfig-rtH1NC 7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/w502-h640/image2.png&quot; width=&quot;502&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æè¿°é—¨çš„æœ‰æ•ˆä½“ç§¯ V&lt;sub>;eff&lt;/sub>;å±€éƒ¨å¯è§‚æµ‹é‡ Bã€‚ç§°ä¸ºæœ‰æ•ˆé¢ç§¯ A&lt;sub>;eff&lt;/sub>; çš„ç›¸å…³é‡ç”±å¹³é¢å’Œåœ†é”¥ä½“çš„æ¨ªæˆªé¢è¡¨ç¤ºã€‚åº•åº§çš„å‘¨é•¿å¯¹åº”äºŽä»¥è´è¶é€Ÿåº¦v&lt;sub>;B&lt;/sub>;ç§»åŠ¨çš„ä¿¡æ¯ä¼ æ’­çš„å‰ç«¯ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>;æˆ‘ä»¬å°†æ­¤æ¡†æž¶åº”ç”¨äºŽæœ€è¿‘çš„&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;å®žéªŒ&lt;/a>;ï¼Œå®žçŽ°æ‰€è°“çš„ Floquet Ising æ¨¡åž‹ï¼ˆä¸€ç§ç‰©ç†æ¨¡åž‹ï¼‰ä¸Žæ—¶é—´æ™¶ä½“å’Œé©¬çº¦æ‹‰çº³å®žéªŒæœ‰å…³ã€‚æ ¹æ®è¯¥å®žéªŒçš„æ•°æ®ï¼Œå¯ä»¥ç›´æŽ¥ä¼°è®¡æœ€å¤§ç”µè·¯çš„æœ‰æ•ˆä¿çœŸåº¦ä¸º 0.37ã€‚ç”±äºŽæµ‹å¾—çš„é—¨é”™è¯¯çŽ‡ä¸º ~1%ï¼Œå› æ­¤ä¼°è®¡çš„æœ‰æ•ˆä½“ç§¯ä¸º ~100ã€‚è¿™æ¯”å…‰é”¥å°å¾—å¤šï¼Œå…‰é”¥åŒ…æ‹¬ 127 ä¸ªé‡å­ä½ä¸Šçš„ 2000 ä¸ªé—¨ã€‚æ‰€ä»¥ï¼Œæœ¬æ¬¡å®žéªŒçš„è´è¶é€Ÿåº¦ç›¸å½“å°ã€‚äº‹å®žä¸Šï¼Œæˆ‘ä»¬è®¤ä¸ºï¼Œä½¿ç”¨èŽ·å¾—æ¯”å®žéªŒæ›´é«˜ç²¾åº¦çš„æ•°å€¼æ¨¡æ‹Ÿï¼Œæœ‰æ•ˆä½“ç§¯ä»…è¦†ç›–çº¦ 28 ä¸ªé‡å­ä½ï¼Œè€Œä¸æ˜¯ 127 ä¸ªã€‚è¿™ç§å°çš„æœ‰æ•ˆä½“ç§¯ä¹Ÿå·²é€šè¿‡ OTOC æŠ€æœ¯&lt;a href=&quot;https://arxiv.org/abs/2306.17839&quot;>;è¯å®ž&lt;/a>;ã€‚å°½ç®¡è¿™æ˜¯ä¸€ä¸ªæ·±åº¦ç”µè·¯ï¼Œä½†ä¼°è®¡çš„è®¡ç®—æˆæœ¬ä¸º 5x10&lt;sup>;11&lt;/sup>;ï¼Œå‡ ä¹Žæ¯”æœ€è¿‘çš„ RCS å®žéªŒå°‘äº†ä¸€ä¸‡äº¿å€ã€‚ç›¸åº”åœ°ï¼Œè¯¥å®žéªŒå¯ä»¥åœ¨å•ä¸ª A100 GPU ä¸Š&lt;a href=&quot;https://arxiv.org/abs/2306.15970&quot;>;æ¨¡æ‹Ÿ&lt;/a>;æ¯ä¸ªæ•°æ®ç‚¹ä¸åˆ°ä¸€ç§’çš„æ—¶é—´ã€‚å› æ­¤ï¼Œè™½ç„¶è¿™ç¡®å®žæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åº”ç”¨ç¨‹åºï¼Œä½†å®ƒå¹¶æ²¡æœ‰æ»¡è¶³è®¡ç®—åº”ç”¨ç¨‹åºçš„ç¬¬äºŒä¸ªè¦æ±‚ï¼šå¤§å¤§ä¼˜äºŽç»å…¸æ¨¡æ‹Ÿã€‚ &lt;/p>; &lt;p>; OTOC çš„ä¿¡æ¯ç½®ä¹±å®žéªŒæ˜¯è®¡ç®—åº”ç”¨çš„ä¸€ä¸ªæœ‰å‰é€”çš„é€”å¾„ã€‚ OTOC å¯ä»¥å‘Šè¯‰æˆ‘ä»¬æœ‰å…³ç³»ç»Ÿçš„é‡è¦ç‰©ç†ä¿¡æ¯ï¼Œä¾‹å¦‚è¶å½¢é€Ÿåº¦ï¼Œè¿™å¯¹äºŽç²¾ç¡®æµ‹é‡ç”µè·¯çš„æœ‰æ•ˆé‡å­ä½“ç§¯è‡³å…³é‡è¦ã€‚ä½¿ç”¨å¿«é€Ÿçº ç¼ é—¨çš„ OTOC å®žéªŒä¸ºé¦–æ¬¡è¶…è¶Šç»å…¸çš„é‡å­å¤„ç†å™¨è®¡ç®—åº”ç”¨æ¼”ç¤ºæä¾›äº†æ½œåœ¨çš„é€”å¾„ã€‚äº‹å®žä¸Šï¼Œåœ¨ 2021 å¹´çš„&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>;å®žéªŒ&lt;/a>;ä¸­ï¼Œæˆ‘ä»¬å®žçŽ°äº† F&lt;sub>;eff &lt;/sub>; çš„æœ‰æ•ˆä¿çœŸåº¦~ 0.06ï¼Œå®žéªŒä¿¡å™ªæ¯”çº¦ä¸º 1ï¼Œå¯¹åº”äºŽ ~250 ä¸ªé—¨çš„æœ‰æ•ˆä½“ç§¯å’Œ 2x10&lt;sup>;12&lt;/sup>; çš„è®¡ç®—æˆæœ¬ã€‚ &lt;/p>; &lt;p>; è™½ç„¶è¿™äº›æ—©æœŸçš„ OTOC å®žéªŒä¸å¤Ÿå¤æ‚ï¼Œæ— æ³•è¶…è¶Šç»å…¸æ¨¡æ‹Ÿï¼Œä½† OTOC å®žéªŒæˆä¸ºè®¡ç®—åº”ç”¨é¦–æ¬¡æ¼”ç¤ºçš„è‰¯å¥½å€™é€‰è€…æœ‰å…¶æ·±åˆ»çš„ç‰©ç†åŽŸå› ã€‚è¿‘æœŸé‡å­å¤„ç†å™¨å¯ä»¥å®žçŽ°çš„å¤§å¤šæ•°æœ‰è¶£çš„é‡å­çŽ°è±¡å¾ˆéš¾ç»å…¸åœ°æ¨¡æ‹Ÿï¼Œè¿™ä¸ŽæŽ¢ç´¢è®¸å¤šé‡å­èƒ½çº§çš„é‡å­ç”µè·¯ç›¸å¯¹åº”ã€‚è¿™ç§æ¼”åŒ–é€šå¸¸æ˜¯æ··ä¹±çš„ï¼Œå¹¶ä¸”æ ‡å‡†æ—¶é—´é¡ºåºç›¸å…³å™¨ï¼ˆTOCï¼‰åœ¨è¿™ç§æƒ…å†µä¸‹å¾ˆå¿«è¡°å‡åˆ°çº¯éšæœºå¹³å‡å€¼ã€‚æ²¡æœ‰ç•™ä¸‹ä»»ä½•å®žéªŒä¿¡å·ã€‚ &lt;a href=&quot;https://arxiv.org/abs/2101.08870&quot;>;OTOC æµ‹é‡&lt;/a>;ä¸ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼Œå®ƒå…è®¸æˆ‘ä»¬éšæ„å¢žåŠ å¤æ‚æ€§ï¼Œä»…å—æ¯ä¸ªé—¨çš„é”™è¯¯çš„é™åˆ¶ã€‚æˆ‘ä»¬é¢„è®¡ï¼Œå°†é”™è¯¯çŽ‡é™ä½Žä¸€åŠå°†ä½¿è®¡ç®—æˆæœ¬åŠ å€ï¼Œä»Žè€Œå°†è¯¥å®žéªŒæŽ¨å‘è¶…è¶Šç»å…¸çš„é˜¶æ®µã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®º&lt;/h2>; &lt;p>;ä½¿ç”¨æˆ‘ä»¬å¼€å‘çš„æœ‰æ•ˆé‡å­ä½“ç§¯æ¡†æž¶ï¼Œæˆ‘ä»¬ç¡®å®šæˆ‘ä»¬çš„ RCS å’Œ OTOC å®žéªŒä»¥åŠæœ€è¿‘çš„ Floquet è¿›åŒ–å®žéªŒçš„è®¡ç®—æˆæœ¬ã€‚è™½ç„¶è¿™äº›éƒ½æ— æ³•æ»¡è¶³è®¡ç®—åº”ç”¨çš„è¦æ±‚ï¼Œä½†æˆ‘ä»¬é¢„è®¡ï¼Œéšç€é”™è¯¯çŽ‡çš„æé«˜ï¼ŒOTOC å®žéªŒå°†æˆä¸ºé‡å­å¤„ç†å™¨çš„ç¬¬ä¸€ä¸ªè¶…è¶Šç»å…¸çš„æœ‰ç”¨åº”ç”¨ã€‚ &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6350776943545232437/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 æ¡è¯„è®ºâ€œ type =â€œtext / htmlâ€/>;&lt;link href =â€œhttp://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437â€rel =â€œeditâ€type =â€œapplication/atom+xmlâ€/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html&quot; rel=&quot;alternate&quot; title=&quot;å¦‚ä½•å°†å˜ˆæ‚çš„é‡å­å¤„ç†å™¨ä¸Žç»å…¸è®¡ç®—æœºè¿›è¡Œæ¯”è¾ƒ&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ï¼šå›¾åƒé«˜åº¦=â€œ16â€rel=â€œhttp://schemas.google.com/g/2005#thumbnailâ€src=â€œhttps://img1.blogblog.com/img/b16-rounded.gifâ€å®½åº¦=â€œ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2 qsEcLuEDPnâ€‹â€‹WZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s72 -c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;æ ‡ç­¾ï¼šblogger.com,1999:blog-8474926331452026626.post-3589347607411181063&lt;/id>;&lt;å·²å‘å¸ƒ>;2023-08-24T12:33:00.002-07:00&lt;/å·²å‘å¸ƒ>;&lt;å·²æ›´æ–°>;2023-08-24T12:33:37.720-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;æ•™æŽˆè¯­è¨€æ¨¡åž‹è¿›è¡Œç®—æ³•æŽ¨ç†&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;å‘å¸ƒè€…ï¼šMILA ç ”ç©¶ç”Ÿ Hattie Zhouã€Google ç ”ç©¶ç§‘å­¦å®¶ Hanie Sedghi&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n _pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm- xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s320/hero.gif&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM)ï¼Œä¾‹å¦‚ &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>; å’Œ&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; è¿‘å¹´æ¥å–å¾—äº†ä»¤äººçž©ç›®çš„è¿›æ­¥ï¼Œç”±&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;æ‰©å¤§æ¨¡åž‹&lt;/a>;å’Œ&lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;>;è®­ç»ƒæ•°æ®å¤§å°é©±åŠ¨&lt;/a>;ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„äº‰è®ºæ˜¯æ³•å­¦ç¡•å£«æ˜¯å¦å¯ä»¥è¿›è¡Œè±¡å¾æ€§æŽ¨ç†ï¼ˆå³æ ¹æ®é€»è¾‘è§„åˆ™æ“çºµç¬¦å·ï¼‰ã€‚ä¾‹å¦‚ï¼Œæ³•å­¦ç¡•å£«èƒ½å¤Ÿåœ¨æ•°å­—è¾ƒå°æ—¶æ‰§è¡Œç®€å•çš„ç®—æœ¯è¿ç®—ï¼Œä½†åœ¨æ•°å­—è¾ƒå¤§æ—¶åˆ™éš¾ä»¥æ‰§è¡Œã€‚è¿™è¡¨æ˜Žæ³•å­¦ç¡•å£«å°šæœªå­¦ä¹ æ‰§è¡Œè¿™äº›ç®—æœ¯è¿ç®—æ‰€éœ€çš„åŸºæœ¬è§„åˆ™ã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; è™½ç„¶ç¥žç»ç½‘ç»œå…·æœ‰å¼ºå¤§çš„&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper .pdf&quot;>;æ¨¡å¼åŒ¹é…åŠŸèƒ½&lt;/a>;ï¼Œå®ƒä»¬å¾ˆå®¹æ˜“è¿‡åº¦æ‹Ÿåˆæ•°æ®ä¸­çš„è™šå‡ç»Ÿè®¡æ¨¡å¼ã€‚å½“è®­ç»ƒæ•°æ®åºžå¤§ä¸”å¤šæ ·åŒ–ä¸”è¯„ä¼°å‘ˆåˆ†å¸ƒæ—¶ï¼Œè¿™å¹¶ä¸å¦¨ç¢è‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºŽéœ€è¦åŸºäºŽè§„åˆ™çš„æŽ¨ç†ï¼ˆä¾‹å¦‚åŠ æ³•ï¼‰çš„ä»»åŠ¡ï¼Œæ³•å­¦ç¡•å£«å¾ˆéš¾åº”å¯¹åˆ†å¸ƒå¤–æ³›åŒ–ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸­çš„è™šå‡ç›¸å…³æ€§é€šå¸¸æ¯”çœŸæ­£çš„åŸºäºŽè§„åˆ™çš„è§£å†³æ–¹æ¡ˆæ›´å®¹æ˜“åˆ©ç”¨ã€‚å› æ­¤ï¼Œå°½ç®¡å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åŠ æ³•ç­‰ç®€å•ç®—æœ¯ä»»åŠ¡çš„æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å³ä½¿åœ¨ &lt;a href=&quot;https://arxiv.org/abs/2103.03874 ä¸Šå¯¹ &lt;a href=&quot;https://openai.com/research/gpt-4&quot;>;GPT-4&lt;/a>; è¿›è¡Œäº†é€‚åº¦æ”¹è¿›&quot;>;MATH&lt;/a>;æ•°æ®é›†ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/2303.12712&quot;>;é”™è¯¯ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºŽç®—æœ¯å’Œè®¡ç®—é”™è¯¯&lt;/a>;ã€‚å› æ­¤ï¼Œä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯æ³•å­¦ç¡•å£«æ˜¯å¦èƒ½å¤Ÿè¿›è¡Œç®—æ³•æŽ¨ç†ï¼Œè¿™æ¶‰åŠé€šè¿‡åº”ç”¨ä¸€ç»„å®šä¹‰ç®—æ³•çš„æŠ½è±¡è§„åˆ™æ¥è§£å†³ä»»åŠ¡ã€‚ &lt;/p>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;é€šè¿‡æƒ…å¢ƒå­¦ä¹ æ•™æŽˆç®—æ³•æŽ¨ç†&lt;/a>;â€ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ç§åˆ©ç”¨&lt;a href =&quot;https://arxiv.org/abs/2005.14165&quot;>;æƒ…å¢ƒå­¦ä¹ &lt;/a>;ï¼Œä»¥å®žçŽ°æ³•å­¦ç¡•å£«çš„ç®—æ³•æŽ¨ç†èƒ½åŠ›ã€‚ä¸Šä¸‹æ–‡å­¦ä¹ æ˜¯æŒ‡æ¨¡åž‹åœ¨æ¨¡åž‹ä¸Šä¸‹æ–‡ä¸­çœ‹åˆ°ä¸€äº›ä»»åŠ¡ç¤ºä¾‹åŽæ‰§è¡Œä»»åŠ¡çš„èƒ½åŠ›ã€‚ä½¿ç”¨æç¤ºå°†ä»»åŠ¡æŒ‡å®šç»™æ¨¡åž‹ï¼Œæ— éœ€æ›´æ–°æƒé‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç®—æ³•æç¤ºæŠ€æœ¯ï¼Œä½¿é€šç”¨è¯­è¨€æ¨¡åž‹èƒ½å¤Ÿå¯¹æ¯”æ‰€è§æ›´éš¾çš„ç®—æœ¯é—®é¢˜å®žçŽ°å¼ºå¤§çš„&lt;a href=&quot;https://arxiv.org/abs/2207.04901&quot;>;æ³›åŒ–&lt;/a>;åœ¨æç¤ºä¸­ã€‚æœ€åŽï¼Œæˆ‘ä»¬è¯æ˜Žæ¨¡åž‹å¯ä»¥é€šè¿‡é€‚å½“é€‰æ‹©æç¤ºç­–ç•¥ï¼Œåœ¨åˆ†å¸ƒå¤–çš„ç¤ºä¾‹ä¸Šå¯é åœ°æ‰§è¡Œç®—æ³•ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNy QMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q -uy_Xt6i7VkZCGiKqA97AGFnlB/s1200/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;1200 â€œ height =â€œ89â€src =â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfV dwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/w640-h89/image1.gif&quot;å®½åº¦=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;é€šè¿‡æä¾›ç®—æ³•æç¤ºï¼Œæˆ‘ä»¬å¯ä»¥æ•™æŽˆé€šè¿‡æƒ…å¢ƒå­¦ä¹ å¯¹ç®—æœ¯è§„åˆ™è¿›è¡Œå»ºæ¨¡ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼ŒLLMï¼ˆå•è¯é¢„æµ‹å™¨ï¼‰åœ¨æç¤ºä¸€ä¸ªç®€å•çš„åŠ æ³•é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œ267+197ï¼‰æ—¶è¾“å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œä½†åœ¨è¯¢é—®å…·æœ‰è¾ƒé•¿æ•°å­—çš„ç±»ä¼¼åŠ æ³•é—®é¢˜æ—¶ä¼šå¤±è´¥ã€‚ç„¶è€Œï¼Œå½“æ›´å›°éš¾çš„é—®é¢˜é™„åŠ äº†æ·»åŠ çš„ç®—æ³•æç¤ºï¼ˆå•è¯é¢„æµ‹å™¨ä¸‹æ–¹æ˜¾ç¤ºå¸¦æœ‰ç™½è‰² +&lt;strong>; &lt;/strong>; çš„è“è‰²æ¡†ï¼‰æ—¶ï¼Œæ¨¡åž‹èƒ½å¤Ÿæ­£ç¡®å›žç­”ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡åž‹è¿˜èƒ½å¤Ÿé€šè¿‡ç»„åˆä¸€ç³»åˆ—åŠ æ³•è®¡ç®—æ¥æ¨¡æ‹Ÿä¹˜æ³•ç®—æ³•ï¼ˆ&lt;strong>;X&lt;/strong>;ï¼‰ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;å°†ç®—æ³•ä½œä¸ºä¸€ç§æŠ€èƒ½è¿›è¡Œæ•™å­¦&lt;/h2>; &lt;p>; ä¸ºäº†å°†ç®—æ³•ä½œä¸ºä¸€ç§æŠ€èƒ½æ¥æ•™æŽˆæ¨¡åž‹ï¼Œæˆ‘ä»¬å¼€å‘ç®—æ³•æç¤ºï¼Œå®ƒå»ºç«‹åœ¨å…¶ä»–åŸºæœ¬åŽŸç†å¢žå¼ºæ–¹æ³•çš„åŸºç¡€ä¸Šï¼ˆä¾‹å¦‚ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpad&lt;/a>;å’Œ&lt;a href=&quot;https://ai. googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;æ€æƒ³é“¾&lt;/a>;ï¼‰ã€‚ç®—æ³•æç¤ºä»Žæ³•å­¦ç¡•å£«ä¸­æå–ç®—æ³•æŽ¨ç†èƒ½åŠ›ï¼Œä¸Žå…¶ä»–æç¤ºæ–¹æ³•ç›¸æ¯”æœ‰ä¸¤ä¸ªæ˜¾ç€åŒºåˆ«ï¼šï¼ˆ1ï¼‰å®ƒé€šè¿‡è¾“å‡ºç®—æ³•è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„æ­¥éª¤æ¥è§£å†³ä»»åŠ¡ï¼Œï¼ˆ2ï¼‰å®ƒä»¥è¶³å¤Ÿçš„ç»†èŠ‚è§£é‡Šæ¯ä¸ªç®—æ³•æ­¥éª¤ï¼Œå› æ­¤æ³•å­¦ç¡•å£«ä¸å…è®¸æœ‰ä»»ä½•è¯¯è§£ã€‚ &lt;/p>; &lt;p>; ä¸ºäº†èŽ·å¾—ç®—æ³•æç¤ºçš„ç›´è§‰ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸¤ä¸ªæ•°å­—ç›¸åŠ çš„ä»»åŠ¡ã€‚åœ¨ä¾¿ç¬ºå¼æç¤ºä¸­ï¼Œæˆ‘ä»¬ä»Žå³åˆ°å·¦å¤„ç†æ¯ä¸ªæ•°å­—ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥è·Ÿè¸ªè¿›ä½å€¼ï¼ˆå³ï¼Œå¦‚æžœå½“å‰æ•°å­—å¤§äºŽ 9ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªæ•°å­—ä¸ŠåŠ  1ï¼‰ã€‚ç„¶è€Œï¼Œåœ¨åªçœ‹åˆ°å‡ ä¸ªè¿›ä½å€¼çš„ä¾‹å­åŽï¼Œè¿›ä½è§„åˆ™å°±å¾ˆæ¨¡ç³Šäº†ã€‚æˆ‘ä»¬å‘çŽ°ï¼ŒåŒ…å«æ˜¾å¼æ–¹ç¨‹æ¥æè¿°è¿›ä½è§„åˆ™æœ‰åŠ©äºŽæ¨¡åž‹å…³æ³¨ç›¸å…³ç»†èŠ‚å¹¶æ›´å‡†ç¡®åœ°è§£é‡Šæç¤ºã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ç§æ´žå¯ŸåŠ›å¼€å‘äº†ä¸¤ä¸ªæ•°å­—åŠ æ³•çš„ç®—æ³•æç¤ºï¼Œå…¶ä¸­æˆ‘ä»¬ä¸ºæ¯ä¸ªè®¡ç®—æ­¥éª¤æä¾›äº†æ˜Žç¡®çš„æ–¹ç¨‹ï¼Œå¹¶ä»¥æ˜Žç¡®çš„æ ¼å¼æè¿°äº†å„ç§ç´¢å¼•æ“ä½œã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260v McydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mlLTkDzy14YMJ32U5mkQT2Y85drKXIVr/s1584 /image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1250&quot; data-original-width=&quot;1584&quot; height=&quot; 505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60h wF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/w640-h505/image4.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;å„ç§æ·»åŠ æç¤ºç­–ç•¥è¯´æ˜Žã€‚&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; ä»…ä½¿ç”¨ä¸‰ä¸ªç­”æ¡ˆé•¿åº¦æœ€å¤šä¸º 5 ä½æ•°å­—çš„åŠ æ³•æç¤ºç¤ºä¾‹ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å¤š 19 ä½æ•°å­—çš„åŠ æ³•æ€§èƒ½ã€‚å‡†ç¡®æ€§æ˜¯é€šè¿‡åœ¨æ•´ä¸ªç­”æ¡ˆèŒƒå›´å†…å‡åŒ€é‡‡æ ·çš„ 2,000 ä¸ªç¤ºä¾‹æ¥è¡¡é‡çš„ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼Œä½¿ç”¨ç®—æ³•æç¤ºå¯ä»¥åœ¨æ¯”æç¤ºä¸­çœ‹åˆ°çš„æ—¶é—´æ›´é•¿çš„æ—¶é—´å†…ä¿æŒé—®é¢˜çš„é«˜ç²¾åº¦ï¼Œè¿™è¡¨æ˜Žè¯¥æ¨¡åž‹ç¡®å®žé€šè¿‡æ‰§è¡Œä¸Žè¾“å…¥æ— å…³çš„ç®—æ³•æ¥è§£å†³ä»»åŠ¡ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZ jl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s1600/image3.png&quot;æ ·å¼=&quot;å·¦è¾¹è·ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;548&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8 YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align :center;&quot;>;æµ‹è¯•ä¸åŒæç¤ºæ–¹å¼ä¸‹é•¿åº¦é€’å¢žçš„åŠ æ³•é¢˜çš„å‡†ç¡®æ€§ã€‚ &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;åˆ©ç”¨ç®—æ³•æŠ€èƒ½ä½œä¸ºå·¥å…·ä½¿ç”¨&lt;/h2>; &lt;p>;ä¸ºäº†è¯„ä¼°æ¨¡åž‹æ˜¯å¦å¯ä»¥åœ¨æ›´å¹¿æ³›çš„æŽ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨ç®—æ³•æŽ¨ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨å°å­¦æ•°å­¦åº”ç”¨é¢˜æ¥è¯„ä¼°æ€§èƒ½ï¼ˆ&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot; >;GSM8k&lt;/a>;ï¼‰ã€‚æˆ‘ä»¬ç‰¹åˆ«å°è¯•ç”¨ç®—æ³•è§£å†³æ–¹æ¡ˆæ›¿æ¢ GSM8k çš„åŠ æ³•è®¡ç®—ã€‚ &lt;/p>; &lt;p>; å—ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶å’Œä¸åŒç®—æ³•ä¹‹é—´å¯èƒ½å­˜åœ¨çš„å¹²æ‰°çš„æŽ¨åŠ¨ï¼Œæˆ‘ä»¬æŽ¢ç´¢äº†ä¸€ç§ç­–ç•¥ï¼Œå…¶ä¸­ä¸åŒæç¤ºçš„æ¨¡åž‹ç›¸äº’äº¤äº’ä»¥è§£å†³å¤æ‚çš„ä»»åŠ¡ã€‚åœ¨ GSM8k çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªä¸“é—¨ä½¿ç”¨&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;æ€æƒ³é“¾æç¤º&lt;/a>;è¿›è¡Œéžæ­£å¼æ•°å­¦æŽ¨ç†çš„æ¨¡åž‹ï¼Œä»¥åŠç¬¬äºŒä¸ªæ¨¡åž‹ä¸“é—¨ç ”ç©¶ä½¿ç”¨&lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;ç®—æ³•æç¤º&lt;/a>;è¿›è¡ŒåŠ æ³•ã€‚éžæ­£å¼æ•°å­¦æŽ¨ç†æ¨¡åž‹è¢«æç¤ºè¾“å‡ºä¸“é—¨çš„æ ‡è®°ï¼Œä»¥ä¾¿è°ƒç”¨åŠ æ³•æç¤ºæ¨¡åž‹æ¥æ‰§è¡Œç®—æœ¯æ­¥éª¤ã€‚æˆ‘ä»¬æå–æ ‡è®°ä¹‹é—´çš„æŸ¥è¯¢ï¼Œå°†å®ƒä»¬å‘é€åˆ°åŠ æ³•æ¨¡åž‹å¹¶å°†ç­”æ¡ˆè¿”å›žåˆ°ç¬¬ä¸€ä¸ªæ¨¡åž‹ï¼Œä¹‹åŽç¬¬ä¸€ä¸ªæ¨¡åž‹ç»§ç»­å…¶è¾“å‡ºã€‚æˆ‘ä»¬ä½¿ç”¨ GSM8k (GSM8k-Hard) ä¸­çš„ä¸€ä¸ªéš¾é¢˜æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå…¶ä¸­æˆ‘ä»¬éšæœºé€‰æ‹© 50 ä¸ªä»…åŠ æ³•é—®é¢˜å¹¶å¢žåŠ é—®é¢˜ä¸­çš„æ•°å€¼ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2 Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/s1268/An%20example%20from%20the %20GSM8k-Hard%20dataset.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;330&quot; data-original-width=&quot;1268 â€œé«˜åº¦=â€œ167â€src=â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358p LdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/w640-h167/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; å®½åº¦=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æ¥è‡ª GSM8k-Hard æ•°æ®é›†çš„ç¤ºä¾‹ã€‚æ€æƒ³é“¾æç¤ºç”¨æ‹¬å·å¢žå¼ºï¼Œä»¥æŒ‡ç¤ºä½•æ—¶åº”æ‰§è¡Œç®—æ³•è°ƒç”¨ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;æˆ‘ä»¬å‘çŽ°ä½¿ç”¨å•ç‹¬çš„ä¸Šä¸‹æ–‡å’Œæ¨¡åž‹ä¸Žä¸“é—¨çš„æç¤ºæ˜¯è§£å†³GSM8k-Hardçš„æœ‰æ•ˆæ–¹æ³•ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä½¿ç”¨ç®—æ³•è°ƒç”¨åŠ æ³•çš„æ¨¡åž‹çš„æ€§èƒ½æ˜¯æ€æƒ³é“¾åŸºçº¿çš„ 2.3 å€ã€‚æœ€åŽï¼Œè¯¥ç­–ç•¥æä¾›äº†ä¸€ä¸ªé€šè¿‡æƒ…å¢ƒå­¦ä¹ ä¿ƒè¿›ä¸“é—¨ä»Žäº‹ä¸åŒæŠ€èƒ½çš„æ³•å­¦ç¡•å£«ä¹‹é—´çš„äº’åŠ¨æ¥è§£å†³å¤æ‚ä»»åŠ¡çš„ç¤ºä¾‹ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C 7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/s716/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;192&quot; data-original-width=&quot;716&quot; height=&quot;108&quot; src= â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-Dr H0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/w400-h108/image2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æœ‰æˆ–æ²¡æœ‰ç®—æ³•è°ƒç”¨çš„ GSM8k-Hard ä¸Šçš„æ€æƒ³é“¾ (CoT) æ€§èƒ½ã€‚&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®º&lt;/h2>; &lt;p>; æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;æƒ…å¢ƒå­¦ä¹ &lt;/a>;å’Œæ–°é¢–çš„ç®—æ³•æç¤ºæŠ€æœ¯æ¥è§£é”æ³•å­¦ç¡•å£«çš„ç®—æ³•æŽ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æžœè¡¨æ˜Žï¼Œé€šè¿‡æä¾›æ›´è¯¦ç»†çš„è§£é‡Šï¼Œå¯èƒ½å¯ä»¥å°†æ›´é•¿çš„ä¸Šä¸‹æ–‡è½¬åŒ–ä¸ºæ›´å¥½çš„æŽ¨ç†æ€§èƒ½ã€‚å› æ­¤ï¼Œè¿™äº›å‘çŽ°è¡¨æ˜Žèƒ½å¤Ÿä½¿ç”¨æˆ–ä»¥å…¶ä»–æ–¹å¼æ¨¡æ‹Ÿé•¿ä¸Šä¸‹æ–‡å¹¶äº§ç”Ÿæ›´å¤šä¿¡æ¯ä¸°å¯Œçš„åŸºæœ¬åŽŸç†ä½œä¸ºæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;æˆ‘ä»¬æ„Ÿè°¢æˆ‘ä»¬çš„åˆè‘—è€… Behnam Neyshaburã€Azade Novaã€Hugo Larochelle å’Œ Aaron Courville å¯¹æœ¬æ–‡åšå‡ºçš„å®è´µè´¡çŒ®ä»¥åŠå¯¹åšå®¢çš„è‰¯å¥½åé¦ˆã€‚æˆ‘ä»¬æ„Ÿè°¢ Tom Small åˆ›å»ºæœ¬æ–‡ä¸­çš„åŠ¨ç”»ã€‚è¿™é¡¹å·¥ä½œæ˜¯ Hattie Zhou åœ¨ Google Research å®žä¹ æœŸé—´å®Œæˆçš„ã€‚&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3589347607411181063/comments/default &quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to- Reason.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 æ¡è¯„è®º&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /3589347607411181063&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;self&quot; ç±»åž‹=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html&quot; rel=&quot;alternate&quot; title=&quot;æ•™å­¦ç®—æ³•æŽ¨ç†çš„è¯­è¨€æ¨¡åž‹â€ type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_C g8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thrï¼štotal>;&lt;/entry>;&lt;entry>;&lt;id>;æ ‡ç­¾ï¼šblogger.comï¼Œ1999ï¼šblog-8474926331452026626.post-6641308922483151364&lt;/id>;&lt;å‘å¸ƒ>;2023-08-22T11ï¼š47ï¼š00.004- 07:00&lt;/å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-24T10:34:33.888-07:00&lt;/æ›´æ–°>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atom/ns#â€æœ¯è¯­=æ·±åº¦å­¦ä¹ &quot;>;&lt;/ç±»åˆ«>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atom/ns#â€æœ¯è¯­=â€œæœºå™¨å­¦ä¹ â€>;&lt;/ç±»åˆ«>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atom/ns#â€ã€‚ blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;æœºå™¨äººæŠ€èƒ½ç»¼åˆå¥–åŠ±è¯­è¨€&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class= &quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šGoogle ç ”ç©¶ç§‘å­¦å®¶ Wenhao Yu å’Œ Fei Xia&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d- RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTneyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw /s320/Language%20to%20reward.jpg&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; ä½¿æœ€ç»ˆç”¨æˆ·èƒ½å¤Ÿä»¥äº¤äº’æ–¹å¼æ•™å¯¼æœºå™¨äººæ‰§è¡Œæ–°é¢–çš„ä»»åŠ¡ï¼Œè¿™æ˜¯æœºå™¨äººæˆåŠŸé›†æˆåˆ°çŽ°å®žä¸–ç•Œåº”ç”¨ç¨‹åºä¸­çš„å…³é”®èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯èƒ½æƒ³è¦æ•™æœºå™¨ç‹—è¡¨æ¼”æ–°æŠ€å·§ï¼Œæˆ–è€…æ•™æœºæ¢°æ‰‹æœºå™¨äººå¦‚ä½•æ ¹æ®ç”¨æˆ·åå¥½æ•´ç†åˆé¤ç›’ã€‚ &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;å¤§åž‹è¯­è¨€æ¨¡åž‹&lt;/a>; (LLM) çš„æœ€æ–°è¿›å±•å¯¹å¤§é‡äº’è”ç½‘æ•°æ®è¿›è¡Œçš„åŸ¹è®­æ˜¾ç¤ºå‡ºå®žçŽ°è¿™ä¸€ç›®æ ‡çš„æœ‰å¸Œæœ›çš„é“è·¯ã€‚äº‹å®žä¸Šï¼Œç ”ç©¶äººå‘˜å·²ç»ä»Ž&lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;åˆ†æ­¥è§„åˆ’ä¸­æŽ¢ç´¢äº†åˆ©ç”¨ LLM è¿›è¡Œæœºå™¨äººæŠ€æœ¯çš„å¤šç§æ–¹æ³•&lt;/a>; å’Œ&lt;a href=&quot;https://interactive-language.github.io/&quot;>;ä»¥ç›®æ ‡ä¸ºå¯¼å‘çš„å¯¹è¯&lt;/a>;åˆ°&lt;a href=&quot;https://ai.googleblog.com/2022/ 11/robots-that-write-their-own-code.html&quot;>;æœºå™¨äººä»£ç ç¼–å†™ä»£ç†&lt;/a>;ã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; è™½ç„¶è¿™äº›æ–¹æ³•èµ‹äºˆäº†ç»„åˆæ¦‚æ‹¬çš„æ–°æ¨¡å¼ï¼Œä½†å®ƒä»¬ä¾§é‡äºŽä½¿ç”¨è¯­è¨€å°†æ¥è‡ª &lt;a href=&quot;https:// çš„æ–°è¡Œä¸ºé“¾æŽ¥åœ¨ä¸€èµ·ã€‚ sites.research.google/palm-saycan&quot;>;çŽ°æœ‰çš„æŽ§åˆ¶åŽŸè¯­åº“&lt;/a>;ï¼Œè¦ä¹ˆæ˜¯æ‰‹åŠ¨è®¾è®¡çš„ï¼Œè¦ä¹ˆæ˜¯å…ˆéªŒå­¦ä¹ çš„ã€‚å°½ç®¡æ‹¥æœ‰æœ‰å…³æœºå™¨äººè¿åŠ¨çš„å†…éƒ¨çŸ¥è¯†ï¼Œä½†ç”±äºŽç›¸å…³è®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§æœ‰é™ï¼Œæ³•å­¦ç¡•å£«å¾ˆéš¾ç›´æŽ¥è¾“å‡ºä½Žçº§æœºå™¨äººå‘½ä»¤ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•çš„è¡¨è¾¾å—åˆ°å¯ç”¨åŽŸè¯­çš„å¹¿åº¦çš„ç“¶é¢ˆï¼Œå…¶è®¾è®¡é€šå¸¸éœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†æˆ–å¤§é‡æ•°æ®æ”¶é›†ã€‚ &lt;/p>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://arxiv.org/abs/2306.08647&quot;>;æœºå™¨äººæŠ€èƒ½ç»¼åˆå¥–åŠ±è¯­è¨€&lt;/a>;â€ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æˆ·èƒ½å¤Ÿæ•™æŽˆæœºå™¨äººçš„æ–¹æ³•é€šè¿‡è‡ªç„¶è¯­è¨€è¾“å…¥è¿›è¡Œæ–°é¢–çš„åŠ¨ä½œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ©ç”¨å¥–åŠ±å‡½æ•°ä½œä¸ºæŽ¥å£ï¼Œå¼¥åˆè¯­è¨€å’Œä½Žçº§æœºå™¨äººåŠ¨ä½œä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé‰´äºŽå¥–åŠ±å‡½æ•°åœ¨è¯­ä¹‰ã€æ¨¡å—åŒ–å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„ä¸°å¯Œæ€§ï¼Œå®ƒä»¬ä¸ºæ­¤ç±»ä»»åŠ¡æä¾›äº†ç†æƒ³çš„æŽ¥å£ã€‚å®ƒä»¬è¿˜é€šè¿‡é»‘ç›’ä¼˜åŒ–æˆ–å¼ºåŒ–å­¦ä¹  (RL) æä¾›ä¸Žä½Žçº§ç­–ç•¥çš„ç›´æŽ¥è¿žæŽ¥ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è¯­è¨€åˆ°å¥–åŠ±ç³»ç»Ÿï¼Œåˆ©ç”¨ LLM å°†è‡ªç„¶è¯­è¨€ç”¨æˆ·æŒ‡ä»¤ç¿»è¯‘ä¸ºå¥–åŠ±æŒ‡å®šä»£ç ï¼Œç„¶åŽåº”ç”¨ &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>;æ‰¾åˆ°æœ€å¤§åŒ–ç”Ÿæˆçš„å¥–åŠ±å‡½æ•°çš„æœ€ä½³ä½Žçº§æœºå™¨äººåŠ¨ä½œã€‚æˆ‘ä»¬ä½¿ç”¨å››è¶³æœºå™¨äººå’Œçµå·§æœºæ¢°æ‰‹æœºå™¨äººæ¨¡æ‹Ÿå„ç§æœºå™¨äººæŽ§åˆ¶ä»»åŠ¡ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„è¯­è¨€å¥–åŠ±ç³»ç»Ÿã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨ç‰©ç†æœºå™¨äººæ“çºµå™¨ä¸ŠéªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ã€‚ &lt;/p>; &lt;p>; è¯­è¨€åˆ°å¥–åŠ±ç³»ç»Ÿç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼š(1) å¥–åŠ±ç¿»è¯‘å™¨ï¼Œä»¥åŠ (2) è¿åŠ¨æŽ§åˆ¶å™¨&lt;em>;ã€‚å¥–åŠ±ç¿»è¯‘å™¨å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°ä»¥ &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;Python ä»£ç è¡¨ç¤ºçš„å¥–åŠ±å‡½æ•°&lt;/a>;ã€‚è¿åŠ¨æŽ§åˆ¶å™¨ä½¿ç”¨&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;åŽé€€åœ°å¹³çº¿ä¼˜åŒ–&lt;/a>;æ¥ä¼˜åŒ–ç»™å®šçš„å¥–åŠ±å‡½æ•°ï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„ä½Žçº§æœºå™¨äººåŠ¨ä½œï¼Œä¾‹å¦‚æ•°é‡åº”æ–½åŠ åˆ°æ¯ä¸ªæœºå™¨äººç”µæœºçš„æ‰­çŸ©ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65 Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s720/image5.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUq G3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;ç”±äºŽé¢„è®­ç»ƒæ•°æ®é›†ä¸­ç¼ºä¹æ•°æ®ï¼ŒLLM æ— æ³•ç›´æŽ¥ç”Ÿæˆä½Žçº§æœºå™¨äººåŠ¨ä½œã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨å¥–åŠ±å‡½æ•°æ¥å¼¥åˆè¯­è¨€å’Œä½Žçº§æœºå™¨äººåŠ¨ä½œä¹‹é—´çš„å·®è·ï¼Œå¹¶æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤å®žçŽ°æ–°é¢–çš„å¤æ‚æœºå™¨äººåŠ¨ä½œã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;å¥–åŠ±ç¿»è¯‘å™¨ï¼šå°†ç”¨æˆ·æŒ‡ä»¤ç¿»è¯‘ä¸ºå¥–åŠ±å‡½æ•°&lt;/h2>; &lt;p>; å¥–åŠ±ç¿»è¯‘å™¨æ¨¡å—çš„æž„å»ºç›®æ ‡æ˜¯å°†è‡ªç„¶è¯­è¨€ç”¨æˆ·æŒ‡ä»¤æ˜ å°„åˆ°å¥–åŠ±å‡½æ•°ã€‚å¥–åŠ±è°ƒæ•´æ˜¯é«˜åº¦ç‰¹å®šé¢†åŸŸçš„ï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†ï¼Œå› æ­¤å½“æˆ‘ä»¬å‘çŽ°åœ¨é€šç”¨è¯­è¨€æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ³•å­¦ç¡•å£«æ— æ³•ç›´æŽ¥ä¸ºç‰¹å®šç¡¬ä»¶ç”Ÿæˆå¥–åŠ±å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬å¹¶ä¸æ„Ÿåˆ°æƒŠè®¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åº”ç”¨äº†æ³•å­¦ç¡•å£«çš„&lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;æƒ…å¢ƒå­¦ä¹ &lt;/a>;èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å¥–åŠ±ç¿»è¯‘å™¨åˆ†ä¸ºä¸¤ä¸ªå­æ¨¡å—ï¼š&lt;em>;åŠ¨ä½œæè¿°ç¬¦&lt;/em>;å’Œ&lt;em>;å¥–åŠ±ç¼–ç å™¨&lt;/em>;ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;è¿åŠ¨æè¿°ç¬¦&lt;/h3>; &lt;p>; é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡ä¸€ä¸ªè¿åŠ¨æè¿°ç¬¦æ¥è§£é‡Šæ¥è‡ªç”¨æˆ·å¹¶å°†å…¶æ‰©å±•ä¸ºéµå¾ªé¢„å®šä¹‰æ¨¡æ¿çš„æ‰€éœ€æœºå™¨äººè¿åŠ¨çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚è¯¥è¿åŠ¨æè¿°ç¬¦å°†æ½œåœ¨ä¸æ˜Žç¡®æˆ–æ¨¡ç³Šçš„ç”¨æˆ·æŒ‡ä»¤è½¬åŒ–ä¸ºæ›´å…·ä½“å’Œæè¿°æ€§çš„æœºå™¨äººè¿åŠ¨ï¼Œä½¿å¥–åŠ±ç¼–ç ä»»åŠ¡æ›´åŠ ç¨³å®šã€‚è€Œä¸”ï¼Œç”¨æˆ·é€šè¿‡åŠ¨ä½œæè¿°å­—æ®µä¸Žç³»ç»Ÿè¿›è¡Œäº¤äº’ï¼Œå› æ­¤ä¸Žç›´æŽ¥æ˜¾ç¤ºå¥–åŠ±å‡½æ•°ç›¸æ¯”ï¼Œè¿™ä¹Ÿä¸ºç”¨æˆ·æä¾›äº†æ›´æ˜“äºŽè§£é‡Šçš„ç•Œé¢ã€‚ &lt;/p>; &lt;p>; ä¸ºäº†åˆ›å»ºè¿åŠ¨æè¿°ç¬¦ï¼Œæˆ‘ä»¬ä½¿ç”¨ LLM å°†ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºæ‰€éœ€æœºå™¨äººè¿åŠ¨çš„è¯¦ç»†æè¿°ã€‚æˆ‘ä»¬è®¾è®¡äº†&lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_motion_descriptor.txt&quot;>;æç¤º&lt;/a>;ï¼ŒæŒ‡å¯¼æ³•å­¦ç¡•å£«è¾“å‡ºåŒ…å«é€‚é‡è¯¦ç»†ä¿¡æ¯çš„åŠ¨ä½œæè¿°å’Œæ ¼å¼ã€‚é€šè¿‡å°†æ¨¡ç³Šçš„ç”¨æˆ·æŒ‡ä»¤è½¬æ¢ä¸ºæ›´è¯¦ç»†çš„æè¿°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨æˆ‘ä»¬çš„ç³»ç»Ÿæ›´å¯é åœ°ç”Ÿæˆå¥–åŠ±å‡½æ•°ã€‚è¿™ä¸ªæƒ³æ³•ä¹Ÿå¯ä»¥æ›´å¹¿æ³›åœ°åº”ç”¨äºŽæœºå™¨äººä»»åŠ¡ä¹‹å¤–ï¼Œå¹¶ä¸”ä¸Ž &lt;a href=&quot;https://innermonologue.github.io/&quot;>;Inner-Monologue&lt;/a>; å’Œ &lt;a href=&quot;https:/ /ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;æ€ç»´é“¾æç¤º&lt;/a>;ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Reward Coder &lt;/h3>; &lt;p>; åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸Ž Motion ç›¸åŒçš„ LLMå¥–åŠ±ç¼–ç å™¨çš„æè¿°ç¬¦ï¼Œå®ƒå°†ç”Ÿæˆçš„è¿åŠ¨æè¿°è½¬æ¢ä¸ºå¥–åŠ±å‡½æ•°ã€‚å¥–åŠ±å‡½æ•°ä½¿ç”¨ Python ä»£ç è¡¨ç¤ºï¼Œä»¥å—ç›ŠäºŽæ³•å­¦ç¡•å£«å…³äºŽå¥–åŠ±ã€ç¼–ç å’Œä»£ç ç»“æž„çš„çŸ¥è¯†ã€‚ &lt;/p>; &lt;p>; ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ LLM ç›´æŽ¥ç”Ÿæˆå¥–åŠ±å‡½æ•° &lt;i>;R&lt;/i>; (&lt;i>;s&lt;/i>;, &lt;i>;t&lt;/i>;)å°†æœºå™¨äººçŠ¶æ€ &lt;i>;s&lt;/i>; å’Œæ—¶é—´ &lt;i>;t&lt;/i>; æ˜ å°„ä¸ºæ ‡é‡å¥–åŠ±å€¼ã€‚ç„¶è€Œï¼Œä»Žå¤´å¼€å§‹ç”Ÿæˆæ­£ç¡®çš„å¥–åŠ±å‡½æ•°å¯¹äºŽæ³•å­¦ç¡•å£«æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œçº æ­£é”™è¯¯éœ€è¦ç”¨æˆ·ç†è§£ç”Ÿæˆçš„ä»£ç ä»¥æä¾›æ­£ç¡®çš„åé¦ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¢„å…ˆå®šä¹‰äº†ä¸€ç»„å¸¸ç”¨äºŽæ„Ÿå…´è¶£çš„æœºå™¨äººçš„å¥–åŠ±é¡¹ï¼Œå¹¶å…è®¸æ³•å­¦ç¡•å£«ç»„åˆä¸åŒçš„å¥–åŠ±é¡¹æ¥åˆ¶å®šæœ€ç»ˆçš„å¥–åŠ±å‡½æ•°ã€‚ä¸ºäº†å®žçŽ°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ª&lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_reward_coder.txt&quot;>;æç¤º&lt;/a>;æ¥æŒ‡å®šå¥–åŠ±æ¡æ¬¾å¹¶æŒ‡å¯¼LLMä¸ºä»»åŠ¡ç”Ÿæˆæ­£ç¡®çš„å¥–åŠ±å‡½æ•°ã€‚&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; &lt;/p>;&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellpacing=&quot;0&quot;class=&quot;tr-caption -container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JX GD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s1293/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= â€œ556â€æ•°æ®åŽŸå§‹å®½åº¦=â€œ1293â€src=â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXU od3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s16000/image4.jpg&quot;/>;&lt;/a>;&lt;/td >;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;å¥–åŠ±ç¿»è¯‘å™¨çš„å†…éƒ¨ç»“æž„ï¼Œå…¶ä»»åŠ¡æ˜¯å°†ç”¨æˆ·è¾“å…¥æ˜ å°„åˆ°å¥–åŠ±å‡½æ•°ã€‚&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è¿åŠ¨æŽ§åˆ¶å™¨ï¼šå°†å¥–åŠ±åŠŸèƒ½è½¬æ¢ä¸ºæœºå™¨äººåŠ¨ä½œ&lt;/h2>; &lt;p>; è¿åŠ¨æŽ§åˆ¶å™¨é‡‡ç”¨å¥–åŠ±è½¬æ¢å™¨ç”Ÿæˆçš„å¥–åŠ±å‡½æ•°ï¼Œå¹¶åˆæˆä¸€ä¸ªæŽ§åˆ¶å™¨ï¼Œå°†æœºå™¨äººè§‚å¯Ÿæ˜ å°„åˆ°ä½Žçº§æœºå™¨äººåŠ¨ä½œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æŽ§åˆ¶å™¨ç»¼åˆé—®é¢˜è¡¨è¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒçš„ç­–ç•¥æ¥è§£å†³ï¼ŒåŒ…æ‹¬å¼ºåŒ–å­¦ä¹ ã€&lt;a href=&quot;https://en.wikipedia.org/wiki/Trajectory_optimization&quot;>;ç¦»çº¿è½¨è¿¹ä¼˜åŒ–&lt;/a>;æˆ–&lt;a href=&quot;https://en.wikipedia.org/wiki/ Model_predictive_control&quot;>;æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶&lt;/a>; (MPC)ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºŽ &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; (MJPC) çš„å¼€æºå®žçŽ°ã€‚ &lt;/p>; &lt;p>; MJPC å±•ç¤ºäº†å¤šç§è¡Œä¸ºçš„äº¤äº’å¼åˆ›å»ºï¼Œä¾‹å¦‚è…¿è¿åŠ¨ã€æŠ“å–å’Œæ‰‹æŒ‡æ­¥æ€ï¼ŒåŒæ—¶æ”¯æŒå¤šç§è§„åˆ’ç®—æ³•ï¼Œä¾‹å¦‚ &lt;a href=&quot;https://homes.cs. Washington.edu/~todorov/papers/TodorovACC05.pdf&quot;>;è¿­ä»£çº¿æ€§-äºŒæ¬¡-é«˜æ–¯&lt;/a>; (iLQG) å’Œ&lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;é¢„æµ‹é‡‡æ ·&lt;/ä¸€ä¸ª>;ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒMJPC ä¸­çš„é¢‘ç¹é‡æ–°è§„åˆ’å¢žå¼ºäº†å…¶å¯¹ç³»ç»Ÿä¸ç¡®å®šæ€§çš„é²æ£’æ€§ï¼Œå¹¶åœ¨ä¸Ž LLM ç»“åˆæ—¶å®žçŽ°äº†äº¤äº’å¼è¿åŠ¨åˆæˆå’Œæ ¡æ­£ç³»ç»Ÿã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç¤ºä¾‹&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;æœºå™¨ç‹—&lt;/h3>; &lt;p>; åœ¨ç¬¬ä¸€ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†è¯­è¨€å¥–åŠ±ç³»ç»Ÿåº”ç”¨äºŽæ¨¡æ‹Ÿå››è¶³æœºå™¨äººï¼Œå¹¶æ•™å®ƒæ‰§è¡Œå„ç§æŠ€èƒ½ã€‚å¯¹äºŽæ¯é¡¹æŠ€èƒ½ï¼Œç”¨æˆ·å°†å‘ç³»ç»Ÿæä¾›ç®€æ´çš„æŒ‡ä»¤ï¼Œç„¶åŽç³»ç»Ÿå°†ä½¿ç”¨å¥–åŠ±å‡½æ•°ä½œä¸ºä¸­é—´æŽ¥å£æ¥åˆæˆæœºå™¨äººè¿åŠ¨ã€‚ &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_quadruped_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;çµå·§çš„æ“çºµå™¨&lt;/h3>; &lt;p>;ç„¶åŽæˆ‘ä»¬å°†è¯­è¨€åº”ç”¨åˆ°-ä¸ºçµå·§çš„æ“çºµæœºå™¨äººæ‰§è¡Œå„ç§æ“çºµä»»åŠ¡æä¾›å¥–åŠ±ç³»ç»Ÿã€‚çµå·§çš„æœºæ¢°è‡‚æœ‰27ä¸ªè‡ªç”±åº¦ï¼ŒæŽ§åˆ¶èµ·æ¥éžå¸¸æœ‰æŒ‘æˆ˜æ€§ã€‚å…¶ä¸­è®¸å¤šä»»åŠ¡éœ€è¦è¶…å‡ºæŽŒæ¡èƒ½åŠ›çš„æ“ä½œæŠ€èƒ½ï¼Œè¿™ä½¿å¾—é¢„å…ˆè®¾è®¡çš„åŸºå…ƒå¾ˆéš¾å·¥ä½œã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªç¤ºä¾‹ï¼Œç”¨æˆ·å¯ä»¥äº¤äº’åœ°æŒ‡ç¤ºæœºå™¨äººå°†è‹¹æžœæ”¾å…¥æŠ½å±‰å†…ã€‚ &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_manipulation_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;åœ¨çœŸå®žæœºå™¨äººä¸Šè¿›è¡ŒéªŒè¯&lt;/h2>; &lt;p>;æˆ‘ä»¬è¿˜éªŒè¯äº†è¯­è¨€ - to-reward æ–¹æ³•ä½¿ç”¨çŽ°å®žä¸–ç•Œçš„æ“çºµæœºå™¨äººæ¥æ‰§è¡Œè¯¸å¦‚æ‹¾å–ç‰©ä½“å’Œæ‰“å¼€æŠ½å±‰ç­‰ä»»åŠ¡ã€‚ä¸ºäº†åœ¨è¿åŠ¨æŽ§åˆ¶å™¨ä¸­æ‰§è¡Œä¼˜åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºå‡†æ ‡è®°ç³»ç»Ÿ &lt;a href=&quot;https://en.wikipedia.org/wiki/ARTag&quot;>;AprilTag&lt;/a>; å’Œ &lt;a href=&quot;https:/ /ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot;>;F-VLM&lt;/a>;ï¼Œä¸€ç§å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å·¥å…·ï¼Œç”¨äºŽè¯†åˆ«è¡¨æ ¼çš„ä½ç½®å’Œè¢«æ“çºµçš„ç‰©ä½“ã€‚ &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;yes&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/real/l2r_demo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®º&lt;/h2>; &lt;p>; åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ç§æ–°çš„èŒƒä¾‹ç”¨äºŽé€šè¿‡å¥–åŠ±å‡½æ•°å°†æ³•å­¦ç¡•å£«ä¸Žæœºå™¨äººè¿žæŽ¥èµ·æ¥ï¼Œç”±ä½Žçº§æ¨¡åž‹é¢„æµ‹æŽ§åˆ¶å·¥å…· MuJoCo MPC æä¾›æ”¯æŒã€‚ä½¿ç”¨å¥–åŠ±å‡½æ•°ä½œä¸ºæŽ¥å£ä½¿ LLM èƒ½å¤Ÿåœ¨è¯­ä¹‰ä¸°å¯Œçš„ç©ºé—´ä¸­å·¥ä½œï¼Œå‘æŒ¥ LLM çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ç¡®ä¿æœ€ç»ˆæŽ§åˆ¶å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ç³»ç»Ÿçš„æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ç»“æž„åŒ–è¿åŠ¨æè¿°æ¨¡æ¿æ¥æ›´å¥½åœ°ä»Žæ³•å­¦ç¡•å£«ä¸­æå–æœ‰å…³æœºå™¨äººè¿åŠ¨çš„å†…éƒ¨çŸ¥è¯†ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ¨¡æ‹Ÿæœºå™¨äººå¹³å°å’Œä¸€ä¸ªçœŸå®žæœºå™¨äººä¸Šæ¼”ç¤ºäº†æˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿï¼Œç”¨äºŽè¿åŠ¨å’Œæ“çºµä»»åŠ¡ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;æˆ‘ä»¬è¦æ„Ÿè°¢æˆ‘ä»¬çš„åˆè‘—è€… Nimrod Gileadiã€Chuyuan Fuã€Sean Kirmaniã€Kuang-Huei Leeã€Montse Gonzalez Arenasã€Hao-Tien Lewis Jiangã€Tom Erezã€Leonard Hasencleverã€Brian Ichterã€Ted Shawã€Peng Xuã€Andy Zengã€Tingnan Zhangã€Nicolas Heessã€Dorsa Sadighã€æ„Ÿè°¢ Jie Tan å’Œ Yuval Tassa åœ¨è¯¥é¡¹ç›®å„ä¸ªæ–¹é¢çš„å¸®åŠ©å’Œæ”¯æŒã€‚æˆ‘ä»¬è¿˜è¦æ„Ÿè°¢ Ken Caluwaertsã€Kristian Hartikainenã€Steven Bohezã€Carolina Paradaã€Marc Toussaint ä»¥åŠ Google DeepMind å›¢é˜Ÿçš„åé¦ˆå’Œè´¡çŒ®ã€‚&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/em>;å†…å®¹>;&lt;link href=&quot;http://blog.research.google/feeds/6641308922483151364/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 æ¡è¯„è®º&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google /2023/08/language-to-rewards-for-robotic-skill.html&quot; rel=&quot;alternate&quot; title=&quot;æœºå™¨äººæŠ€èƒ½åˆæˆå¥–åŠ±è¯­è¨€&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>; Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot; http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt; /author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G _NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s72-c/Language%20to%20reward.jpg â€œ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;æ ‡ç­¾ï¼šblogger.comï¼Œ1999ï¼šblog-8474926331452026626.post-6990000750711940626&lt;/id>;&lt;å‘å¸ƒ>;2023-08-21T00:33:00.002-07:00&lt;/å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-21T00 :35:57.579-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Interspeech&quot;>;&lt;/ category>;&lt;title type=&quot;text&quot;>;Google åœ¨ Interspeech 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šGoogle é¡¹ç›®ç»ç† Catherine Armato&lt;/span>; &lt; img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3 OeZJVLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s1075/Interspeech2023-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; æœ¬å‘¨ï¼Œç¬¬ 24 å±Šå›½é™…è¯­éŸ³äº¤æµåä¼šå¹´ä¼š&lt;/a>;ï¼ˆINTERSPEECH 2023ï¼‰åœ¨çˆ±å°”å…°éƒ½æŸæž—ä¸¾è¡Œï¼Œä»£è¡¨ä¸–ç•Œä¸Šæœ€å¹¿æ³›çš„å£è¯­ç†è§£å’Œå¤„ç†ç ”ç©¶å’ŒæŠ€æœ¯ä¼šè®®ä¹‹ä¸€ã€‚è¯­éŸ³ç›¸å…³ç ”ç©¶é¢†åŸŸçš„ä¸“å®¶é½èšä¸€å ‚ï¼Œå‚åŠ å£å¤´æŠ¥å‘Šå’Œæµ·æŠ¥ä¼šè®®ï¼Œå¹¶åœ¨å…¨çƒèŒƒå›´å†…å»ºç«‹åˆä½œã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; æˆ‘ä»¬å¾ˆé«˜å…´æˆä¸º&lt;a href=&quot;https://interspeech2023.org/sponsorship-exhibition/sponsors/&quot;>;ç™½é‡‘èµžåŠ©å•†&lt;/ INTERSPEECH 2023 çš„ a>;ï¼Œæˆ‘ä»¬å°†åœ¨å…¶ä¸­å±•ç¤º 20 å¤šç¯‡ç ”ç©¶å‡ºç‰ˆç‰©å¹¶æ”¯æŒè®¸å¤šç ”è®¨ä¼šå’Œç‰¹åˆ«ä¼šè®®ã€‚æˆ‘ä»¬æ¬¢è¿ŽçŽ°åœºä¸Žä¼šè€…èŽ…ä¸´ Google Research å±•ä½ï¼Œä¸Žæˆ‘ä»¬çš„ç ”ç©¶äººå‘˜ä¼šé¢ï¼Œå‚ä¸Žé—®ç­”å’Œæ¼”ç¤ºæˆ‘ä»¬çš„ä¸€äº›æœ€æ–°è¯­éŸ³æŠ€æœ¯ï¼Œè¿™æœ‰åŠ©äºŽæé«˜å¯è®¿é—®æ€§ï¼Œä¸ºæ•°åäº¿ç”¨æˆ·çš„æ²Ÿé€šæä¾›ä¾¿åˆ©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¼“åŠ±åœ¨çº¿ä¸Žä¼šè€…è®¿é—®æˆ‘ä»¬&lt;a href=&quot;http://topia.io/google-research&quot;>;Topia è™šæ‹Ÿå±•ä½&lt;/a>;ï¼Œåœ¨è¿™é‡Œæ‚¨å¯ä»¥èŽ·å¾—æœ‰å…³ç ”ç©¶å’Œæœºä¼šçš„æœ€æ–°ä¿¡æ¯åœ¨è°·æ­Œã€‚è®¿é—® &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter å¸æˆ·ï¼Œäº†è§£ Google å±•ä½æ´»åŠ¨ï¼ˆä¾‹å¦‚æ¼”ç¤ºå’Œé—®ç­”çŽ¯èŠ‚ï¼‰ã€‚æ‚¨è¿˜å¯ä»¥åœ¨ä¸‹é¢è¯¦ç»†äº†è§£ INTERSPEECH 2023 ä¸Šå±•ç¤ºçš„ Google ç ”ç©¶ï¼ˆGoogle éš¶å±žå…³ç³»ä»¥&lt;strong>;ç²—ä½“&lt;/strong>;æ˜¾ç¤ºï¼‰ã€‚ &lt;/p>; &lt;br />; &lt;h2>;è‘£äº‹ä¼šå’Œç»„å§”ä¼š&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; ISCA è‘£äº‹ä¼šã€æŠ€æœ¯å§”å‘˜ä¼šä¸»å¸­ï¼š&lt;strong>;&lt;em>;Bhuvana Ramabhadran &lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; åŒºåŸŸä¸»å¸­åŒ…æ‹¬ï¼š&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;è¯­éŸ³å’ŒéŸ³é¢‘ä¿¡å·åˆ†æžï¼š&lt;strong>;&lt;em>;Richard Rose&lt;/em>; &lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;è¯­éŸ³åˆæˆå’Œå£è¯­ç”Ÿæˆï¼š&lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp ;ç‰¹æ®ŠåŒºåŸŸï¼š&lt;strong>;&lt;em>;Tara Sainath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;å«æ˜Ÿäº‹ä»¶&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/competition2023.html&quot;>;2023 å¹´ VoxCeleb æ¼”è®²è€…è¯†åˆ«æŒ‘æˆ˜èµ›&lt;/a>; (VoxSRC-23)&lt; br />; ä¸»åŠžæ–¹åŒ…æ‹¬ï¼š&lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ssw2023.org/&quot;>;ISCA è¯­éŸ³åˆæˆç ”è®¨ä¼š&lt;/ a>; (SSW12)&lt;br />; æ¼”è®²è€…åŒ…æ‹¬ï¼š&lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;ä¸»é¢˜æ¼”è®² â€“ ISCA å¥–ç« èŽ·å¾—è€…&lt;/ h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/narayanan23_interspeech.pdf&quot;>;æ¡¥æ¢è¯­éŸ³ç§‘å­¦å’ŒæŠ€æœ¯ â€” çŽ°åœ¨ä¸Žæœªæ¥&lt;/a>; &lt;br />; æ¼”è®²è€…ï¼š&lt;strong>;&lt;em>;Shrikanth Narayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;è°ƒæŸ¥è°ˆè¯&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; AIæ—¶ä»£çš„è¯­éŸ³åŽ‹ç¼©&lt;br />;æ¼”è®²è€…ï¼š&lt;strong>;&lt;em>;Jan Skoglund&lt;/em>;&lt;br />;&lt;/ strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;ç‰¹åˆ«ä¼šè®®æ–‡ä»¶&lt;â€‹â€‹/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www .isca-speech.org/archive/pdfs/interspeech_2023/rose23_interspeech.pdf&quot;>;ç”¨äºŽåœ¨é‡å è¯­éŸ³ä¸Šå¾®è°ƒ ASR æ¨¡åž‹çš„çº§è”ç¼–ç å™¨&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Richard Rose&lt;/b>;ï¼Œ &lt;b>;å¥¥æ–¯å¡Â·å¼ &lt;/b>;ï¼Œ&lt;b>;å¥¥åˆ©ç»´å°”Â·è¥¿å¥¥æ±‰&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs /interspeech_2023/erdogan23_interspeech.pdf&quot;>;TokenSplitï¼šä½¿ç”¨ç¦»æ•£è¯­éŸ³è¡¨ç¤ºè¿›è¡Œç›´æŽ¥ã€ç²¾ç‚¼å’Œè½¬å½•æ¡ä»¶è¯­éŸ³åˆ†ç¦»å’Œè¯†åˆ«&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Hakan Erdogan&lt;/b>;ï¼Œ&lt;b >;Scott Wisdom&lt;/b>;ã€Xuankai Chang*ã€&lt;b>;ZalÃ¡n Borsos&lt;/b>;ã€&lt;b>;Marco Tagliasacchi&lt;/b>;ã€&lt;b>;Neil Zeghidour&lt;/b>;ã€&lt;b>;John R. Hershey&lt; /b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;è®ºæ–‡&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href =&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/liang23d_interspeech.pdf&quot;>;DeePMOSï¼šæ·±åº¦åŽéªŒå¹³å‡æ„è§-è¨€è¯­å¾—åˆ†&lt;/a>;&lt;br />; &lt;em>;æ¢æ–°å®‡ï¼Œ Fredrik Cumlinã€&lt;strong>; Christian SchÃ¼ldt&lt;/strong>;ã€Saikat Chatterjee&lt;strong>;&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca- peech.org/archive/pdfs/interspeech_2023/baskar23_interspeech.pdf&quot;>;O-1ï¼šä½¿ç”¨ Oracle è¿›è¡Œè‡ªæˆ‘è®­ç»ƒå’Œ 1-æœ€ä½³å‡è®¾&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Murali Karthick Baskar&lt;/b>; ã€&lt;b>;å®‰å¾·é²Â·ç½—æ£®ä¼¯æ ¼&lt;/b>;ã€&lt;b>;å¸ƒç“¦çº³Â·æ‹‰é©¬å·´å¾·å…°&lt;/b>;ã€&lt;b>;å¡è’‚å…‹Â·å¥¥å¾·å¡è¥¿&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot; https://www.isca-speech.org/archive/pdfs/interspeech_2023/huo23b_interspeech.pdf&quot;>;åˆ©ç”¨ç‰¹å¾èžåˆæ–¹æ³•é‡æ–°ç ”ç©¶è¯­éŸ³åŸºç¡€æ¨¡åž‹çš„é«˜æ•ˆè¿ç§»å­¦ä¹ &lt;/a>;&lt;br />; &lt;em>;&lt; b>;Zhouyuan Huor&lt;/b>;ã€&lt;b>;Khe Chai Sim&lt;/b>;ã€&lt;b>;Dongseong Hwang&lt;/b>;ã€&lt;b>;Tsendsuren Munkhdalai&lt;/b>;ã€&lt;b>;Tara N. Sainath&lt;/b>; >;,&lt;b>;ä½©å¾·ç½—Â·èŽ«é›·è¯º&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/camp23_interspeech .pdf&quot;>;MOS ä¸Ž ABï¼šä½¿ç”¨é›†ç¾¤æ ‡å‡†è¯¯å·®å¯é åœ°è¯„ä¼°æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿ&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Joshua Camp&lt;/b>;ã€&lt;b>;Tom Kenter&lt;/b >;ã€&lt;b>;åˆ—å¤«Â·èŠ¬å…‹å°”æ–¯å¦&lt;/b>;ã€&lt;b>;ç½—å¸ƒÂ·å…‹æ‹‰å…‹&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech .org/archive/pdfs/interspeech_2023/gong23c_interspeech.pdf&quot;>;LanSERï¼šè¯­è¨€æ¨¡åž‹æ”¯æŒçš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«&lt;/a>;&lt;br />; &lt;em>;Taesik Kongï¼Œ&lt;strong>; Josh Belanich&lt;/strong>;ï¼Œ&lt;strong>; Krishna Somandepalli&lt;/strong>;ã€&lt;strong>;Arsha Nagrani&lt;/strong>;ã€&lt;strong>;Brian Eoff&lt;/strong>;ã€&lt;strong>;Brendan Jou&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23fa_interspeech.pdf&quot;>;åŸºäºŽä¸€è‡´æ€§çš„æµåª’ä½“ ASR çš„æ¨¡å—åŒ–åŸŸé€‚åº”&lt;/a>;&lt;br />; &lt;em>;&lt; b>;æŽç§‹ä½³&lt;/b>;ã€&lt;b>;æŽæ³¢&lt;/b>;ã€&lt;b>;Dongseong Hwang&lt;/b>;ã€&lt;b>;Tara N. Sainath&lt;/b>;ã€&lt;b>;Pedro M.Mengibar&lt;/b>; b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/panchapagesan23_interspeech.pdf&quot;>;å…³äºŽè®­ç»ƒç¥žç»æ®‹ä½™å£°å­¦å›žå£°æŠ‘åˆ¶å™¨æ”¹è¿›çš„ ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Sankaran Panchapagesan&lt;/b>;ã€&lt;b>;Turaj Zakizadeh Shabestary&lt;/b>;ã€&lt;b>;Arun Narayanan&lt;/b>;&lt;br />;&lt;/em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/eisenstein23_interspeech.pdf&quot;>;MD3ï¼šå¯¹è¯çš„å¤šæ–¹è¨€æ•°æ®é›†&lt;/a>;&lt; br />; &lt;em>;&lt;b>;é›…å„å¸ƒÂ·çˆ±æ£®æ–¯å¦&lt;/b>;ã€&lt;b>;Vinodkumar Prabhakaran&lt;/b>;ã€&lt;b>;å…‹æ‹‰æ‹‰Â·é‡Œç»´æ‹‰&lt;/b>;ã€Dorottya Demszkyã€Devyani Sharma&lt;br />;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/wu23e_interspeech.pdf&quot;>;åŒæ¨¡å¼ NAMï¼šç«¯åˆ°ç«¯çš„æœ‰æ•ˆ Top-K ä¸Šä¸‹æ–‡æ³¨å…¥ç»“æŸ ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Zelin Wu&lt;/b>;ã€&lt;b>;Tsendsuren Munkhdalai&lt;/b>;ã€&lt;b>;Pat Rondon&lt;/b>;ã€&lt;b>;Golan Pundak&lt;/b>; b>;ã€&lt;b>;Khe Chai Sim&lt;/b>;ã€&lt;b>;Christopher Li&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca -speech.org/archive/pdfs/interspeech_2023/blau23_interspeech.pdf&quot;>;ä½¿ç”¨æ–‡æœ¬æ³¨å…¥æ¥æ”¹è¿›è¯­éŸ³ä¸­ä¸ªäººæ ‡è¯†ç¬¦çš„è¯†åˆ«&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yochai Blau&lt;/b>;ï¼Œ&lt;b >;Rohan Agrawal&lt;/b>;ã€&lt;b>;Lior Madmony&lt;/b>;ã€&lt;b>;Gary Wang&lt;/b>;ã€&lt;b>;Andrew Rosenberg&lt;/b>;ã€&lt;b>;é™ˆå“²æ€€&lt;/b>;ã€&lt;b >;ä½é‡Œå…‹Â·æ ¼èµ«æ›¼&lt;/b>;ã€&lt;b>;æ ¼çº³è¿ªÂ·åˆ«å»–å…¹é‡‘&lt;/b>;ã€&lt;b>;å¸•é‡Œè¨Â·å“ˆåŠ å°¼&lt;/b>;ã€&lt;b>;å¸ƒç“¦çº³Â·æ‹‰çŽ›å·´å¾·å…°&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23j_interspeech.pdf&quot;>;å¦‚ä½•ä¼°è®¡é¢„è®­ç»ƒè¯­éŸ³æ¨¡åž‹çš„æ¨¡åž‹å¯è¿ç§»æ€§ï¼Ÿ&lt;/a>;&lt;br />; &lt;em>;Zih-Ching Chenã€Chao-Han Huck Yang*ã€&lt;strong>;æŽæ³¢&lt;/strong>;ã€&lt;strong>;å¼ å®‡&lt;/strong>;ã€&lt;strong>;é™ˆå—æ–°&lt;/strong>;ã€&lt;strong>;Shuo- yiin Chang&lt;/strong>;ã€&lt;strong>;Rohit Prabhavalkarã€&lt;/strong>;æŽé¸¿æ¯…ã€&lt;strong>;Tara N. Sainath&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/peyser23_interspeech.pdf&quot;>;åœ¨ä¸å¯¹é½çš„æƒ…å†µä¸‹æ”¹è¿›è”åˆè¯­éŸ³æ–‡æœ¬è¡¨ç¤º&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Cal Peyser &lt;/b>;ã€&lt;b>;é’ŸçŒ›&lt;/b>;ã€&lt;b>;èƒ¡å¯&lt;/b>;ã€&lt;b>;Rohit Prabhavalkar&lt;/b>;ã€&lt;b>;å®‰å¾·é²Â·ç½—æ£®ä¼¯æ ¼&lt;/b>;ã€&lt;b>;Tara N .Sainath&lt;/b>;ã€Michael Pichenyã€Kyunghyun Cho &lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/ bijwadia23_interspeech.pdf&quot;>;è¯­éŸ³æ¨¡åž‹ä¸­å¤§å†™å’Œè½®æµé¢„æµ‹çš„æ–‡æœ¬æ³¨å…¥&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shaan Bijwadia&lt;/b>;ï¼Œ&lt;b>;Shuo-yiin Chang&lt;/b>;ï¼Œ &lt;b>;çŽ‹è”šç„¶&lt;/b>;ã€&lt;b>;å­Ÿå¿ &lt;/b>;ã€&lt;b>;å¼ æµ©&lt;/b>;ã€&lt;b>;Tara N. Sainath&lt;/b>;&lt;br />;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23_interspeech.pdf&quot;>;ç”¨äºŽè®¾å¤‡ä¸Šè¯­éŸ³åˆ°è¯­éŸ³è½¬æ¢çš„æµå¼ Parrotron&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;ã€&lt;b>;Fadi Biadsy&lt;/b>;ã€&lt;b>;å¼ éœž&lt;/b>;ã€&lt;b>;è’‹ç«‹é˜³&lt;/b>;ã€&lt;b>;å‡¤å‡°è‰åœ°é¹¨&lt;/b>;ã€&lt;b>;Shivani Agrawal&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/ pdfs/interspeech_2023/huang23b_interspeech.pdf&quot;>;ä½¿ç”¨åŒå‘è¯­è¨€æ¨¡åž‹çš„è¯­ä¹‰åˆ†å‰²æ”¹è¿›äº†é•¿æ ¼å¼ ASR&lt;/a>;&lt;br />; &lt;strong>;W. Ronny Huang&lt;/strong>;ã€&lt;strong>;å¼ æµ©&lt;/strong>;ã€&lt;strong>;Shankar Kumar&lt;/strong>;ã€&lt;strong>;å¼ ç¡•ä»ª&lt;/strong>;ã€&lt;strong>;Tara N. Sainath&lt;br />; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/taguchi23_interspeech.pdf&quot;>;é€šç”¨è‡ªåŠ¨è¯­éŸ³è½¬å½•ä¸ºå›½é™…éŸ³æ ‡&lt;/ a>;&lt;br />; &lt;em>;ç”°å£åƒå¯»ã€é…’äº•ä½‘ä»‹ã€&lt;strong>;Parisa Haghani&lt;/strong>;ã€å§œå¤§å«&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /www.isca-speech.org/archive/pdfs/interspeech_2023/hu23c_interspeech.pdf&quot;>;ç”¨äºŽæµå¼å¤šè¯­è¨€ ASR çš„æ··åˆä¸“å®¶ä¸€è‡´æ€§&lt;/a>;&lt;br />; &lt;em>;&lt;b>;èƒ¡æŸ¯&lt;/b>; , &lt;b>;æŽæ³¢&lt;/b>;,&lt;b>;Tara N. Sainath&lt;/b>;,&lt;b>;å¼ å®‡&lt;/b>;,&lt;b>;Francoise Beaufays&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23c_interspeech.pdf&quot;>;æ‰‹æœºå®žæ—¶é¢‘è°±å›¾åæ¼”&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;ã€&lt;b>;Marco Tagliasacchi&lt;/b>;ã€&lt;b>;æŽäº‘é¹&lt;/b>;ã€&lt;b>;è’‹ç«‹é˜³&lt;/b>;ã€&lt;b>;å¼ éœž&lt;/ b>;ã€&lt;b>;Fadi Biadsy&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/ rybakov23b_interspeech.pdf&quot;>;è‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„ 2 ä½ä¸€è‡´æ€§é‡åŒ–&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;ã€&lt;b>;Phoenix Meadowlark&lt;/b>;ã€&lt;b>;Shaojin Ding &lt;/b>;ã€&lt;b>;é‚±å¤§å«&lt;/b>;ã€&lt;b>;æŽå¥&lt;/b>;ã€&lt;b>;æž—å¤§å«&lt;/b>;ã€&lt;b>;ä½•å½¦ç« &lt;/b>;&lt;br />;&lt;/ em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/koizumi23_interspeech.pdf&quot;>;LibriTTS-Rï¼šæ¢å¤çš„å¤šæ‰¬å£°å™¨æ–‡æœ¬åˆ°-è¯­éŸ³è¯­æ–™åº“&lt;/a>;&lt;br />; &lt;em>;&lt;b>;å°æ³‰ä½‘é©¬&lt;/b>;ã€&lt;b>;å¹³è´ºç¦…&lt;/b>;ã€&lt;b>;æˆç”°èŒ‚æ ‘&lt;/b>;ã€&lt;b>;ä¸ä¸€å¸†&lt;/ b>;ã€çŸ¢ç”°éƒ¨èˆªå¹³ã€&lt;b>;ç››å†ˆä¼¸ä¹‹&lt;/b>;ã€&lt;b>;Michiel Bacchiani&lt;/b>;ã€&lt;b>;å¼ å®‡&lt;/b>;ã€&lt;b>;éŸ©ä¼Ÿ&lt;/b>;ã€&lt;b>;Ankur Bapna&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/yu23_interspeech.pdf&quot;>;PronScribeï¼šè¯­éŸ³å’Œæ–‡æœ¬çš„é«˜ç²¾åº¦å¤šæ¨¡æ€éŸ³ç´ è½¬å½•&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yang Yu&lt;/b>;ã€Matthew Perez*ã€&lt;b>;Ankur Bapna&lt;/b>;ã€&lt;b>;Fadi Haik&lt; /b>;ã€&lt;b>;Siamak Tazari&lt;/b>;ã€&lt;b>;å¼ å®‡&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca -speech.org/archive/pdfs/interspeech_2023/vashishth23_interspeech.pdf&quot;>;ç”¨äºŽè¯­è¨€è¯†åˆ«çš„æ ‡ç­¾æ„ŸçŸ¥è¯­éŸ³è¡¨ç¤ºå­¦ä¹ &lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shikhar Vashishth&lt;/b>;ã€&lt;b>;Shikhar Bharadwaj &lt;/b>;ã€&lt;b>;Sriram Ganapathy&lt;/b>;ã€&lt;b>;å®‰åº“å°”Â·å·´æ™®çº³&lt;/b>;ã€&lt;b>;é©¬æ•&lt;/b>;ã€&lt;b>;éŸ©ä¼Ÿ&lt;/b>;ã€&lt;b>;ç»´æ‹‰Â·é˜¿å…‹å¡žå°”ç½—å¾·&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--è„šæ³¨-->; &lt;hr width=&quot;80%&quot; />; &lt;p >; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;åœ¨ Google æœŸé—´å®Œæˆçš„å·¥ä½œ&lt;/span>;&lt;/p>;&lt;/content>; &lt;link href=&quot;http://blog.research.google/feeds/6990000750711940626/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://blog.research.google/2023/08/google-at-interspeech-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 æ¡è¯„è®º&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/google -at-interspeech-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at Interspeech 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http:// www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005 #thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url =â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJ VLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s72-c/Interspeech2023-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http: //search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog- 8474926331452026626.post-362086873015740792&lt;/id>;&lt;å‘å¸ƒ>;2023-08-18T11:28:00.004-07:00&lt;/å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-18T11:30:51.167-07:00&lt;/æ›´æ–°>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atom/ns#â€æœ¯è¯­=â€œè®¡ç®—æœºè§†è§‰â€>;&lt;/ç±»åˆ«>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atom/ns#â€ term=&quot;æœºå™¨å­¦ä¹ &quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;å¤šæ¨¡æ€å­¦ä¹ &quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;ä½¿ç”¨å¤§è¯­è¨€æ¨¡åž‹è¿›è¡Œè‡ªä¸»è§†è§‰ä¿¡æ¯æœç´¢&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šå­¦ç”Ÿç ”ç©¶å‘˜ Ziniu Hu å’Œ Google ç ”ç©¶æ„ŸçŸ¥å›¢é˜Ÿç ”ç©¶ç§‘å­¦å®¶ Alireza Fathi &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUF zF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s2500/AVIS.png&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; åœ¨é€‚åº”å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM) ä»¥é€‚åº”ä»»åŠ¡çš„å¤šæ¨¡å¼è¾“å…¥æ–¹é¢å·²ç»å–å¾—äº†å·¨å¤§è¿›å±•ï¼ŒåŒ…æ‹¬ &lt;a href=&quot;https://huggingface.co/docs/transformers/main/tasks/image_captioning#:~ :text=Image%20captioning%20is%20the%20task,by%20describing%20images%20to%20themã€‚&quot;>;å›¾åƒå­—å¹•&lt;/a>;ï¼Œ&lt;a href=&quot;https://huggingface.co/tasks/visual-question -answering&quot;>;è§†è§‰é—®ç­” (VQA)&lt;/a>; å’Œ&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;å¼€æ”¾è¯æ±‡è¯†åˆ«&lt;/a>;ã€‚å°½ç®¡å–å¾—äº†è¿™äº›æˆå°±ï¼Œå½“å‰æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰åœ¨è§†è§‰ä¿¡æ¯æœç´¢æ•°æ®é›†ä¸Šçš„è¡¨çŽ°è¿˜ä¸å¤Ÿï¼Œä¾‹å¦‚ &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a >; å’Œ &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>;ï¼Œéœ€è¦å¤–éƒ¨çŸ¥è¯†æ¥å›žç­”é—®é¢˜ã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe -vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R 9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-å®½åº¦=â€œ1999â€src=â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpm MLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;éœ€è¦å¤–éƒ¨çŸ¥è¯†æ¥å›žç­”é—®é¢˜çš„è§†è§‰ä¿¡æ¯æœç´¢æŸ¥è¯¢ç¤ºä¾‹ã€‚å›¾åƒå–è‡ª OK-VQA æ•°æ®é›†ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://arxiv.org/abs//2306.08129&quot;>; AVISï¼šä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹è¿›è¡Œè‡ªä¸»è§†è§‰ä¿¡æ¯æœç´¢&lt;/a>;â€ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æžœã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ³•å­¦ç¡•å£«ä¸Žä¸‰ç§ç±»åž‹çš„å·¥å…·é›†æˆï¼šï¼ˆiï¼‰ç”¨äºŽä»Žå›¾åƒä¸­æå–è§†è§‰ä¿¡æ¯çš„è®¡ç®—æœºè§†è§‰å·¥å…·ï¼Œï¼ˆiiï¼‰ç”¨äºŽæ£€ç´¢å¼€æ”¾ä¸–ç•ŒçŸ¥è¯†å’Œäº‹å®žçš„ç½‘ç»œæœç´¢å·¥å…·ï¼Œä»¥åŠï¼ˆiiiï¼‰ç”¨äºŽæ”¶é›†ç›¸å…³ä¿¡æ¯çš„å›¾åƒæœç´¢å·¥å…·æ¥è‡ªä¸Žè§†è§‰ä¸Šç›¸ä¼¼çš„å›¾åƒç›¸å…³çš„å…ƒæ•°æ®ã€‚ AVIS é‡‡ç”¨æ³•å­¦ç¡•å£«æ”¯æŒçš„è§„åˆ’å™¨åœ¨æ¯ä¸ªæ­¥éª¤ä¸­é€‰æ‹©å·¥å…·å’ŒæŸ¥è¯¢ã€‚å®ƒè¿˜ä½¿ç”¨ LLM æ”¯æŒçš„æŽ¨ç†æœºæ¥åˆ†æžå·¥å…·è¾“å‡ºå¹¶æå–å…³é”®ä¿¡æ¯ã€‚å·¥ä½œè®°å¿†ç»„ä»¶åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ä¿ç•™ä¿¡æ¯ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoSâ€‹â€‹FcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s1999/image6.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1758&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3i Fnyn7oy8bB81TCbh_D-I3unIwgxswoSâ€‹â€‹FcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; æ ·å¼=&quot;text-align: center;&quot;>;AVIS ç”Ÿæˆçš„å·¥ä½œæµç¨‹ç¤ºä¾‹ï¼Œç”¨äºŽå›žç­”å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰ä¿¡æ¯æœç´¢é—®é¢˜ã€‚è¾“å…¥å›¾åƒå–è‡ª Infoseek æ•°æ®é›†ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2 >;ä¸Žä¹‹å‰çš„å·¥ä½œæ¯”è¾ƒ&lt;/h2>; &lt;p>;æœ€è¿‘çš„ç ”ç©¶ï¼ˆä¾‹å¦‚ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/2304.09842&quot;>;Chameleon&lt;/a>;ã€&lt;a href=&quot;https:// viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>; å’Œ &lt;a href=&quot;https://multimodal-react.github.io/&quot;>;MM-ReAct&lt;/a>;ï¼‰æŽ¢ç´¢å‘æ³•å­¦ç¡•å£«æ·»åŠ å·¥å…·ä»¥å®žçŽ°å¤šæ¨¡å¼è¾“å…¥ã€‚è¿™äº›ç³»ç»Ÿéµå¾ªä¸¤ä¸ªé˜¶æ®µçš„è¿‡ç¨‹ï¼šè§„åˆ’ï¼ˆå°†é—®é¢˜åˆ†è§£ä¸ºç»“æž„åŒ–ç¨‹åºæˆ–æŒ‡ä»¤ï¼‰å’Œæ‰§è¡Œï¼ˆä½¿ç”¨å·¥å…·æ”¶é›†ä¿¡æ¯ï¼‰ã€‚å°½ç®¡åœ¨åŸºæœ¬ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å¤æ‚çš„çŽ°å®žåœºæ™¯ä¸­å¸¸å¸¸ä¼šå‡ºçŽ°é—®é¢˜ã€‚ &lt;/p>; &lt;p>; äººä»¬å¯¹åº”ç”¨æ³•å­¦ç¡•å£«ä½œä¸ºè‡ªä¸»ä»£ç†çš„å…´è¶£ä¹Ÿæ¿€å¢žï¼ˆä¾‹å¦‚ï¼Œ&lt;a href=&quot;https://openai.com/research/webgpt&quot;>;WebGPT&lt;/a>; å’Œ &lt;a href=&quot;https://react-lm.github.io/&quot;>;ReAct&lt;/a>;ï¼‰ã€‚è¿™äº›ä»£ç†ä¸ŽçŽ¯å¢ƒäº¤äº’ï¼Œæ ¹æ®å®žæ—¶åé¦ˆè¿›è¡Œè°ƒæ•´å¹¶å®žçŽ°ç›®æ ‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¹¶æ²¡æœ‰é™åˆ¶æ¯ä¸ªé˜¶æ®µå¯ä»¥è°ƒç”¨çš„å·¥å…·ï¼Œå¯¼è‡´æœç´¢ç©ºé—´å·¨å¤§ã€‚å› æ­¤ï¼Œå³ä½¿æ˜¯å½“ä»Šæœ€å…ˆè¿›çš„æ³•å­¦ç¡•å£«ä¹Ÿå¯èƒ½é™·å…¥æ— é™å¾ªçŽ¯æˆ–ä¼ æ’­é”™è¯¯ã€‚ AVIS é€šè¿‡æŒ‡å¯¼æ³•å­¦ç¡•å£«ä½¿ç”¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶å—åˆ°ç”¨æˆ·ç ”ç©¶ä¸­äººç±»å†³ç­–çš„å½±å“ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;é€šè¿‡ç”¨æˆ·ç ”ç©¶ä¸º LLM å†³ç­–æä¾›ä¾æ®&lt;/h2>; &lt;p>; ä¸­çš„è®¸å¤šè§†è§‰é—®é¢˜æ•°æ®é›†ï¼Œä¾‹å¦‚ &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; å’Œ &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a >; å³ä½¿å¯¹äººç±»æ¥è¯´ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œé€šå¸¸éœ€è¦å„ç§å·¥å…·å’Œ API çš„å¸®åŠ©ã€‚ä¸‹é¢æ˜¾ç¤ºäº† OK-VQA æ•°æ®é›†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹é—®é¢˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œä»¥äº†è§£ä½¿ç”¨å¤–éƒ¨å·¥å…·æ—¶äººç±»çš„å†³ç­–ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP 8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s1999 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;889&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWx b9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œä»¥äº†è§£äººç±»åœ¨ä½¿ç”¨å¤–éƒ¨å·¥å…·æ—¶çš„å†³ç­–ã€‚å›¾åƒå–è‡ª OK-VQA æ•°æ®é›†ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; ç”¨æˆ·é…å¤‡äº†ä¸Žæˆ‘ä»¬çš„æ–¹æ³•ç›¸åŒçš„å·¥å…·é›†ï¼ŒåŒ…æ‹¬ &lt;a href=&quot;https ://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PALI&lt;/a>;ã€&lt;a href=&quot;https://ai.googleblog.com/2022 /04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; å’Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Search_engine&quot;>;ç½‘ç»œæœç´¢&lt;/a >;ã€‚ä»–ä»¬æŽ¥æ”¶è¾“å…¥å›¾åƒã€é—®é¢˜ã€æ£€æµ‹åˆ°çš„å¯¹è±¡ä½œç‰©ä»¥åŠé“¾æŽ¥åˆ°å›¾åƒæœç´¢ç»“æžœçš„æŒ‰é’®ã€‚è¿™äº›æŒ‰é’®æä¾›äº†æœ‰å…³æ£€æµ‹åˆ°çš„å¯¹è±¡ä½œç‰©çš„å„ç§ä¿¡æ¯ï¼Œä¾‹å¦‚çŸ¥è¯†å›¾å®žä½“ã€ç›¸ä¼¼çš„å›¾åƒæ ‡é¢˜ã€ç›¸å…³äº§å“æ ‡é¢˜å’Œç›¸åŒçš„å›¾åƒæ ‡é¢˜ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬è®°å½•ç”¨æˆ·æ“ä½œå’Œè¾“å‡ºï¼Œå¹¶é€šè¿‡ä¸¤ç§å…³é”®æ–¹å¼å°†å…¶ç”¨ä½œæˆ‘ä»¬ç³»ç»Ÿçš„æŒ‡å—ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æžç”¨æˆ·åšå‡ºçš„å†³ç­–é¡ºåºæž„å»ºä¸€ä¸ªè½¬æ¢å›¾ï¼ˆå¦‚ä¸‹æ‰€ç¤ºï¼‰ã€‚è¯¥å›¾å®šä¹‰äº†ä¸åŒçš„çŠ¶æ€å¹¶é™åˆ¶æ¯ä¸ªçŠ¶æ€ä¸‹å¯ç”¨çš„æ“ä½œé›†ã€‚ä¾‹å¦‚ï¼Œåœ¨å¯åŠ¨çŠ¶æ€ï¼Œç³»ç»Ÿåªèƒ½é‡‡å–ä»¥ä¸‹ä¸‰ç§æ“ä½œä¹‹ä¸€ï¼šPALI å­—å¹•ã€PALI VQA æˆ–å¯¹è±¡æ£€æµ‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨äººç±»å†³ç­–çš„ä¾‹å­æ¥æŒ‡å¯¼æˆ‘ä»¬çš„è§„åˆ’è€…å’ŒæŽ¨ç†è€…ä¸Žç›¸å…³çš„ä¸Šä¸‹æ–‡å®žä¾‹ï¼Œä»¥æé«˜æˆ‘ä»¬ç³»ç»Ÿçš„æ€§èƒ½å’Œæœ‰æ•ˆæ€§ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-I CB-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/s1146/image7.png â€œ style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;845&quot; height=&quot;640&quot; src= â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr 1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/w472-h640/image7.pngâ€œå®½åº¦=â€œ472â€/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS è½¬æ¢å›¾ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot; line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;æ€»ä½“æ¡†æž¶&lt;/h2>; &lt;p>;æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åŠ¨æ€å†³ç­–ç­–ç•¥ï¼Œæ—¨åœ¨å“åº”è§†è§‰ä¿¡æ¯å¯»æ±‚æŸ¥è¯¢ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå…·æœ‰ä¸‰ä¸ªä¸»è¦ç»„ä»¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè§„åˆ’å™¨æ¥ç¡®å®šåŽç»­æ“ä½œï¼ŒåŒ…æ‹¬é€‚å½“çš„ API è°ƒç”¨åŠå…¶éœ€è¦å¤„ç†çš„æŸ¥è¯¢ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª&lt;em>;å·¥ä½œå†…å­˜&lt;/em>;ï¼Œå®ƒä¿ç•™æœ‰å…³ä»Ž API æ‰§è¡ŒèŽ·å¾—çš„ç»“æžœçš„ä¿¡æ¯ã€‚æœ€åŽï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæŽ¨ç†å™¨ï¼Œå…¶ä½œç”¨æ˜¯å¤„ç† API è°ƒç”¨çš„è¾“å‡ºã€‚å®ƒç¡®å®šæ‰€èŽ·å¾—çš„ä¿¡æ¯æ˜¯å¦è¶³ä»¥äº§ç”Ÿæœ€ç»ˆå“åº”ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦é¢å¤–çš„æ•°æ®æ£€ç´¢ã€‚ &lt;/p>; &lt;p>; æ¯æ¬¡éœ€è¦å†³å®šä½¿ç”¨å“ªç§å·¥å…·ä»¥åŠå‘å…¶å‘é€ä»€ä¹ˆæŸ¥è¯¢æ—¶ï¼Œè§„åˆ’å™¨éƒ½ä¼šæ‰§è¡Œä¸€ç³»åˆ—æ­¥éª¤ã€‚æ ¹æ®å½“å‰çŠ¶æ€ï¼Œè§„åˆ’å™¨æä¾›ä¸€ç³»åˆ—æ½œåœ¨çš„åŽç»­è¡ŒåŠ¨ã€‚æ½œåœ¨çš„è¡ŒåŠ¨ç©ºé—´å¯èƒ½å¤ªå¤§ï¼Œä½¿å¾—æœç´¢ç©ºé—´å˜å¾—æ£˜æ‰‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè§„åˆ’å™¨å‚è€ƒè½¬æ¢å›¾æ¥æ¶ˆé™¤ä¸ç›¸å…³çš„åŠ¨ä½œã€‚è§„åˆ’å™¨è¿˜æŽ’é™¤ä¹‹å‰å·²ç»é‡‡å–å¹¶å­˜å‚¨åœ¨å·¥ä½œè®°å¿†ä¸­çš„åŠ¨ä½œã€‚ &lt;/p>; &lt;p>; æŽ¥ä¸‹æ¥ï¼Œè§„åˆ’å™¨æ”¶é›†ä¸€ç»„ç›¸å…³çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹æ˜¯æ ¹æ®äººç±»ä¹‹å‰åœ¨ç”¨æˆ·ç ”ç©¶æœŸé—´åšå‡ºçš„å†³ç­–ç»„åˆè€Œæˆçš„ã€‚é€šè¿‡è¿™äº›ç¤ºä¾‹ä»¥åŠä¿å­˜ä»Žè¿‡åŽ»çš„å·¥å…·äº¤äº’ä¸­æ”¶é›†çš„æ•°æ®çš„å·¥ä½œè®°å¿†ï¼Œè§„åˆ’è€…å¯ä»¥åˆ¶å®šæç¤ºã€‚ç„¶åŽï¼Œæç¤ºè¢«å‘é€åˆ°æ³•å­¦ç¡•å£«ï¼Œæ³•å­¦ç¡•å£«è¿”å›žç»“æž„åŒ–ç­”æ¡ˆï¼Œç¡®å®šä¸‹ä¸€ä¸ªè¦æ¿€æ´»çš„å·¥å…·ä»¥åŠè¦å‘å…¶åˆ†æ´¾çš„æŸ¥è¯¢ã€‚è¿™ç§è®¾è®¡å…è®¸åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­å¤šæ¬¡è°ƒç”¨è§„åˆ’å™¨ï¼Œä»Žè€Œä¿ƒè¿›åŠ¨æ€å†³ç­–ï¼Œé€æ¸å¯¼è‡´å›žç­”è¾“å…¥æŸ¥è¯¢ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬ä½¿ç”¨æŽ¨ç†å™¨æ¥åˆ†æžå·¥å…·æ‰§è¡Œçš„è¾“å‡ºï¼Œæå–æœ‰ç”¨çš„ä¿¡æ¯å¹¶å†³å®šå·¥å…·è¾“å‡ºå±žäºŽå“ªä¸ªç±»åˆ«ï¼šæä¾›ä¿¡æ¯ã€éžä¿¡æ¯æˆ–æœ€ç»ˆç­”æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ³•å­¦ç¡•å£«ä»¥åŠé€‚å½“çš„æç¤ºå’Œä¸Šä¸‹æ–‡ç¤ºä¾‹æ¥æ‰§è¡ŒæŽ¨ç†ã€‚å¦‚æžœæŽ¨ç†æœºæ–­å®šå®ƒå·²å‡†å¤‡å¥½æä¾›ç­”æ¡ˆï¼Œå®ƒå°†è¾“å‡ºæœ€ç»ˆå“åº”ï¼Œä»Žè€Œç»“æŸä»»åŠ¡ã€‚å¦‚æžœå®ƒç¡®å®šå·¥å…·è¾“å‡ºæ²¡æœ‰ä¿¡æ¯ï¼Œå®ƒå°†è¿”å›žç»™è§„åˆ’å™¨ä»¥æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©å¦ä¸€ä¸ªæ“ä½œã€‚å¦‚æžœå®ƒå‘çŽ°å·¥å…·è¾“å‡ºæœ‰ç”¨ï¼Œå®ƒå°†ä¿®æ”¹çŠ¶æ€å¹¶å°†æŽ§åˆ¶æƒè½¬ç§»å›žè§„åˆ’å™¨ä»¥åœ¨æ–°çŠ¶æ€ä¸‹åšå‡ºæ–°å†³ç­–ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQf WpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/s1999/image2.png â€œ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1469&quot; data-original-width=&quot;1999&quot; height= â€œ470â€src =â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSur SEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/w640-h470/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt; /td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS é‡‡ç”¨åŠ¨æ€å†³ç­–ç­–ç•¥æ¥å“åº”è§†è§‰ä¿¡æ¯å¯»æ±‚æŸ¥è¯¢ã€‚&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“æžœ&lt;/h2>; &lt;p>; æˆ‘ä»¬è¯„ä¼° AVIS &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; å’Œ &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; æ•°æ®é›†ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼Œç”šè‡³æ˜¯å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡åž‹ï¼Œä¾‹å¦‚ &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>; å’Œ &lt;a href=&quot;https://ai.googleblogã€‚ com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;ï¼Œåœ¨ Infoseek ä¸Šå¾®è°ƒæ—¶æ— æ³•äº§ç”Ÿé«˜ç²¾åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³• (AVIS) åœ¨æ²¡æœ‰å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå¯¹è¯¥æ•°æ®é›†çš„æœªè§å®žä½“åˆ†å‰²è¾¾åˆ°äº† 50.7% çš„å‡†ç¡®çŽ‡ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2E ikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s1200 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHj qYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Infoseek æ•°æ®é›†ä¸Šçš„ AVIS è§†è§‰é—®ç­”ç»“æžœã€‚ä¸Žä¹‹å‰åŸºäºŽ &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; çš„åŸºçº¿ç›¸æ¯”ï¼ŒAVIS å®žçŽ°äº†æ›´é«˜çš„å‡†ç¡®çŽ‡ã€&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; å’Œ &lt;a href=&quot;https:// arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; æˆ‘ä»¬åœ¨ OK-VQA æ•°æ®é›†ä¸Šçš„ç»“æžœå¦‚ä¸‹æ‰€ç¤ºã€‚ AVIS é€šè¿‡å°‘é‡ä¸Šä¸‹æ–‡ç¤ºä¾‹å®žçŽ°äº† 60.2% çš„å‡†ç¡®çŽ‡ï¼Œé«˜äºŽä¹‹å‰çš„å¤§å¤šæ•°ä½œå“ã€‚ä¸Žåœ¨ OK-VQA ä¸Šå¾®è°ƒçš„ PALI æ¨¡åž‹ç›¸æ¯”ï¼ŒAVIS çš„å‡†ç¡®åº¦è¾ƒä½Žï¼Œä½†ç›¸å½“ã€‚ä¸Ž Infoseek ç›¸æ¯”ï¼ŒAVIS ä¼˜äºŽå¾®è°ƒåŽçš„ PALIï¼Œè¿™ç§å·®å¼‚æ˜¯ç”±äºŽ OK-VQA ä¸­çš„å¤§å¤šæ•°é—®ç­”ç¤ºä¾‹ä¾èµ–äºŽå¸¸è¯†çŸ¥è¯†è€Œä¸æ˜¯ç»†ç²’åº¦çŸ¥è¯†ã€‚å› æ­¤ï¼ŒPaLI èƒ½å¤Ÿå°†æ­¤ç±»é€šç”¨çŸ¥è¯†ç¼–ç åœ¨æ¨¡åž‹å‚æ•°ä¸­ï¼Œå¹¶ä¸”ä¸éœ€è¦å¤–éƒ¨çŸ¥è¯†ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d 5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s1200/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a 6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s16000/image1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;A-OKVQA ä¸Šçš„è§†è§‰é—®ç­”ç»“æžœã€‚ä¸Žä¹‹å‰ä½¿ç”¨å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬å­¦ä¹ çš„ä½œå“ç›¸æ¯”ï¼ŒAVIS å®žçŽ°äº†æ›´é«˜çš„å‡†ç¡®åº¦ï¼ŒåŒ…æ‹¬ &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single- Visual-language-model&quot;>;ç«çƒˆé¸Ÿ&lt;/a>;ã€&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;å·´åˆ©è¯­&lt;/ a>; å’Œ &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>;ã€‚ AVIS è¿˜æ¯”ä¹‹å‰åœ¨ OK-VQA æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒçš„å¤§å¤šæ•°ä½œå“å®žçŽ°äº†æ›´é«˜çš„å‡†ç¡®åº¦ï¼ŒåŒ…æ‹¬ &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language- pre.html&quot;>;REVEAL&lt;/a>;ã€&lt;a href=&quot;https://arxiv.org/abs/2206.01201&quot;>;ReVIVE&lt;/a>;ã€&lt;a href=&quot;https://arxiv.org/abs/ 2112.08614&quot;>;KAT&lt;/a>; å’Œ &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>;ï¼Œå¹¶èŽ·å¾—æŽ¥è¿‘å¾®è°ƒ&lt;a href=&quot;httpsçš„ç»“æžœ://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;æ¨¡åž‹ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®º&lt;/h2>; &lt;p>; æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œä½¿æ³•å­¦ç¡•å£«èƒ½å¤Ÿä½¿ç”¨å„ç§ç”¨äºŽå›žç­”çŸ¥è¯†å¯†é›†åž‹è§†è§‰é—®é¢˜çš„å·¥å…·ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥ä»Žç”¨æˆ·ç ”ç©¶ä¸­æ”¶é›†çš„äººç±»å†³ç­–æ•°æ®ä¸ºåŸºç¡€ï¼Œé‡‡ç”¨äº†ä¸€ä¸ªç»“æž„åŒ–æ¡†æž¶ï¼Œè¯¥æ¡†æž¶ä½¿ç”¨æ³•å­¦ç¡•å£«æ”¯æŒçš„è§„åˆ’å™¨æ¥åŠ¨æ€å†³å®šå·¥å…·é€‰æ‹©å’ŒæŸ¥è¯¢å½¢æˆã€‚ç”± LLM é©±åŠ¨çš„æŽ¨ç†æœºçš„ä»»åŠ¡æ˜¯ä»Žæ‰€é€‰å·¥å…·çš„è¾“å‡ºä¸­å¤„ç†å’Œæå–å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿­ä»£åœ°ä½¿ç”¨è§„åˆ’å™¨å’ŒæŽ¨ç†å™¨æ¥åˆ©ç”¨ä¸åŒçš„å·¥å…·ï¼Œç›´åˆ°æ”¶é›†åˆ°å›žç­”è§†è§‰é—®é¢˜æ‰€éœ€çš„æ‰€æœ‰å¿…è¦ä¿¡æ¯ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;è¿™é¡¹ç ”ç©¶ç”± Ziniu Huã€Ahmet Iscen è¿›è¡Œã€å­™æ™¨ã€å¼ å‡¯ä¼Ÿã€å­™ä¸€èˆŸã€David A. Rossã€Cordelia Schmid å’Œ Alireza Fathiã€‚&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/ feeds/362086873015740792/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous -visual-information-seeking.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 æ¡è¯„è®º&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/ 8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel =&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous-visual-information-seeking.html&quot; rel=&quot;alternate&quot; æ ‡é¢˜=&quot;ä½¿ç”¨å¤§è¯­è¨€æ¨¡åž‹è¿›è¡Œè‡ªä¸»è§†è§‰ä¿¡æ¯æœç´¢&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/ uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1 .blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqY oAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s72-c/AVIS.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:æ€»è®¡>;0&lt;/thrï¼štotal>;&lt;/entry>;&lt;entry>;&lt;id>;æ ‡ç­¾ï¼šblogger.com,1999:blog-8474926331452026626.post-4779594044050650231&lt;/id>;&lt;å‘å¸ƒ>;2023-08-17T11:08:00.000 -07:00&lt;/å·²å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-17T11:08:24.346-07:00&lt;/æ›´æ–°>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atom/ns#â€term= &quot;æœºå™¨å­¦ä¹ &quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;ç¥žç»ç½‘ç»œå‰ªæžç»„åˆä¼˜åŒ–&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šAthena å›¢é˜Ÿç ”ç©¶ç§‘å­¦å®¶ Hussein Hazimeh å’Œéº»çœç†å·¥å­¦é™¢ç ”ç©¶ç”Ÿ Riade Benbaki&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDN jcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s320/CHITA%20hero.gif&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; çŽ°ä»£ç¥žç»ç½‘ç»œåœ¨å„ç§åº”ç”¨ä¸­éƒ½å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä¾‹å¦‚ &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model /&quot;>;è¯­è¨€ã€æ•°å­¦æŽ¨ç†&lt;/a>;å’Œ&lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;æ„¿æ™¯&lt;/a>; ã€‚ç„¶è€Œï¼Œè¿™äº›ç½‘ç»œé€šå¸¸ä½¿ç”¨éœ€è¦å¤§é‡è®¡ç®—èµ„æºçš„å¤§åž‹æž¶æž„ã€‚è¿™ä½¿å¾—å‘ç”¨æˆ·æä¾›æ­¤ç±»æ¨¡åž‹å˜å¾—ä¸åˆ‡å®žé™…ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯ç©¿æˆ´è®¾å¤‡å’Œæ™ºèƒ½æ‰‹æœºç­‰èµ„æºæœ‰é™çš„çŽ¯å¢ƒä¸­ã€‚é™ä½Žé¢„è®­ç»ƒç½‘ç»œçš„æŽ¨ç†æˆæœ¬çš„&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;å¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•&lt;/a>;æ˜¯é€šè¿‡åˆ é™¤ä¸€äº›ç½‘ç»œæ¥ä¿®å‰ªå®ƒä»¬å®ƒä»¬çš„æƒé‡ï¼Œä¸ä¼šæ˜¾ç€å½±å“æ•ˆç”¨ã€‚åœ¨æ ‡å‡†ç¥žç»ç½‘ç»œä¸­ï¼Œæ¯ä¸ªæƒé‡å®šä¹‰ä¸¤ä¸ªç¥žç»å…ƒä¹‹é—´çš„è¿žæŽ¥ã€‚å› æ­¤ï¼Œåœ¨æƒé‡è¢«ä¿®å‰ªåŽï¼Œè¾“å…¥å°†é€šè¿‡è¾ƒå°çš„è¿žæŽ¥é›†ä¼ æ’­ï¼Œå› æ­¤éœ€è¦è¾ƒå°‘çš„è®¡ç®—èµ„æºã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw -bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBRYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEg D-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s1600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width= â€œ1600â€src =â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FonAvjG2Hw J4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;åŽŸå§‹ç½‘ç»œä¸Žä¿®å‰ªåŽçš„ç½‘ç»œã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;å‰ªæžæ–¹æ³•å¯ä»¥åº”ç”¨äºŽç½‘ç»œè®­ç»ƒè¿‡ç¨‹çš„ä¸åŒé˜¶æ®µï¼šè®­ç»ƒåŽã€è®­ç»ƒæœŸé—´æˆ–è®­ç»ƒä¹‹å‰ï¼ˆå³æƒé‡åˆå§‹åŒ–åŽç«‹å³ï¼‰ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨è®­ç»ƒåŽè®¾ç½®ï¼šç»™å®šä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„ç½‘ç»œï¼Œæˆ‘ä»¬å¦‚ä½•ç¡®å®šåº”è¯¥ä¿®å‰ªå“ªäº›æƒé‡ï¼Ÿä¸€ç§æµè¡Œçš„æ–¹æ³•æ˜¯&lt;a href=&quot;https://jmlr.org/papers/volume22/21-0366/21-0366.pdf&quot;>;å¹…åº¦å‰ªæž&lt;/a>;ï¼Œå®ƒåˆ é™¤å¹…åº¦æœ€å°çš„æƒé‡ã€‚è™½ç„¶æœ‰æ•ˆï¼Œä½†è¯¥æ–¹æ³•æ²¡æœ‰ç›´æŽ¥è€ƒè™‘åˆ é™¤æƒé‡å¯¹ç½‘ç»œæ€§èƒ½çš„å½±å“ã€‚å¦ä¸€ä¸ªæµè¡Œçš„èŒƒä¾‹æ˜¯&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;åŸºäºŽä¼˜åŒ–çš„å‰ªæž&lt;/a>;ï¼Œå®ƒæ ¹æ®æƒé‡çš„åˆ é™¤å¯¹æŸå¤±å‡½æ•°çš„å½±å“ç¨‹åº¦æ¥åˆ é™¤æƒé‡ã€‚å°½ç®¡åœ¨æ¦‚å¿µä¸Šå¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†å¤§å¤šæ•°çŽ°æœ‰çš„åŸºäºŽä¼˜åŒ–çš„æ–¹æ³•ä¼¼ä¹Žéƒ½é¢ä¸´ç€æ€§èƒ½å’Œè®¡ç®—è¦æ±‚ä¹‹é—´çš„ä¸¥é‡æƒè¡¡ã€‚è¿›è¡Œç²—ç•¥è¿‘ä¼¼çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œå‡è®¾å¯¹è§’çº¿&lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;>;Hessian_matrix&quot;>;Hessian çŸ©é˜µ&lt;/a>;ï¼‰å¯ä»¥å¾ˆå¥½åœ°æ‰©å±•ï¼Œä½†æ€§èƒ½ç›¸å¯¹è¾ƒä½Žã€‚å¦ä¸€æ–¹é¢ï¼Œè™½ç„¶è¿›è¡Œè¾ƒå°‘è¿‘ä¼¼çš„æ–¹æ³•å¾€å¾€è¡¨çŽ°æ›´å¥½ï¼Œä½†å®ƒä»¬çš„å¯æ‰©å±•æ€§ä¼¼ä¹Žè¦å·®å¾—å¤šã€‚ &lt;/p>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://arxiv.org/abs/2302.14623&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>;â€ä¸­ï¼Œä»‹ç»äºŽ &lt;a href=&quot; https://icml.cc/Conferences/2023/&quot;>;ICML 2023&lt;/a>;ï¼Œæˆ‘ä»¬æè¿°äº†å¦‚ä½•å¼€å‘ä¸€ç§åŸºäºŽä¼˜åŒ–çš„æ–¹æ³•æ¥å¤§è§„æ¨¡ä¿®å‰ªé¢„è®­ç»ƒçš„ç¥žç»ç½‘ç»œã€‚ CHITAï¼ˆä»£è¡¨â€œç»„åˆ Hessian è¿­ä»£é˜ˆå€¼ç®—æ³•â€ï¼‰åœ¨å¯æ‰©å±•æ€§å’Œæ€§èƒ½æƒè¡¡æ–¹é¢ä¼˜äºŽçŽ°æœ‰çš„å‰ªæžæ–¹æ³•ï¼Œå¹¶ä¸”å®ƒé€šè¿‡åˆ©ç”¨å¤šä¸ªé¢†åŸŸçš„è¿›æ­¥æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼ŒåŒ…æ‹¬ &lt;a href=&quot;https://en. wikipedia.org/wiki/High-Dimensional_statistics&quot;>;é«˜ç»´ç»Ÿè®¡&lt;/a>;ã€&lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;ç»„åˆä¼˜åŒ–&lt;/a>;å’Œç¥žç»ç½‘ç»œä¿®å‰ªã€‚ä¾‹å¦‚ï¼ŒCHITA çš„é€Ÿåº¦æ¯”æœ€å…ˆè¿›çš„ä¿®å‰ªæ–¹æ³•å¿« 20 å€åˆ° 1000 å€ &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>;ï¼Œå¹¶å°†å‡†ç¡®æ€§æé«˜ 10 ä»¥ä¸Š% åœ¨è®¸å¤šè®¾ç½®ä¸­ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è´¡çŒ®æ¦‚è¿°&lt;/h2>; &lt;p>; CHITA ç›¸å¯¹äºŽæµè¡Œæ–¹æ³•æœ‰ä¸¤é¡¹æ˜¾ç€çš„æŠ€æœ¯æ”¹è¿›ï¼š&lt; /p>; &lt;ul>; &lt;li>;&lt;strong>;äºŒé˜¶ä¿¡æ¯çš„æœ‰æ•ˆä½¿ç”¨&lt;/strong>;ï¼šä½¿ç”¨äºŒé˜¶ä¿¡æ¯çš„ä¿®å‰ªæ–¹æ³•ï¼ˆå³ï¼Œä¸Ž&lt;a href=&quot;https://en.wikipedia. org/wiki/Second_derivative&quot;>;äºŒé˜¶å¯¼æ•°&lt;/a>;ï¼‰åœ¨è®¸å¤šè®¾ç½®ä¸­å®žçŽ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œæ­¤ä¿¡æ¯é€šå¸¸é€šè¿‡è®¡ç®— Hessian çŸ©é˜µæˆ–å…¶é€†çŸ©é˜µæ¥ä½¿ç”¨ï¼Œè¿™ç§æ“ä½œå¾ˆéš¾æ‰©å±•ï¼Œå› ä¸º Hessian å¤§å°ä¸Žæƒé‡æ•°é‡æˆäºŒæ¬¡æ–¹ã€‚é€šè¿‡ä»”ç»†çš„é‡æ–°è¡¨è¿°ï¼ŒCHITA ä½¿ç”¨äºŒé˜¶ä¿¡æ¯ï¼Œè€Œæ— éœ€æ˜¾å¼è®¡ç®—æˆ–å­˜å‚¨ Hessian çŸ©é˜µï¼Œä»Žè€Œå®žçŽ°æ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚ &lt;/li>;&lt;li>;&lt;strong>;ç»„åˆä¼˜åŒ–&lt;/strong>;ï¼šæµè¡Œçš„åŸºäºŽä¼˜åŒ–çš„æ–¹æ³•ä½¿ç”¨ä¸€ç§ç®€å•çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå•ç‹¬ä¿®å‰ªæƒé‡ï¼Œå³ï¼Œåœ¨å†³å®šä¿®å‰ªæŸä¸ªæƒé‡æ—¶ï¼Œå®ƒä»¬ä¸è€ƒè™‘æ˜¯å¦å…¶ä»–æƒé‡å·²è¢«ä¿®å‰ªã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´é‡è¦æƒé‡çš„ä¿®å‰ªï¼Œå› ä¸ºå½“å…¶ä»–æƒé‡è¢«ä¿®å‰ªæ—¶ï¼Œå­¤ç«‹åœ°è¢«è®¤ä¸ºä¸é‡è¦çš„æƒé‡å¯èƒ½ä¼šå˜å¾—é‡è¦ã€‚ CHITA é€šè¿‡ä½¿ç”¨æ›´å…ˆè¿›çš„ç»„åˆä¼˜åŒ–ç®—æ³•æ¥é¿å…è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç®—æ³•è€ƒè™‘äº†ä¿®å‰ªä¸€ä¸ªæƒé‡å¦‚ä½•å½±å“å…¶ä»–æƒé‡ã€‚ &lt;/li>; &lt;/ul>; &lt;p>; åœ¨ä¸‹é¢çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬è®¨è®º CHITA çš„å‰ªæžå…¬å¼å’Œç®—æ³•ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è®¡ç®—å‹å¥½çš„å‰ªæžå…¬å¼&lt;/h2>; &lt;p>; æœ‰è®¸å¤šå¯èƒ½çš„å‰ªæžå€™é€‰è€…ï¼Œå…¶ä¸­é€šè¿‡ä»…ä¿ç•™åŽŸå§‹ç½‘ç»œæƒé‡çš„å­é›†æ¥èŽ·å¾—ã€‚ä»¤&lt;em>;k&lt;/em>;ä¸ºç”¨æˆ·æŒ‡å®šçš„å‚æ•°ï¼Œè¡¨ç¤ºè¦ä¿ç•™çš„æƒé‡æ•°é‡ã€‚å‰ªæžå¯ä»¥è‡ªç„¶åœ°è¡¨è¿°ä¸ºä¸€ä¸ª&lt;a href=&quot;https://arxiv.org/abs/1803.01454&quot;>;æœ€ä½³å­é›†é€‰æ‹©&lt;/a>;ï¼ˆBSSï¼‰é—®é¢˜ï¼šåœ¨æ‰€æœ‰å¯èƒ½çš„å‰ªæžå€™é€‰è€…ï¼ˆå³æƒé‡å­é›†ï¼‰ä¸­ä»…ä¿ç•™ &lt;em>;k&lt;/em>; ä¸ªæƒé‡ï¼Œé€‰æ‹©æŸå¤±æœ€å°çš„å€™é€‰è€…ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithv XGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s1600/image5.gif&quot; style=&quot;å·¦è¾¹è·ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY 1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;ä½œä¸º BSS é—®é¢˜çš„å‰ªæžï¼šåœ¨å…·æœ‰ç›¸åŒæƒé‡æ€»æ•°çš„æ‰€æœ‰å¯èƒ½çš„å‰ªæžå€™é€‰è€…ä¸­ï¼Œæœ€ä½³å€™é€‰è€…è¢«å®šä¹‰ä¸ºæŸå¤±æœ€å°çš„å€™é€‰è€…ã€‚è¯¥å›¾æ˜¾ç¤ºäº†å››ä¸ªå€™é€‰è€…ï¼Œä½†è¿™ä¸ªæ•°å­—é€šå¸¸è¦å¤§å¾—å¤šã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;è§£å†³åŽŸå§‹æŸå¤±å‡½æ•°ä¸Šçš„å‰ªæžBSSé—®é¢˜é€šå¸¸æ˜¯é€šè¿‡è®¡ç®—æ¥è§£å†³çš„æ£˜æ‰‹çš„ã€‚å› æ­¤ï¼Œä¸Žä¹‹å‰çš„å·¥ä½œç±»ä¼¼ï¼Œä¾‹å¦‚ &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf&quot;>;OBD&lt;/a>; å’Œ &lt;a href=&quot; https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&quot;>;OBS&lt;/a>;ï¼Œæˆ‘ä»¬ç”¨ &lt;a href=&quot; æ¥è¿‘ä¼¼æŸå¤±https://en.wikipedia.org/wiki/Quadratic_form&quot;>;äºŒæ¬¡å‡½æ•°&lt;/a>;ï¼Œä½¿ç”¨äºŒé˜¶&lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor_series&quot;>;æ³°å‹’çº§æ•°&lt; /a>;ï¼Œå…¶ä¸­ Hessian çŸ©é˜µæ˜¯æ ¹æ®ç»éªŒ &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;>;Fisher ä¿¡æ¯çŸ©é˜µ&lt;/a>; è¿›è¡Œä¼°è®¡çš„ã€‚è™½ç„¶æ¢¯åº¦é€šå¸¸å¯ä»¥æœ‰æ•ˆåœ°è®¡ç®—ï¼Œä½†ç”±äºŽå…¶åºžå¤§çš„è§„æ¨¡ï¼Œè®¡ç®—å’Œå­˜å‚¨ Hessian çŸ©é˜µçš„æˆæœ¬è¿‡é«˜ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œé€šå¸¸é€šè¿‡å¯¹ Hessian çŸ©é˜µï¼ˆä¾‹å¦‚ï¼Œå¯¹è§’çŸ©é˜µï¼‰å’Œç®—æ³•ï¼ˆä¾‹å¦‚ï¼Œå•ç‹¬å‰ªæžæƒé‡ï¼‰åšå‡ºé™åˆ¶æ€§å‡è®¾æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚ &lt;/p>; &lt;p>; CHITA ä½¿ç”¨ä¿®å‰ªé—®é¢˜çš„æœ‰æ•ˆé‡æ–°è¡¨è¿°ï¼ˆä½¿ç”¨äºŒæ¬¡æŸå¤±çš„ BSSï¼‰ï¼Œé¿å…æ˜¾å¼è®¡ç®— Hessian çŸ©é˜µï¼ŒåŒæ—¶ä»ç„¶ä½¿ç”¨è¯¥çŸ©é˜µä¸­çš„æ‰€æœ‰ä¿¡æ¯ã€‚è¿™æ˜¯é€šè¿‡åˆ©ç”¨ç»éªŒè´¹å¸Œå°”ä¿¡æ¯çŸ©é˜µçš„ä½Ž&lt;a href=&quot;https://en.wikipedia.org/wiki/Rank_(linear_algebra)&quot;>;ç§©&lt;/a>;ç»“æž„æ¥å®žçŽ°çš„ã€‚è¿™ç§é‡æ–°è¡¨è¿°å¯ä»¥è¢«è§†ä¸ºç¨€ç–&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;>;çº¿æ€§å›žå½’&lt;/a>;é—®é¢˜ï¼Œå…¶ä¸­æ¯ä¸ªå›žå½’ç³»æ•°å¯¹åº”äºŽç¥žç»ç½‘ç»œä¸­çš„ç‰¹å®šæƒé‡ã€‚èŽ·å¾—æ­¤å›žå½’é—®é¢˜çš„è§£å†³æ–¹æ¡ˆåŽï¼Œè®¾ç½®ä¸ºé›¶çš„ç³»æ•°å°†å¯¹åº”äºŽåº”ä¿®å‰ªçš„æƒé‡ã€‚æˆ‘ä»¬çš„å›žå½’æ•°æ®çŸ©é˜µæ˜¯ (&lt;em>;n&lt;/em>; x &lt;em>;p&lt;/em>;)ï¼Œå…¶ä¸­ &lt;em>;n&lt;/em>; æ˜¯æ‰¹æ¬¡ï¼ˆå­æ ·æœ¬ï¼‰å¤§å°ï¼Œ&lt;em>;p&lt;/em>; em>; æ˜¯åŽŸå§‹ç½‘ç»œä¸­çš„æƒé‡æ•°é‡ã€‚é€šå¸¸&lt;em>;n&lt;/em>; &lt;&lt; &lt;em>;p&lt;/em>;ï¼Œå› æ­¤ä½¿ç”¨æ­¤æ•°æ®çŸ©é˜µè¿›è¡Œå­˜å‚¨å’Œæ“ä½œæ¯”ä½¿ç”¨ (&lt;em>;p&lt;/em>; x &lt;em>;p&lt;/em>;) Hessian æ“ä½œçš„å¸¸è§ä¿®å‰ªæ–¹æ³•æ›´å…·å¯æ‰©å±•æ€§ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdK vBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s1601/image3.png&quot;æ ·å¼=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUf ysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;CHITA å°†äºŒæ¬¡æŸå¤±è¿‘ä¼¼é‡æ–°è¡¨è¿°ä¸ºçº¿æ€§å›žå½’ (LR) é—®é¢˜ï¼Œè¿™éœ€è¦æ˜‚è´µçš„ Hessian çŸ©é˜µã€‚ LR çš„æ•°æ®çŸ©é˜µåœ¨ &lt;em>;p&lt;/em>; ä¸­å‘ˆçº¿æ€§ï¼Œè¿™ä½¿å¾—é‡æž„æ¯”åŽŸå§‹äºŒæ¬¡è¿‘ä¼¼æ›´å…·å¯æ‰©å±•æ€§ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;å¯æ‰©å±•çš„ä¼˜åŒ–ç®—æ³•&lt;/h2>; &lt;p>; CHITA åœ¨ä»¥ä¸‹ç¨€ç–æ€§çº¦æŸä¸‹å°†å‰ªæžå‡å°‘ä¸ºçº¿æ€§å›žå½’é—®é¢˜ï¼šæœ€å¤š &lt; em>;k&lt;/em>; ä¸ªå›žå½’ç³»æ•°å¯ä»¥ä¸ä¸ºé›¶ã€‚ä¸ºäº†èŽ·å¾—æ­¤é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬è€ƒè™‘å¯¹è‘—åçš„&lt;a href=&quot;https://arxiv.org/pdf/0805.0510.pdf&quot;>;è¿­ä»£ç¡¬é˜ˆå€¼&lt;/a>; (IHT) ç®—æ³•è¿›è¡Œä¿®æ”¹ã€‚ IHT æ‰§è¡Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;>;æ¢¯åº¦ä¸‹é™&lt;/a>;ï¼Œæ¯æ¬¡æ›´æ–°åŽæ‰§è¡Œä»¥ä¸‹åŽå¤„ç†æ­¥éª¤ï¼šTop-&lt;em >;k&lt;/em>;ï¼ˆå³ï¼Œå…·æœ‰æœ€å¤§å¹…åº¦çš„&lt;em>;k&lt;/em>;ç³»æ•°ï¼‰è®¾ç½®ä¸ºé›¶ã€‚ IHT é€šå¸¸ä¼šä¸ºé—®é¢˜æä¾›è‰¯å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”å®ƒä¼šè¿­ä»£æŽ¢ç´¢ä¸åŒçš„å‰ªæžå€™é€‰å¯¹è±¡å¹¶è”åˆä¼˜åŒ–æƒé‡ã€‚ &lt;/p>; &lt;p>; ç”±äºŽé—®é¢˜çš„è§„æ¨¡ï¼Œå…·æœ‰æ’å®š&lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;>;å­¦ä¹ çŽ‡&lt;/a>;çš„æ ‡å‡† IHT å¯èƒ½ä¼šéžå¸¸æ…¢æ”¶æ•›ã€‚ä¸ºäº†æ›´å¿«åœ°æ”¶æ•›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„&lt;a href=&quot;https://en.wikipedia.org/wiki/Line_search&quot;>;çº¿æœç´¢&lt;/a>;æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é—®é¢˜ç»“æž„æ¥æ‰¾åˆ°åˆé€‚çš„å­¦ä¹ çŽ‡ï¼Œå³å¯¼è‡´æŸå¤±è¶³å¤Ÿå¤§çš„å‡å°‘ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†å¤šç§è®¡ç®—æ–¹æ¡ˆæ¥æé«˜ CHITA çš„æ•ˆçŽ‡å’ŒäºŒé˜¶è¿‘ä¼¼çš„è´¨é‡ï¼Œä»Žè€Œäº§ç”Ÿäº†ä¸€ä¸ªæ”¹è¿›çš„ç‰ˆæœ¬ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º CHITA++ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;å®žéªŒ&lt;/h2>; &lt;p>; æˆ‘ä»¬å°† CHITA çš„è¿è¡Œæ—¶é—´å’Œå‡†ç¡®æ€§ä¸Žå‡ ç§çŠ¶æ€è¿›è¡Œæ¯”è¾ƒä½¿ç”¨ä¸åŒæž¶æž„çš„æœ€å…ˆè¿›çš„ä¿®å‰ªæ–¹æ³•ï¼ŒåŒ…æ‹¬ &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; å’Œ &lt;a href=&quot;https://arxiv.org/abs/1704.04861 &quot;>;ç§»åŠ¨ç½‘ç»œ&lt;/a>;ã€‚ &lt;/p>; &lt;p>; &lt;strong>;è¿è¡Œæ—¶é—´&lt;/strong>;ï¼šCHITA æ¯”æ‰§è¡Œè”åˆä¼˜åŒ–çš„åŒç±»æ–¹æ³•æ›´å…·å¯æ‰©å±•æ€§ï¼ˆè€Œä¸æ˜¯å•ç‹¬ä¿®å‰ªæƒé‡ï¼‰ã€‚ä¾‹å¦‚ï¼ŒCHITA åœ¨å‰ªæž ResNet æ—¶çš„åŠ é€Ÿæ¯”å¯ä»¥è¾¾åˆ° 1000 å€ä»¥ä¸Šã€‚ &lt;/p>; &lt;p>; &lt;strong>;åŽå‰ªæžç²¾åº¦&lt;/strong>;ï¼š&lt;strong>; &lt;/strong>;ä¸‹é¢ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº† CHITA å’Œ CHITA++ ä¸Žå¹…åº¦å‰ªæžï¼ˆMPï¼‰çš„æ€§èƒ½ï¼Œ&lt;a href=&quot;https: //arxiv.org/abs/2004.14340&quot;>;Woodfisher&lt;/a>; (WF) å’Œ&lt;a href=&quot;https://arxiv.org/abs/2203.04466&quot;>;ç»„åˆè„‘å¤–ç§‘åŒ»ç”Ÿ&lt;/a>; (CBS)ï¼Œç”¨äºŽä¿®å‰ª 70% çš„æ¨¡åž‹æƒé‡ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çœ‹åˆ° CHITA å’Œ CHITA++ å–å¾—äº†è‰¯å¥½çš„æ”¹è¿›ã€‚ &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:è‡ªåŠ¨;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1 aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz- KwvpQVIJKDEnkg_/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlk Fi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a >;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ResNet20 ä¸Šå„ç§æ–¹æ³•çš„åŽå‰ªæžç²¾åº¦ã€‚æŠ¥å‘Šä¿®å‰ª 70% æ¨¡åž‹æƒé‡çš„ç»“æžœã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-æ ‡é¢˜å®¹å™¨&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_ AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;imgè¾¹æ¡†=â€œ0â€æ•°æ®åŽŸå§‹é«˜åº¦=â€œ742â€æ•°æ®åŽŸå§‹å®½åº¦=â€œ1200â€é«˜åº¦=â€œ396â€src=â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA -5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-thHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-CP-w2OUUDsPUNMBhGCTkSBFjV6ODFY60 K-VCFnGENDN4LtfA/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MobileNet ä¸Šå„ç§æ–¹æ³•çš„åŽå‰ªæžç²¾åº¦ã€‚æŠ¥å‘Šä¿®å‰ª 70% æ¨¡åž‹æƒé‡çš„ç»“æžœã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠ¥å‘Šä¿®å‰ªæ›´å¤§ç½‘ç»œçš„ç»“æžœï¼šResNet50ï¼ˆåœ¨æ­¤ç½‘ç»œä¸Šï¼Œä¸€äº›ResNet20 å›¾ä¸­åˆ—å‡ºçš„æ–¹æ³•æ— æ³•æ‰©å±•ï¼‰ã€‚è¿™é‡Œæˆ‘ä»¬ä¸Žå¹…åº¦å‰ªæžå’Œ&lt;a href=&quot;https://arxiv.org/abs/2107.03356&quot;>;M-FAC&lt;/a>;è¿›è¡Œæ¯”è¾ƒã€‚ä¸‹å›¾æ˜¾ç¤ºï¼ŒCHITA åœ¨å„ç§ç¨€ç–åº¦æ°´å¹³ä¸‹å®žçŽ°äº†æ›´å¥½çš„æµ‹è¯•ç²¾åº¦ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU 5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/s1050/image6.png&quot;æ ·å¼=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;784&quot; data-original-width=&quot;1050&quot; height=&quot;478&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6J MfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/w640-h478/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æµ‹è¯•ä½¿ç”¨ä¸åŒæ–¹æ³•èŽ·å¾—çš„ä¿®å‰ªç½‘ç»œçš„å‡†ç¡®æ€§ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®ºã€å±€é™æ€§å’Œæœªæ¥å·¥ä½œ&lt;/h2>; &lt;p>; æˆ‘ä»¬æå‡ºäº† CHITAï¼Œä¸€ç§åŸºäºŽä¼˜åŒ–çš„æ–¹æ³•ç”¨äºŽä¿®å‰ªé¢„å…ˆè®­ç»ƒçš„ç¥žç»ç½‘ç»œã€‚ CHITA é€šè¿‡æœ‰æ•ˆåœ°ä½¿ç”¨äºŒé˜¶ä¿¡æ¯å¹¶å€Ÿé‰´ç»„åˆä¼˜åŒ–å’Œé«˜ç»´ç»Ÿè®¡çš„æ€æƒ³ï¼Œæä¾›å¯æ‰©å±•æ€§å’Œæœ‰ç«žäº‰åŠ›çš„æ€§èƒ½ã€‚ &lt;/p>; &lt;p>; CHITA ä¸“ä¸º&lt;em>;éžç»“æž„åŒ–ä¿®å‰ª&lt;/em>;è€Œè®¾è®¡ï¼Œå¯ä»¥åŽ»é™¤ä»»ä½•é‡é‡ã€‚ç†è®ºä¸Šï¼Œéžç»“æž„åŒ–å‰ªæžå¯ä»¥æ˜¾ç€é™ä½Žè®¡ç®—è¦æ±‚ã€‚ç„¶è€Œï¼Œåœ¨å®žè·µä¸­å®žçŽ°è¿™äº›å‡å°‘éœ€è¦æ”¯æŒç¨€ç–è®¡ç®—çš„ç‰¹æ®Šè½¯ä»¶ï¼ˆå¯èƒ½è¿˜æœ‰ç¡¬ä»¶ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç»“æž„åŒ–ä¿®å‰ªä¼šåˆ é™¤ç¥žç»å…ƒç­‰æ•´ä¸ªç»“æž„ï¼Œå¯èƒ½ä¼šæä¾›åœ¨é€šç”¨è½¯ä»¶å’Œç¡¬ä»¶ä¸Šæ›´å®¹æ˜“å®žçŽ°çš„æ”¹è¿›ã€‚å°† CHITA æ‰©å±•åˆ°ç»“æž„åŒ–ä¿®å‰ªå°†ä¼šå¾ˆæœ‰è¶£ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;è¿™é¡¹å·¥ä½œæ˜¯ Google ä¹‹é—´ç ”ç©¶åˆä½œçš„ä¸€éƒ¨åˆ†å’Œéº»çœç†å·¥å­¦é™¢ã€‚æ„Ÿè°¢ Rahul Mazumderã€Natalia Ponomarevaã€Wenyu Chenã€Xiang Menã€Zhe Zhu å’Œ Sergei Vassivitskii åœ¨å‡†å¤‡è¿™ç¯‡æ–‡ç« å’Œè®ºæ–‡æ—¶æä¾›çš„å¸®åŠ©ã€‚è¿˜è¦æ„Ÿè°¢ John Guilyard åœ¨æœ¬æ–‡ä¸­åˆ›å»ºå›¾å½¢ã€‚&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4779594044050650231/comments/default&quot; rel= &quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html#comment -form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel= â€œç¼–è¾‘â€ç±»åž‹=â€œapplication/atom+xmlâ€/>;&lt;link href=â€œhttp://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231â€rel=â€œselfâ€type=â€œapplication/atomâ€ +xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html&quot; rel=&quot;alternate&quot; title=&quot;ç»„åˆä¼˜åŒ–ç¥žç»ç½‘ç»œå‰ªæž&quot; ç±»åž‹=&quot;text/html&quot;/>;&lt;ä½œè€…>;&lt;åç§°>;Google AI&lt;/åç§°>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;ç”µå­é‚®ä»¶>;noreply@blogger.com&lt;/ç”µå­é‚®ä»¶>;&lt;gdï¼šå›¾åƒé«˜åº¦=â€œ16â€rel=â€œhttp://schemas.google.com/g/2005#thumbnailâ€src=â€œhttps://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS 2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K /s72-c/CHITA%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr ï¼šæ€»è®¡>;&lt;/entry>;&lt;entry>;&lt;id>;æ ‡ç­¾ï¼šblogger.com,1999:blog-8474926331452026626.post-8689679451447865270&lt;/id>;&lt;å‘å¸ƒ>;2023-08-15T12:59:00.001-07:00&lt;/å·²å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-15T13:00:54.887-07:00&lt;/æ›´æ–°>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;æ·±åº¦å­¦ä¹ &quot;>;&lt; /ç±»åˆ«>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atom/ns#â€æœ¯è¯­=â€œæŽ¨èç³»ç»Ÿâ€>;&lt;/ç±»åˆ«>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atomâ€ /ns#&quot; term=&quot;ç¤¾äº¤ç½‘ç»œ&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;ç ”ç©¶ï¼šç¤¾ä¼šæ„è¯†æš‚æ—¶å› æžœè§£ç å™¨æŽ¨èç³»ç»Ÿ&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;å‘å¸ƒè€…ï¼šGoogle ç ”ç©¶å·¥ç¨‹å¸ˆ Eltayeb Ahmed å’Œé«˜çº§ç ”ç©¶ç§‘å­¦å®¶ Subhrajit Roy&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt -PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx 8AO1_/s837/study-hero.png&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; é˜…è¯»å¯¹äºŽå¹´è½»å­¦ç”Ÿæœ‰å¾ˆå¤šå¥½å¤„ï¼Œä¾‹å¦‚&lt;a href=&quot;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/284286/reading_for_pleasureã€‚ pdf&quot;>;æ›´å¥½çš„è¯­è¨€å’Œç”Ÿæ´»æŠ€èƒ½&lt;/a>;ï¼Œå¹¶ä¸”å¿«ä¹é˜…è¯»å·²è¢«è¯æ˜Žä¸Ž&lt;a href=&quot;https://files.eric.ed.gov/fulltext/ED541404.pdf&quot;>;å­¦ä¸šæˆåŠŸç›¸å…³&lt; /a>;.æ­¤å¤–ï¼Œå­¦ç”Ÿä»¬è¿˜è¡¨ç¤ºï¼Œé˜…è¯»&lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/1361454042000312284&quot;>;æƒ…ç»ªå¥åº·å¾—åˆ°æ”¹å–„&lt;/a>;ï¼Œå¹¶ä¸”&lt;a href=&quot;https:// eric.ed.gov/?id=ED496343&quot;>;æ›´å¥½çš„å¸¸è¯†å’Œå¯¹å…¶ä»–æ–‡åŒ–çš„æ›´å¥½çš„ç†è§£&lt;/a>;ã€‚ç”±äºŽçº¿ä¸Šå’Œçº¿ä¸‹æœ‰å¤§é‡çš„é˜…è¯»ææ–™ï¼Œæ‰¾åˆ°é€‚åˆå¹´é¾„çš„ã€ç›¸å…³çš„å’Œå¼•äººå…¥èƒœçš„å†…å®¹å¯èƒ½æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä½†å¸®åŠ©å­¦ç”Ÿåšåˆ°è¿™ä¸€ç‚¹æ˜¯è®©ä»–ä»¬å‚ä¸Žé˜…è¯»çš„å¿…è¦æ­¥éª¤ã€‚å‘å­¦ç”Ÿæä¾›ç›¸å…³é˜…è¯»ææ–™çš„æœ‰æ•ˆæŽ¨èæœ‰åŠ©äºŽå­¦ç”ŸæŒç»­é˜…è¯»ï¼Œè€Œè¿™æ­£æ˜¯æœºå™¨å­¦ä¹  (ML) å¯ä»¥æä¾›å¸®åŠ©çš„åœ°æ–¹ã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; æœºå™¨å­¦ä¹ å·²å¹¿æ³›åº”ç”¨äºŽæž„å»º&lt;a href=&quot;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592 -5&quot;>;æŽ¨èç³»ç»Ÿ&lt;/a>;ï¼Œé€‚ç”¨äºŽå„ç§ç±»åž‹çš„æ•°å­—å†…å®¹ï¼Œä»Žè§†é¢‘åˆ°ä¹¦ç±å†åˆ°ç”µå­å•†åŠ¡é¡¹ç›®ã€‚æŽ¨èç³»ç»Ÿåœ¨ä¸€ç³»åˆ—æ•°å­—å¹³å°ä¸Šä½¿ç”¨ï¼Œä»¥å¸®åŠ©å‘ç”¨æˆ·å±•ç¤ºç›¸å…³ä¸”æœ‰å¸å¼•åŠ›çš„å†…å®¹ã€‚åœ¨è¿™äº›ç³»ç»Ÿä¸­ï¼Œæœºå™¨å­¦ä¹ æ¨¡åž‹ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·åå¥½ã€ç”¨æˆ·å‚ä¸Žåº¦å’ŒæŽ¨èé¡¹ç›®å•ç‹¬å‘æ¯ä¸ªç”¨æˆ·æŽ¨èé¡¹ç›®ã€‚è¿™äº›æ•°æ®ä¸ºæ¨¡åž‹æä¾›äº†å¼ºå¤§çš„å­¦ä¹ ä¿¡å·ï¼Œä½¿å…¶èƒ½å¤ŸæŽ¨èå¯èƒ½æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼Œä»Žè€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚ &lt;/p>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://arxiv.org/abs/2306.07946&quot;>;ç ”ç©¶ï¼šç¤¾äº¤æ„è¯†æš‚æ—¶å› æžœè§£ç å™¨æŽ¨èç³»ç»Ÿ&lt;/a>;â€ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæœ‰å£°è¯»ç‰©çš„å†…å®¹æŽ¨èç³»ç»Ÿåœ¨æ•™è‚²çŽ¯å¢ƒä¸­è€ƒè™‘é˜…è¯»çš„ç¤¾ä¼šæ€§è´¨ã€‚æˆ‘ä»¬ä¸Ž &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; åˆä½œå¼€å‘äº† STUDY ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªæ•™è‚²éžè¥åˆ©ç»„ç»‡ï¼Œæ—¨åœ¨ä¿ƒè¿›é˜…è¯»éšœç¢å­¦ç”Ÿçš„é˜…è¯»ï¼Œé€šè¿‡å­¦æ ¡å‘å­¦ç”Ÿæä¾›æœ‰å£°è¯»ç‰©å¹¿æ³›çš„è®¢é˜…è®¡åˆ’ã€‚åˆ©ç”¨ Learning Ally å›¾ä¹¦é¦†ä¸­çš„å„ç§æœ‰å£°è¯»ç‰©ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¸®åŠ©å­¦ç”Ÿæ‰¾åˆ°åˆé€‚çš„å†…å®¹ï¼Œä»¥å¸®åŠ©æé«˜ä»–ä»¬çš„é˜…è¯»ä½“éªŒå’Œå‚ä¸Žåº¦ã€‚ç”±äºŽä¸€ä¸ªäººçš„åŒé¾„äººå½“å‰æ­£åœ¨é˜…è¯»çš„å†…å®¹ä¼šå¯¹ä»–ä»¬æ„Ÿå…´è¶£çš„é˜…è¯»å†…å®¹äº§ç”Ÿé‡å¤§å½±å“ï¼Œå› æ­¤æˆ‘ä»¬å…±åŒå¤„ç†åŒä¸€æ•™å®¤ä¸­å­¦ç”Ÿçš„é˜…è¯»å‚ä¸ŽåŽ†å²ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ¨¡åž‹èƒ½å¤Ÿä»Žæœ‰å…³å­¦ç”Ÿæœ¬åœ°ç¤¾äº¤ç¾¤ä½“ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸ºä»–ä»¬çš„æ•™å®¤ï¼‰å½“å‰è¶‹åŠ¿çš„å®žæ—¶ä¿¡æ¯ä¸­å—ç›Šã€‚ &lt;/p>; &lt;br />; &lt;h2>;æ•°æ®&lt;/h2>; &lt;p>; &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; æ‹¥æœ‰ä¸€ä¸ªå¤§åž‹æ•°å­—å›¾ä¹¦é¦†ï¼Œå…¶ä¸­åŒ…å«é’ˆå¯¹ä»¥ä¸‹äººç¾¤çš„ç²¾é€‰æœ‰å£°è¯»ç‰©ï¼šå­¦ç”Ÿï¼Œä½¿å…¶éžå¸¸é€‚åˆæž„å»ºç¤¾ä¼šæŽ¨èæ¨¡åž‹ï¼Œä»¥å¸®åŠ©æé«˜å­¦ç”Ÿçš„å­¦ä¹ æˆæžœã€‚æˆ‘ä»¬æ”¶åˆ°äº†ä¸¤å¹´çš„åŒ¿åæœ‰å£°è¯»ç‰©æ¶ˆè´¹æ•°æ®ã€‚æ•°æ®ä¸­çš„æ‰€æœ‰å­¦ç”Ÿã€å­¦æ ¡å’Œåˆ†ç»„éƒ½æ˜¯åŒ¿åçš„ï¼Œä»…é€šè¿‡éšæœºç”Ÿæˆçš„ ID è¿›è¡Œè¯†åˆ«ï¼Œæ— æ³•é€šè¿‡ Google è¿½æº¯åˆ°çœŸå®žå®žä½“ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ½œåœ¨å¯è¯†åˆ«å…ƒæ•°æ®ä»…ä»¥èšåˆå½¢å¼å…±äº«ï¼Œä»¥ä¿æŠ¤å­¦ç”Ÿå’Œæœºæž„ä¸è¢«é‡æ–°è¯†åˆ«ã€‚è¿™äº›æ•°æ®åŒ…æ‹¬å­¦ç”Ÿä¸Žæœ‰å£°è¯»ç‰©äº’åŠ¨çš„å¸¦æ—¶é—´æˆ³çš„è®°å½•ã€‚å¯¹äºŽæ¯æ¬¡äº¤äº’ï¼Œæˆ‘ä»¬éƒ½æœ‰ä¸€ä¸ªåŒ¿åçš„å­¦ç”Ÿ IDï¼ˆåŒ…æ‹¬å­¦ç”Ÿçš„å¹´çº§å’ŒåŒ¿åçš„å­¦æ ¡ IDï¼‰ã€æœ‰å£°è¯»ç‰©æ ‡è¯†ç¬¦å’Œæ—¥æœŸã€‚è™½ç„¶è®¸å¤šå­¦æ ¡å°†å•ä¸€å¹´çº§çš„å­¦ç”Ÿåˆ†å¸ƒåœ¨å¤šä¸ªæ•™å®¤ä¸­ï¼Œä½†æˆ‘ä»¬åˆ©ç”¨æ­¤å…ƒæ•°æ®åšå‡ºç®€åŒ–çš„å‡è®¾ï¼Œå³åŒä¸€æ‰€å­¦æ ¡å’ŒåŒä¸€å¹´çº§çš„æ‰€æœ‰å­¦ç”Ÿéƒ½åœ¨åŒä¸€æ•™å®¤ã€‚è™½ç„¶è¿™ä¸ºå»ºç«‹æ›´å¥½çš„ç¤¾äº¤æŽ¨èæ¨¡åž‹æä¾›äº†åŸºç¡€ï¼Œä½†å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™å¹¶ä¸èƒ½è®©æˆ‘ä»¬é‡æ–°è¯†åˆ«ä¸ªäººã€ç­çº§ç¾¤ä½“æˆ–å­¦æ ¡ã€‚ &lt;/p>; &lt;br />; &lt;h2>;STUDY ç®—æ³•&lt;/h2>; &lt;p>; æˆ‘ä»¬å°†æŽ¨èé—®é¢˜æè¿°ä¸º&lt;a href=&quot;https://arxiv.org/abs/2104.10584&quot;>;ç‚¹å‡»çŽ‡&lt;/a>; é¢„æµ‹é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹ç”¨æˆ·ä¸Žæ¯ä¸ªç‰¹å®šé¡¹ç›®äº¤äº’çš„æ¡ä»¶æ¦‚çŽ‡è¿›è¡Œå»ºæ¨¡ï¼Œæ¡ä»¶æ˜¯ 1ï¼‰ç”¨æˆ·å’Œé¡¹ç›®ç‰¹å¾ä»¥åŠ 2ï¼‰å½“å‰ç”¨æˆ·çš„é¡¹ç›®äº¤äº’åŽ†å²åºåˆ—ã€‚ &lt;a href=&quot;https://arxiv.org/abs/1905.06874&quot;>;ä¹‹å‰çš„å·¥ä½œ&lt;/a>;å»ºè®®&lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer&lt;åŸºäºŽ /a>; çš„æ¨¡åž‹æ˜¯ Google Research å¼€å‘çš„ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ¨¡åž‹ç±»ï¼Œéžå¸¸é€‚åˆå¯¹è¿™ä¸ªé—®é¢˜è¿›è¡Œå»ºæ¨¡ã€‚å½“å•ç‹¬å¤„ç†æ¯ä¸ªç”¨æˆ·æ—¶ï¼Œè¿™å°†æˆä¸ºä¸€ä¸ª&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;è‡ªå›žå½’åºåˆ—å»ºæ¨¡é—®é¢˜&lt;/a>;ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ¦‚å¿µæ¡†æž¶æ¥å»ºæ¨¡æˆ‘ä»¬çš„æ•°æ®ï¼Œç„¶åŽæ‰©å±•è¿™ä¸ªæ¡†æž¶æ¥åˆ›å»ºç ”ç©¶æ–¹æ³•ã€‚ &lt;/p>; &lt;p>; è™½ç„¶è¿™ç§ç‚¹å‡»çŽ‡é¢„æµ‹æ–¹æ³•å¯ä»¥å¯¹å•ä¸ªç”¨æˆ·è¿‡åŽ»å’Œæœªæ¥çš„é¡¹ç›®åå¥½ä¹‹é—´çš„ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä¸”å¯ä»¥åœ¨è®­ç»ƒæ—¶å­¦ä¹ ç”¨æˆ·ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¨¡å¼ï¼Œä½†å®ƒæ— æ³•åœ¨æŽ¨ç†æ—¶å¯¹ä¸åŒç”¨æˆ·ä¹‹é—´çš„ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡æ—¶é—´ã€‚ä¸ºäº†è®¤è¯†é˜…è¯»çš„ç¤¾ä¼šæ€§å¹¶å¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬å¼€å‘äº† STUDY æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹å°†æ¯ä¸ªå­¦ç”Ÿé˜…è¯»çš„å¤šä¸ªä¹¦ç±åºåˆ—è¿žæŽ¥æˆä¸€ä¸ªåºåˆ—ï¼Œè¯¥åºåˆ—æ”¶é›†å•ä¸ªæ•™å®¤ä¸­å¤šä¸ªå­¦ç”Ÿçš„æ•°æ®ã€‚ &lt;/p>; &lt;p>; ç„¶è€Œï¼Œå¦‚æžœè¦é€šè¿‡ Transformer å»ºæ¨¡ï¼Œè¿™ç§æ•°æ®è¡¨ç¤ºéœ€è¦ä»”ç»†ç ”ç©¶ã€‚åœ¨ Transformer ä¸­ï¼Œæ³¨æ„åŠ›æŽ©ç æ˜¯æŽ§åˆ¶å“ªäº›è¾“å…¥å¯ç”¨äºŽé€šçŸ¥å“ªäº›è¾“å‡ºçš„é¢„æµ‹çš„çŸ©é˜µã€‚ä½¿ç”¨åºåˆ—ä¸­çš„æ‰€æœ‰å…ˆéªŒæ ‡è®°æ¥é€šçŸ¥è¾“å‡ºé¢„æµ‹çš„æ¨¡å¼å¯¼è‡´ä¼ ç»Ÿä¸Šåœ¨å› æžœè§£ç å™¨ä¸­å‘çŽ°çš„ä¸Šä¸‰è§’æ³¨æ„çŸ©é˜µã€‚ç„¶è€Œï¼Œç”±äºŽè¾“å…¥ STUDY æ¨¡åž‹çš„åºåˆ—ä¸æ˜¯æŒ‰æ—¶é—´æŽ’åºçš„ï¼Œå³ä½¿å®ƒçš„æ¯ä¸ªç»„æˆå­åºåˆ—éƒ½æ˜¯æ ‡å‡†çš„&lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;>;å› æžœè§£ç å™¨&lt;/a>;ä¸å†é€‚åˆè¿™ä¸ªåºåˆ—ã€‚å½“å°è¯•é¢„æµ‹æ¯ä¸ªæ ‡è®°æ—¶ï¼Œæ¨¡åž‹ä¸å…è®¸å…³æ³¨åºåˆ—ä¸­ä½äºŽå…¶ä¹‹å‰çš„æ¯ä¸ªæ ‡è®°ï¼›å…¶ä¸­ä¸€äº›ä»¤ç‰Œå¯èƒ½å…·æœ‰è¾ƒæ™šçš„æ—¶é—´æˆ³ï¼Œå¹¶ä¸”åŒ…å«åœ¨éƒ¨ç½²æ—¶ä¸å¯ç”¨çš„ä¿¡æ¯ã€‚ &lt;br />; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tq Rd-ZWDXOCRpHAy0FZE9sV0xB8reyJ-- Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/s1787/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1651&quot; data-original-width = =ç¬¬1787ç« pHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/w400-h370/image1.png&quot;å®½åº¦=&quot; 400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;åœ¨æ­¤å›¾ä¸­ï¼Œæˆ‘ä»¬æ˜¾ç¤ºäº†é€šå¸¸ç”¨äºŽå› æžœè§£ç å™¨ã€‚æ¯åˆ—ä»£è¡¨ä¸€ä¸ªè¾“å‡ºï¼Œæ¯åˆ—ä»£è¡¨ä¸€ä¸ªè¾“å‡ºã€‚ç‰¹å®šä½ç½®å¤„çš„çŸ©é˜µæ¡ç›®çš„å€¼ä¸º 1ï¼ˆæ˜¾ç¤ºä¸ºè“è‰²ï¼‰è¡¨ç¤ºæ¨¡åž‹åœ¨é¢„æµ‹ç›¸åº”åˆ—çš„è¾“å‡ºæ—¶å¯ä»¥è§‚å¯Ÿåˆ°è¯¥è¡Œçš„è¾“å…¥ï¼Œè€Œå€¼ä¸º 0ï¼ˆæ˜¾ç¤ºä¸ºç™½è‰²ï¼‰è¡¨ç¤ºç›¸åçš„æƒ…å†µ.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; STUDY æ¨¡åž‹å»ºç«‹åœ¨å› æžœå˜æ¢å™¨çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å°†ä¸‰è§’çŸ©é˜µæ³¨æ„æŽ©ç æ›¿æ¢ä¸ºçµæ´»çš„æ³¨æ„æŽ©ç ï¼Œå…¶å€¼åŸºäºŽæ—¶é—´æˆ³ï¼Œä»¥å…è®¸å…³æ³¨ä¸åŒçš„å­åºåˆ—ã€‚ä¸Žå¸¸è§„ Transformer ç›¸æ¯”ï¼Œå¸¸è§„ Transformer ä¸å…è®¸è·¨ä¸åŒå­åºåˆ—è¿›è¡Œå…³æ³¨ï¼Œå¹¶ä¸”åœ¨åºåˆ—å†…å…·æœ‰ä¸‰è§’çŸ©é˜µæŽ©ç ï¼Œè€Œ STUDY åœ¨åºåˆ—å†…ç»´æŠ¤å› æžœä¸‰è§’å…³æ³¨çŸ©é˜µï¼Œå¹¶ä¸”è·¨åºåˆ—å…·æœ‰çµæ´»çš„å€¼ï¼Œå…¶å€¼å–å†³äºŽæ—¶é—´æˆ³ã€‚å› æ­¤ï¼Œåºåˆ—ä¸­ä»»ä½•è¾“å‡ºç‚¹çš„é¢„æµ‹éƒ½æ˜¯ç”±è¿‡åŽ»ç›¸å¯¹äºŽå½“å‰æ—¶é—´ç‚¹å‘ç”Ÿçš„æ‰€æœ‰è¾“å…¥ç‚¹é€šçŸ¥çš„ï¼Œæ— è®ºå®ƒä»¬æ˜¯å‡ºçŽ°åœ¨åºåˆ—ä¸­å½“å‰è¾“å…¥ä¹‹å‰è¿˜æ˜¯ä¹‹åŽã€‚è¿™ç§å› æžœçº¦æŸå¾ˆé‡è¦ï¼Œå› ä¸ºå¦‚æžœä¸åœ¨è®­ç»ƒæ—¶å¼ºåˆ¶æ‰§è¡Œï¼Œæ¨¡åž‹å¯èƒ½ä¼šå­¦ä¹ ä½¿ç”¨æœªæ¥çš„ä¿¡æ¯è¿›è¡Œé¢„æµ‹ï¼Œè€Œè¿™å¯¹äºŽçŽ°å®žä¸–ç•Œçš„éƒ¨ç½²æ¥è¯´æ˜¯ä¸å¯ç”¨çš„ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6Zm UAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s1104/image3.jpg&quot; style=&quot;margin-å·¦ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFHANO2eJdeFMIWt7K 58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;åœ¨ï¼ˆaï¼‰ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªå…·æœ‰å› æžœæ³¨æ„åŠ›çš„é¡ºåºè‡ªå›žå½’å˜åŽ‹å™¨ï¼Œå¯ä»¥å•ç‹¬å¤„ç†æ¯ä¸ªç”¨æˆ·ï¼›åœ¨ï¼ˆbï¼‰ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç­‰æ•ˆçš„è”åˆå‰å‘ä¼ é€’ï¼Œå…¶ç»“æžœä¸Žï¼ˆaï¼‰ç›¸åŒçš„è®¡ç®—ï¼›æœ€åŽï¼Œåœ¨ï¼ˆcï¼‰ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜Žï¼Œé€šè¿‡å‘æ³¨æ„åŠ›æŽ©æ¨¡å¼•å…¥æ–°çš„éžé›¶å€¼ï¼ˆä»¥ç´«è‰²æ˜¾ç¤ºï¼‰ï¼Œæˆ‘ä»¬å…è®¸ä¿¡æ¯åœ¨ç”¨æˆ·ä¹‹é—´æµåŠ¨ã€‚æˆ‘ä»¬é€šè¿‡å…è®¸é¢„æµ‹ä»¥å…·æœ‰è¾ƒæ—©æ—¶é—´æˆ³çš„æ‰€æœ‰äº¤äº’ä¸ºæ¡ä»¶æ¥å®žçŽ°è¿™ä¸€ç‚¹ï¼Œæ— è®ºäº¤äº’æ˜¯å¦æ¥è‡ªåŒä¸€ç”¨æˆ·ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQf vFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s1035/Study.png&quot; style=&quot;å·¦è¾¹è·ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1035&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB 8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s16000/Study.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;åœ¨ (a) ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªå…·æœ‰å› æžœæ³¨æ„åŠ›çš„é¡ºåºè‡ªå›žå½’è½¬æ¢å™¨ï¼Œå¯ä»¥å•ç‹¬å¤„ç†æ¯ä¸ªç”¨æˆ·ï¼›åœ¨ï¼ˆbï¼‰ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç­‰æ•ˆçš„è”åˆå‰å‘ä¼ é€’ï¼Œå…¶ç»“æžœä¸Žï¼ˆaï¼‰ç›¸åŒçš„è®¡ç®—ï¼›æœ€åŽï¼Œåœ¨ï¼ˆcï¼‰ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜Žï¼Œé€šè¿‡å‘æ³¨æ„åŠ›æŽ©æ¨¡å¼•å…¥æ–°çš„éžé›¶å€¼ï¼ˆä»¥ç´«è‰²æ˜¾ç¤ºï¼‰ï¼Œæˆ‘ä»¬å…è®¸ä¿¡æ¯åœ¨ç”¨æˆ·ä¹‹é—´æµåŠ¨ã€‚æˆ‘ä»¬é€šè¿‡å…è®¸é¢„æµ‹ä»¥å…·æœ‰è¾ƒæ—©æ—¶é—´æˆ³çš„æ‰€æœ‰äº¤äº’ä¸ºæ¡ä»¶æ¥å®žçŽ°è¿™ä¸€ç‚¹ï¼Œæ— è®ºäº¤äº’æ˜¯å¦æ¥è‡ªåŒä¸€ç”¨æˆ·ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; !--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt; tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2 _wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/s1104/image3.png&quot;æ ·å¼= â€œå·¦è¾¹è·ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›â€&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;548&quot; height=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9Fd caJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/w318-h640/image3.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>; &lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;åœ¨ (a) ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªå…·æœ‰å› æžœæ³¨æ„åŠ›çš„é¡ºåºè‡ªå›žå½’è½¬æ¢å™¨ï¼Œå¯ä»¥å•ç‹¬å¤„ç†æ¯ä¸ªç”¨æˆ·ï¼›åœ¨ï¼ˆbï¼‰ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç­‰æ•ˆçš„è”åˆå‰å‘ä¼ é€’ï¼Œå…¶ç»“æžœä¸Žï¼ˆaï¼‰ç›¸åŒçš„è®¡ç®—ï¼›æœ€åŽï¼Œåœ¨ï¼ˆcï¼‰ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜Žï¼Œé€šè¿‡å‘æ³¨æ„åŠ›æŽ©æ¨¡å¼•å…¥æ–°çš„éžé›¶å€¼ï¼ˆä»¥ç´«è‰²æ˜¾ç¤ºï¼‰ï¼Œæˆ‘ä»¬å…è®¸ä¿¡æ¯åœ¨ç”¨æˆ·ä¹‹é—´æµåŠ¨ã€‚æˆ‘ä»¬é€šè¿‡å…è®¸é¢„æµ‹ä»¥å…·æœ‰è¾ƒæ—©æ—¶é—´æˆ³çš„æ‰€æœ‰äº¤äº’ä¸ºæ¡ä»¶æ¥å®žçŽ°è¿™ä¸€ç‚¹ï¼Œæ— è®ºäº¤äº’æ˜¯å¦æ¥è‡ªåŒä¸€ç”¨æˆ·ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;å®žéªŒ&lt;/h2>; &lt;p>; æˆ‘ä»¬ä½¿ç”¨ Learning Ally æ•°æ®é›†æ¥è®­ç»ƒ STUDY æ¨¡åž‹ä»¥åŠå¤šä¸ªåŸºçº¿ä»¥è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å®žçŽ°äº†ä¸€ä¸ªè‡ªå›žå½’ç‚¹å‡»çŽ‡è½¬æ¢å™¨è§£ç å™¨ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºâ€œä¸ªä½“â€ï¼Œä¸€ä¸ª&lt;em>;k&lt;/em>;æœ€è¿‘é‚»åŸºçº¿ï¼ˆKNNï¼‰ï¼Œä»¥åŠä¸€ä¸ªå¯æ¯”è¾ƒçš„ç¤¾äº¤åŸºçº¿ï¼Œç¤¾äº¤æ³¨æ„åŠ›è®°å¿†ç½‘ç»œï¼ˆSAMNï¼‰ ã€‚æˆ‘ä»¬ä½¿ç”¨ç¬¬ä¸€å­¦å¹´çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨ç¬¬äºŒå­¦å¹´çš„æ•°æ®è¿›è¡ŒéªŒè¯å’Œæµ‹è¯•ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬é€šè¿‡æµ‹é‡ç”¨æˆ·å®žé™…äº¤äº’çš„ä¸‹ä¸€ä¸ªé¡¹ç›®åœ¨æ¨¡åž‹çš„å‰ &lt;em>;n&lt;/em>; ä¸ªæŽ¨èä¸­çš„æ—¶é—´ç™¾åˆ†æ¯”æ¥è¯„ä¼°è¿™äº›æ¨¡åž‹ï¼Œå³ï¼Œhits@&lt;em>;nï¼Œ &lt;/em>;å¯¹äºŽ&lt;em>;n&lt;/em>;çš„ä¸åŒå€¼ã€‚é™¤äº†åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡åž‹ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æŠ¥å‘Šäº†æ¨¡åž‹åœ¨æµ‹è¯•é›†çš„ä¸¤ä¸ªå­é›†ä¸Šçš„å¾—åˆ†ï¼Œè¿™ä¸¤ä¸ªå­é›†æ¯”æ•´ä¸ªæ•°æ®é›†æ›´å…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå­¦ç”Ÿé€šå¸¸ä¼šåœ¨å¤šä¸ªä¼šè¯ä¸­ä¸Žæœ‰å£°è¯»ç‰©è¿›è¡Œäº¤äº’ï¼Œå› æ­¤ç®€å•åœ°æŽ¨èç”¨æˆ·æœ€è¿‘è¯»è¿‡çš„ä¸€æœ¬ä¹¦å°†æ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„ç®€å•æŽ¨èã€‚å› æ­¤ï¼Œç¬¬ä¸€ä¸ªæµ‹è¯•å­é›†ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œéžå»¶ç»­â€ï¼‰æ˜¯æŒ‡å½“å­¦ç”Ÿä¸Žä¸Žä¹‹å‰äº¤äº’ä¸åŒçš„ä¹¦ç±è¿›è¡Œäº¤äº’æ—¶ï¼Œæˆ‘ä»¬ä»…æŸ¥çœ‹æ¯ä¸ªæ¨¡åž‹åœ¨æŽ¨èä¸Šçš„è¡¨çŽ°ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°å­¦ç”Ÿä¼šé‡æ¸©ä»–ä»¬è¿‡åŽ»è¯»è¿‡çš„ä¹¦ç±ï¼Œå› æ­¤é€šè¿‡å°†é’ˆå¯¹æ¯ä¸ªå­¦ç”Ÿçš„æŽ¨èä»…é™äºŽä»–ä»¬è¿‡åŽ»è¯»è¿‡çš„ä¹¦ç±ï¼Œå¯ä»¥åœ¨æµ‹è¯•é›†ä¸Šå–å¾—å‡ºè‰²çš„è¡¨çŽ°ã€‚å°½ç®¡å‘å­¦ç”ŸæŽ¨èæ—§çš„æ”¶è—å¤¹å¯èƒ½å¾ˆæœ‰ä»·å€¼ï¼Œä½†æŽ¨èç³»ç»Ÿçš„å¤§éƒ¨åˆ†ä»·å€¼æ¥è‡ªäºŽå‘ç”¨æˆ·å±•ç¤ºæ–°çš„å’ŒæœªçŸ¥çš„å†…å®¹ã€‚ä¸ºäº†è¡¡é‡è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨å­¦ç”Ÿç¬¬ä¸€æ¬¡ä¸Žæ ‡é¢˜äº¤äº’çš„æµ‹è¯•é›†å­é›†ä¸Šè¯„ä¼°æ¨¡åž‹ã€‚æˆ‘ä»¬å°†è¿™ä¸ªè¯„ä¼°å­é›†å‘½åä¸ºâ€œå°è¯´â€ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬å‘çŽ°ï¼Œåœ¨æˆ‘ä»¬è¯„ä¼°çš„å‡ ä¹Žæ¯ä¸ªåˆ‡ç‰‡ä¸­ï¼ŒSTUDY éƒ½ä¼˜äºŽæ‰€æœ‰å…¶ä»–æµ‹è¯•æ¨¡åž‹ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr 7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/s1526/Study-img2.png â€œ style=â€margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1486&quot; data-original-width=&quot;1526&quot; height=&quot;390&quot; src= â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx 0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/w400-h390/Study-img2.png&quot;å®½åº¦=â€œ400â€/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;åœ¨æ­¤å›¾ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº† Studyã€Individualã€KNN å’Œ SAMN å››ç§æ¨¡åž‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬ç”¨ hist@5 æ¥è¡¡é‡æ€§èƒ½ï¼Œå³æ¨¡åž‹åœ¨æ¨¡åž‹çš„å‰ 5 ä¸ªæŽ¨èä¸­å»ºè®®ç”¨æˆ·é˜…è¯»çš„ä¸‹ä¸€ä¸ªæ ‡é¢˜çš„å¯èƒ½æ€§æœ‰å¤šå¤§ã€‚æˆ‘ä»¬åœ¨æ•´ä¸ªæµ‹è¯•é›†ï¼ˆå…¨éƒ¨ï¼‰ä»¥åŠæ–°é¢–çš„å’Œéžè¿žç»­çš„åˆ†å‰²ä¸Šè¯„ä¼°æ¨¡åž‹ã€‚æˆ‘ä»¬å‘çŽ° STUDY åœ¨æ‰€æœ‰åˆ†ç»„ä¸­å§‹ç»ˆä¼˜äºŽå…¶ä»–ä¸‰ä¸ªæ¨¡åž‹ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;é€‚å½“åˆ†ç»„çš„é‡è¦æ€§&lt;/h2>; &lt;p>; STUDY ç®—æ³•çš„æ ¸å¿ƒæ˜¯å°†ç”¨æˆ·ç»„ç»‡æˆç»„ï¼Œå¹¶åœ¨æ¨¡åž‹çš„å•æ¬¡å‰å‘ä¼ é€’ä¸­å¯¹åŒä¸€ç»„ä¸­çš„å¤šä¸ªç”¨æˆ·è¿›è¡Œè”åˆæŽ¨ç†ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ¶ˆèžç ”ç©¶ï¼Œç ”ç©¶äº†å®žé™…åˆ†ç»„å¯¹æ¨¡åž‹æ€§èƒ½çš„é‡è¦æ€§ã€‚åœ¨æˆ‘ä»¬æå‡ºçš„æ¨¡åž‹ä¸­ï¼Œæˆ‘ä»¬å°†åŒä¸€å¹´çº§å’Œå­¦æ ¡çš„æ‰€æœ‰å­¦ç”Ÿåˆ†ç»„åœ¨ä¸€èµ·ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å¯¹ç”±åŒä¸€å¹´çº§å’Œå­¦åŒºçš„æ‰€æœ‰å­¦ç”Ÿå®šä¹‰çš„ç»„è¿›è¡Œå®žéªŒï¼Œå¹¶å°†æ‰€æœ‰å­¦ç”Ÿæ”¾å…¥ä¸€ä¸ªç»„ä¸­ï¼Œå¹¶åœ¨æ¯æ¬¡å‰å‘ä¼ é€’ä¸­ä½¿ç”¨éšæœºå­é›†ã€‚æˆ‘ä»¬è¿˜å°†è¿™äº›æ¨¡åž‹ä¸Žä¸ªä½“æ¨¡åž‹è¿›è¡Œæ¯”è¾ƒä»¥ä¾›å‚è€ƒã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬å‘çŽ°ä½¿ç”¨æ›´åŠ æœ¬åœ°åŒ–çš„åˆ†ç»„æ›´ä¸ºæœ‰æ•ˆï¼Œå­¦æ ¡å’Œå¹´çº§åˆ†ç»„çš„æ•ˆæžœä¼˜äºŽåœ°åŒºå’Œå¹´çº§åˆ†ç»„ã€‚è¿™æ”¯æŒäº†è¿™æ ·çš„å‡è®¾ï¼šå­¦ä¹ æ¨¡åž‹ä¹‹æ‰€ä»¥æˆåŠŸï¼Œæ˜¯å› ä¸ºé˜…è¯»ç­‰æ´»åŠ¨çš„ç¤¾ä¼šæ€§è´¨â€”â€”äººä»¬çš„é˜…è¯»é€‰æ‹©å¯èƒ½ä¸Žå‘¨å›´äººçš„é˜…è¯»é€‰æ‹©ç›¸å…³ã€‚è¿™ä¸¤ç§æ¨¡åž‹éƒ½ä¼˜äºŽå…¶ä»–ä¸¤ç§æ¨¡åž‹ï¼ˆå•ç»„å’Œä¸ªäººï¼‰ï¼Œå…¶ä¸­ä¸ä½¿ç”¨å¹´çº§æ°´å¹³å¯¹å­¦ç”Ÿè¿›è¡Œåˆ†ç»„ã€‚è¿™è¡¨æ˜Žæ¥è‡ªå…·æœ‰ç›¸ä¼¼é˜…è¯»æ°´å¹³å’Œå…´è¶£çš„ç”¨æˆ·çš„æ•°æ®æœ‰åˆ©äºŽæ€§èƒ½ã€‚ &lt;/p>; &lt;br />; &lt;h2>;æœªæ¥çš„å·¥ä½œ&lt;/h2>; &lt;p>; è¿™é¡¹å·¥ä½œä»…é™äºŽä¸ºå‡è®¾ç¤¾äº¤å…³ç³»åŒè´¨çš„ç”¨æˆ·ç¾¤è¿›è¡ŒæŽ¨èå»ºæ¨¡ã€‚å°†æ¥ï¼Œå¯¹å…³ç³»ä¸å‡åŒ€çš„ç”¨æˆ·ç¾¤ä½“è¿›è¡Œå»ºæ¨¡å°†æ˜¯æœ‰ç›Šçš„ï¼Œå³ï¼Œå­˜åœ¨æ˜Žæ˜¾ä¸åŒç±»åž‹çš„å…³ç³»æˆ–å·²çŸ¥ä¸åŒå…³ç³»çš„ç›¸å¯¹å¼ºåº¦æˆ–å½±å“ã€‚ &lt;/p>; &lt;br />; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;è¿™é¡¹å·¥ä½œæ¶‰åŠç”±ç ”ç©¶äººå‘˜ã€è½¯ä»¶å·¥ç¨‹å¸ˆå’Œæ•™è‚²ä¸»é¢˜ä¸“å®¶ç»„æˆçš„å¤šå­¦ç§‘å›¢é˜Ÿçš„åä½œåŠªåŠ›ã€‚æˆ‘ä»¬æ„Ÿè°¢æˆ‘ä»¬çš„åˆè‘—è€…ï¼šæ¥è‡ª Google çš„ Diana Mincuã€Lauren Harrell å’Œ Katherine Hellerã€‚æˆ‘ä»¬è¿˜è¦æ„Ÿè°¢ Learning Ally çš„åŒäº‹ Jeff Hoã€Akshat Shahã€Erin Walker å’Œ Tyler Bastianï¼Œä»¥åŠ Google çš„åˆä½œè€… Marc Repnyekã€Aki Estrellaã€Fernando Diazã€Scott Sannerã€Emily Salkey å’Œ Lev Proleevã€‚&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8689679451447865270/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/study-socially-aware-temporally-causal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 æ¡è¯„è®º&quot; ç±»åž‹=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;é“¾æŽ¥ href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog .research.google/2023/08/study-socially-aware-temporally-causal.html&quot; rel=&quot;alternate&quot; title=&quot;ç ”ç©¶ï¼šç¤¾ä¼šæ„è¯†æ—¶é—´å› æžœè§£ç å™¨æŽ¨èç³»ç»Ÿ&quot; type=&quot;text/html&quot;/>;&lt;author >;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16 â€œrel =â€œhttp://schemas.google.com/g/2005#thumbnailâ€src =â€œhttps://img1.blogblog.com/img/b16-rounded.gifâ€å®½åº¦=â€œ16â€>;&lt;/gd :image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqa JM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s72-c/study-hero.png â€œ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;æ ‡ç­¾ï¼šblogger.comï¼Œ1999ï¼šblog-8474926331452026626.post-3240052415427459327&lt;/id>;&lt;å‘å¸ƒ>;2023-08-09T11:32:00.001-07:00&lt;/å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-09T12 :26:53.831-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;è‡ªç„¶è¯­è¨€ç†è§£&quot;>;&lt;/category>;&lt;title type=&quot;text &quot;>;æ–‡æ¡£ç†è§£æ–¹é¢çš„è¿›å±•&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šAthena å›¢é˜Ÿ Google ç ”ç©¶éƒ¨è½¯ä»¶å·¥ç¨‹å¸ˆ Sandeep Tata&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85z UbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s320/Document%20understanding%20hero.png&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; è¿‡åŽ»å‡ å¹´ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å¤„ç†å¤æ‚ä¸šåŠ¡æ–‡æ¡£å¹¶å°†å…¶è½¬æ¢ä¸ºç»“æž„åŒ–å¯¹è±¡çš„ç³»ç»Ÿå–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ä¸€ä¸ªå¯ä»¥ä»Žæ”¶æ®ã€ä¿é™©æŠ¥ä»·ç­‰æ–‡æ¡£ä¸­&lt;a href=&quot;https://ai.googleblog.com/2020/06/extracting-structured-data-from.html&quot;>;è‡ªåŠ¨æå–æ•°æ®&lt;/a>;çš„ç³»ç»Ÿå’Œè´¢åŠ¡æŠ¥è¡¨ï¼Œé€šè¿‡é¿å…å®¹æ˜“å‡ºé”™çš„æ‰‹åŠ¨å·¥ä½œï¼Œæœ‰å¯èƒ½æ˜¾ç€æé«˜ä¸šåŠ¡å·¥ä½œæµç¨‹çš„æ•ˆçŽ‡ã€‚åŸºäºŽ &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; æž¶æž„çš„æœ€æ–°æ¨¡åž‹å·²ç»å±•ç¤ºäº†&lt;a href=&quot; https://ai.googleblog.com/2022/04/formnet-beyond-sequential-modeling-for.html&quot;>;å‡†ç¡®åº¦æ˜¾ç€æå‡&lt;/a>;ã€‚æ›´å¤§çš„æ¨¡åž‹ï¼Œä¾‹å¦‚ &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>;ï¼Œä¹Ÿè¢«ç”¨æ¥è¿›ä¸€æ­¥ç®€åŒ–è¿™äº›ä¸šåŠ¡å·¥ä½œæµç¨‹ã€‚ç„¶è€Œï¼Œå­¦æœ¯æ–‡çŒ®ä¸­ä½¿ç”¨çš„æ•°æ®é›†æœªèƒ½æ•æ‰åˆ°çŽ°å®žä¸–ç•Œç”¨ä¾‹ä¸­é‡åˆ°çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå­¦æœ¯åŸºå‡†æŠ¥å‘Šäº†å¾ˆå¼ºçš„æ¨¡åž‹å‡†ç¡®æ€§ï¼Œä½†è¿™äº›ç›¸åŒçš„æ¨¡åž‹åœ¨ç”¨äºŽå¤æ‚çš„çŽ°å®žåº”ç”¨ç¨‹åºæ—¶è¡¨çŽ°ä¸ä½³ã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; åœ¨â€œ&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;VRDUï¼šè§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£çš„åŸºå‡†&lt; /a>;â€ï¼Œåœ¨ &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>; ä¸Šå‘å¸ƒï¼Œæˆ‘ä»¬å®£å¸ƒå‘å¸ƒæ–°çš„&lt;a href=&quot;https://research.google /resources/datasets/visually-rich-document-understanding/&quot;>;è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£&lt;/a>; (VRDU) æ•°æ®é›†æ—¨åœ¨å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°è·Ÿè¸ªæ–‡æ¡£ç†è§£ä»»åŠ¡çš„è¿›å±•ã€‚æ ¹æ®ç»å¸¸ä½¿ç”¨æ–‡æ¡£ç†è§£æ¨¡åž‹çš„çŽ°å®žæ–‡æ¡£ç±»åž‹ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†è‰¯å¥½çš„æ–‡æ¡£ç†è§£åŸºå‡†çš„äº”ä¸ªè¦æ±‚ã€‚ç„¶åŽï¼Œæˆ‘ä»¬æè¿°äº†ç ”ç©¶ç•Œå½“å‰ä½¿ç”¨çš„å¤§å¤šæ•°æ•°æ®é›†ä¸ºä½•æ— æ³•æ»¡è¶³å…¶ä¸­ä¸€é¡¹æˆ–å¤šé¡¹è¦æ±‚ï¼Œè€Œ VRDU å´æ»¡è¶³äº†æ‰€æœ‰è¿™äº›è¦æ±‚ã€‚æˆ‘ä»¬å¾ˆé«˜å…´åœ°å®£å¸ƒå…¬å¼€å‘å¸ƒ VRDU &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;æ•°æ®é›†&lt;/a>;å’Œ&lt;a href=&quot;https ://github.com/google-research-datasets/vrdu&quot;>;è¯„ä¼°ä»£ç &lt;/a>;ï¼Œé‡‡ç”¨&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;çŸ¥è¯†å…±äº«è®¸å¯&lt;/a>;ä¸€ä¸ª>;ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;åŸºå‡†è¦æ±‚&lt;/h2>; &lt;p>; é¦–å…ˆï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†æœ€å…ˆè¿›çš„æ¨¡åž‹å‡†ç¡®æ€§ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ &lt;a href=&quot;https://arxiv.org/abs/2203.08411&quot;>;FormNet&lt;/a>; å’Œ &lt;a href=&quot;https://arxiv.org/abs/2012.14740&quot;>;LayoutLMv2&lt;/ a>;ï¼‰å…³äºŽçŽ°å®žä¸–ç•Œç”¨ä¾‹ä¸Žå­¦æœ¯åŸºå‡†ï¼ˆä¾‹å¦‚ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;ã€&lt;a href=&quot;https://openreview.a>;ï¼‰ net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;ã€&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;ï¼‰ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæœ€å…ˆè¿›çš„æ¨¡åž‹ä¸Žå­¦æœ¯åŸºå‡†ç»“æžœä¸ç¬¦ï¼Œå¹¶ä¸”åœ¨çŽ°å®žä¸–ç•Œä¸­çš„å‡†ç¡®æ€§è¦ä½Žå¾—å¤šã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ç»å¸¸ä½¿ç”¨æ–‡æ¡£ç†è§£æ¨¡åž‹çš„å…¸åž‹æ•°æ®é›†ä¸Žå­¦æœ¯åŸºå‡†è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ç¡®å®šäº†äº”ä¸ªæ•°æ®é›†è¦æ±‚ï¼Œä½¿æ•°æ®é›†èƒ½å¤Ÿæ›´å¥½åœ°æ•èŽ·çŽ°å®žåº”ç”¨ç¨‹åºçš„å¤æ‚æ€§ï¼š&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;ä¸°å¯Œæ¨¡å¼ï¼š&lt;/strong>;åœ¨å®žè·µä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å„ç§ç”¨äºŽç»“æž„åŒ–æå–çš„ä¸°å¯Œæ¨¡å¼ã€‚å®žä½“å…·æœ‰ä¸åŒçš„æ•°æ®ç±»åž‹ï¼ˆæ•°å­—ã€å­—ç¬¦ä¸²ã€æ—¥æœŸç­‰ï¼‰ï¼Œè¿™äº›æ•°æ®ç±»åž‹å¯èƒ½æ˜¯å¿…éœ€çš„ã€å¯é€‰çš„æˆ–åœ¨å•ä¸ªæ–‡æ¡£ä¸­é‡å¤ï¼Œç”šè‡³å¯èƒ½æ˜¯åµŒå¥—çš„ã€‚åƒï¼ˆæ ‡é¢˜ã€é—®é¢˜ã€ç­”æ¡ˆï¼‰è¿™æ ·çš„ç®€å•å¹³é¢æ¨¡å¼çš„æå–ä»»åŠ¡å¹¶ä¸èƒ½åæ˜ å®žè·µä¸­é‡åˆ°çš„å…¸åž‹é—®é¢˜ã€‚ &lt;/li>;&lt;li>;&lt;strong>;å¸ƒå±€ä¸°å¯Œçš„æ–‡æ¡£ï¼š&lt;/strong>;æ–‡æ¡£åº”è¯¥å…·æœ‰å¤æ‚çš„å¸ƒå±€å…ƒç´ ã€‚å®žé™…è®¾ç½®ä¸­çš„æŒ‘æˆ˜æ¥è‡ªä»¥ä¸‹äº‹å®žï¼šæ–‡æ¡£å¯èƒ½åŒ…å«è¡¨æ ¼ã€é”®å€¼å¯¹ã€åœ¨å•åˆ—å’ŒåŒåˆ—å¸ƒå±€ä¹‹é—´åˆ‡æ¢ã€ä¸åŒéƒ¨åˆ†å…·æœ‰ä¸åŒçš„å­—ä½“å¤§å°ã€åŒ…æ‹¬å¸¦æœ‰æ ‡é¢˜ç”šè‡³è„šæ³¨çš„å›¾ç‰‡ã€‚å°†æ­¤ä¸Žå¤§å¤šæ•°æ–‡æ¡£ä»¥å¥å­ã€æ®µè½å’Œå¸¦æœ‰èŠ‚æ ‡é¢˜çš„ç« èŠ‚ç»„ç»‡çš„æ•°æ®é›†è¿›è¡Œå¯¹æ¯” - è¿™äº›æ–‡æ¡£é€šå¸¸æ˜¯ &lt;a href=&quot;https://arxiv.org/abs ä¸Šç»å…¸è‡ªç„¶è¯­è¨€å¤„ç†æ–‡çŒ®çš„ç„¦ç‚¹/2007.14062&quot;>;é•¿&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2004.08483&quot;>;è¾“å…¥&lt;/a>;ã€‚ &lt;/li>;&lt;li>;&lt;strong>;å¤šæ ·åŒ–æ¨¡æ¿ï¼š&lt;/strong>;åŸºå‡†æµ‹è¯•åº”åŒ…æ‹¬ä¸åŒçš„ç»“æž„å¸ƒå±€æˆ–æ¨¡æ¿ã€‚å¯¹äºŽå¤§å®¹é‡æ¨¡åž‹æ¥è¯´ï¼Œé€šè¿‡è®°å¿†ç»“æž„ä»Žç‰¹å®šæ¨¡æ¿ä¸­æå–æ•°æ®æ˜¯å¾®ä¸è¶³é“çš„ã€‚ç„¶è€Œï¼Œåœ¨å®žè·µä¸­ï¼Œäººä»¬éœ€è¦èƒ½å¤ŸæŽ¨å¹¿åˆ°æ–°çš„æ¨¡æ¿/å¸ƒå±€ï¼Œè¿™æ˜¯åŸºå‡†æµ‹è¯•ä¸­çš„è®­ç»ƒ-æµ‹è¯•åˆ†å‰²åº”è¯¥è¡¡é‡çš„ä¸€ç§èƒ½åŠ›ã€‚ &lt;/li>;&lt;li>;&lt;strong>;é«˜è´¨é‡ OCR&lt;/strong>;ï¼šæ–‡æ¡£åº”å…·æœ‰é«˜è´¨é‡&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;å…‰å­¦å­—ç¬¦è¯†åˆ«&lt;/a>; (OCR) ç»“æžœã€‚æˆ‘ä»¬æ­¤åŸºå‡†æµ‹è¯•çš„ç›®æ ‡æ˜¯ä¸“æ³¨äºŽ VRDU ä»»åŠ¡æœ¬èº«ï¼Œå¹¶æŽ’é™¤ OCR å¼•æ“Žé€‰æ‹©å¸¦æ¥çš„å¯å˜æ€§ã€‚ &lt;/li>;&lt;li>;&lt;strong>;ä»¤ç‰Œçº§æ³¨é‡Š&lt;/strong>;ï¼šæ–‡æ¡£åº”åŒ…å«å¯ä»¥æ˜ å°„å›žç›¸åº”è¾“å…¥æ–‡æœ¬çš„çœŸå®žæ³¨é‡Šï¼Œä»¥ä¾¿æ¯ä¸ªä»¤ç‰Œéƒ½å¯ä»¥æ³¨é‡Šä¸ºç›¸åº”å®žä½“çš„ä¸€éƒ¨åˆ†ã€‚è¿™ä¸Žç®€å•åœ°æä¾›è¦ä¸ºå®žä½“æå–çš„å€¼çš„æ–‡æœ¬å½¢æˆå¯¹æ¯”ã€‚è¿™æ˜¯ç”Ÿæˆå¹²å‡€çš„è®­ç»ƒæ•°æ®çš„å…³é”®ï¼Œæˆ‘ä»¬ä¸å¿…æ‹…å¿ƒä¸Žç»™å®šå€¼çš„å¶ç„¶åŒ¹é…ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æ”¶æ®ä¸­ï¼Œå¦‚æžœç¨Žé¢ä¸ºé›¶ï¼Œâ€œç¨Žå‰æ€»è®¡â€å­—æ®µå¯èƒ½å…·æœ‰ä¸Žâ€œæ€»è®¡â€å­—æ®µç›¸åŒçš„å€¼ã€‚å…·æœ‰ä»¤ç‰Œçº§åˆ«æ³¨é‡Šä¼šé˜»æ­¢æˆ‘ä»¬ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå…¶ä¸­åŒ¹é…å€¼çš„ä¸¤ä¸ªå®žä¾‹éƒ½è¢«æ ‡è®°ä¸ºâ€œtotalâ€å­—æ®µçš„çœŸå®žå€¼ï¼Œä»Žè€Œäº§ç”Ÿå˜ˆæ‚çš„ç¤ºä¾‹ã€‚ &lt;/li>; &lt;/ul>; &lt;tablealign=â€œcenterâ€cellpadding=â€œ0â€cellspacing=â€œ0â€class=â€œtr-caption-containerâ€style=â€œmargin-leftï¼šè‡ªåŠ¨ï¼›margin-rightï¼šè‡ªåŠ¨ï¼›â€ >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn 5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s1600 /Benchmark%20GIF.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src =â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6Ai WX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s16000/Benchmark%20GIF.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;VRDU æ•°æ®é›†å’Œä»»åŠ¡&lt;/h2>; &lt;p>; VRDU æ•°æ®é›†æ˜¯ä¸¤ä¸ªå…¬å¼€å¯ç”¨æ•°æ®é›†çš„ç»„åˆï¼Œ&lt;a href=&quot;https://efile.40%;&quot;>; fara.gov/ords/fara/f?p=1235:10&quot;>;æ³¨å†Œè¡¨&lt;/a>;å’Œ&lt;a href=&quot;https://publicfiles.fcc.gov/&quot;>;å¹¿å‘Šè´­ä¹°è¡¨å•&lt;/a>;ã€‚è¿™äº›æ•°æ®é›†æä¾›äº†ä»£è¡¨çŽ°å®žä¸–ç•Œç”¨ä¾‹çš„ç¤ºä¾‹ï¼Œå¹¶æ»¡è¶³ä¸Šè¿°äº”ä¸ªåŸºå‡†è¦æ±‚ã€‚ &lt;/p>; &lt;p>; Ad-buy Forms æ•°æ®é›†åŒ…å« 641 ä¸ªåŒ…å«æ”¿æ²»å¹¿å‘Šè¯¦ç»†ä¿¡æ¯çš„æ–‡æ¡£ã€‚æ¯ä»½æ–‡ä»¶éƒ½æ˜¯ç”±ç”µè§†å°å’Œç«žé€‰å›¢é˜Ÿç­¾ç½²çš„å‘ç¥¨æˆ–æ”¶æ®ã€‚æ–‡æ¡£ä½¿ç”¨è¡¨æ ¼ã€å¤šåˆ—å’Œé”®å€¼å¯¹æ¥è®°å½•å¹¿å‘Šä¿¡æ¯ï¼Œä¾‹å¦‚äº§å“åç§°ã€æ’­æ”¾æ—¥æœŸã€æ€»ä»·ã€å‘å¸ƒæ—¥æœŸå’Œæ—¶é—´ã€‚ &lt;/p>; &lt;p>; ç™»è®°è¡¨æ•°æ®é›†åŒ…å« 1,915 ä¸ªæ–‡æ¡£ï¼Œå…¶ä¸­åŒ…å«æœ‰å…³å¤–å›½ä»£ç†äººåœ¨ç¾Žå›½æ”¿åºœç™»è®°çš„ä¿¡æ¯ã€‚æ¯ä»½æ–‡ä»¶éƒ½è®°å½•äº†æ¶‰åŠéœ€è¦å…¬å¼€æŠ«éœ²çš„æ´»åŠ¨çš„å¤–å›½ä»£ç†äººçš„åŸºæœ¬ä¿¡æ¯ã€‚å†…å®¹åŒ…æ‹¬æ³¨å†Œäººåç§°ã€ç›¸å…³éƒ¨é—¨åœ°å€ã€æ´»åŠ¨ç›®çš„ç­‰è¯¦ç»†ä¿¡æ¯ã€‚ &lt;/p>; &lt;p>; æˆ‘ä»¬ä»Žå…¬ä¼—&lt;a href=&quot;https://www.fcc.gov/&quot;>;è”é‚¦é€šä¿¡å§”å‘˜ä¼š&lt;/a>; (FCC) å’Œ&lt;a href=&quot; https://www.justice.gov/nsd-fara&quot;>;å¤–å›½ä»£ç†äººç™»è®°æ³•&lt;/a>; (FARA) ç½‘ç«™ï¼Œå¹¶ä½¿ç”¨ &lt;a href=&quot;https://cloud.google.com/ å°†å›¾åƒè½¬æ¢ä¸ºæ–‡æœ¬&quot;>;Google Cloud çš„&lt;/a>; &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;OCR&lt;/a>;ã€‚æˆ‘ä»¬ä¸¢å¼ƒäº†å°‘é‡å‡ é¡µé•¿çš„æ–‡æ¡£ï¼Œå¹¶ä¸”å¤„ç†åœ¨ä¸¤åˆ†é’Ÿå†…æ²¡æœ‰å®Œæˆã€‚è¿™ä¹Ÿä½¿æˆ‘ä»¬èƒ½å¤Ÿé¿å…å‘é€å¾ˆé•¿çš„æ–‡æ¡£è¿›è¡Œæ‰‹åŠ¨æ³¨é‡Šâ€”â€”å¯¹äºŽå•ä¸ªæ–‡æ¡£æ¥è¯´ï¼Œè¿™é¡¹ä»»åŠ¡å¯èƒ½éœ€è¦ä¸€ä¸ªå¤šå°æ—¶ã€‚ç„¶åŽï¼Œæˆ‘ä»¬ä¸ºå…·æœ‰æ–‡æ¡£æ ‡è®°ä»»åŠ¡ç»éªŒçš„æ³¨é‡Šè€…å›¢é˜Ÿå®šä¹‰äº†æ¨¡å¼å’Œç›¸åº”çš„æ ‡è®°æŒ‡ä»¤ã€‚ &lt;/p>; &lt;p>; è¿˜å‘æ³¨é‡Šè€…æä¾›äº†ä¸€äº›æˆ‘ä»¬è‡ªå·±æ ‡è®°çš„ç¤ºä¾‹æ ‡è®°æ–‡æ¡£ã€‚è¯¥ä»»åŠ¡è¦æ±‚æ³¨é‡Šè€…æ£€æŸ¥æ¯ä¸ªæ–‡æ¡£ï¼Œå›´ç»•æ¯ä¸ªæ–‡æ¡£æž¶æž„ä¸­å®žä½“çš„æ¯æ¬¡å‡ºçŽ°ç»˜åˆ¶è¾¹ç•Œæ¡†ï¼Œå¹¶å°†è¯¥è¾¹ç•Œæ¡†ä¸Žç›®æ ‡å®žä½“ç›¸å…³è”ã€‚ç¬¬ä¸€è½®æ ‡æ³¨åŽï¼Œä¸€ç»„ä¸“å®¶è¢«æŒ‡æ´¾å¯¹ç»“æžœè¿›è¡Œå®¡æ ¸ã€‚ä¿®æ­£åŽçš„ç»“æžœåŒ…å«åœ¨å·²å‘å¸ƒçš„ VRDU æ•°æ®é›†ä¸­ã€‚æœ‰å…³æ ‡è®°åè®®å’Œæ¯ä¸ªæ•°æ®é›†æž¶æž„çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;è®ºæ–‡&lt;/a>;ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG 2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s1600/Chart.png&quot;æ ·å¼=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i7 2av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s16000/Chart.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;çŽ°æœ‰å­¦æœ¯åŸºå‡† (&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;ã€&lt;a href=&quot;https://openreview.net/ pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;ã€&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;ã€&lt;a href=&quot;https:// /arxiv.org/abs/2105.05796&quot;>;Kleister-NDA&lt;/a>;ã€&lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-Charity&lt;/a>;ã€&lt;a href=&quot;https ://wandb.ai/stacey/deepform_v1/reports/DeepForm-Understand-Structured-Documents-at-Scale--VmlldzoyODQ3Njg&quot;>;DeepForm&lt;/a>;ï¼‰æœªè¾¾åˆ°æˆ‘ä»¬ä¸ºé¡¹ç›®ç¡®å®šçš„äº”é¡¹è¦æ±‚ä¸­çš„ä¸€é¡¹æˆ–å¤šé¡¹è‰¯å¥½çš„æ–‡æ¡£ç†è§£åŸºå‡†ã€‚ VRDU æ»¡è¶³äº†æ‰€æœ‰è¿™äº›è¦æ±‚ã€‚è¯·å‚é˜…æˆ‘ä»¬çš„&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;è®ºæ–‡&lt;/a>;ï¼Œäº†è§£æ¯ä¸ªæ•°æ®é›†çš„èƒŒæ™¯ä»¥åŠå®ƒä»¬å¦‚ä½•æœªèƒ½æ»¡è¶³ä¸€é¡¹æˆ–å¤šé¡¹è¦æ±‚çš„è®¨è®ºã€‚&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; æˆ‘ä»¬æž„å»ºäº†å››ä¸ªä¸åŒçš„æ¨¡åž‹è®­ç»ƒé›†ï¼Œåˆ†åˆ«æœ‰ 10ã€50ã€100 å’Œ 200 ä¸ªæ ·æœ¬ã€‚ç„¶åŽï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªä»»åŠ¡ï¼ˆå¦‚ä¸‹æ‰€è¿°ï¼‰è¯„ä¼° VRDU æ•°æ®é›†ï¼š(1) å•æ¨¡æ¿å­¦ä¹ ã€(2) æ··åˆæ¨¡æ¿å­¦ä¹ å’Œ (3) çœ‹ä¸è§çš„æ¨¡æ¿å­¦ä¹ ã€‚å¯¹äºŽæ¯é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸­åŒ…å« 300 ä¸ªæ–‡æ¡£ã€‚æˆ‘ä»¬ä½¿ç”¨æµ‹è¯•é›†ä¸Šçš„ &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1 åˆ†æ•°&lt;/a>; æ¥è¯„ä¼°æ¨¡åž‹ã€‚ &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;å•æ¨¡æ¿å­¦ä¹ &lt;/em>; (STL)ï¼šè¿™æ˜¯æœ€ç®€å•çš„åœºæ™¯ï¼Œå…¶ä¸­è®­ç»ƒã€æµ‹è¯•å’ŒéªŒè¯é›†ä»…åŒ…å«å•ä¸ªæ¨¡æ¿ã€‚è¿™ä¸ªç®€å•çš„ä»»åŠ¡æ—¨åœ¨è¯„ä¼°æ¨¡åž‹å¤„ç†å›ºå®šæ¨¡æ¿çš„èƒ½åŠ›ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬æœŸæœ›æ­¤ä»»åŠ¡çš„ F1 åˆ†æ•°éžå¸¸é«˜ï¼ˆ0.90+ï¼‰ã€‚ &lt;/li>;&lt;li>;&lt;em>;æ··åˆæ¨¡æ¿å­¦ä¹ &lt;/em>;ï¼ˆMTLï¼‰ï¼šæ­¤ä»»åŠ¡ä¸Žå¤§å¤šæ•°ç›¸å…³è®ºæ–‡ä½¿ç”¨çš„ä»»åŠ¡ç±»ä¼¼ï¼šè®­ç»ƒé›†ã€æµ‹è¯•é›†å’ŒéªŒè¯é›†éƒ½åŒ…å«å±žäºŽåŒä¸€é›†çš„æ–‡æ¡£æ¨¡æ¿ã€‚æˆ‘ä»¬ä»Žæ•°æ®é›†ä¸­éšæœºæŠ½å–æ–‡æ¡£å¹¶æž„å»ºåˆ†å‰²ï¼Œä»¥ç¡®ä¿æ¯ä¸ªæ¨¡æ¿çš„åˆ†å¸ƒåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ä¸ä¼šæ”¹å˜ã€‚ &lt;/li>;&lt;li>;&lt;em>;æœªè§æ¨¡æ¿å­¦ä¹ &lt;/em>;ï¼ˆUTLï¼‰ï¼šè¿™æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„è®¾ç½®ï¼Œæˆ‘ä»¬è¯„ä¼°æ¨¡åž‹æ˜¯å¦å¯ä»¥æ³›åŒ–åˆ°æœªè§æ¨¡æ¿ã€‚ä¾‹å¦‚ï¼Œåœ¨æ³¨å†Œè¡¨æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªæ¨¡æ¿ä¸­çš„ä¸¤ä¸ªæ¥è®­ç»ƒæ¨¡åž‹ï¼Œå¹¶ä½¿ç”¨å…¶ä½™ä¸€ä¸ªæ¥æµ‹è¯•æ¨¡åž‹ã€‚è®­ç»ƒã€æµ‹è¯•å’ŒéªŒè¯é›†ä¸­çš„æ–‡æ¡£æ¥è‡ªä¸ç›¸äº¤çš„æ¨¡æ¿é›†ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œä»¥å‰çš„åŸºå‡†æµ‹è¯•å’Œæ•°æ®é›†æ²¡æœ‰æ˜Žç¡®æä¾›è¿™æ ·çš„ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡åž‹æ³›åŒ–åˆ°è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ¨¡æ¿çš„èƒ½åŠ›ã€‚ &lt;/li>; &lt;/ul>; &lt;p>; ç›®æ ‡æ˜¯èƒ½å¤Ÿè¯„ä¼°æ¨¡åž‹çš„æ•°æ®æ•ˆçŽ‡ã€‚åœ¨æˆ‘ä»¬çš„&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;è®ºæ–‡&lt;/a>;ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä½¿ç”¨ STLã€MTL å’Œ UTL ä»»åŠ¡çš„ä¸¤ä¸ªæœ€æ–°æ¨¡åž‹ï¼Œå¹¶æå‡ºäº†ä¸‰ä¸ªè§‚å¯Ÿç»“æžœã€‚é¦–å…ˆï¼Œä¸Žå…¶ä»–åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒVRDU å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶è¡¨æ˜Žæ¨¡åž‹æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¡¨æ˜Žï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡åž‹ï¼Œfew-shot æ€§èƒ½ä¹Ÿä½Žå¾—æƒŠäººï¼Œå³ä½¿æ˜¯æœ€å¥½çš„æ¨¡åž‹ï¼Œå…¶ F1 åˆ†æ•°ä¹Ÿä½ŽäºŽ 0.60ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è¡¨æ˜Žæ¨¡åž‹å¾ˆéš¾å¤„ç†ç»“æž„åŒ–çš„é‡å¤å­—æ®µï¼Œå¹¶ä¸”åœ¨è¿™äº›å­—æ®µä¸Šçš„è¡¨çŽ°ç‰¹åˆ«å·®ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®º&lt;/h2>; &lt;p>; æˆ‘ä»¬å‘å¸ƒäº†æ–°çš„&lt;a href=&quot;https:// Research.google/resources/datasets/visually-rich-document-understanding/&quot;>;è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£&lt;/a>; (VRDU) æ•°æ®é›†ï¼Œå¯å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°è·Ÿè¸ªæ–‡æ¡£ç†è§£ä»»åŠ¡çš„è¿›åº¦ã€‚æˆ‘ä»¬æè¿°äº†ä¸ºä»€ä¹ˆ VRDU æ›´å¥½åœ°åæ˜ äº†è¯¥é¢†åŸŸçš„å®žé™…æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å®žéªŒï¼Œè¡¨æ˜Ž VRDU ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”ä¸Žæ–‡çŒ®ä¸­é€šå¸¸ä½¿ç”¨çš„æ•°æ®é›†ï¼ˆå…¸åž‹çš„ F1 åˆ†æ•°ä¸º 0.90+ï¼‰ç›¸æ¯”ï¼Œæœ€è¿‘çš„æ¨¡åž‹æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬å¸Œæœ› VRDU æ•°æ®é›†å’Œè¯„ä¼°ä»£ç çš„å‘å¸ƒæœ‰åŠ©äºŽç ”ç©¶å›¢é˜ŸæŽ¨è¿›æ–‡æ¡£ç†è§£çš„æœ€æ–°æŠ€æœ¯ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;éžå¸¸æ„Ÿè°¢çŽ‹å­é¾™ã€å‘¨ä¸€è¶…ã€é­Wei å’Œ Chen-Yu Lee ä¸Ž Sandeep Tata å…±åŒæ’°å†™äº†è¿™ç¯‡è®ºæ–‡ã€‚æ„Ÿè°¢ Marc Najorkã€Riham Mansour ä»¥åŠ Google Research å’Œ Cloud AI å›¢é˜Ÿçš„ä¼—å¤šåˆä½œä¼™ä¼´æä¾›äº†å®è´µçš„è§è§£ã€‚æ„Ÿè°¢ John Guilyard åˆ›å»ºæœ¬æ–‡ä¸­çš„åŠ¨ç”»ã€‚ &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3240052415427459327/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/advances-in-document-understanding.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 æ¡è¯„è®ºâ€œ type =â€œtext / htmlâ€/>;&lt;link href =â€œhttp://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327â€rel =â€œeditâ€type =â€œapplication/atom+xmlâ€/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/08/advances-in-document-understanding.html&quot; rel=&quot;alternate&quot; title=&quot;æ–‡æ¡£ç†è§£æ–¹é¢çš„è¿›å±•&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Googleäººå·¥æ™ºèƒ½&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http ://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/ä½œè€…>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV 8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s72-c/Document%20understanding%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt; id>;æ ‡ç­¾ï¼šblogger.comï¼Œ1999ï¼šblog-8474926331452026626.post-595171581401765238&lt;/id>;&lt;å‘å¸ƒ>;2023-08-08T14:02:00.000-07:00&lt;/å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-08T14ï¼š 02:34.659-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;æ·±åº¦å­¦ä¹ &quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/ category>;&lt;title type=&quot;text&quot;>;AdaTapeï¼šå…·æœ‰è‡ªé€‚åº”è®¡ç®—å’ŒåŠ¨æ€è¯»å†™çš„åŸºç¡€æ¨¡åž‹&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šè–›ç¦å…†ã€ç ”ç©¶å®žä¹ ç”Ÿå’Œç ”ç©¶ç§‘å­¦å®¶ Mostafa Dehghaniï¼ŒGoogle&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIq JhdQrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudIEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s2000/adatape.png&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›â€ />; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;>;è‡ªé€‚åº”è®¡ç®—&lt;/a>;æ˜¯æŒ‡æœºå™¨å­¦ä¹ ç³»ç»Ÿæ ¹æ®çŽ¯å¢ƒå˜åŒ–è°ƒæ•´å…¶è¡Œä¸ºçš„èƒ½åŠ›ã€‚è™½ç„¶ä¼ ç»Ÿçš„ç¥žç»ç½‘ç»œå…·æœ‰å›ºå®šçš„åŠŸèƒ½å’Œè®¡ç®—èƒ½åŠ›ï¼Œå³å®ƒä»¬èŠ±è´¹ç›¸åŒæ•°é‡çš„ FLOP æ¥å¤„ç†ä¸åŒçš„è¾“å…¥ï¼Œä½†å…·æœ‰è‡ªé€‚åº”å’ŒåŠ¨æ€è®¡ç®—çš„æ¨¡åž‹ä¼šæ ¹æ®è¾“å…¥çš„å¤æ‚æ€§æ¥è°ƒæ•´å…¶ä¸“ç”¨äºŽå¤„ç†æ¯ä¸ªè¾“å…¥çš„è®¡ç®—é¢„ç®—ã€‚è¾“å…¥ã€‚ &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ç¥žç»ç½‘ç»œä¸­çš„è‡ªé€‚åº”è®¡ç®—å…·æœ‰å¸å¼•åŠ›æœ‰ä¸¤ä¸ªå…³é”®åŽŸå› ã€‚é¦–å…ˆï¼Œå¼•å…¥é€‚åº”æ€§çš„æœºåˆ¶æä¾›äº†å½’çº³åå·®ï¼Œå¯ä»¥åœ¨è§£å†³ä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œä¸ºä¸åŒçš„è¾“å…¥å¯ç”¨ä¸åŒæ•°é‡çš„è®¡ç®—æ­¥éª¤å¯¹äºŽè§£å†³éœ€è¦ä¸åŒæ·±åº¦çš„å»ºæ¨¡å±‚æ¬¡ç»“æž„çš„ç®—æœ¯é—®é¢˜è‡³å…³é‡è¦ã€‚å…¶æ¬¡ï¼Œå®ƒä½¿ä»Žä¸šè€…èƒ½å¤Ÿé€šè¿‡åŠ¨æ€è®¡ç®—æä¾›çš„æ›´å¤§çµæ´»æ€§æ¥è°ƒæ•´æŽ¨ç†æˆæœ¬ï¼Œå› ä¸ºå¯ä»¥è°ƒæ•´è¿™äº›æ¨¡åž‹ä»¥èŠ±è´¹æ›´å¤šçš„ FLOP æ¥å¤„ç†æ–°è¾“å…¥ã€‚ &lt;/p>; &lt;p>; ç¥žç»ç½‘ç»œå¯ä»¥é€šè¿‡å¯¹å„ç§è¾“å…¥ä½¿ç”¨ä¸åŒçš„å‡½æ•°æˆ–è®¡ç®—é¢„ç®—æ¥å®žçŽ°è‡ªé€‚åº”ã€‚æ·±åº¦ç¥žç»ç½‘ç»œå¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªåŸºäºŽè¾“å…¥åŠå…¶å‚æ•°è¾“å‡ºç»“æžœçš„å‡½æ•°ã€‚ä¸ºäº†å®žçŽ°è‡ªé€‚åº”å‡½æ•°ç±»åž‹ï¼Œæ ¹æ®è¾“å…¥é€‰æ‹©æ€§åœ°æ¿€æ´»å‚æ•°å­é›†ï¼Œè¿™ä¸€è¿‡ç¨‹ç§°ä¸ºæ¡ä»¶è®¡ç®—ã€‚åœ¨ &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;mixture-of-experts&lt;/a>; çš„ç ”ç©¶ä¸­æŽ¢ç´¢äº†åŸºäºŽå‡½æ•°ç±»åž‹çš„è‡ªé€‚åº”æ€§ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å…¥çš„ç¨€ç–æ¿€æ´»å‚æ•°æ ·æœ¬æ˜¯é€šè¿‡è·¯ç”±ç¡®å®šçš„ã€‚è‡ªé€‚åº”è®¡ç®—çš„å¦ä¸€ä¸ªç ”ç©¶é¢†åŸŸæ¶‰åŠåŠ¨æ€è®¡ç®—é¢„ç®—ã€‚ä¸Žæ ‡å‡†ç¥žç»ç½‘ç»œï¼ˆä¾‹å¦‚ &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;ï¼‰ä¸åŒï¼Œ&lt;a href= &quot;https://arxiv.org/abs/2005.14165&quot;>;GPT-3&lt;/a>;ã€&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling -to.html&quot;>;PaLM&lt;/a>; å’Œ &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;ViT&lt;/a>;å…¶è®¡ç®—é¢„ç®—å¯¹äºŽä¸åŒçš„æ ·æœ¬æ˜¯å›ºå®šçš„ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;æœ€è¿‘çš„ç ”ç©¶&lt;/a>;è¡¨æ˜Žï¼Œè‡ªé€‚åº”è®¡ç®—é¢„ç®—å¯ä»¥æé«˜ Transformer ä¸è¶³çš„ä»»åŠ¡çš„æ€§èƒ½ã€‚å…¶ä¸­è®¸å¤šå·¥ä½œé€šè¿‡ä½¿ç”¨åŠ¨æ€æ·±åº¦æ¥åˆ†é…è®¡ç®—é¢„ç®—æ¥å®žçŽ°è‡ªé€‚åº”æ€§ã€‚ä¾‹å¦‚ï¼Œæå‡ºäº†è‡ªé€‚åº”è®¡ç®—æ—¶é—´ï¼ˆACTï¼‰ç®—æ³•æ¥ä¸ºé€’å½’ç¥žç»ç½‘ç»œæä¾›è‡ªé€‚åº”è®¡ç®—é¢„ç®—ã€‚ &lt;a href=&quot;https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html&quot;>;é€šç”¨è½¬æ¢å™¨&lt;/a>;å°† ACT ç®—æ³•æ‰©å±•åˆ°è½¬æ¢å™¨ï¼Œä½¿è®¡ç®—é¢„ç®—å–å†³äºŽç”¨äºŽæ¯ä¸ªè¾“å…¥ç¤ºä¾‹æˆ–æ ‡è®°çš„è½¬æ¢å™¨å±‚æ•°ã€‚æœ€è¿‘çš„ç ”ç©¶ï¼Œä¾‹å¦‚ &lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;PonderNet&lt;/a>;ï¼Œåœ¨æ”¹è¿›åŠ¨æ€åœæ­¢æœºåˆ¶çš„åŒæ—¶ä¹Ÿéµå¾ªç±»ä¼¼çš„æ–¹æ³•ã€‚ &lt;/p>; &lt;p>; åœ¨è®ºæ–‡â€œ&lt;a href=&quot;https://arxiv.org/abs/2301.13195&quot;>;å…·æœ‰å¼¹æ€§è¾“å…¥åºåˆ—çš„è‡ªé€‚åº”è®¡ç®—&lt;/a>;â€ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ©ç”¨è‡ªé€‚åº”è®¡ç®—çš„æ–°æ¨¡åž‹ï¼Œç§°ä¸º&lt;em>;AdaTape&lt;/em>;ã€‚è¯¥æ¨¡åž‹æ˜¯åŸºäºŽ Transformer çš„æž¶æž„ï¼Œä½¿ç”¨ä¸€ç»„åŠ¨æ€ä»¤ç‰Œæ¥åˆ›å»ºå¼¹æ€§è¾“å…¥åºåˆ—ï¼Œä¸Žä»¥å‰çš„ä½œå“ç›¸æ¯”ï¼Œæä¾›äº†å…³äºŽé€‚åº”æ€§çš„ç‹¬ç‰¹è§†è§’ã€‚ AdaTape ä½¿ç”¨è‡ªé€‚åº”ç£å¸¦è¯»å–æœºåˆ¶æ¥æ ¹æ®è¾“å…¥çš„å¤æ‚æ€§ç¡®å®šæ·»åŠ åˆ°æ¯ä¸ªè¾“å…¥çš„ä¸åŒæ•°é‡çš„ç£å¸¦æ ‡è®°ã€‚ AdaTape å®žçŽ°èµ·æ¥éžå¸¸ç®€å•ï¼Œæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„æ—‹é’®æ¥åœ¨éœ€è¦æ—¶æé«˜å‡†ç¡®æ€§ï¼Œè€Œä¸”ä¸Ž&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;å…¶ä»–è‡ªé€‚åº”åŸºçº¿&lt;/aç›¸æ¯”ä¹Ÿæ›´åŠ é«˜æ•ˆ>; å› ä¸ºå®ƒç›´æŽ¥å°†è‡ªé€‚åº”æ€§æ³¨å…¥åˆ°è¾“å…¥åºåˆ—ä¸­ï¼Œè€Œä¸æ˜¯æ¨¡åž‹æ·±åº¦ä¸­ã€‚æœ€åŽï¼ŒAdatape åœ¨æ ‡å‡†ä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒåˆ†ç±»ï¼‰ä»¥åŠç®—æ³•ä»»åŠ¡ä¸Šæä¾›æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„è´¨é‡å’Œæˆæœ¬æƒè¡¡ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;å…·æœ‰å¼¹æ€§è¾“å…¥åºåˆ—çš„è‡ªé€‚åº”è®¡ç®—è½¬æ¢å™¨&lt;/h2>; &lt;p>; AdaTape ä½¿ç”¨ä¸¤ç§è‡ªé€‚åº”å‡½æ•°ç±»åž‹å’ŒåŠ¨æ€è®¡ç®—é¢„ç®—ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºŽæ ‡è®°åŒ–åŽçš„ä¸€æ‰¹è¾“å…¥åºåˆ—ï¼ˆä¾‹å¦‚ï¼Œæ¥è‡ªè§†è§‰å˜æ¢å™¨ä¸­çš„å›¾åƒçš„éžé‡å è¡¥ä¸çš„çº¿æ€§æŠ•å½±ï¼‰ï¼ŒAdaTape ä½¿ç”¨è¡¨ç¤ºæ¯ä¸ªè¾“å…¥çš„å‘é‡æ¥åŠ¨æ€é€‰æ‹©å¯å˜å¤§å°çš„ç£å¸¦æ ‡è®°åºåˆ—ã€‚ &lt;/p>; &lt;p>; AdaTape ä½¿ç”¨ç§°ä¸ºâ€œç£å¸¦åº“â€çš„ä»¤ç‰Œåº“æ¥å­˜å‚¨é€šè¿‡è‡ªé€‚åº”ç£å¸¦è¯»å–æœºåˆ¶ä¸Žæ¨¡åž‹äº¤äº’çš„æ‰€æœ‰å€™é€‰ç£å¸¦ä»¤ç‰Œã€‚æˆ‘ä»¬æŽ¢ç´¢äº†ä¸¤ç§ä¸åŒçš„åˆ›å»ºç£å¸¦åº“çš„æ–¹æ³•ï¼šè¾“å…¥é©±åŠ¨çš„ç£å¸¦åº“å’Œå¯å­¦ä¹ çš„ç£å¸¦åº“ã€‚ &lt;/p>; &lt;p>; è¾“å…¥é©±åŠ¨åº“çš„æ€»ä½“æ€æƒ³æ˜¯ä»Žè¾“å…¥ä¸­æå–ä¸€ç»„ä»¤ç‰Œï¼ŒåŒæ—¶é‡‡ç”¨ä¸ŽåŽŸå§‹æ¨¡åž‹ä»¤ç‰Œç”Ÿæˆå™¨ä¸åŒçš„æ–¹æ³•å°†åŽŸå§‹è¾“å…¥æ˜ å°„åˆ°è¾“å…¥æ ‡è®°çš„åºåˆ—ã€‚è¿™ä½¿å¾—èƒ½å¤ŸåŠ¨æ€åœ°æŒ‰éœ€è®¿é—®æ¥è‡ªä½¿ç”¨ä¸åŒè§‚ç‚¹ï¼ˆä¾‹å¦‚ï¼Œä¸åŒå›¾åƒåˆ†è¾¨çŽ‡æˆ–ä¸åŒæŠ½è±¡çº§åˆ«ï¼‰èŽ·å¾—çš„è¾“å…¥çš„ä¿¡æ¯ã€‚ &lt;/p>; &lt;p>; åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸åŒæŠ½è±¡çº§åˆ«çš„æ ‡è®°åŒ–æ˜¯ä¸å¯èƒ½çš„ï¼Œå› æ­¤è¾“å…¥é©±åŠ¨çš„ç£å¸¦åº“æ˜¯ä¸å¯è¡Œçš„ï¼Œä¾‹å¦‚å½“å¾ˆéš¾è¿›ä¸€æ­¥æ‹†åˆ† &lt;a href=&quot; ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æ—¶https://arxiv.org/abs/2012.09699&quot;>;å›¾å½¢è½¬æ¢å™¨&lt;/a>;ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒAdaTape æä¾›äº†ä¸€ç§æ›´é€šç”¨çš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ä¸€ç»„å¯è®­ç»ƒå‘é‡ä½œä¸ºç£å¸¦ä»¤ç‰Œæ¥ç”Ÿæˆç£å¸¦åº“ã€‚è¿™ç§æ–¹æ³•è¢«ç§°ä¸ºå¯å­¦ä¹ åº“ï¼Œå¯ä»¥è¢«è§†ä¸ºåµŒå…¥å±‚ï¼Œå…¶ä¸­æ¨¡åž‹å¯ä»¥æ ¹æ®è¾“å…¥ç¤ºä¾‹çš„å¤æ‚æ€§åŠ¨æ€æ£€ç´¢æ ‡è®°ã€‚å¯å­¦ä¹ çš„é“¶è¡Œä½¿ AdaTape èƒ½å¤Ÿç”Ÿæˆæ›´çµæ´»çš„ç£å¸¦åº“ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªè¾“å…¥ç¤ºä¾‹çš„å¤æ‚æ€§åŠ¨æ€è°ƒæ•´å…¶è®¡ç®—é¢„ç®—ï¼Œä¾‹å¦‚ï¼Œæ›´å¤æ‚çš„ç¤ºä¾‹ä»Žç£å¸¦åº“ä¸­æ£€ç´¢æ›´å¤šçš„ä»¤ç‰Œï¼Œè¿™ä½¿å¾—æ¨¡åž‹ä¸ä¼šåªä½¿ç”¨å­˜å‚¨åœ¨é“¶è¡Œä¸­çš„çŸ¥è¯†ï¼Œä½†ç”±äºŽè¾“å…¥çŽ°åœ¨æ›´å¤§ï¼Œæ‰€ä»¥è¿˜è¦èŠ±è´¹æ›´å¤šçš„ FLOP æ¥å¤„ç†å®ƒã€‚ &lt;/p>; &lt;p>; æœ€åŽï¼Œé€‰å®šçš„ç£å¸¦æ ‡è®°å°†é™„åŠ åˆ°åŽŸå§‹è¾“å…¥å¹¶é¦ˆé€åˆ°ä»¥ä¸‹è½¬æ¢å™¨å±‚ã€‚å¯¹äºŽæ¯ä¸ªè½¬æ¢å™¨å±‚ï¼Œåœ¨æ‰€æœ‰è¾“å…¥å’Œç£å¸¦æ ‡è®°ä¸Šä½¿ç”¨ç›¸åŒçš„å¤šå¤´æ³¨æ„åŠ›ã€‚ç„¶è€Œï¼Œä½¿ç”¨äº†ä¸¤ç§ä¸åŒçš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼šä¸€ç§ç”¨äºŽæ¥è‡ªåŽŸå§‹è¾“å…¥çš„æ‰€æœ‰ä»¤ç‰Œï¼Œå¦ä¸€ç§ç”¨äºŽæ‰€æœ‰ç£å¸¦ä»¤ç‰Œã€‚é€šè¿‡å¯¹è¾“å…¥å’Œç£å¸¦ä»¤ç‰Œä½¿ç”¨å•ç‹¬çš„å‰é¦ˆç½‘ç»œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è´¨é‡ç¨å¥½ä¸€äº›ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfda EU1-w8kv7USzuc-VBwGumvHxfccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s1786/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;871&quot; data-original-width=&quot;1786&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfdaEU1-w8kv7USzuc-VBwGumvHxf ccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td ç±»=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AdaTape æ¦‚è¿°ã€‚å¯¹äºŽä¸åŒçš„æ ·æœ¬ï¼Œæˆ‘ä»¬ä»Žç£å¸¦åº“ä¸­æŒ‘é€‰ä¸åŒæ•°é‡çš„ä¸åŒæ ‡è®°ã€‚ç£å¸¦åº“å¯ä»¥ç”±è¾“å…¥é©±åŠ¨ï¼Œä¾‹å¦‚é€šè¿‡æå–ä¸€äº›é¢å¤–çš„ç»†ç²’åº¦ä¿¡æ¯ï¼Œæˆ–è€…å®ƒå¯ä»¥æ˜¯ä¸€ç»„å¯è®­ç»ƒå‘é‡ã€‚è‡ªé€‚åº”ç£å¸¦è¯»å–ç”¨äºŽé’ˆå¯¹ä¸åŒçš„è¾“å…¥é€’å½’åœ°é€‰æ‹©ä¸åŒé•¿åº¦çš„ç£å¸¦ä»¤ç‰Œåºåˆ—ã€‚ç„¶åŽå°†è¿™äº›æ ‡è®°ç®€å•åœ°é™„åŠ åˆ°è¾“å…¥å¹¶é¦ˆé€åˆ°å˜åŽ‹å™¨ç¼–ç å™¨ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;AdaTape æä¾›æœ‰ç”¨çš„å½’çº³åå·®&lt;/h2>; &lt;p>; æˆ‘ä»¬åœ¨å¥‡å¶æ ¡éªŒä¸Šè¯„ä¼° AdaTapeï¼Œè¿™å¯¹äºŽæ ‡å‡† Transformer æ¥è¯´æ˜¯ä¸€é¡¹éžå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä»¥ç ”ç©¶ AdaTape ä¸­å½’çº³åå·®çš„å½±å“ã€‚å¯¹äºŽå¥‡å¶æ ¡éªŒä»»åŠ¡ï¼Œç»™å®šåºåˆ— 1ã€0 å’Œ -1ï¼Œæ¨¡åž‹å¿…é¡»é¢„æµ‹åºåˆ—ä¸­ 1 æ•°é‡çš„å¶æ•°æˆ–å¥‡æ•°ã€‚ Parity æ˜¯æœ€ç®€å•çš„éžè®¡æ•°å™¨è‡ªç”±æˆ–&lt;a href=&quot;https://arxiv.org/abs/2009.11264&quot;>;å‘¨æœŸæ€§æ­£åˆ™è¯­è¨€&lt;/a>;ï¼Œä½†ä¹Ÿè®¸ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ ‡å‡† Transformer æ— æ³•è§£å†³è¯¥ä»»åŠ¡ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3 EoYw4UhzcKK0fGIscOJSv36zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/s866/image4.png&quot;æ ·å¼=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;432&quot; data-original-width=&quot;866&quot; height=&quot;319&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3EoYw4UhzcKK0fGIscOJSv36 zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/w640-h319/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;å¯¹å¥‡å¶æ ¡éªŒä»»åŠ¡çš„è¯„ä¼°ã€‚æ ‡å‡† Transformer å’Œ Universal Transformer æ— æ³•æ‰§è¡Œæ­¤ä»»åŠ¡ï¼Œä¸¤è€…éƒ½æ˜¾ç¤ºå‡ºéšæœºçŒœæµ‹åŸºçº¿æ°´å¹³çš„æ€§èƒ½ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;å°½ç®¡è¿›è¡Œäº†çŸ­æœŸè¯„ä¼°ï¼Œå¯¹äºŽç®€å•çš„åºåˆ—ï¼Œæ ‡å‡† Transformer å’Œé€šç”¨ Transformer éƒ½æ— æ³•æ‰§è¡Œå¥‡å¶æ ¡éªŒä»»åŠ¡ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•åœ¨æ¨¡åž‹ä¸­ç»´æŠ¤è®¡æ•°å™¨ã€‚ç„¶è€Œï¼ŒAdaTape çš„æ€§èƒ½ä¼˜äºŽæ‰€æœ‰åŸºçº¿ï¼Œå› ä¸ºå®ƒåœ¨å…¶è¾“å…¥é€‰æ‹©æœºåˆ¶ä¸­èžå…¥äº†è½»é‡çº§å¾ªçŽ¯ï¼Œæä¾›äº†å½’çº³åå·®ï¼Œå¯ä»¥å®žçŽ°è®¡æ•°å™¨çš„éšå¼ç»´æŠ¤ï¼Œè¿™åœ¨æ ‡å‡† Transformer ä¸­æ˜¯ä¸å¯èƒ½çš„ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;å›¾åƒåˆ†ç±»è¯„ä¼°&lt;/h2>; &lt;p>; æˆ‘ä»¬è¿˜åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¯„ä¼° AdaTapeã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»Žå¤´å¼€å§‹åœ¨ &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet-1K&lt;/a>; ä¸Šè®­ç»ƒ AdaTapeã€‚ä¸‹å›¾æ˜¾ç¤ºäº† AdaTape å’ŒåŸºçº¿æ–¹æ³•çš„å‡†ç¡®æ€§ï¼ŒåŒ…æ‹¬ &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>; å’Œ Universal Transformer ViTï¼ˆUViT å’Œ U2Tï¼‰ä¸Žå®ƒä»¬çš„é€Ÿåº¦ï¼ˆä»¥æ¯ç§’ç”±æ¯ä¸ªä»£ç å¤„ç†çš„å›¾åƒæ•°é‡æ¥è¡¡é‡ï¼‰ã€‚åœ¨è´¨é‡å’Œæˆæœ¬æƒè¡¡æ–¹é¢ï¼ŒAdaTape çš„æ€§èƒ½æ¯”æ›¿ä»£è‡ªé€‚åº”å˜åŽ‹å™¨åŸºçº¿è¦å¥½å¾—å¤šã€‚å°±æ•ˆçŽ‡è€Œè¨€ï¼Œè¾ƒå¤§çš„ AdaTape æ¨¡åž‹ï¼ˆå°±å‚æ•°æ•°é‡è€Œè¨€ï¼‰æ¯”è¾ƒå°çš„åŸºçº¿æ›´å¿«ã€‚è¿™äº›ç»“æžœä¸Ž&lt;a href=&quot;https://arxiv.org/abs/2110.12894&quot;>;ä¹‹å‰çš„å·¥ä½œ&lt;/a>;çš„å‘çŽ°ä¸€è‡´ï¼Œè¯¥å‘çŽ°è¡¨æ˜Žè‡ªé€‚åº”æ¨¡åž‹æ·±åº¦æž¶æž„ä¸å¤ªé€‚åˆè®¸å¤šåŠ é€Ÿå™¨ï¼Œä¾‹å¦‚çƒ­å¡‘æ€§èšæ°¨é…¯ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7Z Ih6Rujxnd-H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H -Hxzek9ziXZsw5adSEMZU/s1473/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;657&quot; data-original-width=&quot;1473 â€œé«˜åº¦=â€œ285â€src=â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7ZIh6Rujxnd- H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H-Hxzek9ziXZsw5adSEMZU/w640-h285/image2.png&quot; å®½åº¦=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æˆ‘ä»¬é€šè¿‡ä»Žå¤´å¼€å§‹åœ¨ ImageNet ä¸Šè¿›è¡Œè®­ç»ƒæ¥è¯„ä¼° AdaTape ã€‚å¯¹äºŽ&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>;ï¼Œæˆ‘ä»¬ä¸ä»…ä»Žè®ºæ–‡ä¸­æŠ¥å‘Šäº†ä»–ä»¬çš„ç»“æžœï¼Œè¿˜é€šè¿‡ä»Žå¤´å¼€å§‹è®­ç»ƒé‡æ–°å®žâ€‹â€‹çŽ°äº†A-ViTï¼Œå³ï¼ŒA-ViTï¼ˆæˆ‘ä»¬çš„ï¼‰ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A AdaTape è¡Œä¸ºçš„ç ”ç©¶&lt;/h2>; &lt;p>; é™¤äº†å¥‡å¶æ ¡éªŒä»»åŠ¡å’Œ ImageNet-1K ä¸Šçš„æ€§èƒ½ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº† AdaTape åœ¨ &lt;a href=&quot;https:// /arxiv.org/abs/1707.02968&quot;>;JFT-300M&lt;/a>;éªŒè¯é›†ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£æ¨¡åž‹çš„è¡Œä¸ºï¼Œæˆ‘ä»¬å°†è¾“å…¥é©±åŠ¨é“¶è¡Œä¸Šçš„ä»£å¸é€‰æ‹©ç»“æžœå¯è§†åŒ–ä¸ºçƒ­å›¾ï¼Œå…¶ä¸­è¾ƒæµ…çš„é¢œè‰²æ„å‘³ç€è¯¥ä½ç½®è¢«æ›´é¢‘ç¹åœ°é€‰æ‹©ã€‚çƒ­å›¾æ˜¾ç¤º AdaTape æ›´é¢‘ç¹åœ°é€‰æ‹©ä¸­å¿ƒè¡¥ä¸ã€‚è¿™ä¸Žæˆ‘ä»¬çš„å…ˆéªŒçŸ¥è¯†ç›¸ä¸€è‡´ï¼Œå› ä¸ºä¸­å¿ƒè¡¥ä¸é€šå¸¸åŒ…å«æ›´å¤šä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰è‡ªç„¶å›¾åƒçš„æ•°æ®é›†çš„èƒŒæ™¯ä¸‹ï¼Œå…¶ä¸­ä¸»è¦å¯¹è±¡ä½äºŽå›¾åƒçš„ä¸­é—´ã€‚è¿™ä¸€ç»“æžœå‡¸æ˜¾äº† AdaTape çš„æ™ºèƒ½æ€§ï¼Œå› ä¸ºå®ƒå¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«ä¿¡æ¯æ›´ä¸°å¯Œçš„è¡¥ä¸å¹¶ç¡®å®šä¼˜å…ˆçº§ï¼Œä»¥æé«˜å…¶æ€§èƒ½ã€‚ &lt;/p>; &lt;p>;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:è‡ªåŠ¨;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgk R2VcDoKTicWQ_hufXwrlAcPMoeYqcrCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/s839/ image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;839&quot; height=&quot;270 â€œ src =â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgkR2VcDoKTicWQ_hufXwrlAcPMoeYq crCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/w640-h270/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr >;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;æˆ‘ä»¬å¯è§†åŒ– AdaTape-B/32ï¼ˆå·¦ï¼‰å’Œ AdaTape-B/16ï¼ˆå³ï¼‰çš„ç£å¸¦ä»¤ç‰Œé€‰æ‹©çƒ­å›¾ã€‚é¢œè‰²è¶Šçƒ­/è¶Šæµ…ï¼Œæ„å‘³ç€è¯¥ä½ç½®çš„è¡¥ä¸è¢«æ›´é¢‘ç¹åœ°é€‰æ‹©ã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç»“è®º&lt;/h2>; &lt;p>; AdaTape çš„ç‰¹ç‚¹æ˜¯è‡ªé€‚åº”ç£å¸¦è¯»å–æœºåˆ¶ç”Ÿæˆçš„å¼¹æ€§åºåˆ—é•¿åº¦ã€‚è¿™è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„æ„Ÿåº”åç½®ï¼Œä½¿ AdaTape èƒ½å¤Ÿè§£å†³å¯¹æ ‡å‡†å˜åŽ‹å™¨å’ŒçŽ°æœ‰è‡ªé€‚åº”å˜åŽ‹å™¨éƒ½å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚é€šè¿‡å¯¹å›¾åƒè¯†åˆ«åŸºå‡†è¿›è¡Œå…¨é¢çš„å®žéªŒï¼Œæˆ‘ä»¬è¯æ˜Žäº†å½“è®¡ç®—ä¿æŒæ’å®šæ—¶ï¼ŒAdaTape ä¼˜äºŽæ ‡å‡†è½¬æ¢å™¨å’Œè‡ªé€‚åº”æž¶æž„è½¬æ¢å™¨ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;è‡´è°¢&lt;/h2>; &lt;p>; &lt;em>;æœ¬æ–‡ä½œè€…ä¹‹ä¸€ï¼ŒMostafa Dehghani ï¼ŒçŽ°ä¾›èŒäºŽ Google DeepMindã€‚ &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/595171581401765238/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘è¡¨è¯„è®º&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 æ¡è¯„è®º&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html&quot; rel=&quot;alternate&quot; title=&quot;AdaTapeï¼šå…·æœ‰è‡ªé€‚åº”è®¡ç®—å’ŒåŠ¨æ€è¯»å†™çš„åŸºç¡€æ¨¡åž‹&quot; type= &quot;text/html&quot;/>;&lt;ä½œè€…>;&lt;åç§°>;Google AI&lt;/åç§°>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;ç”µå­é‚®ä»¶>;noreply@blogger.com&lt;/ç”µå­é‚®ä»¶>;&lt;gd:å›¾åƒé«˜åº¦=â€œ16â€rel=â€œhttp://schemas.google.com/g/2005#thumbnailâ€src=â€œhttps://img1.blogblog.com/img/b16-rounded.gifâ€ width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIqJhd Qrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudiEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s72- c/adatape.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/æ¡ç›®>;&lt;entry>;&lt;id>;æ ‡ç­¾ï¼šblogger.com,1999:blog-8474926331452026626.post-4436946873570093049&lt;/id>;&lt;å·²å‘å¸ƒ>;2023-08-03T11:24:00.001-07:00&lt;/å·²å‘å¸ƒ>;&lt;å·²æ›´æ–°>; 2023-08-03T11:24:40.465-07:00&lt;/æ›´æ–°>;&lt;ç±»åˆ«æ–¹æ¡ˆ=â€œhttp://www.blogger.com/atom/ns#â€æœ¯è¯­=â€œAIâ€>;&lt;/ç±»åˆ«>;&lt;ç±»åˆ«æ–¹æ¡ˆ=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;æ·±åº¦å­¦ä¹ &quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term= &quot;å¥åº·&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;å¤šæ¨¡å¼åŒ»ç–—äººå·¥æ™ºèƒ½&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒäººï¼šå¥åº·äººå·¥æ™ºèƒ½ä¸»ç®¡ Greg Corrado ã€Google ç ”ç©¶éƒ¨å’Œ Google ç ”ç©¶éƒ¨å·¥ç¨‹ä¸Žç ”ç©¶å‰¯æ€»è£ Yossi Matias&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjm G9lDTBWdy5kH_bR3-tqN8KzdhgmrLL_N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3 /s1600/medpalm.png&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; åŒ»å­¦æœ¬è´¨ä¸Šæ˜¯ä¸€é—¨å¤šæ¨¡å¼å­¦ç§‘ã€‚åœ¨æä¾›æŠ¤ç†æ—¶ï¼Œä¸´åºŠåŒ»ç”Ÿé€šå¸¸ä¼šè§£è¯»å„ç§æ¨¡å¼çš„æ•°æ®ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒã€ä¸´åºŠè®°å½•ã€å®žéªŒå®¤æµ‹è¯•ã€ç”µå­å¥åº·è®°å½•ã€åŸºå› ç»„å­¦ç­‰ã€‚åœ¨è¿‡åŽ»åå¹´å·¦å³çš„æ—¶é—´é‡Œï¼Œäººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ç‰¹å®šæ¨¡å¼ä¸‹çš„ç‰¹å®šä»»åŠ¡ä¸Šå–å¾—äº†ä¸“å®¶çº§çš„æ€§èƒ½â€”â€”ä¸€äº›äººå·¥æ™ºèƒ½ç³»ç»Ÿ&lt;a href=&quot;https://www.nature.com/articles/s41591 -019-0447-x&quot;>;å¤„ç† CT æ‰«æ&lt;/a>;ï¼Œè€Œå…¶ä»–äºº&lt;a href=&quot;https://jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;åˆ†æžé«˜å€ç—…ç†åˆ‡ç‰‡&lt;/a>;ï¼Œè¿˜æœ‰ä¸€äº›&lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/NEJMc2112090&quot;>;å¯»æ‰¾ç½•è§çš„é—ä¼ å˜å¼‚&lt;/a>;ã€‚è¿™äº›ç³»ç»Ÿçš„è¾“å…¥å¾€å¾€æ˜¯å¤æ‚çš„æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒï¼Œå®ƒä»¬é€šå¸¸æä¾›ç»“æž„åŒ–è¾“å‡ºï¼Œæ— è®ºæ˜¯ç¦»æ•£ç­‰çº§çš„å½¢å¼è¿˜æ˜¯&lt;a href=&quot;https://www.nature.com/articles/s41591-018- 0107-6&quot;>;å¯†é›†å›¾åƒåˆ†å‰²æŽ©æ¨¡ã€‚&lt;/a>;åŒæ—¶ï¼Œå¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM) çš„å®¹é‡å’ŒåŠŸèƒ½å…·æœ‰&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;å˜å¾—å¦‚æ­¤å…ˆè¿›&lt;/a>;ï¼Œä»¥è‡³äºŽä»–ä»¬é€šè¿‡ç”¨ç®€å•çš„è¯­è¨€è¿›è¡Œè§£é‡Šå’Œå›žåº”ï¼Œè¡¨çŽ°å‡ºäº†å¯¹åŒ»å­¦çŸ¥è¯†çš„ç†è§£å’Œä¸“ä¸šçŸ¥è¯†ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å¦‚ä½•å°†è¿™äº›åŠŸèƒ½ç»“åˆåœ¨ä¸€èµ·æ¥æž„å»ºå¯ä»¥åˆ©ç”¨æ‰€æœ‰è¿™äº›æ¥æºçš„ä¿¡æ¯çš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Ÿ &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; åœ¨ä»Šå¤©çš„åšæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ä¸ºæ³•å­¦ç¡•å£«å¸¦æ¥å¤šæ¨¡å¼èƒ½åŠ›çš„ä¸€ç³»åˆ—æ–¹æ³•ï¼Œå¹¶åˆ†äº«äº†å…³äºŽæž„å»ºå¤šæ¨¡å¼åŒ»å­¦æ³•å­¦ç¡•å£«çš„å¯å¤„ç†æ€§çš„ä¸€äº›ä»¤äººå…´å¥‹çš„ç»“æžœï¼Œæ­£å¦‚æœ€è¿‘çš„ä¸‰ç¯‡ç ”ç©¶è®ºæ–‡æ‰€è¿°ã€‚è¿™äº›è®ºæ–‡åè¿‡æ¥æ¦‚è¿°äº†å¦‚ä½•å°†&lt;em>;ä»Žå¤´&lt;/em>;æ¨¡å¼å¼•å…¥æ³•å­¦ç¡•å£«ï¼Œå¦‚ä½•å°†æœ€å…ˆè¿›çš„åŒ»å­¦æˆåƒåŸºç¡€æ¨¡åž‹ç§»æ¤åˆ°å¯¹è¯å¼æ³•å­¦ç¡•å£«ä¸Šï¼Œä»¥åŠå»ºç«‹æ³•å­¦ç¡•å£«çš„ç¬¬ä¸€æ­¥çœŸæ­£çš„å¤šæ¨¡å¼åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚å¦‚æžœæˆåŠŸæˆç†Ÿï¼Œå¤šæ¨¡å¼åŒ»å­¦æ³•å­¦ç¡•å£«å¯èƒ½ä¼šæˆä¸ºæ¶µç›–ä¸“ä¸šåŒ»å­¦ã€åŒ»å­¦ç ”ç©¶å’Œæ¶ˆè´¹è€…åº”ç”¨çš„æ–°è¾…åŠ©æŠ€æœ¯çš„åŸºç¡€ã€‚ä¸Žæˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œä¸€æ ·ï¼Œæˆ‘ä»¬å¼ºè°ƒéœ€è¦ä¸ŽåŒ»å­¦ç•Œå’ŒåŒ»ç–—ä¿å¥ç”Ÿæ€ç³»ç»Ÿåˆä½œä»”ç»†è¯„ä¼°è¿™äº›æŠ€æœ¯ã€‚ &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;ä¸€ç³»åˆ—æ–¹æ³•&lt;/h2>; &lt;p>; æœ€è¿‘æå‡ºäº†å‡ ç§æž„å»ºå¤šæ¨¡å¼æ³•å­¦ç¡•å£«çš„æ–¹æ³•æœˆ [&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;1&lt;/a>;ï¼Œ&lt;a href=&quot;https:// /www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;2&lt;/a>;ï¼Œ&lt;a href=&quot;https://ai.googleblog.com/2023 /03/palm-e-embodied-multimodal-language.html&quot;>;3&lt;/a>;]ï¼Œæ¯«æ— ç–‘é—®ï¼Œæ–°æ–¹æ³•å°†åœ¨ä¸€æ®µæ—¶é—´å†…ç»§ç»­å‡ºçŽ°ã€‚ä¸ºäº†äº†è§£ä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿå¸¦æ¥æ–°æ¨¡å¼çš„æœºä¼šï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä¸‰ç§å¹¿æ³›å®šä¹‰çš„æ–¹æ³•ï¼šå·¥å…·ä½¿ç”¨ã€æ¨¡åž‹ç§»æ¤å’Œé€šæ‰ç³»ç»Ÿã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ ZKRMY82JiKLD26G2skhiJEEh8Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s1600/image1.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ZKRMY82JiKLD26G2skhiJEEh8 Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; æ ·å¼&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;å·¥å…·ä½¿ç”¨&lt;/h2>; &lt;p>; åœ¨ &lt;em >;å·¥å…·ä½¿ç”¨æ–¹æ³•ï¼Œä¸€ä¸ªä¸­å¿ƒåŒ»å­¦æ³•å­¦ç¡•å£«å°†å„ç§æ¨¡å¼çš„æ•°æ®åˆ†æžå¤–åŒ…ç»™ä¸€ç»„é’ˆå¯¹è¿™äº›ä»»åŠ¡ç‹¬ç«‹ä¼˜åŒ–çš„è½¯ä»¶å­ç³»ç»Ÿï¼šå·¥å…·ã€‚å·¥å…·ä½¿ç”¨çš„å¸¸è§åŠ©è®°ç¤ºä¾‹æ˜¯æ•™æ³•å­¦ç¡•å£«ä½¿ç”¨è®¡ç®—å™¨è€Œä¸æ˜¯è‡ªå·±åšç®—æœ¯ã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼Œé¢å¯¹èƒ¸éƒ¨ X å…‰æ£€æŸ¥çš„åŒ»å­¦æ³•å­¦ç¡•å£«å¯ä»¥å°†è¯¥å›¾åƒè½¬å‘åˆ°æ”¾å°„å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿå¹¶æ•´åˆè¯¥å“åº”ã€‚è¿™å¯ä»¥é€šè¿‡å­ç³»ç»Ÿæä¾›çš„åº”ç”¨ç¨‹åºç¼–ç¨‹æŽ¥å£ï¼ˆAPIï¼‰æ¥å®Œæˆï¼Œæˆ–è€…æ›´å¥‡ç‰¹çš„æ˜¯ï¼Œä¸¤ä¸ªå…·æœ‰ä¸åŒä¸“ä¸šçš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿè¿›è¡Œå¯¹è¯æ¥å®Œæˆã€‚ &lt;/p>; &lt;p>; è¿™ç§æ–¹æ³•æœ‰ä¸€äº›é‡è¦çš„å¥½å¤„ã€‚å®ƒå…è®¸å­ç³»ç»Ÿä¹‹é—´å®žçŽ°æœ€å¤§çš„çµæ´»æ€§å’Œç‹¬ç«‹æ€§ï¼Œä½¿å«ç”Ÿç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®ç»è¿‡éªŒè¯çš„å­ç³»ç»Ÿæ€§èƒ½ç‰¹å¾åœ¨æŠ€æœ¯æä¾›å•†ä¹‹é—´æ··åˆå’ŒåŒ¹é…äº§å“ã€‚æ­¤å¤–ï¼Œå­ç³»ç»Ÿä¹‹é—´çš„äººç±»å¯è¯»é€šä¿¡é€šé“æœ€å¤§é™åº¦åœ°æé«˜äº†å¯å®¡æ ¸æ€§å’Œå¯è°ƒè¯•æ€§ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨ç‹¬ç«‹å­ç³»ç»Ÿä¹‹é—´å®žçŽ°æ­£ç¡®çš„é€šä¿¡å¯èƒ½å¾ˆæ£˜æ‰‹ï¼Œè¿™ä¼šç¼©å°ä¿¡æ¯ä¼ è¾“èŒƒå›´ï¼Œæˆ–æš´éœ²é€šä¿¡é”™è¯¯å’Œä¿¡æ¯ä¸¢å¤±çš„é£Žé™©ã€‚ &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;æ¨¡åž‹å«æŽ¥&lt;/h2>; &lt;p>; ä¸€ç§æ›´ç»¼åˆçš„æ–¹æ³•æ˜¯é‡‡ç”¨ä¸“é—¨ç”¨äºŽæ¯ä¸ªç›¸å…³é¢†åŸŸï¼Œå¹¶å¯¹å…¶è¿›è¡Œè°ƒæ•´ä»¥ç›´æŽ¥æ’å…¥æ³•å­¦ç¡•å£« - å°†è§†è§‰æ¨¡åž‹&lt;em>;å«æŽ¥&lt;/em>;åˆ°æ ¸å¿ƒæŽ¨ç†ä»£ç†ä¸Šã€‚ä¸Žç”±æ³•å­¦ç¡•å£«ç¡®å®šæ‰€ä½¿ç”¨çš„å…·ä½“å·¥å…·çš„å·¥å…·ä½¿ç”¨ç›¸åï¼Œåœ¨æ¨¡åž‹ç§»æ¤ä¸­ï¼Œç ”ç©¶äººå‘˜å¯ä»¥åœ¨å¼€å‘è¿‡ç¨‹ä¸­é€‰æ‹©ä½¿ç”¨ã€å®Œå–„æˆ–å¼€å‘ç‰¹å®šæ¨¡åž‹ã€‚åœ¨è°·æ­Œç ”ç©¶ä¸­å¿ƒæœ€è¿‘çš„ä¸¤ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜Žè¿™å®žé™…ä¸Šæ˜¯å¯è¡Œçš„ã€‚ç¥žç»æ³•å­¦ç¡•å£«é€šå¸¸é€šè¿‡é¦–å…ˆå°†å•è¯æ˜ å°„åˆ°&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings&quot;>;å‘é‡åµŒå…¥ç©ºé—´&lt;/ä¸€ä¸ª>;ã€‚è¿™ä¸¤ç¯‡è®ºæ–‡éƒ½åŸºäºŽå°†æ•°æ®ä»Žæ–°æ¨¡æ€æ˜ å°„åˆ°æ³•å­¦ç¡•å£«å·²ç»ç†Ÿæ‚‰çš„è¾“å…¥è¯åµŒå…¥ç©ºé—´çš„æƒ³æ³•ã€‚ç¬¬ä¸€ç¯‡è®ºæ–‡â€œ&lt;a href=&quot;https://arxiv.org/abs/2307.09018&quot;>;åŸºäºŽä¸ªäººç‰¹å®šæ•°æ®çš„å¥åº·å¤šæ¨¡å¼æ³•å­¦ç¡•å£«&lt;/a>;â€è¡¨æ˜Ž&lt;a href= â€œhttps://www.ukbiobank.ac.uk/&quot;>;è‹±å›½ç”Ÿç‰©é“¶è¡Œ&lt;/a>;å¦‚æžœæˆ‘ä»¬é¦–å…ˆè®­ç»ƒç¥žç»ç½‘ç»œåˆ†ç±»å™¨æ¥è§£é‡Š&lt;a href=&quot;https://www.mayoclinic.org/tests -procedures/spirometry/about/pac-20385201&quot;>;å‘¼å¸å›¾&lt;/a>;ï¼ˆä¸€ç§ç”¨äºŽè¯„ä¼°å‘¼å¸èƒ½åŠ›çš„æ–¹å¼ï¼‰ï¼Œç„¶åŽè°ƒæ•´è¯¥ç½‘ç»œçš„è¾“å‡ºä½œä¸º LLM çš„è¾“å…¥ã€‚ &lt;/p>; &lt;p>; ç¬¬äºŒç¯‡è®ºæ–‡ï¼Œâ€œ&lt;a href=&quot;https://arxiv.org/abs/2308.01317&quot;>;ELIXRï¼šé€šè¿‡å¤§è¯­è¨€æ¨¡åž‹å’Œæ”¾å°„å­¦çš„ç»“åˆå®žçŽ°é€šç”¨ X å°„çº¿äººå·¥æ™ºèƒ½ç³»ç»Ÿè§†è§‰ç¼–ç å™¨&lt;/a>;â€ï¼Œé‡‡ç”¨ç›¸åŒçš„ç­–ç•¥ï¼Œä½†å°†å…¶åº”ç”¨äºŽæ”¾å°„å­¦ä¸­çš„å…¨å°ºå¯¸å›¾åƒç¼–ç å™¨æ¨¡åž‹ã€‚ä»Ž&lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;äº†è§£èƒ¸éƒ¨ X å…‰æ£€æŸ¥çš„åŸºç¡€æ¨¡åž‹&lt;/a>;å¼€å§‹ï¼Œå·²å±•ç¤ºä¸ºäº†æˆä¸ºåœ¨æ­¤æ¨¡å¼ä¸­æž„å»ºå„ç§åˆ†ç±»å™¨çš„è‰¯å¥½åŸºç¡€ï¼Œæœ¬æ–‡æè¿°äº†è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„åŒ»ç–—ä¿¡æ¯é€‚é…å™¨ï¼Œè¯¥é€‚é…å™¨å°†åŸºç¡€æ¨¡åž‹çš„é¡¶å±‚è¾“å‡ºé‡æ–°è¡¨ç¤ºä¸ºä¸€ç³»åˆ—æ ‡è®°LLM çš„è¾“å…¥åµŒå…¥ç©ºé—´ã€‚å°½ç®¡å¯¹è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡åž‹éƒ½æ²¡æœ‰è¿›è¡Œå¾®è°ƒï¼Œä½†ç”Ÿæˆçš„ç³»ç»Ÿæ˜¾ç¤ºäº†æœªç»è®­ç»ƒçš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬&lt;a href=&quot;https://arxiv.org/abs/2210.10163&quot;>;è¯­ä¹‰æœç´¢&lt;/a>;å’Œ&lt;a href=&quot;https://www.nature.com/articles/sdata2018251&quot;>;è§†è§‰é—®ç­”&lt;/a>;ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFM aMMzTSUu4fif7t9SVsr9MduEcISlA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s1600/image3.gif&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFMaMMzTSUu4fif7t9SVsr9MduEc ISLA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; æ ·å¼=&quot;text-align: center;&quot;>;æˆ‘ä»¬å«æŽ¥æ¨¡åž‹çš„æ–¹æ³•æ˜¯é€šè¿‡è®­ç»ƒåŒ»ç–—ä¿¡æ¯é€‚é…å™¨æ¥å°†çŽ°æœ‰æˆ–æ”¹è¿›çš„å›¾åƒç¼–ç å™¨çš„è¾“å‡ºæ˜ å°„ä¸ºæ³•å­¦ç¡•å£«å¯ç†è§£çš„å½¢å¼ã€‚&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; æ¨¡åž‹å«æŽ¥æœ‰å¾ˆå¤šä¼˜ç‚¹ã€‚å®ƒä½¿ç”¨ç›¸å¯¹é€‚åº¦çš„è®¡ç®—èµ„æºæ¥è®­ç»ƒé€‚é…å™¨å±‚ï¼Œä½†å…è®¸æ³•å­¦ç¡•å£«åœ¨æ¯ä¸ªæ•°æ®åŸŸä¸­çŽ°æœ‰çš„é«˜åº¦ä¼˜åŒ–å’ŒéªŒè¯çš„æ¨¡åž‹ä¸Šæž„å»ºã€‚å°†é—®é¢˜æ¨¡å—åŒ–ä¸ºç¼–ç å™¨ã€é€‚é…å™¨å’Œ LLM ç»„ä»¶è¿˜å¯ä»¥æ–¹ä¾¿åœ¨å¼€å‘å’Œéƒ¨ç½²æ­¤ç±»ç³»ç»Ÿæ—¶å¯¹å„ä¸ªè½¯ä»¶ç»„ä»¶è¿›è¡Œæµ‹è¯•å’Œè°ƒè¯•ã€‚ç›¸åº”çš„ç¼ºç‚¹æ˜¯ï¼Œä¸“ä¸šç¼–ç å™¨å’Œ LLM ä¹‹é—´çš„é€šä¿¡ä¸å†æ˜¯äººç±»å¯è¯»çš„ï¼ˆä½œä¸ºä¸€ç³»åˆ—é«˜ç»´å‘é‡ï¼‰ï¼Œå¹¶ä¸”å«æŽ¥è¿‡ç¨‹ä¸ä»…éœ€è¦ä¸ºæ¯ä¸ªç‰¹å®šé¢†åŸŸçš„ç¼–ç å™¨æž„å»ºä¸€ä¸ªæ–°çš„é€‚é…å™¨ï¼Œè¿˜éœ€è¦ä¸ºæ¯ä¸ªæ¯ä¸ªç¼–ç å™¨çš„&lt;em>;ä¿®è®¢&lt;/em>;ã€‚ &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;é€šæ‰ç³»ç»Ÿ&lt;/h2>; &lt;p>; å¤šæ¨¡å¼åŒ»ç–—äººå·¥æ™ºèƒ½æœ€æ ¹æœ¬çš„æ–¹æ³•æ˜¯æž„å»ºä¸€ä¸ªé›†æˆçš„ç³»ç»Ÿï¼Œå®Œå…¨é€šæ‰çš„ç³»ç»Ÿæœ¬èº«å°±èƒ½å¤Ÿå¸æ”¶æ‰€æœ‰æ¥æºçš„ä¿¡æ¯ã€‚åœ¨è¯¥é¢†åŸŸçš„ç¬¬ä¸‰ç¯‡è®ºæ–‡â€œ&lt;a href=&quot;https://arxiv.org/abs/2307.14334&quot;>;è¿ˆå‘é€šæ‰ç”Ÿç‰©åŒ»å­¦äººå·¥æ™ºèƒ½&lt;/a>;â€ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰ä¸ºæ¯ç§æ•°æ®æ¨¡æ€ä½¿ç”¨å•ç‹¬çš„ç¼–ç å™¨å’Œé€‚é…å™¨ï¼Œä»¥ &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;PaLM-E&lt;/a>; ä¸ºåŸºç¡€æž„å»ºï¼ŒPaLM-E æ˜¯æœ€è¿‘å‘å¸ƒçš„å¤šæ¨¡å¼æ¨¡åž‹å•ä¸ª LLM (&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;) å’Œ &lt; a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;å•è§†è§‰ç¼–ç å™¨ (ViT)&lt;/a>;ã€‚åœ¨æ­¤è®¾ç½®ä¸­ï¼ŒLLM æ–‡æœ¬ç¼–ç å™¨æ¶µç›–æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®æ¨¡å¼ï¼Œä½†çŽ°åœ¨æ‰€æœ‰å…¶ä»–æ•°æ®éƒ½è¢«è§†ä¸ºå›¾åƒå¹¶é¦ˆé€åˆ°è§†è§‰ç¼–ç å™¨ã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42 Hepv4DNsEWebRmtorN05xdpkJ2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s1600/image2.gif&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42Hepv4DNsEWebRmtorN05xdpkJ 2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s16000/image2.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Med-PaLM M æ˜¯ä¸€ä¸ªå¤§åž‹å¤šæ¨¡æ€ç”Ÿæˆæ¨¡åž‹ï¼Œå¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ¨¡åž‹æƒé‡çµæ´»åœ°ç¼–ç å’Œè§£é‡Šç”Ÿç‰©åŒ»å­¦æ•°æ®ï¼ŒåŒ…æ‹¬ä¸´åºŠè¯­è¨€ã€æˆåƒå’ŒåŸºå› ç»„å­¦ã€‚&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; æˆ‘ä»¬é€šè¿‡åœ¨è®ºæ–‡ä¸­æè¿°çš„åŒ»å­¦æ•°æ®é›†ä¸Šå¾®è°ƒæ•´å¥—æ¨¡åž‹å‚æ•°ï¼Œå°† PaLM-E ä¸“é—¨åº”ç”¨äºŽåŒ»å­¦é¢†åŸŸã€‚ç”±æ­¤äº§ç”Ÿçš„é€šç”¨åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿæ˜¯ &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM&lt;/a>; çš„å¤šæ¨¡å¼ç‰ˆæœ¬ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º Med-PaLM Mã€‚çµæ´»çš„å¤šæ¨¡å¼åºåˆ—åˆ°åºåˆ—çš„æž¶æž„ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸€æ¬¡äº¤äº’ä¸­äº¤ç»‡å„ç§ç±»åž‹çš„å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯å•ä¸ªç»Ÿä¸€æ¨¡åž‹çš„é¦–æ¬¡æ¼”ç¤ºï¼Œè¯¥æ¨¡åž‹å¯ä»¥è§£é‡Šå¤šæ¨¡å¼ç”Ÿç‰©åŒ»å­¦æ•°æ®å¹¶åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ä½¿ç”¨ç›¸åŒçš„æ¨¡åž‹æƒé‡é›†æ¥å¤„ç†å„ç§ä»»åŠ¡ï¼ˆè®ºæ–‡ä¸­æœ‰è¯¦ç»†è¯„ä¼°ï¼‰ã€‚ &lt;/p>; &lt;p>; è¿™ç§å¤šæ¨¡æ€é€šæ‰ç³»ç»Ÿæ–¹æ³•æ˜¯æˆ‘ä»¬æè¿°çš„æ–¹æ³•ä¸­æœ€é›„å¿ƒå‹ƒå‹ƒã€åŒæ—¶ä¹Ÿæ˜¯æœ€ä¼˜é›…çš„æ–¹æ³•ã€‚åŽŸåˆ™ä¸Šï¼Œè¿™ç§ç›´æŽ¥æ–¹æ³•æœ€å¤§é™åº¦åœ°æé«˜äº†æ¨¡å¼ä¹‹é—´çš„çµæ´»æ€§å’Œä¿¡æ¯ä¼ è¾“ã€‚ç”±äºŽæ²¡æœ‰ API æ¥ç»´æŒå…¼å®¹æ€§ï¼Œä¹Ÿæ²¡æœ‰é€‚é…å™¨å±‚çš„æ¿€å¢žï¼Œé€šç”¨æ–¹æ³•å¯ä»¥è¯´æ˜¯æœ€ç®€å•çš„è®¾è®¡ã€‚ä½†åŒæ ·çš„ä¼˜é›…ä¹Ÿæ˜¯å…¶ä¸€äº›ç¼ºç‚¹çš„æ ¹æºã€‚è®¡ç®—æˆæœ¬é€šå¸¸æ›´é«˜ï¼Œå¹¶ä¸”ç”±äºŽå•ä¸€è§†è§‰ç¼–ç å™¨æœåŠ¡äºŽå¤šç§æ¨¡å¼ï¼Œé¢†åŸŸä¸“ä¸šåŒ–æˆ–ç³»ç»Ÿå¯è°ƒè¯•æ€§å¯èƒ½ä¼šå—åˆ°å½±å“ã€‚ &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;å¤šæ¨¡å¼åŒ»ç–—äººå·¥æ™ºèƒ½çš„çŽ°å®ž&lt;/h2>; &lt;p>;ä¸ºäº†åœ¨åŒ»å­¦ä¸­å……åˆ†åˆ©ç”¨äººå·¥æ™ºèƒ½ï¼Œæˆ‘ä»¬éœ€è¦å°†ç»è¿‡é¢„æµ‹äººå·¥æ™ºèƒ½è®­ç»ƒçš„ä¸“å®¶ç³»ç»Ÿçš„ä¼˜åŠ¿ä¸Žé€šè¿‡ç”Ÿæˆäººå·¥æ™ºèƒ½å®žçŽ°çš„çµæ´»æ€§ç»“åˆèµ·æ¥ã€‚å“ªç§æ–¹æ³•ï¼ˆæˆ–æ–¹æ³•ç»„åˆï¼‰åœ¨è¯¥é¢†åŸŸæœ€æœ‰ç”¨å–å†³äºŽè®¸å¤šå°šæœªè¯„ä¼°çš„å› ç´ ã€‚é€šæ‰æ¨¡åž‹çš„çµæ´»æ€§å’Œç®€å•æ€§æ˜¯å¦æ¯”æ¨¡åž‹å«æŽ¥æˆ–å·¥å…·ä½¿ç”¨çš„æ¨¡å—åŒ–æ›´æœ‰ä»·å€¼ï¼Ÿå“ªç§æ–¹æ³•å¯ä»¥ä¸ºç‰¹å®šçš„å®žé™…ç”¨ä¾‹æä¾›æœ€é«˜è´¨é‡çš„ç»“æžœï¼Ÿæ”¯æŒåŒ»å­¦ç ”ç©¶æˆ–åŒ»å­¦æ•™è‚²ä¸Žå¢žå¼ºåŒ»ç–—å®žè·µçš„é¦–é€‰æ–¹æ³•æ˜¯å¦ä¸åŒï¼Ÿå›žç­”è¿™äº›é—®é¢˜éœ€è¦æŒç»­è¿›è¡Œä¸¥æ ¼çš„å®žè¯ç ”ç©¶ï¼Œå¹¶ä¸ŽåŒ»ç–—ä¿å¥æä¾›è€…ã€åŒ»ç–—æœºæž„ã€æ”¿åºœå®žä½“å’ŒåŒ»ç–—ä¿å¥è¡Œä¸šåˆä½œä¼™ä¼´ç»§ç»­è¿›è¡Œå¹¿æ³›çš„ç›´æŽ¥åˆä½œã€‚æˆ‘ä»¬æœŸå¾…ç€å…±åŒå¯»æ‰¾ç­”æ¡ˆã€‚ &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4436946873570093049/comments/default&quot; rel=&quot;replies&quot; title=&quot;å‘å¸ƒè¯„è®º&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/multimodal-medical-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 æ¡è¯„è®º&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google /2023/08/multimodal-medical-ai.html&quot; rel=&quot;alternate&quot; title=&quot;å¤šæ¨¡å¼åŒ»ç–— AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http ://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/ g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot; 72â€œç½‘å€=â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjmG9lDTBWdy5kH_bR3-tqN8KzdhgmrLL _N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3/s72-c/medpalm.pngâ€œå®½åº¦=â€œ72â€xmlnsï¼šåª’ä½“=â€œhttp ://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;æ ‡ç­¾ï¼šblogger.comï¼Œ1999ï¼šåšå®¢-8474926331452026626.post-5980448298988153366&lt;/id>;&lt;å‘å¸ƒ>;2023-07-26T09:33:00.003-07:00&lt;/å‘å¸ƒ>;&lt;æ›´æ–°>;2023-08-01T08:31:25.216-07:00&lt;/æ›´æ–°>; &lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;å¯»æ‰¾æ— æºåŸŸçš„é€šç”¨æ–¹æ³•é€‚åº”&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;å‘å¸ƒè€…ï¼šGoogle ç ”ç©¶ç§‘å­¦å®¶ Eleni Triantafillou å’Œå­¦ç”Ÿç ”ç©¶å‘˜ Malik Boudiaf &lt;/span>; &lt;img src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4epC-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgy M4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/s1500/notela.png&quot; style=&quot;æ˜¾ç¤ºï¼šæ— ï¼›&quot; />; &lt;p>; æ·±åº¦å­¦ä¹ æœ€è¿‘åœ¨å¹¿æ³›çš„é—®é¢˜å’Œåº”ç”¨ä¸­å–å¾—äº†å·¨å¤§è¿›å±•ï¼Œä½†æ¨¡åž‹åœ¨éƒ¨ç½²åœ¨æœªçŸ¥çš„é¢†åŸŸæˆ–åˆ†å¸ƒä¸­æ—¶å¸¸å¸¸ä¼šå‡ºçŽ°ä¸å¯é¢„æµ‹çš„å¤±è´¥ã€‚ &lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;æ— æºåŸŸé€‚åº”&lt;/a>; (SFDA) æ˜¯ä¸€ä¸ªç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨è®¾è®¡é€‚åº”é¢„è®­ç»ƒæ¨¡åž‹çš„æ–¹æ³•ï¼ˆåœ¨ä¸€ä¸ªâ€œæºåŸŸâ€ï¼‰åˆ°ä¸€ä¸ªæ–°çš„â€œç›®æ ‡åŸŸâ€ï¼Œä»…ä½¿ç”¨åŽè€…çš„æœªæ ‡è®°æ•°æ®ã€‚ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; è®¾è®¡æ·±åº¦æ¨¡åž‹çš„é€‚åº”æ–¹æ³•æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é¢†åŸŸã€‚è™½ç„¶æ¨¡åž‹å’Œè®­ç»ƒæ•°æ®é›†è§„æ¨¡çš„ä¸æ–­æ‰©å¤§æ˜¯å…¶æˆåŠŸçš„å…³é”®å› ç´ ï¼Œä½†è¿™ç§è¶‹åŠ¿çš„è´Ÿé¢åŽæžœæ˜¯è®­ç»ƒæ­¤ç±»æ¨¡åž‹çš„è®¡ç®—æˆæœ¬è¶Šæ¥è¶Šé«˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ä½¿å¾—å¤§åž‹æ¨¡åž‹è®­ç»ƒ&lt;a href=&quot;https:// spectrum.ieee.org/deep-learning-computational-cost&quot;>;ä¸å¤ªå®¹æ˜“è®¿é—®&lt;/a>;å¹¶ä¸”ä¸å¿…è¦&lt;a href=&quot;https://penntoday.upenn.edu/news/hidden-costs-ai-impending-energy- and-resource-strain&quot;>;å¢žåŠ ç¢³è¶³è¿¹&lt;/a>;ã€‚ç¼“è§£è¿™ä¸€é—®é¢˜çš„ä¸€ä¸ªé€”å¾„æ˜¯é€šè¿‡è®¾è®¡æŠ€æœ¯æ¥åˆ©ç”¨å’Œé‡ç”¨å·²ç»è®­ç»ƒå¥½çš„æ¨¡åž‹æ¥å¤„ç†æ–°ä»»åŠ¡æˆ–æŽ¨å¹¿åˆ°æ–°é¢†åŸŸã€‚äº‹å®žä¸Šï¼Œåœ¨&lt;a href=&quot;https://arxiv.org/abs/1911.02685&quot;>;è¿ç§»å­¦ä¹ &lt;/a>;çš„æ¡†æž¶ä¸‹ï¼Œå¯¹æ¨¡åž‹é€‚åº”æ–°ä»»åŠ¡è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ã€‚ &lt;/p>; &lt;p>; SFDA æ˜¯è¿™é¡¹ç ”ç©¶çš„ä¸€ä¸ªç‰¹åˆ«å®žç”¨çš„é¢†åŸŸï¼Œå› ä¸ºä¸€äº›éœ€è¦é€‚åº”çš„çŽ°å®žåº”ç”¨ç¨‹åºä¼šé‡åˆ°æ— æ³•èŽ·å¾—æ¥è‡ªç›®æ ‡åŸŸçš„æ ‡è®°ç¤ºä¾‹çš„é—®é¢˜ã€‚äº‹å®žä¸Šï¼ŒSFDA æ­£å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ [&lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;1&lt;/a>;ã€&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot; >;2&lt;/a>;ã€&lt;a href=&quot;https://arxiv.org/abs/2110.04202&quot;>;3&lt;/a>;ã€&lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;4 &lt;/a>;]ã€‚ç„¶è€Œï¼Œå°½ç®¡å‡ºäºŽé›„å¿ƒå‹ƒå‹ƒçš„ç›®æ ‡ï¼Œå¤§å¤šæ•° SFDA ç ”ç©¶éƒ½åŸºäºŽä¸€ä¸ªéžå¸¸ç‹­çª„çš„æ¡†æž¶ï¼Œè€ƒè™‘å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ç®€å•çš„&lt;a href=&quot;https://arxiv.org/abs/2108.13624&quot;>;åˆ†å¸ƒå˜åŒ–&lt;/a>;ã€‚ &lt;/p>; &lt;p>; ä¸Žè¿™ä¸€è¶‹åŠ¿æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘ç”Ÿç‰©å£°å­¦é¢†åŸŸï¼Œå…¶ä¸­è‡ªç„¶å‘ç”Ÿçš„åˆ†å¸ƒå˜åŒ–æ— å¤„ä¸åœ¨ï¼Œé€šå¸¸ä»¥ç›®æ ‡æ ‡è®°æ•°æ®ä¸è¶³ä¸ºç‰¹å¾ï¼Œè¿™å¯¹ä»Žä¸šè€…æ¥è¯´æ˜¯ä¸€ä¸ªéšœç¢ã€‚å› æ­¤ï¼Œåœ¨æ­¤åº”ç”¨ä¸­ç ”ç©¶ SFDA ä¸ä»…å¯ä»¥è®©å­¦æœ¯ç•Œäº†è§£çŽ°â€‹â€‹æœ‰æ–¹æ³•çš„æ™®éæ€§å¹¶ç¡®å®šå¼€æ”¾çš„ç ”ç©¶æ–¹å‘ï¼Œè€Œä¸”è¿˜å¯ä»¥ç›´æŽ¥ä½¿è¯¥é¢†åŸŸçš„ä»Žä¸šè€…å—ç›Šï¼Œå¹¶æœ‰åŠ©äºŽè§£å†³æœ¬ä¸–çºªæœ€å¤§çš„æŒ‘æˆ˜ä¹‹ä¸€ï¼šç”Ÿç‰©å¤šæ ·æ€§ä¿å­˜ã€‚ &lt;/p>; &lt;p>; åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å®£å¸ƒâ€œ&lt;a href=&quot;https://arxiv.org/abs/2302.06658&quot;>;å¯»æ‰¾ä¸€ç§é€šç”¨çš„æ— æºåŸŸé€‚åº”æ–¹æ³•&lt;/a>;â€ï¼Œå‡ºçŽ°åœ¨ &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023ã€‚&lt;/a>;æˆ‘ä»¬è¡¨æ˜Žï¼Œæœ€å…ˆè¿›çš„ SFDA æ–¹æ³•åœ¨é¢å¯¹çŽ°å®žçš„åˆ†å¸ƒå˜åŒ–æ—¶å¯èƒ½è¡¨çŽ°ä¸ä½³ç”šè‡³å´©æºƒåœ¨ç”Ÿç‰©å£°å­¦ä¸­ã€‚æ­¤å¤–ï¼ŒçŽ°æœ‰æ–¹æ³•ç›¸å¯¹äºŽå½¼æ­¤çš„è¡¨çŽ°ä¸Žè§†è§‰åŸºå‡†ä¸­è§‚å¯Ÿåˆ°çš„ä¸åŒï¼Œå¹¶ä¸”ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæœ‰æ—¶è¡¨çŽ°æ¯”æ ¹æœ¬æ²¡æœ‰é€‚åº”æ›´å·®ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†NOTELAï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç®€å•æ–¹æ³•ï¼Œå®ƒåœ¨è¿™äº›è½¬å˜ä¸Šä¼˜äºŽçŽ°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶åœ¨ä¸€ç³»åˆ—è§†è§‰æ•°æ®é›†ä¸Šè¡¨çŽ°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å¾—å‡ºçš„ç»“è®ºæ˜¯ï¼Œï¼ˆä»…ï¼‰åœ¨å¸¸ç”¨æ•°æ®é›†å’Œåˆ†å¸ƒå˜åŒ–ä¸Šè¯„ä¼° SFDA æ–¹æ³•ï¼Œè®©æˆ‘ä»¬å¯¹å…¶ç›¸å¯¹æ€§èƒ½å’Œæ™®éæ€§äº§ç”Ÿäº†çŸ­è§†çš„çœ‹æ³•ã€‚ä¸ºäº†å…‘çŽ°ä»–ä»¬çš„æ‰¿è¯ºï¼ŒSFDA æ–¹æ³•éœ€è¦åœ¨æ›´å¹¿æ³›çš„åˆ†å¸ƒå˜åŒ–ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬ä¸»å¼ è€ƒè™‘å¯ä»¥æœ‰åˆ©äºŽé«˜å½±å“åŠ›åº”ç”¨çš„è‡ªç„¶å‘ç”Ÿçš„æ–¹æ³•ã€‚ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ç”Ÿç‰©å£°å­¦ä¸­çš„åˆ†å¸ƒå˜åŒ–&lt;/h2>; &lt;p>; è‡ªç„¶å‘ç”Ÿçš„åˆ†å¸ƒå˜åŒ–åœ¨ç”Ÿç‰©å£°å­¦ä¸­æ™®éå­˜åœ¨ã€‚æœ€å¤§çš„é¸Ÿç±»æ­Œæ›²æ ‡è®°æ•°æ®é›†æ˜¯ &lt;a href=&quot;https://www.researchgate.net/publication/280978332_The_Xeno-canto_collection_and_its_relation_to_sound_recognition_and_classification&quot;>;Xeno-Canto&lt;/a>; (XC)ï¼Œè¿™æ˜¯ç”¨æˆ·è´¡çŒ®çš„é‡Žç”Ÿé¸Ÿç±»å½•éŸ³çš„é›†åˆæ¥è‡ªä¸–ç•Œå„åœ°çš„é¸Ÿç±»ã€‚ XC ä¸­çš„å½•éŸ³æ˜¯â€œç„¦ç‚¹â€çš„ï¼šå®ƒä»¬é’ˆå¯¹çš„æ˜¯åœ¨è‡ªç„¶æ¡ä»¶ä¸‹æ•èŽ·çš„ä¸ªä½“ï¼Œå…¶ä¸­æ‰€è¯†åˆ«çš„é¸Ÿçš„æ­Œå£°ä½äºŽå‰æ™¯ã€‚ç„¶è€Œï¼Œå‡ºäºŽæŒç»­ç›‘æŽ§å’Œè·Ÿè¸ªçš„ç›®çš„ï¼Œä»Žä¸šè€…é€šå¸¸æ›´æ„Ÿå…´è¶£çš„æ˜¯åœ¨é€šè¿‡å…¨å‘éº¦å…‹é£ŽèŽ·å¾—çš„è¢«åŠ¨å½•éŸ³ï¼ˆâ€œå£°æ™¯â€ï¼‰ä¸­è¯†åˆ«é¸Ÿç±»ã€‚è¿™æ˜¯ä¸€ä¸ª&lt;a href=&quot;https://www.researchgate.net/publication/344893963_Overview_of_BirdCLEF_2020_Bird_Sound_Recognition_in_Complex_Acoustic_Environments&quot;>;æœ€è¿‘&lt;/a>;&lt;a href=&quot;https://www.sciencedirect.com/science/ Article/pii/S1574954121000273&quot;>;å·¥ä½œ&lt;/a>; å±•ç¤ºéžå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å—åˆ°è¿™ä¸ªçŽ°å®žåº”ç”¨çš„å¯å‘ï¼Œæˆ‘ä»¬ä½¿ç”¨åœ¨ XC ä¸Šé¢„å…ˆè®­ç»ƒçš„é¸Ÿç±»åˆ†ç±»å™¨ä½œä¸ºæºæ¨¡åž‹ï¼Œä»¥åŠæ¥è‡ªä¸åŒåœ°ç†ä½ç½®çš„å‡ ä¸ªâ€œå£°æ™¯â€æ¥ç ”ç©¶ SFDA çš„ç”Ÿç‰©å£°å­¦ â€” &lt;a href=&quot;https://doi. org/10.5281/zenodo.7050013&quot;>;å†…åŽè¾¾å±±è„‰&lt;/a>;ï¼ˆå†…åŽè¾¾å·žï¼‰ï¼› &lt;a href=&quot;https://doi.org/10.1002/ecy.3329&quot;>;Powdermill&lt;/a>; è‡ªç„¶ä¿æŠ¤åŒºï¼Œç¾Žå›½å®¾å¤•æ³•å°¼äºšå·žï¼› &lt;a href=&quot;https://doi.org/10.5281/zenodo.7078498&quot;>;å¤å¨å¤·&lt;/a>;ï¼›ç¾Žå›½åŠ åˆ©ç¦å°¼äºšå·žå¡æ™®å°”æ–¯åˆ†æ°´å²­ï¼› &lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;Sapsucker Woods&lt;/a>;ï¼Œç¾Žå›½çº½çº¦ (SSW)ï¼›å’Œ &lt;a href=&quot;https://www.google.com/url?q=https://zenodo.org/record/7525349%23.ZB8z_-xudhE&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1688498539746392&amp;amp; usg=AOvVaw07CsIKIE-dcNyMKFT-n_JT&quot;>;å“¥ä¼¦æ¯”äºš&lt;/a>; â€” ä½œä¸ºæˆ‘ä»¬çš„ç›®æ ‡åŸŸã€‚ &lt;/p>; &lt;p>; è¿™ç§ä»Žèšç„¦åŸŸåˆ°æ— æºåŸŸçš„è½¬å˜æ˜¯å·¨å¤§çš„ï¼šåŽè€…çš„å½•éŸ³é€šå¸¸å…·æœ‰ä½Žå¾—å¤šçš„ä¿¡å™ªæ¯”ï¼Œå‡ åªé¸ŸåŒæ—¶å‘å£°ï¼Œä»¥åŠæ˜Žæ˜¾çš„å¹²æ‰°å› ç´ å’ŒçŽ¯å¢ƒå™ªéŸ³ï¼Œä¾‹å¦‚é›¨æˆ–é£Žã€‚æ­¤å¤–ï¼Œä¸åŒçš„éŸ³æ™¯æºè‡ªä¸åŒçš„åœ°ç†ä½ç½®ï¼Œä»Žè€Œå¯¼è‡´æžç«¯çš„æ ‡ç­¾å˜åŒ–ï¼Œå› ä¸º XC ä¸­çš„ç‰©ç§çš„ä¸€å°éƒ¨åˆ†å°†å‡ºçŽ°åœ¨ç»™å®šä½ç½®ã€‚æ­¤å¤–ï¼Œæ­£å¦‚çŽ°å®žä¸–ç•Œæ•°æ®ä¸­å¸¸è§çš„é‚£æ ·ï¼ŒæºåŸŸå’Œç›®æ ‡åŸŸéƒ½å­˜åœ¨æ˜Žæ˜¾çš„ç±»åˆ«ä¸å¹³è¡¡ï¼Œå› ä¸ºæŸäº›ç‰©ç§æ¯”å…¶ä»–ç‰©ç§æ›´å¸¸è§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è€ƒè™‘äº†å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜ï¼Œå› ä¸ºæ¯ä¸ªè®°å½•ä¸­å¯èƒ½ä¼šè¯†åˆ«å‡ºå‡ åªé¸Ÿç±»ï¼Œè¿™ä¸Žé€šå¸¸ç ”ç©¶ SFDA çš„æ ‡å‡†å•æ ‡ç­¾å›¾åƒåˆ†ç±»åœºæ™¯æœ‰å¾ˆå¤§ä¸åŒã€‚ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeG afX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQOzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s1378/image3.png&quot;æ ·å¼=&quot;å·¦è¾¹è·ï¼šè‡ªåŠ¨ï¼›å³è¾¹è·ï¼šè‡ªåŠ¨ï¼›&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;1378&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeGafX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQ OzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;â€œç„¦ç‚¹â†’éŸ³æ™¯â€è½¬å˜çš„å›¾ç¤ºã€‚åœ¨èšç„¦åŸŸä¸­ï¼Œå½•éŸ³é€šå¸¸ç”±å‰æ™¯ä¸­çš„å•ä¸ªé¸Ÿç±»å‘å£°ç»„æˆï¼Œä»¥é«˜ä¿¡å™ªæ¯” (SNR) æ•èŽ·ï¼Œå°½ç®¡èƒŒæ™¯ä¸­å¯èƒ½è¿˜æœ‰å…¶ä»–é¸Ÿç±»å‘å£°ã€‚&lt;strong>; &lt;/strong>;å¦ä¸€æ–¹é¢ï¼ŒéŸ³æ™¯åŒ…å«å…¨å‘éº¦å…‹é£Žçš„å½•éŸ³ï¼Œå¯ä»¥ç”±å¤šåªé¸ŸåŒæ—¶å‘å£°ä»¥åŠæ˜†è™«ã€é›¨ã€æ±½è½¦ã€é£žæœºç­‰çŽ¯å¢ƒå™ªéŸ³ç»„æˆã€‚&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/è¡¨>; &lt;br />; &lt;è¡¨align =â€œä¸­å¿ƒâ€cellpadding =â€œ0â€cellspacing =â€œ0â€class =â€œtr-caption-containerâ€>; &lt;tbody>; &lt;tr>; &lt;td style =â€œtext-alignï¼šå·¦; &quot;>;&lt;b>;éŸ³é¢‘æ–‡ä»¶&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align : left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;ç„¦ç‚¹åŸŸ&lt;em>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audiocontrols=&quot;controls&quot; src=&quot;https:// /storage.googleapis.com/chirp-public-bucket/notela-blog-post/XC417991%20-%20Yellow-throated%20Vireo%20-%20Vireo%20flavifrons.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/ em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;éŸ³æ™¯åŸŸå&lt;em>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>; &lt;/sup>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://storage.googleapis.com/chirp-public-bucket/notela-blog-post/ Yetvir-soundscape.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/em>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;é¢‘è°±å›¾å›¾åƒ&lt;/ b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href= â€œhttps://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR 4VQQSIFLm6u1t0NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/s936/left.png&quot; style=&quot;margin-left: è‡ªåŠ¨; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR4VQQSIFLm6u1t0 NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/w640-h546/left.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;td>; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iy jBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTCmlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/s936/å³.png&quot; style=&quot;margin-left: è‡ªåŠ¨; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iyjBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTC mlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/w640-h546/right.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;tableå¯¹é½=â€œä¸­å¿ƒâ€cellpadding=â€œ0â€cellspacing=â€œ0â€ç±»=â€œtr-caption-containerâ€æ ·å¼=â€œmargin-leftï¼šè‡ªåŠ¨ï¼› margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ä»Žç„¦ç‚¹åŸŸï¼ˆ&lt;b>;å‘å·¦&lt;/ b>;ï¼‰åˆ°éŸ³æ™¯åŸŸï¼ˆ&lt;b>;å³&lt;/b>;ï¼‰ï¼Œæ ¹æ®ä»£è¡¨æ€§çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆ&lt;b>;é¡¶éƒ¨&lt;/b>;ï¼‰å’Œé¢‘è°±å›¾å›¾åƒï¼ˆ&lt;b>;åº•éƒ¨&lt;/b>;ï¼‰æ¯ä¸ªæ•°æ®é›†çš„å½•éŸ³ã€‚è¯·æ³¨æ„ï¼Œåœ¨ç¬¬äºŒä¸ªéŸ³é¢‘å‰ªè¾‘ä¸­ï¼Œé¸Ÿé¸£å£°éžå¸¸å¾®å¼±ï¼›è¿™æ˜¯éŸ³æ™¯å½•éŸ³ä¸­çš„ä¸€ä¸ªå¸¸è§å±žæ€§ï¼Œå…¶ä¸­é¸Ÿå«å£°ä¸åœ¨â€œå‰æ™¯â€ã€‚å­¦åˆ†ï¼š&lt;b>;å·¦ï¼š&lt;/b>; XC Sue Riffe &lt;a href=&quot;https://xeno-canto.org/417991&quot;>;å½•éŸ³&lt;/a>; (&lt;a href=&quot;https://creativecommons.org/licenses/by-nc-sa/4.0/ &quot;>;CC-BY-NC license&lt;/a>;). &lt;b>;Right:&lt;/b>; Excerpt from a recording made available by Kahl, Charif, &amp;amp; Klinck. (2022) &quot;A collection of fully-annotated soundscape recordings from the Northeastern United States&quot; [&lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;link&lt;/a>;] from the SSW soundscape dataset (&lt;a href=&quot;https://creativecommons. org/licenses/by/4.0/&quot;>;CC-BY license&lt;/a>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;State-of-the-art SFDA models perform poorly on bioacoustics shifts&lt;/h2>; &lt;p>; As a starting point, we benchmark six state-of-the-art SFDA methods on our bioacoustics benchmark, and compare them to the &lt;em>;non-adapted&lt;/em>; baseline (the source model). Our findings are surprising: without exception, existing methods are unable to consistently outperform the source model on all target domains. In fact, they often underperform it significantly. &lt;/p>; &lt;p>; As an example, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>;, a recent method, aims to make models produce confident predictions for each example by reducing the uncertainty of the model&#39;s output probabilities. While Tent performs well in various tasks, it doesn&#39;t work effectively for our bioacoustics task. In the single-label scenario, minimizing entropy forces the model to choose a single class for each example confidently. However, in our multi-label scenario, there&#39;s no such constraint that any class should be selected as being present. Combined with significant distribution shifts, this can cause the model to collapse, leading to zero probabilities for all classes. Other benchmarked methods like &lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;, &lt;a href=&quot;https://openreview.net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;DUST&lt;/a>; and &lt;a href=&quot;https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;Pseudo-Labelling&lt;/a>;, which are strong baselines for standard SFDA benchmarks, also struggle with this bioacoustics task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj38druynuW9zh26no15rOJCj1aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s1434/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;730&quot; data-original-width=&quot;1434&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj38druynuW9zh26no15rOJCj1aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Evolution of the test &lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Mean_average_precision&quot;>;mean average precision&lt;/a>; (mAP), a standard metric for multilabel classification, throughout the adaptation procedure on the six soundscape datasets. We benchmark our proposed NOTELA and Dropout Student (see below), as well as &lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;, &lt;a href=&quot;https://openreview.net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;DUST&lt;/a>; and &lt;a href=&quot;https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;Pseudo-Labelling&lt;/a>;. Aside from NOTELA, all other methods fail to consistently improve the source model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Introducing NOisy student TEacher with Laplacian Adjustment (NOTELA)&lt;/h2>; &lt;p>; Nonetheless, a surprisingly positive result stands out: the less celebrated &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;>;Noisy Student&lt;/a>; principle appears promising. This unsupervised approach encourages the model to reconstruct its own predictions on some target dataset, but under the application of random noise. While noise may be introduced through various channels, we strive for simplicity and use &lt;a href=&quot;https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9&quot;>;model dropout&lt;/a>; as the only noise source: we therefore refer to this approach as&lt;em>; Dropout Student (DS)&lt;/em>;. In a nutshell, it encourages the model to limit the influence of individual neurons (or filters) when making predictions on a specific target dataset. &lt;/p>; &lt;p>; DS, while effective, faces a model collapse issue on various target domains. We hypothesize this happens because the source model initially lacks confidence in those target domains. We propose improving DS stability by using the feature space directly as an auxiliary source of truth. NOTELA does this by encouraging similar pseudo-labels for nearby points in the feature space, inspired by &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&#39;s method&lt;/a>; and Laplacian &lt;a href=&quot;https://www.researchgate.net/publication/220319905_Manifold_Regularization_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples&quot;>;regularization&lt;/a>;. This simple approach is visualized below, and consistently and significantly outperforms the source model in both audio and visual tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4VZgRaN-w1jovOwJ4gEXoupksTEpvb5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s1870/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;124&quot; data-original-width=&quot;1870&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4VZgRaN-w1jovOwJ4gEXoupksTEpvb5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9QnJb5JxwLFwbmJJN_3jmlGB9-CTug92969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s1080/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9QnJb5JxwLFwbmJJN_3jmlGB9-CTug92969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;NOTELA in action. The audio recordings are forwarded through the full model to obtain a first set of predictions, which are then refined through Laplacian regularization, a form of post-processing based on clustering nearby points. Finally, the refined predictions are used as targets for the &lt;em>;noisy model &lt;/em>;to reconstruct.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The standard artificial image classification benchmarks have inadvertently limited our understanding of the true generalizability and robustness of SFDA methods. We advocate for broadening the scope and adopt a new assessment framework that incorporates naturally-occurring distribution shifts from bioacoustics. We also hope that NOTELA serves as a robust baseline to facilitate research in that direction. NOTELA&#39;s strong performance perhaps points to two factors that can lead to developing more generalizable models: first, developing methods with an eye towards harder problems and second, favoring simple modeling principles. However, there is still future work to be done to pinpoint and comprehend existing methods&#39; failure modes on harder problems. We believe that our research represents a significant step in this direction, serving as a foundation for designing SFDA methods with greater generalizability. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;One of the authors of this post, Eleni Triantafillou, is now at Google DeepMind. We are posting this blog post on behalf of the authors of the NOTELA paper: Malik Boudiaf, Tom Denton, Bart van MerriÃ«nboer, Vincent Dumoulin*, Eleni Triantafillou* (where * denotes equal contribution). We thank our co-authors for the hard work on this paper and the rest of the Perch team for their support and feedback.&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;Note that in this audio clip, the bird song is very faint; a common property in soundscape recordings where bird calls aren&#39;t at the â€œforegroundâ€.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;â†©&lt;/sup>;&lt;/a>;&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5980448298988153366/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/in-search-of-generalizable-method-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/in-search-of-generalizable-method-for.html&quot; rel=&quot;alternate&quot; title=&quot;In search of a generalizable method for source-free domain adaptation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4epC-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgyM4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/s72-c/notela.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2519516457542613363&lt;/id>;&lt;published>;2023-07-23T14:13:00.002-07:00&lt;/published>;&lt;updated>;2023-08-07T10:08:27.713-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ICML 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Cat Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4nvXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s320/Google-ICML-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Groups across Google actively pursue research in the field of machine learning (ML), ranging from theory and application. We build ML systems to solve deep scientific and engineering challenges in areas of language, music, visual processing, algorithm development, and more. We aim to build a more collaborative ecosystem with the broader ML research community through open-sourcing tools and datasets, publishing our work, and actively participating in conferences. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Google is proud to be a &lt;a href=&quot;https://icml.cc/virtual/2023/sponsor_list&quot;>;Diamond Sponsor&lt;/a>; of the 40th &lt;a href=&quot;https://icml.cc/virtual/2023/index.html&quot;>;International Conference on Machine Learning&lt;/a>; (ICML 2023), a premier annual conference, which is being held this week in Honolulu, Hawaii. As a leader in ML research, Google has a strong presence at this year&#39;s conference with over 120 accepted papers and active involvement in a number of workshops and tutorials. Google is also proud to be a Platinum Sponsor for both the &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; and &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;Women in Machine Learning&lt;/a>; workshops. We look forward to sharing some of our extensive ML research and expanding our partnership with the broader ML research community. &lt;/p>; &lt;p>; Registered for ICML 2023? We hope you&#39;ll visit the Google booth to learn more about the exciting work, creativity, and fun that goes into solving a portion of the field&#39;s most interesting challenges.è®¿é—® &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter å¸æˆ·ï¼Œäº†è§£ Google å±•ä½æ´»åŠ¨ï¼ˆä¾‹å¦‚æ¼”ç¤ºå’Œé—®ç­”çŽ¯èŠ‚ï¼‰ã€‚ See &lt;a href=&quot;http://www.deepmind.com/blog/google-deepmind-research-at-icml-2023&quot;>;Google DeepMind&#39;s blog&lt;/a>; to learn about their technical participation at ICML 2023. &lt;/p>; &lt;p>; Take a look below to learn more about the Google research being presented at ICML 2023 (Google affiliations in &lt;strong>;bold&lt;/strong>;).&lt;/p>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Board Members include: &lt;strong>;&lt;em>;Corinna Cortes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>; &lt;br>; Tutorial Chairs include: &lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Lhyy8H75KA&quot;>;Scaling Vision Transformers to 22 Billion Parameters&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Josip Djolonga&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Basil Mustafa&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Piotr Padlewski&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Heek&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Justin Gilmer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andreas Steiner&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Robert Geirhos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ibrahim Alabdulmohsin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lucas Beyer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Tschannen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiao Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Carlos Riquelme&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthias Minderer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joan Puigcerver,&lt;/em>; &lt;em>;Utku Evci&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Manoj Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gamaleldin F. Elsayed&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fisher Yu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Avital Oliver&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fantine Huot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mark Patrick Collier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alexey Gritsenko&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vighnesh Birodkar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Cristina Vasconcelos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Mensink&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alexander Kolesnikov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Filip PavetiÄ‡&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dustin Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mario LuÄiÄ‡&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiaohua Zhai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel Keysers&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremiah Harmsen&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Neil Houlsby&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=C9NEblP8vS&quot;>;Fast Inference from Transformers via Speculative Decoding&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yaniv Leviathan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matan Kalman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yossi Matias&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=bUFUaawOTk&quot;>;Best of Both Worlds Policy Optimization&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;, &lt;em>;Chen-Yu Wei&lt;/em>;, &lt;strong>;&lt;em>;Julian Zimmert&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=9PJ2V6qvQL&quot;>;Inflow, Outflow, and Reciprocity in Machine Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Walid Krichene&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;Transformers Learn In-Context by Gradient Descent&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Eyvind Niklasson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ettore Randazzo&lt;/em>;&lt;/strong>;, &lt;em>;JoÃ£o Sacramento&lt;/em>;,&lt;strong>; &lt;em>;Alexander Mordvintsev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andrey Zhmoginov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Max Vladymyrov&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=EfhmBBrXY2&quot;>;Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Luke Vilnis&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;, &lt;em>;Patrick Murray*&lt;/em>;, &lt;em>;Alexandre Passos*&lt;/em>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ayBKRjGDEI&quot;>;Differentially Private Hierarchical Clustering with Provable Approximation Guarantees&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/differentially-private-clustering-for.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Jacob Imola&lt;/em>;*,&lt;strong>; &lt;em>;Alessandro Epasto&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mohammad Mahdian&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vincent Cohen-Addad&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ZVxT2ToHR5&quot;>;Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;H. Brendan McMahan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Keith Rush&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=1UaGAhLAsL&quot;>;Random Classification Noise Does Not Defeat All Convex Potential Boosters Irrespective of Model Choice&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Richard Nock&lt;/em>;&lt;/strong>;, &lt;em>;Robert Williamson&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=qw8zAw6mzJ&quot;>;Simplex Random Features&lt;/a>; &lt;br>; &lt;em>;Isaac Reid&lt;/em>;,&lt;strong>; &lt;em>;Krzysztof Choromanski&lt;/em>;&lt;/strong>;, &lt;em>;Valerii Likhosherstov&lt;/em>;, &lt;em>;Adrian Weller&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=bF1LVbP493&quot;>;Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mandar Joshi&lt;/em>;&lt;/strong>;, &lt;em>;Iulia Turc&lt;/em>;,&lt;strong>; &lt;em>;Hexiang Hu&lt;/em>;&lt;/strong>;, &lt;em>;Fangyu Liu&lt;/em>;,&lt;strong>; &lt;em>;Julian Eisenschlos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Shaw&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Wei Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kristina Toutanova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=eIQIcUKs0T&quot;>;Mu&lt;sup>;2&lt;/sup>;SLAM: Multitask, Multilingual Speech and Language Models&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yong Cheng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yu Zhang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Melvin Johnson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wolfgang Macherey&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ankur Bapna&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=5h42xM0pwn&quot;>;Robust Budget Pacing with a Single Sample&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Santiago Balseiro&lt;/em>;&lt;/strong>;, &lt;em>;Rachitesh Kumar&lt;/em>;*,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Balasubramanian Sivan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Di Wang&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=0bR5JuxaoN&amp;amp;name=pdf&quot;>;A Statistical Perspective on Retrieval-Based Models&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Soumya Basu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;, &lt;em>;Manzil Zaheer&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=XjTcC4EA4P&quot;>;Approximately Optimal Core Shapes for Tensor Decompositions&lt;/a>; &lt;br>; &lt;em>;Mehrdad Ghadiri&lt;/em>;,&lt;strong>; &lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Fu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=rWGp9FbS0Q&amp;amp;name=pdf&quot;>;Efficient List-Decodable Regression Using Batches&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Abhimanyu Das&lt;/em>;&lt;/strong>;, &lt;em>;Ayush Jain&lt;/em>;*,&lt;strong>; &lt;em>;Weihao Kong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rajat Sen&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=SpFIO5Mdso&amp;amp;name=pdf&quot;>;Efficient Training of Language Models Using Few-Shot Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Sashank J. Reddi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sobhan Miryoosefi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Stefani Karp&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shankar Krishnan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Seungyeon Kim&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sanjiv Kumar&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=Bj76bauv1Q&amp;amp;name=pdf&quot;>;Fully Dynamic Submodular Maximization Over Matroids&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;, &lt;em>;Federico Fusco&lt;/em>;, &lt;strong>;&lt;em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Morteza Zadimoghaddam&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=VlEAJkmlMs&amp;amp;name=pdf&quot;>;GFlowNet-EM for Learning Compositional Latent Variable Models&lt;/a>; &lt;br>; &lt;em>;Edward J Hu&lt;/em>;, &lt;em>;Nikolay Malkin&lt;/em>;, &lt;em>;Moksh Jain&lt;/em>;, &lt;strong>;&lt;em>;Katie Everett&lt;/em>;&lt;/strong>;, &lt;em>;Alexandros Graikos&lt;/em>;, &lt;em>;Yoshua Bengio&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rB0VaD44FZ&quot;>;Improved Online Learning Algorithms for CTR Prediction in Ad Auctions&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Zhe Feng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>;, &lt;em>;Zixin Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sfdKdeczaw&amp;amp;name=pdf&quot;>;Large Language Models Struggle to Learn Long-Tail Knowledge&lt;/a>; &lt;br>; &lt;em>;Nikhil Kandpal&lt;/em>;, &lt;em>;Haikang Deng&lt;/em>;,&lt;strong>; &lt;em>;Adam Roberts&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Eric Wallace&lt;/em>;&lt;/strong>;, &lt;em>;Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=UdiUd99I81&quot;>;Multi-channel Autobidding with Budget and ROI Constraints&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yuan Deng&lt;/em>;&lt;/strong>;, &lt;em>;Negin Golrezaei&lt;/em>;, &lt;em>;Patrick Jaillet&lt;/em>;, &lt;em>;Jason Cheuk Nam Liang&lt;/em>;, &lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZMvv6laV5b&amp;amp;name=pdf&quot;>;Multi-layer Neural Networks as Trainable Ladders of Hilbert Spaces&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Zhengdao Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfkSyUJyqg&quot;>;On User-Level Private Convex Optimization&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Badih Ghazi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pritish Kamath&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ravi Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghu Meka&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pasin Manurangsi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Chiyuan Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zAgouWgI7b&amp;amp;name=pdf&quot;>;PAC Generalization via Invariant Representations&lt;/a>; &lt;br>; &lt;em>;Advait U Parulekar&lt;/em>;,&lt;strong>; &lt;em>;Karthikeyan Shanmugam&lt;/em>;&lt;/strong>;, &lt;em>;Sanjay Shakkottai&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0rZvMIfECW&amp;amp;name=pdf&quot;>;Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice&lt;/a>; &lt;br>; &lt;em>;Toshinori Kitamura&lt;/em>;, &lt;em>;Tadashi Kozuno&lt;/em>;, &lt;em>;Yunhao Tang&lt;/em>;, &lt;strong>;&lt;em>;Nino Vieillard&lt;/em>;&lt;/strong>;, &lt;em>;Michal Valko&lt;/em>;, &lt;em>;Wenhao Yang&lt;/em>;,&lt;strong>; &lt;em>;Jincheng Mei&lt;/em>;&lt;/strong>;, &lt;em>;Pierre Menard&lt;/em>;, &lt;em>;Mohammad Gheshlaghi Azar&lt;/em>;, &lt;em>;Remi Munos&lt;/em>;,&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;,&lt;em>;Csaba Szepesvari&lt;/em>;, &lt;em>;Wataru Kumagai&lt;/em>;, &lt;em>;Yutaka Matsuo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mrykt39VUw&amp;amp;name=pdf&quot;>;Speeding Up Bellman Ford via Minimum Violation Permutations&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;, &lt;em>;Ola Svensson&lt;/em>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LxodbQa62n&amp;amp;name=pdf&quot;>;Statistical Indistinguishability of Learning Algorithms&lt;/a>; &lt;br>; &lt;em>;Alkis Kalavasis&lt;/em>;,&lt;strong>; &lt;em>;Amin Karbasi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shay Moran&lt;/em>;&lt;/strong>;, &lt;em>;Grigoris Velegkas&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=G5vKSJVhJL&quot;>;Test-Time Adaptation with Slot-Centric Models&lt;/a>; &lt;br>; &lt;em>;Mihir Prabhudesai&lt;/em>;, &lt;em>;Anirudh Goyal&lt;/em>;,&lt;strong>; &lt;em>;Sujoy Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Gaurav Aggarwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;, &lt;em>;Deepak Pathak&lt;/em>;, &lt;em>;Katerina Fragkiadaki>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=2WEMW6rGgG&quot;>;Algorithms for Bounding Contribution for Histogram Estimation Under User-Level Privacy&lt;/a>; &lt;br>; &lt;em>;Yuhan Liu&lt;/em>;*, &lt;strong>;&lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Wennan Zhu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Marco Gruteser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=SgeIqUvo4w&amp;amp;name=pdf&quot;>;Bandit Online Linear Optimization with Hints and Queries&lt;/a>; &lt;br>; &lt;em>;Aditya Bhaskara&lt;/em>;, &lt;em>;Ashok Cutkosky&lt;/em>;,&lt;strong>; &lt;em>;Ravi Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Manish Purohit&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=wagsJnR5GO&amp;amp;name=pdf&quot;>;CLUTR: Curriculum Learning via Unsupervised Task Representation Learning&lt;/a>; &lt;br>; &lt;em>;Abdus Salam Azad&lt;/em>;,&lt;strong>; &lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;, &lt;em>;Jasper Emhoff&lt;/em>;, &lt;em>;Nathaniel Alexis&lt;/em>;,&lt;strong>; &lt;em>;Aleksandra Faust&lt;/em>;&lt;/strong>;, &lt;em>;Pieter Abbeel&lt;/em>;, &lt;em>;Ion Stoica&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=R3WrLjtzG8&amp;amp;name=pdf&quot;>;CSP: Self-Supervised Contrastive Spatial Pre-training for Geospatial-Visual Representations&lt;/a>; &lt;br>; &lt;em>;Gengchen Mai&lt;/em>;, &lt;strong>;&lt;em>;Ni Lao&lt;/em>;&lt;/strong>;, &lt;em>;Yutong He&lt;/em>;, &lt;em>;Jiaming Song&lt;/em>;, &lt;em>;Stefano Ermon&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=vd5JYAml0A&amp;amp;name=pdf&quot;>;Ewald-Based Long-Range Message Passing for Molecular Graphs&lt;/a>; &lt;br>; &lt;em>;Arthur Kosmala&lt;/em>;,&lt;strong>; &lt;em>;Johannes Gasteiger&lt;/em>;&lt;/strong>;, &lt;em>;Nicholas Gao&lt;/em>;, &lt;em>;Stephan GÃ¼nnemann&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Iey50XHA3g&amp;amp;name=pdf&quot;>;Fast (1+Îµ)-Approximation Algorithms for Binary Matrix Factorization&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;, &lt;em>;Maximilian VÃ¶tsch&lt;/em>;, &lt;strong>;&lt;em>;David Woodruff&lt;/em>;&lt;/strong>;, &lt;em>;Samson Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=b9opfVNw6O&amp;amp;name=pdf&quot;>;Federated Linear Contextual Bandits with User-Level Differential Privacy&lt;/a>; &lt;br>; &lt;em>;Ruiquan Huang&lt;/em>;, &lt;em>;Huanyu Zhang&lt;/em>;, &lt;em>;Luca Melis&lt;/em>;, &lt;em>;Milan Shen&lt;/em>;,&lt;strong>; &lt;em>;Meisam Hejazinia&lt;/em>;&lt;/strong>;, &lt;em>;Jing Yang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=m21SgZnBWZ&amp;amp;name=pdf&quot;>;Investigating the Role of Model-Based Learning in Exploration and Transfer&lt;/a>; &lt;br>; &lt;em>;Jacob C Walker&lt;/em>;, &lt;em>;Eszter VÃ©rtes&lt;/em>;, &lt;em>;Yazhe Li&lt;/em>;,&lt;strong>; &lt;em>;Gabriel Dulac-Arnold&lt;/em>;&lt;/strong>;, &lt;em>;Ankesh Anand&lt;/em>;, &lt;em>;Theophane Weber&lt;/em>;,&lt;em>; Jessica B Hamrick&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=K1sJiHvy02&quot;>;Label Differential Privacy and Private Training Data Release&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Robert Busa-Fekete&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andres Munoz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Umar Syed&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Q4QFG5Fe4O&amp;amp;name=pdf&quot;>;Lifelong Language Pretraining with Distribution-Specialized Experts&lt;/a>; &lt;br>; &lt;em>;Wuyang Chen&lt;/em>;*, &lt;strong>;&lt;em>;Yanqi Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Nan Du&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yanping Huang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;James Laudon&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Zhifeng Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Claire Cui&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=06djx2x2Rf&amp;amp;name=pdf&quot;>;Multi-User Reinforcement Learning with Low Rank Rewards&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Dheeraj Mysore Nagaraj&lt;/em>;&lt;/strong>;, &lt;em>;Suhas S Kowshik&lt;/em>;, &lt;strong>;&lt;em>;Naman Agarwal&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Praneeth Netrapalli&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Prateek Jain&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DwOUndjwiV&amp;amp;name=pdf&quot;>;Multi-View Masked World Models for Visual Robotic Manipulation&lt;/a>; &lt;br>; &lt;em>;Younggyo Seo&lt;/em>;, &lt;em>;Junsu Kim&lt;/em>;, &lt;em>;Stephen James&lt;/em>;,&lt;strong>; &lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;, &lt;em>;Jinwoo Shin&lt;/em>;, &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VTpHpqM3Cf&amp;amp;name=pdf&quot;>;PaLM-E: An Embodied Multimodal Language Model&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Danny Driess&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Fei Xia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Corey Lynch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Ichter&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;Ayzaan Wahid&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan Tompson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Quan Vuong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tianhe Yu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wenlong Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yevgen Chebotar&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pierre Sermanet&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Duckworth&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vincent Vanhoucke&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Karol Hausman&lt;/em>;&lt;/strong>;, &lt;em>;Marc Toussaint&lt;/em>;,&lt;strong>; &lt;em>;Klaus Greff&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Igor Mordatch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pete Florence&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=y8qAZhWbNs&quot;>;Private Federated Learning with Autotuned Compression&lt;/a>; &lt;br>; &lt;em>;Enayat Ullah&lt;/em>;*, &lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=7WdMBofQFx&amp;amp;name=pdf&quot;>;Refined Regret for Adversarial MDPs with Linear Function Approximation&lt;/a>; &lt;br>; &lt;em>;Yan Dai&lt;/em>;, &lt;em>;Haipeng Luo&lt;/em>;, &lt;em>;Chen-Yu Wei&lt;/em>;, &lt;strong>;&lt;em>;Julian Zimmert&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ccwSdYv1GI&amp;amp;name=pdf&quot;>;Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory&lt;/a>; &lt;br>; &lt;em>;Justin Cui&lt;/em>;,&lt;em>; Ruoche Wan&lt;/em>;, &lt;strong>;&lt;em>;Si Si&lt;/em>;&lt;/strong>;, &lt;em>;Cho-Jui Hsieh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=X7jMTrwuCz&amp;amp;name=pdf&quot;>;SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance&lt;/a>; &lt;br>; &lt;em>;Amit Attia&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6EVUnWGBMU&quot;>;The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation&lt;/a>; &lt;br>; &lt;em>;Mark Rowland&lt;/em>;, &lt;em>;Yunhao Tang&lt;/em>;, &lt;em>;Clare Lyle&lt;/em>;, &lt;em>;RÃ©mi Munos&lt;/em>;, &lt;strong>;&lt;em>;Marc G. Bellemare&lt;/em>;&lt;/strong>;, &lt;em>;Will Dabney&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=iMHNLJRSVz&amp;amp;name=pdf&quot;>;Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features&lt;/a>; &lt;br>; &lt;em>;Chieh Hubert Lin&lt;/em>;, &lt;em>;Hung-Yu Tseng&lt;/em>;, &lt;em>;Hsin-Ying Lee&lt;/em>;, &lt;em>;Maneesh Kumar Singh&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4UStsbnfVT&quot;>;User-Level Private Stochastic Convex Optimization with Optimal Rates&lt;/a>; &lt;br>; &lt;em>;Raef Bassily&lt;/em>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6MU5xdrO7t&amp;amp;name=pdf&quot;>;A Simple Zero-Shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models&lt;/a>; &lt;br>; &lt;em>;James Urquhart Allingham&lt;/em>;*, &lt;strong>;&lt;em>;Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael W Dusenberry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiuye Gu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yin Cui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dustin Tran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremiah Zhe Liu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mXv2aVqUGG&amp;amp;name=pdf&quot;>;Can Large Language Models Reason About Program Invariants?&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kexin Pei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Bieber&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kensen Shi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Charles Sutton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pengcheng Yin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=pWeQdceMHL&amp;amp;name=pdf&quot;>;Concurrent Shuffle Differential Privacy Under Continual Observation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jay Tenenbaum&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Haim Kaplan&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Uri Stemmer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Xqedp0Iu1S&amp;amp;name=pdf&quot;>;Constant Matters: Fine-Grained Error Bound on Differentially Private Continual Observation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Hendrik Fichtenberger&lt;/em>;&lt;/strong>;, &lt;em>;Monika Henzinger&lt;/em>;, &lt;em>;Jalaj Upadhyay&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=NfCA622s8O&amp;amp;name=pdf&quot;>;Cross-Entropy Loss Functions: Theoretical Analysis and Applications&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;, &lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;, &lt;em>;Yutao Zhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=5UZYtGEPTt&amp;amp;name=pdf&quot;>;Efficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation&lt;/a>; &lt;br>; &lt;em>;Orin Levy&lt;/em>;, &lt;strong>;&lt;em>;Alon Cohen&lt;/em>;&lt;/strong>;,&lt;em>; Asaf Cassel&lt;/em>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KrsaROSs8b&amp;amp;name=pdf&quot;>;Fairness in Streaming Submodular Maximization Over a Matroid Constraint&lt;/a>; &lt;br>; &lt;em>;Marwa El Halabi&lt;/em>;, &lt;em>;Federico Fusco&lt;/em>;, &lt;strong>;&lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jakab Tardos&lt;/em>;&lt;/strong>;, &lt;em>;Jakub Tarnawski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZX4uS605XV&amp;amp;name=pdf&quot;>;The Flan Collection: Designing Data and Methods for Effective Instruction Tuning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Shayne Longpre&lt;/em>;, &lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tu Vu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Albert Webson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hyung Won Chung&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Quoc V Le&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Barret Zoph&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jason Wei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adam Roberts&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=rzN05i4GOE&amp;amp;name=pdf&quot;>;Graph Reinforcement Learning for Network Control via Bi-level Optimization&lt;/a>; &lt;br>; &lt;em>;Daniele Gammelli&lt;/em>;,&lt;strong>; &lt;em>;James Harrison&lt;/em>;&lt;/strong>;, &lt;em>;Kaidi Yang&lt;/em>;, &lt;em>;Marco Pavone&lt;/em>;, &lt;em>;Filipe Rodrigues&lt;/em>;, &lt;em>;Francisco C. Pereira&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Fgn23Fsmtv&amp;amp;name=pdf&quot;>;Learning-Augmented Private Algorithms for Multiple Quantile Release&lt;/a>; &lt;br>; &lt;em>;Mikhail Khodak&lt;/em>;*, &lt;strong>;&lt;em>;Kareem Amin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Travis Dick&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=uSHBQdWmuC&amp;amp;name=pdf&quot;>;LegendreTron: Uprising Proper Multiclass Loss Learning&lt;/a>; &lt;br>; &lt;em>;Kevin H Lam&lt;/em>;, &lt;strong>;&lt;em>;Christian Walder&lt;/em>;&lt;/strong>;, &lt;em>;Spiridon Penev&lt;/em>;,&lt;strong>; &lt;em>;Richard Nock&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VnGIZsmxDG&amp;amp;name=pdf&quot;>;Measuring the Impact of Programming Language Distribution&lt;/a>; &lt;br>; &lt;em>;Gabriel Orlanski&lt;/em>;*, &lt;strong>;&lt;em>;Kefan Xiao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeffrey Hui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Howland&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Malmaud&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jacob Austin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rishabh Singh&lt;/em>;&lt;/strong>;, &lt;em>;Michele Catasta&lt;/em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=f69OtekDi4&quot;>;Multi-task Differential Privacy Under Distribution Skew&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Walid Krichene&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shuang Song&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Li Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=hi9UssZdHR&quot;>;Muse: Text-to-Image Generation via Masked Generative Transformers&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Huiwen Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Han Zhang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jarred Barber&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;AJ Maschinot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;JosÃ© Lezama&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lu Jiang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kevin Murphy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;William T. Freeman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Rubinstein&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuanzhen Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dilip Krishnan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;On the Convergence of Federated Averaging with Cyclic Client Participation&lt;/a>; &lt;br>; &lt;em>;Yae Jee Cho&lt;/em>;, &lt;em>;Pranay Sharma&lt;/em>;, &lt;em>;Gauri Joshi&lt;/em>;, &lt;strong>;&lt;em>;Zheng Xu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tong Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=GimajxXNc0&amp;amp;name=pdf&quot;>;Optimal Stochastic Non-smooth Non-convex Optimization Through Online-to-Non-convex Conversion&lt;/a>; &lt;br>; &lt;em>;Ashok Cutkosky&lt;/em>;, &lt;strong>;&lt;em>;Harsh Mehta&lt;/em>;&lt;/strong>;, &lt;em>;Francesco Orabona&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=4SHQv4cp3I&amp;amp;name=pdf&quot;>;Out-of-Domain Robustness via Targeted Augmentations&lt;/a>; &lt;br>; &lt;em>;Irena Gao&lt;/em>;, &lt;em>;Shiori Sagawa&lt;/em>;,&lt;strong>; &lt;em>;Pang Wei Koh&lt;/em>;&lt;/strong>;, &lt;em>;Tatsunori Hashimoto&lt;/em>;,&lt;em>; Percy Liang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=b6Hxt4Jw10&quot;>;Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models&lt;/a>; &lt;br>; &lt;em>;Jamil Arbas&lt;/em>;, &lt;em>;Hassan Ashtiani&lt;/em>;,&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=nlUAvrMbUZ&amp;amp;name=pdf&quot;>;Pre-computed Memory or On-the-Fly Encoding? A Hybrid Approach to Retrieval Augmentation Makes the Most of Your Compute&lt;/a>; &lt;br>; &lt;em>;Michiel de Jong&lt;/em>;, &lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nicholas FitzGerald&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fei Sha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;William W. Cohen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1FldU7JzGh&amp;amp;name=pdf&quot;>;Scalable Adaptive Computation for Iterative Generation&lt;/a>; &lt;br>; &lt;em>;Allan Jabri&lt;/em>;*, &lt;strong>;&lt;em>;David J. Fleet&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ting Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HiKPaeowPB&quot;>;Scaling Spherical CNNs&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Carlos Esteves&lt;/em>;&lt;/strong>;, &lt;em>;Jean-Jacques Slotine&lt;/em>;, &lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0O7b2Y198V&amp;amp;name=pdf&quot;>;STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition&lt;/a>; &lt;br>; &lt;em>;Yucheng Lu&lt;/em>;, &lt;strong>;&lt;em>;Shivani Agrawal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Suvinay Subramanian&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Oleg Rybakov&lt;/em>;&lt;/strong>;, &lt;em>;Christopher De Sa&lt;/em>;, &lt;em>;Amir Yazdanbakhsh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LZt1HIEoAf&amp;amp;name=pdf&quot;>;Stratified Adversarial Robustness with Rejection&lt;/a>; &lt;br>; &lt;em>;Jiefeng Chen&lt;/em>;, &lt;em>;Jayaram Raghuram&lt;/em>;, &lt;em>;Jihye Choi&lt;/em>;,&lt;strong>; &lt;em>;Xi Wu&lt;/em>;&lt;/strong>;, &lt;em>;Yingyu Liang&lt;/em>;, &lt;em>;Somesh Jha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=CnHxxjqkMi&amp;amp;name=pdf&quot;>;When Does Privileged information Explain Away Label Noise?&lt;/a>; &lt;br>; &lt;em>;Guillermo Ortiz-Jimenez&lt;/em>;*,&lt;strong>; &lt;em>;Mark Collier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anant Nawalgaria&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alexander D&#39;Amour&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jesse Berent&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Effrosyni Kokiopoulou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2bGTacOn8v&amp;amp;name=pdf&quot;>;Adaptive Computation with Elastic Input Sequence&lt;/a>; &lt;br>; &lt;em>;Fuzhao Xue&lt;/em>;*, &lt;strong>;&lt;em>;Valerii Likhosherstov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;, &lt;em>;Yang You&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Pbaiy3fRCt&amp;amp;name=pdf&quot;>;Can Neural Network Memorization Be Localized?&lt;/a>; &lt;br>; &lt;em>;Pratyush Maini&lt;/em>;,&lt;strong>; &lt;em>;Michael C. Mozer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>;, &lt;em>;Zachary C. Lipton&lt;/em>;, &lt;em>;J. Zico Kolter&lt;/em>;,&lt;strong>; &lt;em>;Chiyuan Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Ct2N6RWZpQ&amp;amp;name=pdf&quot;>;Controllability-Aware Unsupervised Skill Discovery&lt;/a>; &lt;br>; &lt;em>;Seohong Park&lt;/em>;,&lt;strong>; &lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;, &lt;em>;Youngwoon Lee&lt;/em>;, &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2Mbo7IEtZW&amp;amp;name=pdf&quot;>;Efficient Learning of Mesh-Based Physical Simulation with Bi-Stride Multi-Scale Graph Neural Network&lt;/a>; &lt;br>; &lt;em>;Yadi Cao&lt;/em>;,&lt;strong>; &lt;em>;Menglei Chai&lt;/em>;&lt;/strong>;, &lt;em>;Minchen Li&lt;/em>;, &lt;em>;Chenfanfu Jiang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zN4oRCrlnM&amp;amp;name=pdf&quot;>;Federated Heavy Hitter Recovery Under Linear Sketching&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Adria Gascon&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Peter Kairouz&lt;/strong>;&lt;/em>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;Graph Generative Model for Benchmarking Graph Neural Networks&lt;/a>; &lt;br>; &lt;em>;Minji Yoon&lt;/em>;, &lt;em>;Yue Wu&lt;/em>;, &lt;strong>;&lt;em>;John Palowitch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;em>;Russ Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=IFhGrPAn8f&amp;amp;name=pdf&quot;>;H-Consistency Bounds for Pairwise Misranking Loss Surrogates&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;, &lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;, &lt;em>;Yutao Zhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DF6ypWrepg&amp;amp;name=pdf&quot;>;Improved Regret for Efficient Online Reinforcement Learning with Linear Function Approximation&lt;/a>; &lt;br>; &lt;em>;Uri Sherman&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZXeTCRZJp9&amp;amp;name=pdf&quot;>;Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames&lt;/a>; &lt;br>; &lt;em>;Ondrej Biza&lt;/em>;*,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gamaleldin Fathy Elsayed&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=a35tteW8if&quot;>;Multi-task Off-Policy Learning from Bandit Feedback&lt;/a>; &lt;br>; &lt;em>;Joey Hong&lt;/em>;, &lt;em>;Branislav Kveton&lt;/em>;, &lt;em>;Manzil Zaheer&lt;/em>;, &lt;em>;Sumeet Katariya&lt;/em>;,&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=yv8GUQREda&amp;amp;name=pdf&quot;>;Optimal No-Regret Learning for One-Sided Lipschitz Functions&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Guru Guruganesh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jon Schneider&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Joshua Ruizhi Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=AwxfYvdPZV&amp;amp;name=pdf&quot;>;Policy Mirror Ascent for Efficient and Independent Learning in Mean Field Games&lt;/a>; &lt;br>; &lt;em>;Batuhan Yardim&lt;/em>;, &lt;em>;Semih Cayci&lt;/em>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;, &lt;em>;Niao He&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ILMHlUn4k6&amp;amp;name=pdf&quot;>;Regret Minimization and Convergence to Equilibria in General-Sum Markov Games&lt;/a>; &lt;br>; &lt;em>;Liad Erez&lt;/em>;, &lt;em>;Tal Lancewicki&lt;/em>;, &lt;em>;Uri Sherman&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=skDVsmXjPR&amp;amp;name=pdf&quot;>;Reinforcement Learning Can Be More Efficient with Multiple Rewards&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rdOuTlTUMX&quot;>;Reinforcement Learning with History-Dependent Dynamic Contexts&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Guy Tennenholtz&lt;/em>;&lt;/strong>;, &lt;em>;Nadav Merlis&lt;/em>;, &lt;strong>;&lt;em>;Lior Shani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Martin Mladenov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Craig Boutlier&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sdhcjMzhHN&amp;amp;name=pdf&quot;>;User-Defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems&lt;/a>; &lt;br>; &lt;em>;Marc Anton Finzi&lt;/em>;*,&lt;strong>; &lt;em>;Anudhyan Boral&lt;/em>;&lt;/strong>;, &lt;em>;Andrew Gordon Wilson&lt;/em>;,&lt;strong>; &lt;em>;Fei Sha&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Leonardo Zepeda-Nunez&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LDBIVZCnLl&amp;amp;name=pdf&quot;>;Discrete Key-Value Bottleneck&lt;/a>; &lt;br>; &lt;em>;Frederik TrÃ¤uble&lt;/em>;, &lt;em>;Anirudh Goyal&lt;/em>;, &lt;em>;Nasim Rahaman&lt;/em>;,&lt;strong>; &lt;em>;Michael Curtis Mozer&lt;/em>;&lt;/strong>;, &lt;em>;Kenji Kawaguchi&lt;/em>;, &lt;em>;Yoshua Bengio&lt;/em>;, &lt;em>;Bernhard SchÃ¶lkopf&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nVO6YTca8O&quot;>;DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm&lt;/a>; &lt;br>; &lt;em>;Lisang Ding&lt;/em>;, &lt;em>;Kexin Jin&lt;/em>;,&lt;strong>; &lt;em>;Bicheng Ying&lt;/em>;&lt;/strong>;, &lt;em>;Kun Yuan&lt;/em>;, &lt;em>;Wotao Yin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3Ge74dgjjU&amp;amp;name=pdf&quot;>;Exphormer: Sparse Transformers for Graphs&lt;/a>; &lt;br>; &lt;em>;Hamed Shirzad&lt;/em>;, &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Balaji Venkatachalam&lt;/em>;&lt;/strong>;, &lt;em>;Danica J. Sutherland&lt;/em>;,&lt;strong>; &lt;em>;Ali Kemal Sinop&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=dolp65Z6re&amp;amp;name=pdf&quot;>;Fast, Differentiable and Sparse Top-k: A Convex Analysis Perspective&lt;/a>; &lt;br>; &lt;em>;Michael Eli Sander&lt;/em>;*, &lt;strong>;&lt;em>;Joan Puigcerver&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Josip Djolonga&lt;/em>;&lt;/strong>;, &lt;em>;Gabriel PeyrÃ©&lt;/em>;,&lt;strong>; &lt;em>;Mathieu Blondel&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=priTMs7n6e&quot;>;Improved Policy Evaluation for Randomized Trials of Algorithmic Resource Allocation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Aditya Mate&lt;/em>;&lt;/strong>;, &lt;em>;Bryan Wilder&lt;/em>;,&lt;strong>; &lt;em>;Aparna Taneja&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Milind Tambe&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Yh9sFZQk7Y&amp;amp;name=pdf&quot;>;In Search for a Generalizable Method for Source Free Domain Adaptation&lt;/a>; &lt;br>; &lt;em>;Malik Boudiaf&lt;/em>;*, &lt;strong>;&lt;em>;Tom Denton&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bart van Merrienboer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vincent Dumoulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Eleni Triantafillou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mSofpvUxCL&amp;amp;name=pdf&quot;>;Learning Rate Schedules in the Presence of Distribution Shift&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adel Javanmard&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pratik Worah&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=slM2r4bRD1&amp;amp;name=pdf&quot;>;Not All Semantics Are Created Equal: Contrastive Self-Supervised Learning with Automatic Temperature Individualization&lt;/a>; &lt;br>; &lt;em>;Zi-Hao Qiu&lt;/em>;, &lt;em>;Quanqi Hu&lt;/em>;, &lt;em>;Zhuoning Yuan&lt;/em>;,&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;Lijun Zhang&lt;/em>;, &lt;em>;Tianbao Yang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=EUQsBO975P&amp;amp;name=pdf&quot;>;On the Relationship Between Explanation and Prediction: A Causal View&lt;/a>; &lt;br>; &lt;em>;Amir-Hossein Karimi&lt;/em>;*, &lt;em>;Krikamol Muandet&lt;/em>;,&lt;strong>; &lt;em>;Simon Kornblith&lt;/em>;&lt;/strong>;, &lt;em>;Bernhard SchÃ¶lkopf&lt;/em>;,&lt;strong>; &lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=qorOnDor89&amp;amp;name=pdf&quot;>;On the Role of Attention in Prompt-Tuning&lt;/a>; &lt;br>; &lt;em>;Samet Oymak&lt;/em>;,&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;, &lt;em>;Mahdi Soltanolkotabi&lt;/em>;, &lt;em>;Christos Thrampoulidis&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2jvwyTm6Pk&amp;amp;name=pdf&quot;>;PLay: Parametrically Conditioned Layout Generation Using Latent Diffusion&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Chin-Yi Cheng&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yang Li&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=631FTQB0UB&amp;amp;name=pdf&quot;>;The Power of Learned Locally Linear Models for Nonlinear Policy Optimization&lt;/a>; &lt;br>; &lt;em>;Daniel Pfrommer&lt;/em>;, &lt;em>;Max Simchowitz&lt;/em>;, &lt;em>;Tyler Westenbroek&lt;/em>;, &lt;em>;Nikolai Matni&lt;/em>;,&lt;strong>; &lt;em>;Stephen Tu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=BDYIci7bVs&amp;amp;name=pdf&quot;>;Relevant Walk Search for Explaining Graph Neural Networks&lt;/a>; &lt;br>; &lt;em>;Ping Xiong&lt;/em>;, &lt;em>;Thomas Schnake&lt;/em>;, &lt;em>;Michael Gastegger&lt;/em>;, &lt;em>;GrÃ©goire Montavon&lt;/em>;,&lt;strong>; &lt;em>;Klaus Robert Muller&lt;/em>;&lt;/strong>;,&lt;em>;Shinichi Nakajima&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RX70NHEPE0&amp;amp;name=pdf&quot;>;Repository-Level Prompt Generation for Large Language Models of Code&lt;/a>; &lt;br>; &lt;em>;Disha Shrivastava&lt;/em>;,&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel Tarlow&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=r3M5cBtpYq&amp;amp;name=pdf&quot;>;Robust and Private Stochastic Linear Bandits&lt;/a>; &lt;br>; &lt;em>;Vasileios Charisopoulos&lt;/em>;*, &lt;strong>;&lt;em>;Hossein Esfandiari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6l9YG3wHA9&amp;amp;name=pdf&quot;>;Simple Diffusion: End-to-End Diffusion for High Resolution Images&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Emiel Hoogeboom&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Heek&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tim Salimans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=cw6Zb0sEiT&amp;amp;name=pdf&quot;>;Tied-Augment: Controlling Representation Similarity Improves Data Augmentation&lt;/a>; &lt;br>; &lt;em>;Emirhan Kurtulus&lt;/em>;, &lt;strong>;&lt;em>;Zichao Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yann Dauphin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ekin D. Cubuk&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1d3O0b1rbL&amp;amp;name=pdf&quot;>;Why Is Public Pre-Training Necessary for Private Model Training?&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Arun Ganesh&lt;/em>;&lt;/strong>;, &lt;em>;Mahdi Haghifam&lt;/em>;*, &lt;strong>;&lt;em>;Milad Nasr&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Steinke&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Om Thakkar&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Abhradeep Guha Thakurta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lun Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XXC601YWgq&amp;amp;name=pdf&quot;>;A Connection Between One-Step RL and Critic Regularization in Reinforcement Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Benjamin Eysenbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;, &lt;em>;Ruslan Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;Beyond Uniform Lipschitz Condition in Differentially Private Optimization&lt;/a>; &lt;br>; &lt;em>;Rudrajit Das&lt;/em>;*, &lt;strong>;&lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tong Zhang&lt;/em>;&lt;/strong>;, &lt;em>;Sujay Sanghavi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Y5jGkbZ0W3&amp;amp;name=pdf&quot;>;Efficient Graph Field Integrators Meet Point Clouds&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Krzysztof Choromanski&lt;/em>;&lt;/strong>;, &lt;em>;Arijit Sehanobish&lt;/em>;, &lt;em>;Han Lin&lt;/em>;, &lt;em>;Yunfan Zhao&lt;/em>;, &lt;em>;Eli Berger,&lt;/em>; &lt;em>;Tetiana Parshakova&lt;/em>;, &lt;em>;Alvin Pan&lt;/em>;, &lt;em>;David Watkins&lt;/em>;, &lt;em>;Tianyi Zhang&lt;/em>;, &lt;em>;Valerii Likhosherstov&lt;/em>;, &lt;em>;Somnath Basu Roy Chowdhury&lt;/em>;,&lt;strong>; &lt;em>;Avinava Dubey&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Deepali Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tamas Sarlos&lt;/em>;&lt;/strong>;, &lt;em>;Snigdha Chaturvedi&lt;/em>;, &lt;em>;Adrian Weller&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RAeN6s9RZV&amp;amp;name=pdf&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>; &lt;br>; &lt;em>;Riade Benbaki&lt;/em>;, &lt;em>;Wenyu Chen&lt;/em>;, &lt;em>;Xiang Meng&lt;/em>;, &lt;strong>;&lt;em>;Hussein Hazimeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhe Zhao&lt;/em>;&lt;/strong>;, &lt;em>;Rahul Mazumder&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2M7lwN0DTp&amp;amp;name=pdf&quot;>;Jump-Start Reinforcement Learning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Ikechukwu Uchendu&lt;/em>;*, &lt;strong>;&lt;em>;Ted Xiao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yao Lu&lt;/em>;&lt;/strong>;, &lt;em>;Banghua Zhu&lt;/em>;, &lt;em>;Mengyuan Yan&lt;/em>;, &lt;em>;JosÃ©phine Simon&lt;/em>;, &lt;em>;Matthew Bennice&lt;/em>;, &lt;em>;Chuyuan Fu&lt;/em>;, &lt;em>;Cong Ma&lt;/em>;, &lt;em>;Jiantao Jiao&lt;/em>;,&lt;strong>; &lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Karol Hausman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=WPjMrOi1KE&amp;amp;name=pdf&quot;>;Learning in POMDPs is Sample-Efficient with Hindsight Observability&lt;/a>; &lt;br>; &lt;em>;Jonathan Lee&lt;/em>;,&lt;strong>; &lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;, &lt;em>;Tong Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=K0InBsKODr&amp;amp;name=pdf&quot;>;Low-Variance Gradient Estimation in Unrolled Computation Graphs with ES-Single&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Vicol&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Qh0Gbq3lkh&amp;amp;name=pdf&quot;>;Masked Trajectory Models for Prediction, Representation, and Control&lt;/a>; &lt;br>; &lt;em>;Philipp Wu&lt;/em>;, &lt;em>;Arjun Majumdar&lt;/em>;, &lt;em>;Kevin Stone&lt;/em>;, &lt;em>;Yixin Lin&lt;/em>;,&lt;strong>; &lt;em>;Igor Mordatch&lt;/em>;&lt;/strong>;, &lt;em>;Pieter Abbeel&lt;/em>;, &lt;em>;Aravind Rajeswaran&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DnTVBs6zbz&amp;amp;name=pdf&quot;>;Overcoming Simplicity Bias in Deep Networks Using a Feature Sieve&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Rishabh Tiwari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pradeep Shenoy&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KKaTURYcKG&amp;amp;name=pdf&quot;>;Pairwise Ranking Losses of Click-Through Rates Prediction for Welfare Maximization in Ad Auctions&lt;/a>; &lt;br>; &lt;em>;Boxiang Lyu&lt;/em>;,&lt;strong>; &lt;em>;Zhe Feng&lt;/em>;&lt;/strong>;, &lt;em>;Zachary Robertson&lt;/em>;,&lt;strong>; &lt;em>;Sanmi Koyejo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=UTtYSDO1MK&amp;amp;name=pdf&quot;>;Predictive Flows for Faster Ford-Fulkerson&lt;/a>; &lt;br>; &lt;em>;Sami Davies&lt;/em>;, &lt;em>;Benjamin Moseley&lt;/em>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuyan Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=SVCYSBgFIr&amp;amp;name=pdf&quot;>;Scaling Laws for Multilingual Neural Machine Translation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Patrick Fernandes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Behrooz Ghorbani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=msZrQQAlBA&amp;amp;name=pdf&quot;>;Sequential Monte Carlo Learning for Time Series Structure Discovery&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Feras Saad&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Patton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthew Douglas Hoffman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rif A. Saurous&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vikash Mansinghka&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XqyXhjVRxR&amp;amp;name=pdf&quot;>;Stochastic Gradient Succeeds for Bandits&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jincheng Mei&lt;/em>;&lt;/strong>;, &lt;em>;Zixin Zhong&lt;/em>;,&lt;strong>; &lt;em>;Bo Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;, &lt;em>;Csaba Szepesvari&lt;/em>;,&lt;strong>; &lt;em>;Dale Schuurmans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nm4NwFfp7a&quot;>;Subset-Based Instance Optimality in Private Estimation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Travis Dick&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Kulesza&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zvCSNsoyKW&amp;amp;name=pdf&quot;>;The Unreasonable Effectiveness of Few-Shot Learning for Machine Translation&lt;/a>; &lt;br>; &lt;em>;Xavier Garcia&lt;/em>;, &lt;em>;Yamini Bansal&lt;/em>;,&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;George Foster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Maxim Krikun&lt;/em>;&lt;/strong>;, &lt;em>;Melvin Johnson&lt;/em>;, &lt;em>;Orhan Firat&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21552&quot;>;Self-Supervised Learning in Vision: from Research Advances to Best Practices&lt;/a>; &lt;br>; &lt;em>;Xinlei Chen&lt;/em>;, &lt;em>;Ishan Misra&lt;/em>;, &lt;em>;Randall Balestriero&lt;/em>;, &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;, &lt;em>;Christoph Feichtenhofer&lt;/em>;, &lt;em>;Mark Ibrahim&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21560&quot;>;How to DP-fy ML: A Practical Tutorial to Machine Learning with Differential Privacy&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21558&quot;>;Recent Advances in the Generalization Theory of Neural Networks&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Tengyu Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Damian&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EXPO Day workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;Graph Neural Networks in Tensorflow: A Practical Guide&lt;/a>; &lt;br>; Workshop Organizers include: &lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan Halcrow&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google sponsored affinity workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; (LAXAI) &lt;br>; Platinum Sponsor &lt;br>; Keynote Speaker:&lt;em>; &lt;strong>;Monica Ribero&lt;/strong>;&lt;/em>; &lt;br>; Panelist: &lt;strong>;&lt;em>;Yao Qin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;Women in Machine Learning&lt;/a>; (WiML) &lt;br>; Platinum Sponsor &lt;br>; Panelists:&lt;strong>;&lt;em>; Yao Qin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://fl-icml2023.github.io/&quot;>;Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/imlh2023/home?authuser=1&quot;>;Interpretable Machine Learning in Healthcare&lt;/a>; (IMLH) &lt;br>; Organizer: &lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://klr-icml2023.github.io/&quot;>;Knowledge and Logical Reasoning in the Era of Data-Driven Learning&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Beliz GÃ¼nel&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mfpl-icml-2023&quot;>;The Many Facets of Preference-Based Learning&lt;/a>; (MFPL) &lt;br>; Organizer: &lt;strong>;&lt;em>;Robert Busa-Fekete&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://syns-ml.github.io/2023/&quot;>;The Synergy of Scientific and Machine Learning Modelling&lt;/a>; (SynS &amp;amp; ML) &lt;br>; Speaker: &lt;strong>;&lt;em>;Sercan Arik&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://tomworkshop.github.io/&quot;>;Theory of Mind in Communicating Agents&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Pei Zhou&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/aihci/home&quot;>;Artificial Intelligence &amp;amp; Human Computer Interaction&lt;/a>; &lt;br>; Organizer:&lt;em>; &lt;strong>;Yang Li&lt;/strong>;&lt;/em>;,&lt;strong>; &lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://dmlr.ai/&quot;>;Data-Centric Machine Learning Research&lt;/a>; (DMLR) &lt;br>; Organizer: &lt;strong>;&lt;em>;Alicia Parrish&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Peter Mattson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Isabelle Guyon&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://neuralcompression.github.io/workshop23&quot;>;Neural Compression: from Information Theory to Applications&lt;/a>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Johannes BallÃ©&lt;/em>;&lt;/strong>; &lt;br>; Panelist: &lt;strong>;&lt;em>;George Toderici&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/teach-icml-23/home&quot;>;Neural Conversational AI Workshop - What&#39;s Left to TEACH (Trustworthy, Enhanced, Adaptable, Capable and Human-centric) Chatbots?&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/scis-workshop-23&quot;>;Spurious Correlations, Invariance and Stability&lt;/a>; (SCIS) &lt;br>; Organizer: &lt;strong>;&lt;em>;Amir Feder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>; &lt;br>; Title: Unsupervised Graph Embedding @ Google (&lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;paper&lt;/a>;, &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;EXPO workshop&lt;/a>;) &lt;br>; Tuesday, July 25th at 10:30 AM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;br>; Title: Federated Learning of Gboard Language Models with Differential Privacy (&lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;paper 1&lt;/a>;, &lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;paper 2&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot;>;blog post&lt;/a>;) &lt;br>; Tuesday, July 25th at 3:30 PM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;br>; Title: Self-supervised scene understanding (&lt;a href=&quot;https://arxiv.org/abs/2302.04973&quot;>;paper 1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2203.11194&quot;>;paper 2&lt;/a>;) &lt;br>; Wednesday, July 26th at 10:30 AM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Max Vladymyrov&lt;/em>;&lt;/strong>; &lt;br>; Title: Transformers learn in-context by gradient descent (&lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;paper&lt;/a>;) &lt;br>; Wednesday, July 26th at 3:30 PM HST &lt;/p>; &lt;/div>; &lt;br>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2519516457542613363/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-icml-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-icml-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICML 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4nvXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s72-c/Google-ICML-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4421751509192561388&lt;/id>;&lt;published>;2023-07-20T09:22:00.002-07:00&lt;/published>;&lt;updated>;2023-07-20T15:31:26.737-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Using societal context knowledge to foster the responsible application of AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Donald Martin, Jr., Technical Program Manager, Head of Societal Context Understanding Tools and Solutions (SCOUTS), Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s1100/scouts.png&quot; style=&quot;display: none;&quot; />; &lt;p>; AI-related products and technologies are constructed and deployed in a &lt;em>;societal context&lt;/em>;: that is, a dynamic and complex collection of social, cultural, historical, political and economic circumstances. Because societal contexts by nature are dynamic, complex, non-linear, contested, subjective, and highly qualitative, they are challenging to translate into the quantitative representations, methods, and practices that dominate standard machine learning (ML) approaches and responsible AI product development practices. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The first phase of AI product development is &lt;em>;problem understanding&lt;/em>;, and this phase has tremendous influence over how problems (eg, increasing cancer screening availability and accuracy) are formulated for ML systems to solve as well many other downstream decisions, such as dataset and ML architecture choice. When the societal context in which a product will operate is not articulated well enough to result in robust problem understanding, the resulting ML solutions can be fragile and even propagate unfair biases. &lt;/p>; &lt;p>; When AI product developers lack access to the knowledge and tools necessary to effectively understand and consider societal context during development, they tend to abstract it away. This abstraction leaves them with a shallow, quantitative understanding of the problems they seek to solve, while product users and society stakeholders â€” who are proximate to these problems and embedded in related societal contexts â€” tend to have a deep qualitative understanding of those same problems. This qualitativeâ€“quantitative divergence in ways of understanding complex problems that separates product users and society from developers is what we call the &lt;em>;problem understanding chasm&lt;/em>;. &lt;/p>; &lt;p>; This chasm has repercussions in the real world: for example, it was the root cause of &lt;a href=&quot;https://www.science.org/doi/10.1126/science.aax2342&quot;>;racial bias discovered by a widely used healthcare algorithm&lt;/a>; intended to solve the problem of choosing patients with the most complex healthcare needs for special programs. Incomplete understanding of the societal context in which the algorithm would operate led system designers to form incorrect and oversimplified causal theories about what the key problem factors were. Critical socio-structural factors, including lack of access to healthcare, lack of trust in the health care system, and underdiagnosis due to human bias,&lt;em>; &lt;/em>;were left out while spending on healthcare was highlighted as a predictor of complex health need. &lt;/p>; &lt;p>; To bridge the problem understanding chasm responsibly, AI product developers need tools that put community-validated and structured knowledge of societal context about complex societal problems at their fingertips â€” starting with problem understanding, but also throughout the product development lifecycle. To that end, &lt;a href=&quot;https://sites.research.google/scouts/&quot;>;Societal Context Understanding Tools and Solutions&lt;/a>; (SCOUTS) â€” part of the &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; (RAI-HCT) team within Google Research â€” is a dedicated research team focused on the mission to â€œempower people with the scalable, trustworthy societal context knowledge required to realize responsible, robust AI and solve the world&#39;s most complex societal problems.â€ SCOUTS is motivated by the significant challenge of articulating societal context, and it conducts innovative foundational and applied research to produce structured societal context knowledge and to integrate it into all phases of the AI-related product development lifecycle. Last year we &lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;announced&lt;/a>; that &lt;a href=&quot;https://jigsaw.google.com/&quot;>;Jigsaw&lt;/a>;, Google&#39;s incubator for building technology that explores solutions to threats to open societies, leveraged our structured societal context knowledge approach during the data preparation and evaluation phases of model development to scale bias mitigation for their widely used &lt;a href=&quot;https://perspectiveapi.com/&quot;>;Perspective API&lt;/a>; toxicity classifier. Going forward SCOUTS&#39; research agenda focuses on the problem understanding phase of AI-related product development with the goal of bridging the problem understanding chasm. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Bridging the AI problem understanding chasm&lt;/h2>; &lt;p>; Bridging the AI problem understanding chasm requires two key ingredients: 1) a reference frame for organizing structured societal context knowledge and 2) participatory, non-extractive methods to elicit community expertise about complex problems and represent it as structured knowledge. SCOUTS has published innovative research in both areas.&lt;/p>; &lt;br>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 0%; margin-right: 0%;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://sites.research.google/scouts/videos/scouts_pull_intro.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;div style=&quot;line-height: 80%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the problem understanding chasm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;A societal context reference frame&lt;/h3>; &lt;p>; An essential ingredient for producing structured knowledge is a taxonomy for creating the structure to organize it. SCOUTS collaborated with other RAI-HCT teams (&lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;TasC&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/03/responsible-ai-at-google-research.html&quot;>;Impact Lab&lt;/a>;), &lt;a href=&quot;https://www.deepmind.com/&quot;>;Google DeepMind&lt;/a>;, and external system dynamics experts to &lt;a href=&quot;https://arxiv.org/abs/2006.09663&quot;>;develop a taxonomic reference frame&lt;/a>; for societal context. To contend with the complex, dynamic, and adaptive nature of societal context, we leverage &lt;a href=&quot;https://en.wikipedia.org/wiki/Complex_adaptive_system&quot;>;complex adaptive systems&lt;/a>; (CAS) theory to propose a high-level taxonomic model for organizing societal context knowledge. The model pinpoints three key elements of societal context and the dynamic feedback loops that bind them together&lt;strong>;: &lt;/strong>;agents, precepts, and artifacts. &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Agents&lt;/em>;: These can be individuals or institutions. &lt;/li>;&lt;li>;&lt;em>;Precepts&lt;/em>;: The preconceptions â€” including beliefs, values, stereotypes and biases â€” that constrain and drive the behavior of agents. An example of a basic precept is that â€œall basketball players are over 6 feet tall.â€ That limiting assumption can lead to failures in identifying basketball players of smaller stature. &lt;/li>;&lt;li>;&lt;em>;Artifacts&lt;/em>;: Agent behaviors produce many kinds of artifacts, including language, data, technologies, societal problems and products. &lt;/li>; &lt;/ul>; &lt;p>; The relationships between these entities are dynamic and complex. Our work hypothesizes that precepts are the most critical element of societal context and we highlight &lt;em>;the problems people perceive&lt;/em>; and &lt;em>;the causal theories they hold about why those problems exist&lt;/em>; as particularly influential precepts that are core to understanding societal context. For example, in the case of racial bias in a medical algorithm described earlier, the causal theory precept held by designers was that&lt;em>; complex health problems would cause healthcare expenditures to go up for all populations&lt;/em>;. That incorrect precept directly led to the choice of healthcare spending as the proxy variable for the model to predict complex healthcare need, which in turn led to the model being biased against Black patients who, due to societal factors such as lack of access to healthcare and underdiagnosis due to bias on average, do not always spend more on healthcare when they have complex healthcare needs. A key open question is how can we ethically and equitably elicit causal theories from the people and communities who are most proximate to problems of inequity and transform them into useful structured knowledge? &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s1600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrative version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s1600/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Taxonomic version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Working with communities to foster the responsible application of AI to healthcare&lt;/h3>; &lt;p>; Since its inception, SCOUTS has worked&lt;a href=&quot;https://accelerate.withgoogle.com/stories/exploring-systems-dynamics-inclusive-ml-and-societal-impact-meet-googlers-donald-martin-and-jamaal-sebastian-barnes&quot;>; to build capacity&lt;/a>; in historically marginalized communities to articulate the broader societal context of the complex problems that matter to them using a practice called community based system dynamics (CBSD). &lt;a href=&quot;https://en.wikipedia.org/wiki/System_dynamics&quot;>;System dynamics&lt;/a>; (SD) is a methodology for articulating causal theories about complex problems, both &lt;em>;qualitatively&lt;strong>; &lt;/strong>;&lt;/em>;as causal loop and &lt;a href=&quot;https://online.visual-paradigm.com/knowledge/business-design/what-is-stock-and-flow-diagram/&quot;>;stock and flow diagrams&lt;/a>; (CLDs and SFDs, respectively) and &lt;em>;quantitatively&lt;/em>; as simulation models. The inherent support of visual qualitative tools, quantitative methods, and collaborative model building makes it an ideal ingredient for bridging the problem understanding chasm. CBSD is a &lt;a href=&quot;https://medium.com/people-ai-research/qa-donald-martin-on-community-based-system-dynamics-and-machine-learning-3fa21e42c680&quot;>;community-based, participatory variant of SD&lt;/a>; specifically focused on building capacity within communities to collaboratively describe and model the problems they face as causal theories, directly without intermediaries. With CBSD we&#39;ve witnessed community groups learn the basics and begin drawing CLDs within 2 hours. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s1147/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;863&quot; data-original-width=&quot;1147&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community members learning system dynamics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There is a huge potential for AI to &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9955430/#:~:text=AI%20algorithms%20can%20analyze%20medical,diseases%20more%20accurately%20and%20quickly.&quot;>;improve medical diagnosis&lt;/a>;. But the safety, equity, and reliability of AI-related health diagnostic algorithms depends on diverse and balanced training datasets. An open challenge in the health diagnostic space is the dearth of training sample data from historically marginalized groups. SCOUTS collaborated with the &lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community and CBSD experts to produce &lt;a href=&quot;https://arxiv.org/abs/2305.13485&quot;>;qualitative and quantitative causal theories&lt;/a>; for the data gap problem. The theories include critical factors that make up the broader societal context surrounding health diagnostics, including cultural memory of death and trust in medical care. &lt;/p>; &lt;p>; The figure below depicts the causal theory generated during the collaboration described above as a CLD. It hypothesizes that trust in medical care influences all parts of this complex system and is the key lever for increasing screening, which in turn generates data to overcome the data diversity gap. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s1024/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;835&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s1072/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;561&quot; data-original-width=&quot;1072&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Causal loop diagram of the health diagnostics data gap&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; These community-sourced causal theories are a first step to bridge the problem understanding chasm with trustworthy societal context knowledge. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; As discussed in this blog, the problem understanding chasm is a critical open challenge in responsible AI. SCOUTS conducts exploratory and applied research in collaboration with other teams within Google Research, external community, and academic partners across multiple disciplines to make meaningful progress solving it. Going forward our work will focus on three key elements, guided by our &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;: &lt;/p>; &lt;ol>; &lt;li>;Increase awareness and understanding of the problem understanding chasm and its implications through talks, publications, and training. &lt;/li>;&lt;li>;Conduct foundational and applied research for representing and integrating societal context knowledge into AI product development tools and workflows, from conception to monitoring, evaluation and adaptation. &lt;/li>;&lt;li>;Apply community-based causal modeling methods to the AI health equity domain to realize impact and build society&#39;s and Google&#39;s capability to produce and leverage global-scale societal context knowledge to realize responsible AI. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s960/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SCOUTS flywheel for bridging the problem understanding chasm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;Thank you to John Guilyard for graphics development, everyone in SCOUTS, and all of our collaborators and sponsors.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4421751509192561388/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/using-societal-context-knowledge-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/using-societal-context-knowledge-to.html&quot; rel=&quot;alternate&quot; title=&quot;Using societal context knowledge to foster the responsible application of AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s72-c/scouts.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8123471024413959829&lt;/id>;&lt;published>;2023-07-18T13:15:00.000-07:00&lt;/published>;&lt;updated>;2023-07-18T13:15:15.633-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Self-Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SimPer: Simple self-supervised learning of periodic targets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Daniel McDuff, Staff Research Scientist, and Yuzhe Yang, Student Researcher, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s320/SimPer%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; Learning from periodic data (signals that repeat, such as a heart beat or the daily temperature changes on Earth&#39;s surface) is crucial for many real-world applications, from &lt;a href=&quot;https://cloud.google.com/blog/topics/sustainability/weather-prediction-with-ai&quot;>;monitoring weather systems&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/take-pulse-health-and-wellness-your-phone/&quot;>;detecting vital signs&lt;/a>;. For example, in the environmental remote sensing domain, periodic learning is often needed to enable nowcasting of environmental changes, such as &lt;a href=&quot;https://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html&quot;>;precipitation patterns or land surface temperature&lt;/a>;. In the health domain, learning from video measurement has shown to extract (quasi-)periodic vital signs such as &lt;a href=&quot;https://www.ahajournals.org/doi/full/10.1161/JAHA.118.008585&quot;>;atrial fibrillation&lt;/a>; and &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9298820&quot;>;sleep apnea episodes&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Approaches like &lt;a href=&quot;https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html&quot;>;RepNet&lt;/a>; highlight the importance of these types of tasks, and present a solution that recognizes repetitive activities within a single video. However, these are supervised approaches that require a significant amount of data to capture repetitive activities, all labeled to indicate the number of times an action was repeated. Labeling such data is often challenging and resource-intensive, requiring researchers to manually capture gold-standard temporal measurements that are synchronized with the modality of interest (eg, video or satellite imagery). &lt;/p>; &lt;p>; Alternatively, self-supervised learning (SSL) methods (eg, &lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&quot;>;SimCLR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;), which leverage a large amount of unlabeled data to learn representations that capture periodic or quasi-periodic temporal dynamics, have demonstrated success in &lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j.html&quot;>;solving classification tasks&lt;/a>;. However, they overlook the intrinsic periodicity (ie, the ability to identify if a frame is part of a periodic process) in data and fail to learn robust representations that capture periodic or frequency attributes. This is because periodic learning exhibits characteristics that are distinct from prevailing learning tasks. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s1338/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1338&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Feature similarity is different in the context of periodic representations as compared to static features (eg, images). For example, videos that are offset by short time delays or are reversed should be similar to the original sample, whereas videos that have been upsampled or downsampled by a factor &lt;em>;x&lt;/em>; should be different from the original sample by a factor of &lt;em>;x&lt;/em>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To address these challenges, in â€œ&lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;SimPer: Simple Self-Supervised Learning of Periodic Targets&lt;/a>;â€, published at the eleventh &lt;a href=&quot;https://iclr.cc/&quot;>;International Conference on Learning Representations&lt;/a>; (ICLR 2023), we introduced a self-supervised contrastive framework for learning periodic information in data. Specifically, SimPer leverages the temporal properties of periodic targets using &lt;em>;temporal self-contrastive learning&lt;/em>;, where positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the &lt;em>;same&lt;/em>; input instance. We propose &lt;em>;periodic feature similarity&lt;/em>; that explicitly defines how to measure similarity in the context of periodic learning. Moreover, we design a generalized contrastive loss&lt;em>; &lt;/em>;that extends the classic &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE loss&lt;/a>; to a soft regression variant that enables contrasting over continuous labels (frequency). Next, we demonstrate that SimPer effectively learns period feature representations compared to state-of-the-art SSL methods, highlighting its intriguing properties including better data efficiency, robustness to spurious correlations, and generalization to distribution shifts. Finally, we are excited to release the &lt;a href=&quot;https://github.com/YyzHarry/SimPer&quot;>;SimPer code repo&lt;/a>; with the research community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The SimPer framework&lt;/h2>; &lt;p>; SimPer introduces a temporal self-contrastive learning framework. Positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the same input instance. For temporal video examples, periodicity-invariant changes are cropping, rotation or flipping, whereas periodicity-variant changes involve increasing or decreasing the speed of a video. &lt;/p>; &lt;p>; To explicitly define how to measure similarity in the context of periodic learning, SimPer proposes periodic feature similarity. This construction allows us to formulate training as a contrastive learning task. A model can be trained with data without any labels and then fine-tuned if necessary to map the learned features to specific frequency values. &lt;/p>; &lt;p>; Given an input sequence &lt;em>;x&lt;/em>;, we know there&#39;s an underlying associated periodic signal. We then transform &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo- speed or frequency labels for the unlabeled input x. &lt;/p>; &lt;p>; Conventional &lt;a href=&quot;https://en.wikipedia.org/wiki/Similarity_measure&quot;>;similarity measures&lt;/a>; such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; emphasize strict proximity between two feature vectors, and are sensitive to index shifted features (which represent different time stamps), reversed features, and features with changed frequencies. In contrast, periodic feature similarity should be high for samples with small temporal shifts and or reversed indexes, while capturing a continuous similarity change when the feature frequency varies. This can be achieved via a similarity metric in the frequency domain, such as the distance between two &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;>;Fourier transforms&lt;/a>;. &lt;/p>; &lt;p>; To harness the intrinsic continuity of augmented samples in the frequency domain, SimPer designs a generalized contrastive loss that extends the classic &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE&lt;/a>; loss to a soft regression variant that enables contrasting over continuous labels (frequency). This makes it suitable for regression tasks, where the goal is to recover a continuous signal, such as a heart beat. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;361&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SimPer constructs negative views of data through transformations in the frequency domain. The input sequence &lt;em>;x&lt;/em>; has an underlying associated periodic signal. SimPer transforms &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo speed or frequency labels for unlabeled input &lt;em>;x&lt;/em>; (periodicity-variant augmentations &lt;em>;Ï„&lt;/em>;). SimPer takes transformations that do not change the identity of the input and defines these as periodicity-invariant augmentations &lt;em>;Ïƒ&lt;/em>;, thus creating different positive views of the sample. Then, it sends these augmented views to the encoder &lt;em>;f&lt;/em>;,&lt;em>; &lt;/em>;which extracts corresponding features. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate SimPer&#39;s performance, we benchmarked it against state-of-the-art SSL schemes (eg, &lt;a href=&quot;https://github.com/google-research/simclr&quot;>;SimCLR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf&quot;>;BYOL&lt;/a>;, &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Spatiotemporal_Contrastive_Video_Representation_Learning_CVPR_2021_paper.html&quot;>;CVRL&lt;/a>;) on a set of six diverse periodic learning datasets for common real-world tasks in human behavior analysis, environmental remote sensing, and healthcare. Specifically, below we present results on heart rate measurement and exercise repetition counting from video. The results show that SimPer outperforms the state-of-the-art SSL schemes across all six datasets, highlighting its superior performance in terms of data efficiency, robustness to spurious correlations, and generalization to unseen targets. &lt;/p>; &lt;p>; Here we show quantitative results on two representative datasets using SimPer pre-trained using various SSL methods and fine-tuned on the labeled data. First, we pre-train SimPer using the &lt;a href=&quot;https://sites.google.com/corp/view/ybenezeth/ubfcrppg&quot;>;Univ. Bourgogne Franche-ComtÃ© Remote PhotoPlethysmoGraphy&lt;/a>; (UBFC) dataset, a human &lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram#Remote_photoplethysmography&quot;>;photoplethysmography&lt;/a>; and heart rate prediction dataset, and compare its performance to state-of-the-art SSL methods. We observe that SimPer outperforms SimCLR, MoCo v2, BYOL, and CVRL methods. The results on the human action counting dataset, &lt;a href=&quot;https://sites.google.com/corp/view/repnet&quot;>;Countix&lt;/a>;, further confirm the benefits of SimPer over others methods as it notably outperforms the supervised baseline. For the feature evaluation results and performance on other datasets, please refer to the &lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results of SimCLR, MoCo v2, BYOL, CVRL and SimPer on the Univ. Bourgogne Franche-ComtÃ© Remote PhotoPlethysmoGraphy (UBFC) and Countix datasets. Heart rate and repetition count performance is reported as &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;mean absolute error&lt;/a>; (MAE).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and applications&lt;/h2>; &lt;p>; We present SimPer, a self-supervised contrastive framework for learning periodic information in data. We demonstrate that by combining a temporal self-contrastive learning framework, periodicity-invariant and periodicity-variant augmentations, and continuous periodic feature similarity, SimPer provides an intuitive and flexible approach for learning strong feature representations for periodic signals. Moreover, SimPer can be applied to various fields, ranging from environmental remote sensing to healthcare. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Yuzhe Yang, Xin Liu, Ming-Zher Poh, Jiang Wu, Silviu Borac, and Dina Katabi for their contributions to this work. &lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8123471024413959829/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/simper-simple-self-supervised-learning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/simper-simple-self-supervised-learning.html&quot; rel=&quot;alternate&quot; title=&quot;SimPer: Simple self-supervised learning of periodic targets&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s72-c/SimPer%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2013323302512835157&lt;/id>;&lt;published>;2023-07-13T14:01:00.001-07:00&lt;/published>;&lt;updated>;2023-07-13T14:01:18.428-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Symbol tuning improves in-context learning in language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jerry Wei, Student Researcher, and Denny Zhou, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s1200/SymbolTuning.png&quot; style=&quot;display: none;&quot; />; &lt;p>; A key feature of human intelligence is that humans can learn to perform new tasks by reasoning using only a few examples. Scaling up language models has unlocked a range of new applications and paradigms in machine learning, including the ability to perform challenging reasoning tasks via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>;. Language models, however, are still sensitive to the way that prompts are given, indicating that they are not reasoning in a robust manner. For instance, language models often require heavy prompt engineering or phrasing tasks as instructions, and they exhibit unexpected behaviors such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;performance on tasks being unaffected even when shown incorrect labels&lt;/a>;. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In â€œ&lt;a href=&quot;https://arxiv.org/abs/2305.08298&quot;>;Symbol tuning improves in-context learning in language models&lt;/a>;â€, we propose a simple fine-tuning procedure that we call &lt;em>;symbol tuning&lt;/em>;, which can improve in-context learning by emphasizing inputâ€“label mappings. We experiment with symbol tuning across &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Flan-PaLM&lt;/a>; models and observe benefits across various settings. &lt;/p>; &lt;ul>; &lt;li>;Symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. &lt;/li>;&lt;li>;Symbol-tuned models are much stronger at algorithmic reasoning tasks. &lt;/li>;&lt;li>;Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior knowledge. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s1035/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;663&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of symbol tuning, where models are fine-tuned on tasks where natural language labels are replaced with arbitrary symbols. Symbol tuning relies on the intuition that when instruction and relevant labels are not available, models must use in-context examples to learn the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Motivation&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a common fine-tuning method that has been shown to improve performance and allow models to better follow in-context examples. One shortcoming, however, is that models are not forced to learn to use the examples because the task is redundantly defined in the evaluation example via instructions and natural language labels. For example, on the left in the figure above, although the examples can help the model understand the task (sentiment analysis), they are not strictly necessary since the model could ignore the examples and just read the instruction that indicates what the task is. &lt;/p>; &lt;p>; In symbol tuning, the model is fine-tuned on examples where the instructions are removed and natural language labels are replaced with semantically-unrelated labels (eg, â€œFoo,â€ â€œBar,â€ etc.). In this setup, the task is unclear without looking at the in-context examples. For example, on the right in the figure above, multiple in-context examples would be needed to figure out the task. Because symbol tuning teaches the model to reason over the in-context examples, symbol-tuned models should have better performance on tasks that require reasoning between in-context examples and their labels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s1035/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;237&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Datasets and task types used for symbol tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Symbol-tuning procedure&lt;/h2>; &lt;p>; We selected 22 publicly-available &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) datasets that we use for our symbol-tuning procedure. These tasks have been widely used in the past, and we only chose classification-type tasks since our method requires discrete labels. We then remap labels to a random label from a set of ~30K arbitrary labels selected from one of three categories: integers, character combinations, and words. &lt;/p>; &lt;p>; For our experiments, we symbol tune &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, the instruction-tuned variants of &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;. We use three different sizes of Flan-PaLM models: Flan-PaLM-8B, Flan-PaLM-62B, and Flan-PaLM-540B. We also tested &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;Flan-cont-PaLM-62B&lt;/a>; (Flan-PaLM-62B at 1.3T tokens instead of 780B tokens), which we abbreviate as 62B-c. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s861/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;168&quot; data-original-width=&quot;861&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a set of âˆ¼300K arbitrary symbols from three categories (integers, character combinations, and words). âˆ¼30K symbols are used during tuning and the rest are held out for evaluation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experimental setup&lt;/h2>; &lt;p>; We want to evaluate a model&#39;s ability to perform unseen tasks, so we cannot evaluate on tasks used in symbol tuning (22 datasets) or used during instruction tuning (1.8K tasks). Hence, we choose 11 NLP datasets that were not used during fine-tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;In-context learning&lt;/h2>; &lt;p>; In the symbol-tuning procedure, models must learn to reason with in-context examples in order to successfully perform tasks because prompts are modified to ensure that tasks cannot simply be learned from relevant labels or instructions. Symbol-tuned models should perform better in settings where tasks are unclear and require reasoning between in-context examples and their labels. To explore these settings, we define four in-context learning settings that vary the amount of reasoning required between inputs and labels in order to learn the task (based on the availability of instructions/relevant labels) &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s936/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;936&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depending on the availability of instructions and relevant natural language labels, models may need to do varying amounts of reasoning with in-context examples. When these features are not available, models must reason with the given in-context examples to successfully perform the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Symbol tuning improves performance across all settings for models 62B and larger, with small improvements in settings with relevant natural language labels (+0.8% to +4.2%) and substantial improvements in settings without relevant natural language labels (+5.5% to +15.5%). Strikingly, when relevant labels are unavailable, symbol-tuned Flan-PaLM-8B outperforms FlanPaLM-62B, and symbol-tuned Flan-PaLM-62B outperforms Flan-PaLM-540B. This performance difference suggests that symbol tuning can allow much smaller models to perform as well as large models on these tasks (effectively saving âˆ¼10X inference compute). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s900/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;495&quot; data-original-width=&quot;900&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Large-enough symbol-tuned models are better at in-context learning than baselines, especially in settings where relevant labels are not available. Performance is shown as average model accuracy (%) across eleven tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Algorithmic reasoning&lt;/h2>; &lt;p>; We also experiment on algorithmic reasoning tasks from &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;. There are two main groups of tasks: 1) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/list_functions&quot;>;List functions&lt;/a>; â€” identify a transformation function (eg, remove the last element in a list) between input and output lists containing non-negative integers; and 2) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/simp_turing_concept&quot;>;simple turing concepts&lt;/a>; â€” reason with binary strings to learn the concept that maps an input to an output (eg, swapping 0s and 1s in a string). &lt;/p>; &lt;p>; On the list function and simple turing concept tasks, symbol tuning results in an average performance improvement of 18.2% and 15.3%, respectively. Additionally, Flan-cont-PaLM-62B with symbol tuning outperforms Flan-PaLM-540B on the list function tasks on average, which is equivalent to a âˆ¼10x reduction in inference compute. These improvements suggest that symbol tuning strengthens the model&#39;s ability to learn in-context for unseen task types, as symbol tuning did not include any algorithmic data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s908/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;496&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models achieve higher performance on list function tasks and simple turing concept tasks. (Aâ€“E): categories of list functions tasks. (F): simple turing concepts task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In the flipped-label experiment, labels of in-context and evaluation examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as â€œnegative sentimentâ€), thereby allowing us to study whether models can override prior knowledge. &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;Previous work&lt;/a>; has shown that while pre-trained models (without instruction tuning) can, to some extent, follow flipped labels presented in-context, instruction tuning degraded this ability. &lt;/p>; &lt;p>; We see that there is a similar trend across all model sizes â€” symbol-tuned models are much more capable of following flipped labels than instruction-tuned models. We found that after symbol tuning, Flan-PaLM-8B sees an average improvement across all datasets of 26.5%, Flan-PaLM-62B sees an improvement of 33.7%, and Flan-PaLM-540B sees an improvement of 34.0%. Additionally, symbol-tuned models achieve similar or better than average performance as pre-trainingâ€“only models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s914/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;914&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models are much better at following flipped labels presented in-context than instruction-tuned models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented symbol tuning, a new method of tuning models on tasks where natural language labels are remapped to arbitrary symbols. Symbol tuning is based off of the intuition that when models cannot use instructions or relevant labels to determine a presented task, it must do so by instead learning from in-context examples. We tuned four language models using our symbol-tuning procedure, utilizing a tuning mixture of 22 datasets and approximately 30K arbitrary symbols as labels. &lt;/p>; &lt;p>; We first showed that symbol tuning improves performance on unseen in-context learning tasks, especially when prompts do not contain instructions or relevant labels. We also found that symbol-tuned models were much better at algorithmic reasoning tasks, despite the lack of numerical or algorithmic data in the symbol-tuning procedure. Finally, in an in-context learning setting where inputs have flipped labels, symbol tuning (for some datasets) restores the ability to follow flipped labels that was lost during instruction tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; Through symbol tuning, we aim to increase the degree to which models can examine and learn from inputâ€“label mappings during in-context learning. We hope that our results encourage further work towards improving language models&#39; ability to reason over symbols presented in-context. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. This work was conducted by Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. We would like to thank our colleagues at Google Research and Google DeepMind for their advice and helpful discussions.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2013323302512835157/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/symbol-tuning-improves-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/symbol-tuning-improves-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Symbol tuning improves in-context learning in language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s72-c/SymbolTuning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8984134419460793359&lt;/id>;&lt;published>;2023-07-11T10:00:00.010-07:00&lt;/published>;&lt;updated>;2023-07-11T10:44:15.111-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;An open-source gymnasium for machine learning assisted computer architecture design&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Amir Yazdanbakhsh, Research Scientist, and Vijay Janapa Reddi, Visiting Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s1200/ArchGym-animation2.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_architecture&quot;>;Computer Architecture&lt;/a>; research has a long history of developing simulators and tools to evaluate and shape the design of computer systems. For example, the &lt;a href=&quot;https://ieeexplore.ieee.org/iel5/2/21180/00982917.pdf?casa_token=M_ZAQmgCbKkAAAAA:Y9wFTB9OQBwzXXpw7kNbq4asdlCCHYRaq7qsqcRBpYyh6734aHmr57ll4Vb1_zcG5ukNvNONNQ&quot;>;SimpleScalar&lt;/a>; simulator was introduced in the late 1990s and allowed researchers to explore various &lt;a href=&quot;https://en.wikipedia.org/wiki/Microarchitecture&quot;>;microarchitectural&lt;/a>; ideas. Computer architecture simulators and tools, such as &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;gem5&lt;/a>;, &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>;, and many more have played a significant role in advancing computer architecture research. Since then, these shared resources and infrastructure have benefited industry and academia and have enabled researchers to systematically build on each other&#39;s work, leading to significant advances in the field. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Nonetheless, computer architecture research is evolving, with industry and academia turning towards machine learning (ML) optimization to meet stringent domain-specific requirements, such as &lt;a href=&quot;https://ai.googleblog.com/2021/02/machine-learning-for-computer.html&quot;>;ML for computer architecture&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2201.01863.pdf&quot;>;ML for TinyML acceleration&lt;/a>;,&amp;nbsp;&lt;a href=&quot;https://ai.googleblog.com/2022/03/offline-optimization-for-architecting.html&quot;>;DNN&lt;/a>; &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9804604&quot;>;accelerator&lt;/a>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3503222.3507767&quot;>;datapath optimization&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1394608.1382172?casa_token=3g46kAHeN0QAAAAA:5pKdYXamA_vstsO5LqA0_4rRy5o2Z46Ks-OJLDUe7ZSLtDfkaSS5i0zLfMe3Y7gAuY2bFMZ1yGOEMA&quot;>;memory controllers&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40&quot;>;power consumption&lt;/a>;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/7430287&quot;>;security&lt;/a>;, and &lt;a href=&quot;https://www.sigarch.org/tag/privacy-preserving-computing/&quot;>;privacy&lt;/a>;. Although prior work has demonstrated the benefits of ML in design optimization, the lack of strong, reproducible baselines hinders fair and objective comparison across different methods and poses several challenges to their deployment. To ensure steady progress, it is imperative to understand and tackle these challenges collectively. &lt;/p>; &lt;p>; To alleviate these challenges, in â€œ&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3579371.3589049&quot;>;ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design&lt;/a>;â€, accepted at &lt;a href=&quot;https://www.iscaconf.org/isca2023/program/&quot;>;ISCA 2023&lt;/a>;, we introduced ArchGym, which includes a variety of computer architecture simulators and ML algorithms. Enabled by ArchGym, our results indicate that with a sufficiently large number of samples, any of a diverse collection of ML algorithms are capable of finding the optimal set of architecture design parameters for each target problem; &lt;i>;no one solution is necessarily better than another&lt;/i>;. These results further indicate that selecting the optimal hyperparameters for a given ML algorithm is essential for finding the optimal architecture design, but choosing them is non-trivial. We &lt;a href=&quot;https://bit.ly/ArchGym&quot;>;release&lt;/a>; the code and dataset across multiple computer architecture simulations and ML algorithms.&lt;/p>; &lt;br />; &lt;h2>;Challenges in ML-assisted architecture research &lt;/h2>; &lt;p>; ML-assisted architecture research poses several challenges, including: &lt;/p>; &lt;ol>; &lt;li>;For a specific ML-assisted computer architecture problem (eg, finding an optimal solution for a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_random-access_memory&quot;>;DRAM&lt;/a>; controller) there is no systematic way to identify optimal ML algorithms or hyperparameters (eg, learning rate, warm-up steps, etc.). There is a wider range of ML and heuristic methods, from &lt;a href=&quot;https://arxiv.org/abs/2008.03639&quot;>;random walk&lt;/a>; to &lt;a href=&quot;http://incompleteideas.net/book/RLbook2020.pdf&quot;>;reinforcement learning&lt;/a>; (RL), that can be employed for &lt;a href=&quot;https://en.wikipedia.org/wiki/Design_space_exploration&quot;>;design space exploration&lt;/a>; (DSE). While these methods have shown noticeable performance improvement over their choice of baselines, it is not evident whether the improvements are because of the choice of optimization algorithms or hyperparameters.&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;Thus, to ensure reproducibility and facilitate widespread adoption of ML-aided architecture DSE, it is necessary to outline a systematic benchmarking methodology.&lt;/em>; &lt;br />;&lt;br />; &lt;/li>; &lt;li>;While computer architecture simulators have been the backbone of architectural innovations, there is an emerging need to address the trade-offs between accuracy, speed, and cost in architecture exploration. The accuracy and speed of performance estimation widely varies from one simulator to another, depending on the underlying modeling details (eg, &lt;a href=&quot;https://biblio.ugent.be/publication/2968322/file/6776727.pdf&quot;>;cycle&lt;/a>;-&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;accurate&lt;/a>; vs. &lt;a href=&quot;https://arxiv.org/abs/2210.03894&quot;>;ML&lt;/a>;-&lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;based&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1808.07412&quot;>;proxy&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1803.02329&quot;>;models&lt;/a>;). While analytical or ML-based proxy models are nimble by virtue of discarding low-level details, they generally suffer from high prediction error. Also, due to commercial licensing, there can be strict &lt;a href=&quot;https://ieeexplore.ieee.org/document/7945172&quot;>;limits on the number of runs collected from a simulator&lt;/a>;. Overall, these constraints exhibit distinct performance vs. sample efficiency trade-offs, affecting the choice of optimization algorithm for architecture exploration. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;It is challenging to delineate how to systematically compare the effectiveness of various ML algorithms under these constraints.&lt;/em>; &lt;br />; &lt;br />; &lt;/li>; &lt;li>;Finally, the landscape of ML algorithms is rapidly evolving and some ML algorithms need data to be useful. Additionally, rendering the outcome of DSE into meaningful artifacts such as datasets is critical for drawing insights about the design space. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;In this rapidly evolving ecosystem, it is consequential to ensure how to amortize the overhead of search algorithms for architecture exploration. It is not apparent, nor systematically studied how to leverage exploration data while being agnostic to the underlying search algorithm.&lt;/em>; &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;ArchGym design&lt;/h2>; &lt;p>; ArchGym addresses these challenges by providing a unified framework for evaluating different ML-based search algorithms fairly. It comprises two main components: 1) the ArchGym environment and 2) the ArchGym agent. The environment is an encapsulation of the architecture cost model â€” which includes latency, throughput, area, energy, etc., to determine the computational cost of running the workload, given a set of architectural parameters â€” paired with the target workload(s). The agent is an encapsulation of the ML algorithm used for the search and consists of hyperparameters and a guiding policy. The hyperparameters are intrinsic to the algorithm for which the model is to be optimized and can significantly influence performance. The policy, on the other hand, determines how the agent selects a parameter iteratively to optimize the target objective. &lt;/p>; &lt;p>; Notably, ArchGym also includes a standardized interface that connects these two components, while also saving the exploration data as the ArchGym Dataset. At its core, the interface entails three main signals: &lt;i>;hardware state&lt;/i>;, &lt;i>;hardware parameters&lt;/i>;, and &lt;i>;metrics&lt;/i>;. These signals are the bare minimum to establish a meaningful communication channel between the environment and the agent.&amp;nbsp;Using these signals, the agent observes the state of the hardware and suggests a set of hardware parameters to iteratively optimize a (user-defined) reward. The reward is a function of hardware performance metrics, such as performance, energy consumption, etc.&amp;nbsp;&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s1226/ArchGym-animation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;722&quot; data-original-width=&quot;1226&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s16000/ArchGym-animation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ArchGym comprises two main components: the ArchGym environment and the ArchGym agent. The ArchGym environment encapsulates the cost model and the agent is an abstraction of a policy and hyperparameters. With a standardized interface that connects these two components, ArchGym provides a unified framework for evaluating different ML-based search algorithms fairly while also saving the exploration data as the ArchGym Dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;ML algorithms could be equally favorable to meet user-defined target specifications&lt;/h2>; &lt;p>; Using ArchGym, we empirically demonstrate that across different optimization objectives and DSE problems, &lt;em>;at least one set of hyperparameters exists that results in the same hardware performance as other ML algorithms&lt;/em>;. A poorly selected (random selection) hyperparameter for the ML algorithm or its baseline can lead to a misleading conclusion that a particular family of ML algorithms is better than another. We show that with sufficient hyperparameter tuning, different search algorithms, even &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;random walk&lt;/a>; (RW), are able to identify the best possible reward. However, note that finding the right set of hyperparameters may require exhaustive search or even luck to make it competitive. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1158&quot; data-original-width=&quot;1999&quot; height=&quot;371&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/w640-h371/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With a sufficient number of samples, there exists at least one set of hyperparameters that results in the same performance across a range of search algorithms. Here the dashed line represents the maximum normalized reward. &lt;i>;Cloud-1&lt;/i>;, &lt;i>;cloud-2&lt;/i>;, &lt;i>;stream&lt;/i>;, and &lt;i>;random&lt;/i>; indicate four different memory traces for &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>; (DRAM subsystem design space exploration framework).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset construction and high-fidelity proxy model training&lt;/h2>; &lt;p>; Creating a unified interface using ArchGym also enables the creation of datasets that can be used to design better data-driven ML-based proxy architecture cost models to improve the speed of architecture simulation. To evaluate the benefits of datasets in building an ML model to approximate architecture cost, we leverage ArchGym&#39;s ability to log the data from each run from DRAMSys to create four dataset variants, each with a different number of data points. For each variant, we create two categories: (a) Diverse Dataset, which represents the data collected from different agents (&lt;a href=&quot;https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms&quot;>;ACO&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_algorithm&quot;>;GA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;RW&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions.&quot;>;BO&lt;/a>;), and (b) ACO only, which shows the data collected exclusively from the ACO agent, both of which are released along with ArchGym. We train a proxy model on each dataset using &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;>;random forest regression&lt;/a>; with the objective to predict the latency of designs for a &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAM simulator&lt;/a>;. Our results show that: &lt;/p>; &lt;ol>; &lt;li>;As we increase the dataset size, the average normalized &lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;root mean squared error&lt;/a>; (RMSE) slightly decreases. &lt;/li>;&lt;li>;However, as we introduce diversity in the dataset (eg, collecting data from different agents), we observe 9Ã— to 42Ã— lower RMSE across different dataset sizes. &lt;/li>; &lt;/ol>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/s1826/ArchGym2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1143&quot; data-original-width=&quot;1826&quot; height=&quot;401&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/w640-h401/ArchGym2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diverse dataset collection across different agents using ArchGym interface.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/s1803/ArchGym1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1803&quot; height=&quot;407&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/w640-h407/ArchGym1.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The impact of a diverse dataset and dataset size on the normalized RMSE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;The need for a community-driven ecosystem for ML-assisted architecture research&lt;/h2>; &lt;p>;While, ArchGym is an initial effort towards creating an open-source ecosystem that (1) connects a broad range of search algorithms to computer architecture simulators in an unified and easy-to-extend manner, (2) facilitates research in ML-assisted computer architecture, and (3) forms the scaffold to develop reproducible baselines, there are a lot of open challenges that need community-wide support. Below we outline some of the open challenges in ML-assisted architecture design. Addressing these challenges requires a well coordinated effort and a community driven ecosystem.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;969&quot; data-original-width=&quot;1999&quot; height=&quot;310&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/w640-h310/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Key challenges in ML-assisted architecture design.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We call this ecosystem &lt;a href=&quot;https://www.sigarch.org/architecture-2-0-why-computer-architects-need-a-data-centric-ai-gymnasium/&quot;>;Architecture 2.0&lt;/a>;.&amp;nbsp;We outline the key challenges and a vision for building an inclusive ecosystem of interdisciplinary researchers to tackle the long-standing open problems in applying ML for computer architecture research.&amp;nbsp;If you are interested in helping shape this ecosystem, please fill out the &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSfIYeSBoEi-DIHizPx4-FTEcZUSY_uUcKe0rdHC0tkCWp3Gag/viewform&quot;>;interest survey&lt;/a>;.&lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>;&lt;a href=&quot;https://bit.ly/ArchGym&quot;>;ArchGym&lt;/a>; is an open source gymnasium for ML architecture DSE and enables an standardized interface that can be readily extended to suit different use cases. Additionally, ArchGym enables fair and reproducible comparison between different ML algorithms and helps to establish stronger baselines for computer architecture research problems. &lt;/p>; &lt;p>; We invite the computer architecture community as well as the ML community to actively participate in the development of ArchGym. We believe that the creation of a gymnasium-type environment for computer architecture research would be a significant step forward in the field and provide a platform for researchers to use ML to accelerate research and lead to new and innovative designs. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;This blogpost is based on joint work with several co-authors at Google and Harvard University.&amp;nbsp;&lt;/i>;&lt;i>;We would like to acknowledge and highlight Srivatsan Krishnan (Harvard) who contributed several ideas to this project in collaboration with Shvetank Prakash (Harvard), Jason Jabbour (Harvard), Ikechukwu Uchendu (Harvard), Susobhan Ghosh (Harvard), Behzad Boroujerdian (Harvard), Daniel Richins (Harvard), Devashree Tripathy (Harvard), and Thierry Thambe (Harvard).&amp;nbsp; In addition, we would also like to thank James Laudon, Douglas Eck, Cliff Young, and Aleksandra Faust for their support, feedback, and motivation for this work. We would also like to thank John Guilyard for the animated figure used in this post. Amir Yazdanbakhsh is now a Research Scientist at Google DeepMind and Vijay Janapa Reddi is an Associate Professor at Harvard.&lt;/i>;&lt;/p>;&lt;div>;&lt;br />;&lt;/div>;&lt;br />;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8984134419460793359/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/an-open-source-gymnasium-for-computer.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/an-open-source-gymnasium-for-computer.html&quot; rel=&quot;alternate&quot; title=&quot;An open-source gymnasium for machine learning assisted computer architecture design&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s72-c/ArchGym-animation2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7093565002706757508&lt;/id>;&lt;published>;2023-07-10T06:02:00.005-07:00&lt;/published>;&lt;updated>;2023-07-21T13:24:14.452-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ACL 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s1040/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the &lt;a href=&quot;https://2023.aclweb.org/&quot;>;61st annual meeting&lt;/a>; of the &lt;a href=&quot;https://www.aclweb.org/&quot;>;Association for Computational Linguistics&lt;/a>; (ACL), a premier conference covering a broad spectrum of research areas that are concerned with computational approaches to natural language, is taking place in Vancouver, BC. As a leader in natural language processing and understanding, and a&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/sponsors/&quot;>;Diamond Level sponsor&lt;/a>;&amp;nbsp;of&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, Google will showcase the latest research in the field with over 50 publications, and active involvement in a variety of workshops and tutorials.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;If you&#39;re registered for ACL 2023, we hope that you&#39;ll visit the Google booth to learn more about the projects at Google that go into solving interesting problems for billions of people. You can also learn more about Google&#39;s participation below (Google affiliations in &lt;b>;bold&lt;/b>;).&lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Area chairs include: &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>; &lt;br />; Workshop chairs include: &lt;strong>;&lt;em>;Annie Louis&lt;/em>;&lt;/strong>; &lt;br />; Publication chairs include: &lt;strong>;&lt;em>;Lei Shu&lt;/em>;&lt;/strong>; &lt;br />; Program Committee includes: &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Spotlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09648.pdf&quot;>;NusaCrowd: Open Source Initiative for Indonesian NLP Resources&lt;/a>; &lt;br />; &lt;em>;Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Winata, Bryan Wilie, Fajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Muhammad Satrio Wicaksono, Ivan Parmonangan, Ika Alfina, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh Dhole, Arie Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Adilazuarda, Ryan Hadiwijaya, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Haryo Wibowo, Cuk Tho, Ichwanul Karo Karo, Tirana Fatyanosa, Ziwei Ji, Graham Neubig, Timothy Baldwin, &lt;strong>;Sebastian Ruder&lt;/strong>;, Pascale Fung, Herry Sujaini, Sakriani Sakti, Ayu Purwarianti&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2205.12680.pdf&quot;>;Optimizing Test-Time Query Representations for Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Mujeen Sung, Jungsoo Park, Jaewoo Kang, Danqi Chen, &lt;strong>;Jinhyuk Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10750.pdf&quot;>;PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition&lt;/a>; &lt;br />; &lt;em>;Sihao Chen*, &lt;strong>;Senaka Buthpitiya&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, Dan Roth, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02301.pdf&quot;>;Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes&lt;/a>; &lt;br />; &lt;em>;Cheng-Yu Hsieh*, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chih-Kuan Yeh&lt;/strong>;, &lt;strong>;Hootan Nakhost&lt;/strong>;, &lt;strong>;Yasuhisa Fujii&lt;/strong>;, Alex Ratner, Ranjay Krishna, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05110.pdf&quot;>;Large Language Models with Controllable Working Memory&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Daliang Li&lt;/b>;, &lt;b>;Ankit Singh Rawat&lt;/b>;, &lt;b>;Manzil Zaheer&lt;/b>;, &lt;b>;Xin Wang&lt;/b>;, &lt;b>;Michal Lukasik&lt;/b>;, &lt;b>;Andreas Veit&lt;/b>;, &lt;b>;Felix Yu&lt;/b>;,&lt;b>; Sanjiv Kumar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10791.pdf&quot;>;OpineSum: Entailment-Based Self-Training for Abstractive Opinion Summarization&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Annie Louis&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08775.pdf&quot;>;RISE: Leveraging Retrieval Techniques for Summarization Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Uthus&lt;/b>;, &lt;b>;Jianmo Ni&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b5b9bb4ec1e1d7416f88f8b3a1649d1235d747b8.pdf&quot;>;Follow the Leader(board) with Confidence: Estimating p-Values from a Single Test Set with Item and Response Variance&lt;/a>; &lt;br />; &lt;i>; Shira Wein*, Christopher Homan, &lt;strong>;Lora Aroyo&lt;/strong>;, &lt;strong>;Chris Welty&lt;/strong>;&lt;/i>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.02516.pdf&quot;>;SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives&lt;/a>; &lt;br />; &lt;i>;&lt;strong>;Fedor Moiseev&lt;/strong>;, &lt;strong>;Gustavo Hernandez Abrego&lt;/strong>;, &lt;strong>;Peter Dornbach&lt;/strong>;, &lt;strong>;Imed Zitouni&lt;/strong>;, &lt;strong>;Enrique Alfonseca&lt;/strong>;, &lt;strong>;Zhe Dong&lt;/strong>;&lt;/i>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10266.pdf&quot;>;Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM&#39;s Translation Capability&lt;/a>; &lt;br />; &lt;em>;Eleftheria Briakou, &lt;strong>;Colin Cherry&lt;/strong>;, &lt;strong>;George Foster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09102.pdf&quot;>;Prompting PaLM for Translation: Assessing Strategies and Performance&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Vilar&lt;/b>;, &lt;b>;Markus Freitag&lt;/b>;, &lt;b>;Colin Cherry&lt;/b>;, &lt;b>;Jiaming Luo&lt;/b>;, &lt;b>;Viresh Ratnakar&lt;/b>;, &lt;b>;George Foster&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.17525.pdf&quot;>;Query Refinement Prompts for Closed-Book Long-Form QA&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kellie Webster&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10381.pdf&quot;>;To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering&lt;/a>; &lt;br />; &lt;em>;Dheeru Dua*, &lt;strong>;Emma Strubell&lt;/strong>;, Sameer Singh, &lt;strong>;Pat Verga&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00193.pdf&quot;>;FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/02/frmt-benchmark-for-few-shot-region.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;b>;Parker Riley&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Jan A. Botha&lt;/b>;, &lt;b>;Xavier Garcia&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Jason Riesa&lt;/b>;, &lt;b>;Orhan Firat&lt;/b>;,&lt;b>; Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.00397.pdf&quot;>;Conditional Generation with a Question-Answering Blueprint&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kuzman Ganchev&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;, &lt;b>;Fantine Huot&lt;/b>;, &lt;b>;Anders Sandholm&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Mirella Lapata&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12142.pdf&quot;>;Coreference Resolution Through a Seq2Seq Transition-Based System&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Bernd Bohnet&lt;/b>;, &lt;b>;Chris Alberti&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00106.pdf&quot;>;Cross-Lingual Transfer with Language-Specific Subnetworks for Low-Resource Dependency Parsing&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni, &lt;strong>;Dan Garrette&lt;/strong>;, Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08054.pdf&quot;>;DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue&lt;/a>; &lt;br />; &lt;em>;William Held*, &lt;strong>;Christopher Hidey&lt;/strong>;, &lt;strong>;Fei Liu&lt;/strong>;, &lt;strong>;Eric Zhu&lt;/strong>;, &lt;strong>;Rahul Goel&lt;/strong>;, Diyi Yang, &lt;strong>;Rushin Shah&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.08726.pdf&quot;>;RARR: Researching and Revising What Language Models Say, Using Language Models&lt;/a>; &lt;br />; Luyu Gao*, &lt;strong>;Zhuyun Dai&lt;/strong>;, &lt;strong>;Panupong Pasupat&lt;/strong>;, Anthony Chen*, &lt;strong>;Arun Tejasvi Chaganty&lt;/strong>;, &lt;strong>;Yicheng Fan&lt;/strong>;, &lt;strong>;Vincent Y. Zhao&lt;/strong>;, &lt;strong>;Ni Lao&lt;/strong>;, &lt;strong>;Hongrae Lee&lt;/strong>;, &lt;strong>;Da-Cheng Juan&lt;/strong>;, &lt;strong>;Kelvin Guu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.16793.pdf&quot;>;Benchmarking Large Language Model Capabilities for Conditional Generation&lt;/a>; &lt;br />; &lt;em>;Joshua Maynez, Priyanka Agrawal, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.01786.pdf&quot;>;Crosslingual Generalization Through Multitask Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Niklas Muennighoff, Thomas Wang, Lintang Sutawika, &lt;strong>;Adam Roberts&lt;/strong>;, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05655.pdf&quot;>;DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering&lt;/a>; &lt;br />; &lt;em>;Ella Neeman, &lt;strong>;Roee Aharoni&lt;/strong>;, Or Honovich, Leshem Choshen, &lt;strong>;Idan Szpektor&lt;/strong>;, Omri Abend&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10933.pdf&quot;>;Resolving Indirect Referring Expressions for Entity Selection&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mohammad Javad Hosseini&lt;/b>;, &lt;b>;Filip Radlinski&lt;/b>;, &lt;b>;Silvia Pareti&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11840.pdf&quot;>;SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models&lt;/a>; &lt;br />; &lt;em>;Akshita Jha*, &lt;strong>;Aida Mostafazadeh Davani&lt;/strong>;, Chandan K Reddy, &lt;strong>;Shachi Dave&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;, &lt;strong>;Sunipa Dev&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.10040.pdf&quot;>;The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks&lt;/a>; &lt;br />; &lt;em>;Nikil Selvam, &lt;strong>;Sunipa Dev&lt;/strong>;, Daniel Khashabi, Tushar Khot, Kai-Wei Chang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10562.pdf&quot;>;Character-Aware Models Improve Visual Text Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Rosanne Liu&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Chitwan Saharia&lt;/b>;, &lt;b>;William Chan&lt;/b>;, &lt;b>;Adam Roberts&lt;/b>;, &lt;b>;Sharan Narang&lt;/b>;, &lt;b>;Irina Blok&lt;/b>;, &lt;b>;RJ Mical&lt;/b>;, &lt;b>;Mohammad Norouzi&lt;/b>;, &lt;b>;Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06995.pdf&quot;>;Cold-Start Data Selection for Better Few-Shot Language Model Fine-Tuning: A Prompt-Based Uncertainty Propagation Approach&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment &lt;br />; &lt;em>;&lt;b>;Roni Rabin&lt;/b>;, &lt;b>;Alexandre Djerbetian&lt;/b>;, &lt;b>;Roee Engelberg&lt;/b>;, &lt;b>;Lidan Hackmon&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;, &lt;b>;Amir Globerson&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02549.pdf&quot;>;FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Chen-Yu Lee&lt;/b>;, &lt;b>;Chun-Liang Li&lt;/b>;, &lt;b>;Hao Zhang&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Vincent Perot&lt;/b>;, &lt;b>;Guolong Su&lt;/b>;, &lt;b>;Xiang Zhang&lt;/b>;,&lt;b>; Kihyuk Sohn&lt;/b>;, &lt;b>;Nikolay Glushinev&lt;/b>;, &lt;b>;Renshen Wang&lt;/b>;, &lt;b>;Joshua Ainslie&lt;/b>;, &lt;b>;Shangbang Long&lt;/b>;, &lt;b>;Siyang Qin&lt;/b>;,&lt;b>; Yasuhisa Fujii&lt;/b>;, &lt;b>;Nan Hua&lt;/b>;, &lt;b>;Tomas Pfister&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00922.pdf&quot;>;Dialect-Robust Evaluation of Generated Text&lt;/a>; &lt;br />; &lt;em>;Jiao Sun*, &lt;strong>;Thibault Sellam&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Tu Vu*, &lt;strong>;Timothy Dozat&lt;/strong>;, &lt;strong>;Dan Garrette&lt;/strong>;, &lt;strong>;Aditya Siddhant&lt;/strong>;, &lt;strong>;Jacob Eisenstein&lt;/strong>;, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.03950.pdf&quot;>;MISGENDERED: Limits of Large Language Models in Understanding Pronouns&lt;/a>; &lt;br />; &lt;em>;Tamanna Hossain, &lt;strong>;Sunipa Dev&lt;/strong>;, Sameer Singh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13894.pdf&quot;>;LAMBADA: Backward Chaining for Automated Reasoning in Natural Language&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mehran Kazemi&lt;/b>;, &lt;b>;Najoung Kim&lt;/b>;, &lt;b>;Deepti Bhatia&lt;/b>;, &lt;b>;Xin Xu&lt;/b>;, &lt;b>;Deepak Ramachandran&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.19585.pdf&quot;>;LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction&lt;/a>; &lt;br />; &lt;em>;Jeremiah Milbauer*, &lt;strong>;Annie Louis&lt;/strong>;, &lt;strong>;Mohammad Javad Hosseini&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, &lt;strong>;Donald Metzler&lt;/strong>;, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05392.pdf&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Andy Zeng&lt;/strong>;, Trevor Darrell, Dan Klein&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10001.pdf&quot;>;Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters&lt;/a>; &lt;br />; &lt;em>;Boshi Wang, Sewon Min, Xiang Deng, &lt;strong>;Jiaming Shen&lt;/strong>;, &lt;strong>;You Wu&lt;/strong>;, Luke Zettlemoyer and Huan Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14106.pdf&quot;>;Better Zero-Shot Reasoning with Self-Adaptive Prompting&lt;/a>; &lt;br />; &lt;em>;Xingchen Wan*, &lt;strong>;Ruoxi Sun&lt;/strong>;, Hanjun Dai, &lt;strong>;Sercan Ã–. Arik&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.00186.pdf&quot;>;Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Paul Roit&lt;/b>;, &lt;b>;Johan Ferret&lt;/b>;, &lt;b>;Lior Shani&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Geoffrey Cideron&lt;/b>;, &lt;b>;Robert Dadashi&lt;/b>;, &lt;b>;Matthieu Geist&lt;/b>;,&lt;b>; Sertan Girgin&lt;/b>;, &lt;b>;LÃ©onard Hussenot&lt;/b>;, &lt;b>;Orgad Keller&lt;/b>;, &lt;b>;Nikola Momchev&lt;/b>;, &lt;b>;Sabela Ramos&lt;/b>;, &lt;b>;Piotr Stanczyk&lt;/b>;, &lt;b>;Nino Vieillard&lt;/b>;, &lt;b>;Olivier Bachem&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Avinatan Hassidim&lt;/b>;, &lt;b>;Olivier Pietquin&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09248.pdf&quot;>;Natural Language to Code Generation in Interactive Data Science Notebooks&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Pengcheng Yin&lt;/b>;, &lt;b>;Wen-Ding Li&lt;/b>;, &lt;b>;Kefan Xiao&lt;/b>;, &lt;b>;Abhishek Rao&lt;/b>;, &lt;b>;Yeming Wen&lt;/b>;, &lt;b>;Kensen Shi&lt;/b>;, &lt;b>;Joshua Howland&lt;/b>;,&lt;b>; Paige Bailey&lt;/b>;, &lt;b>;Michele Catasta&lt;/b>;, &lt;b>;Henryk Michalewski&lt;/b>;, &lt;b>;Oleksandr Polozov&lt;/b>;, &lt;b>;Charles Sutton&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08410.pdf&quot;>;Teaching Small Language Models to Reason&lt;/a>; &lt;br />; &lt;em>;Lucie Charlotte Magister*, &lt;strong>;Jonathan Mallinson&lt;/strong>;, &lt;strong>;Jakub Adamek&lt;/strong>;, &lt;strong>;Eric Malmi&lt;/strong>;, &lt;strong>;Aliaksei Severyn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/assets/pdf/papers/NeuPSL.pdf&quot;>;Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic&lt;/a>; &lt;br />; &lt;em>;Connor Pryor*, &lt;strong>;Quan Yuan&lt;/strong>;, &lt;strong>;Jeremiah Liu&lt;/strong>;, &lt;strong>;Mehran Kazemi&lt;/strong>;, &lt;strong>;Deepak Ramachandran&lt;/strong>;, &lt;strong>;Tania Bedrax-Weiss&lt;/strong>;, Lise Getoor&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10397.pdf&quot;>;A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization&lt;/a>; &lt;br />; &lt;em>;Lining Zhang, Simon Mille, Yufang Hou, &lt;strong>;Daniel Deutsch&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Yixin Liu, Saad Mahamood, &lt;strong>;Sebastian Gehrmann&lt;/strong>;, Miruna Clinciu, Khyathi Raghavi Chandu and JoÃ£o Sedoc&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Industry Track papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18465.pdf&quot;>;Federated Learning of Gboard Language Models with Differential Privacy&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Zheng Xu&lt;/b>;, &lt;b>;Yanxiang Zhang&lt;/b>;, &lt;b>;Galen Andrew&lt;/b>;, &lt;b>;Christopher Choquette&lt;/b>;, &lt;b>;Peter Kairouz&lt;/b>;, &lt;b>;Brendan McMahan&lt;/b>;, &lt;b>;Jesse Rosenstock&lt;/b>;, &lt;b>;Yuanbo Zhang&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18373.pdf&quot;>;KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models&lt;/a>; &lt;br />; &lt;em>;Zhiwei Jia*, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Arjun Akula&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, Hao Su, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;ACL Findings papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10622.pdf&quot;>;Multilingual Summarization with Factual Consistency Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Jonathan Herzig&lt;/b>;, &lt;b>;Elizabeth Clark&lt;/b>;, &lt;b>;Mirella Lapata &lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06767.pdf&quot;>;Parameter-Efficient Fine-Tuning for Robust Continual Multilingual Learning&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Kartikeya Badola&lt;/b>;, &lt;b>;Shachi Dave&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08153.pdf&quot;>;FiDO: Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference&lt;/a>; &lt;br />; &lt;em>;Michiel de Jong*, &lt;strong>;Yury Zemlyanskiy&lt;/strong>;, &lt;strong>;Joshua Ainslie&lt;/strong>;, &lt;strong>;Nicholas FitzGerald&lt;/strong>;, &lt;strong>;Sumit Sanghai&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;William Cohen&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00609.pdf&quot;>;A Simple, Yet Effective Approach to Finding Biases in Code Generation&lt;/a>; &lt;br />; &lt;em>;Spyridon Mouselinos, Mateusz Malinowski, &lt;strong>;Henryk Michalewski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09261.pdf&quot;>;Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mirac Suzgun&lt;/b>;, &lt;b>;Nathan Scales&lt;/b>;, &lt;b>;Nathanael Scharli&lt;/b>;, &lt;b>;Sebastian Gehrmann&lt;/b>;, &lt;b>;Yi Tay&lt;/b>;, &lt;b>;Hyung Won Chung&lt;/b>;,&lt;b>; Aakanksha Chowdhery&lt;/b>;, &lt;b>;Quoc Le&lt;/b>;, &lt;b>;Ed Chi&lt;/b>;, &lt;b>;Denny Zhou&lt;/b>;, &lt;b>;Jason Wei&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.07730.pdf&quot;>;QueryForm: A Simple Zero-Shot Form Entity Query Framework&lt;/a>; &lt;br />; &lt;em>;Zifeng Wang*, &lt;strong>;Zizhao Zhang&lt;/strong>;, &lt;strong>;Jacob Devlin&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Guolong Su&lt;/strong>;, &lt;strong>;Hao Zhang&lt;/strong>;, Jennifer Dy, &lt;strong>;Vincent Perot&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10703.pdf&quot;>;ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09682.pdf&quot;>;Multilingual Sequence-to-Sequence Models for Hebrew NLP&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Matan Eyal&lt;/b>;, &lt;b>;Hila Noga&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.04009.pdf&quot;>;Triggering Multi-Hop Reasoning for Question Answering in Language Models Using Soft Prompts and Random Walks&lt;/a>; &lt;br />; &lt;em>;Kanishka Misra*, &lt;strong>;Cicero Nogueira dos Santos&lt;/strong>;, Siamak Shakeri&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Complex Reasoning in Natural Language&lt;/a>;&lt;br />; &lt;em>;Wenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, &lt;strong>;Aman Madaan&lt;/strong>;, Tao Yu&lt;/em>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Generating Text from Language Models&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Afra Amini&lt;/b>;, &lt;b>;Ryan Cotterell&lt;/b>;, &lt;b>;John Hewitt&lt;/b>;, &lt;b>;Clara Meister&lt;/b>;, &lt;b>;Tiago Pimentel&lt;/b>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/sustainlp2023&quot;>;Simple and Efficient Natural Language Processing (SustaiNLP)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Tal Schuster&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.workshopononlineabuse.com/&quot;>;Workshop on Online Abuse and Harms (WOAH)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Aida Mostafazadeh Davani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://doc2dial.github.io/workshop2023/&quot;>;Document-Grounded Dialogue and Conversational Question Answering (DialDoc)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/5thnlp4convai/home?authuser=0&quot;>;NLP for Conversational AI&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cawl.wellformedness.com/&quot;>;Computation and Written Language (CAWL)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;b>;Kyle Gorman&lt;/b>;, &lt;b>;Brian Roark&lt;/b>;, &lt;b>;Richard Sproat&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sigmorphon.github.io/workshops/2023/&quot;>;Computational Morphology and Phonology (SIGMORPHON)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Kyle Gorman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/umass.edu/wnu2023&quot;>;Workshop on Narrative Understanding (WNU)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7093565002706757508/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-acl-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-acl-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ACL 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s72-c/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6926843365781180400&lt;/id>;&lt;published>;2023-07-07T11:01:00.001-07:00&lt;/published>;&lt;updated>;2023-07-07T11:09:20.724-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;video&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Modular visual question answering via code generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sanjay Subramanian, PhD student, UC Berkeley, and Arsha Nagrani, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://visualqa.org/&quot;>;Visual question answering&lt;/a>; (VQA) is a machine learning task that requires a model to answer a question about an image or &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;a set of images&lt;/a>;. Conventional VQA approaches need a large amount of labeled training data consisting of thousands of human-annotated question-answer pairs associated with images. In recent years, advances in large-scale pre-training have led to the development of VQA methods that perform well with &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;fewer than fifty training examples&lt;/a>; (few-shot) and &lt;a href=&quot;https://ai.googleblog.com/2022/07/rewriting-image-captions-for-visual.html&quot;>;without any human-annotated VQA training data&lt;/a>; (zero-shot). However, there is still a significant performance gap between these methods and state-of-the-art fully supervised VQA methods, such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html&quot;>;MaMMUT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;. In particular, few-shot methods struggle with spatial reasoning, counting, and multi-hop reasoning. Furthermore, few-shot methods have generally been limited to answering questions about single images. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To improve accuracy on VQA examples that involve complex reasoning, in â€œ&lt;a href=&quot;https://arxiv.org/abs/2306.05392&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>;,â€ to appear at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, we introduce CodeVQA, a framework that answers visual questions using program synthesis. Specifically, when given a question about an image or set of images, CodeVQA generates a Python program (code) with simple visual functions that allow it to process images, and executes this program to determine the answer. We demonstrate that in the few-shot setting, CodeVQA outperforms prior work by roughly 3% on the &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; dataset and 2% on the &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; dataset. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;CodeVQA&lt;/h2>; &lt;p>; The CodeVQA approach uses a code-writing large language model (LLM), such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PALM&lt;/a>;, to generate &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;Python&lt;/a>; programs (code). We guide the LLM to correctly use visual functions by crafting a prompt consisting of a description of these functions and fewer than fifteen â€œin-contextâ€ examples of visual questions paired with the associated Python code for them. To select these examples, we compute embeddings for the input question and of all of the questions for which we have annotated programs (a randomly chosen set of fifty). Then, we select questions that have the highest similarity to the input and use them as in-context examples. Given the prompt and question that we want to answer, the LLM generates a Python program representing that question. &lt;/p>; &lt;p>; We instantiate the CodeVQA framework using three visual functions: (1) &lt;code>;query&lt;/code>;, (2) &lt;code>;get_pos&lt;/code>;, and (3) &lt;code>;find_matching_image&lt;/code>;. &lt;/p>; &lt;ul>; &lt;li>;&lt;code>;Query&lt;/code>;, which answers a question about a single image, is implemented using the few-shot &lt;a href=&quot;https://arxiv.org/abs/2210.08773&quot;>;Plug-and-Play VQA&lt;/a>; (PnP-VQA) method. PnP-VQA generates captions using &lt;a href=&quot;https://arxiv.org/abs/2201.12086&quot;>;BLIP&lt;/a>; â€” an image-captioning &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformer&lt;/a>; pre-trained on millions of image-caption pairs â€” and feeds these into a LLM that outputs the answers to the question. &lt;/li>;&lt;li>;&lt;code>;Get_pos&lt;/code>;, which is an object localizer that takes a description of an object as input and returns its position in the image, is implemented using &lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GradCAM&lt;/a>;. Specifically, the description and the image are passed through the BLIP joint text-image encoder, which predicts an image-text matching score. GradCAM takes the gradient of this score with respect to the image features to find the region most relevant to the text. &lt;/li>;&lt;li>;&lt;code>;Find_matching_image&lt;/code>;, which is used in multi-image questions to find the image that best matches a given input phrase, is implemented by using BLIP text and image encoders to compute a text embedding for the phrase and an image embedding for each image. Then the dot products of the text embedding with each image embedding represent the relevance of each image to the phrase, and we pick the image that maximizes this relevance. &lt;/li>; &lt;/ul>; &lt;p>; The three functions can be implemented using models that require very little annotation (eg, text and image-text pairs collected from the web and a small number of VQA examples). Furthermore, the CodeVQA framework can be easily generalized beyond these functions to others that a user might implement (eg, object detection, image segmentation, or knowledge base retrieval). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s1024/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;501&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the CodeVQA method. First, a large language model generates a Python program (code), which invokes visual functions that represent the question. In this example, a simple VQA method (&lt;code>;query&lt;/code>;) is used to answer one part of the question, and an object localizer (&lt;code>;get_pos&lt;/code>;) is used to find the positions of the objects mentioned. Then the program produces an answer to the original question by combining the outputs of these functions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; The CodeVQA framework correctly generates and executes Python programs not only for single-image questions, but also for multi-image questions. For example, if given two images, each showing two pandas, a question one might ask is, â€œIs it true that there are four pandas?â€ In this case, the LLM converts the counting question about the pair of images into a program in which an object count is obtained for each image (using the &lt;em>;query&lt;/em>; function). Then the counts for both images are added to compute a total count, which is then compared to the number in the original question to yield a yes or no answer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s1080/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We evaluate CodeVQA on three visual reasoning datasets: &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; (single-image), &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; (multi-image), and &lt;a href=&quot;https://lil.nlp.cornell.edu/nlvr/&quot;>;NLVR2&lt;/a>; (multi-image). For GQA, we provide 12 in-context examples to each method, and for COVR and NLVR2, we provide six in-context examples to each method. The table below shows that CodeVQA improves consistently over the baseline few-shot VQA method on all three datasets. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;GQA&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;COVR&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;NLVR2&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.56 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.06 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;63.37 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.03 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;54.11 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64.04 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the GQA, COVR, and NLVR2 datasets, showing that CodeVQA consistently improves over few-shot PnP-VQA. The metric is exact-match accuracy, ie, the percentage of examples in which the predicted answer exactly matches the ground-truth answer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that in GQA, CodeVQA&#39;s accuracy is roughly 30% higher than the baseline on spatial reasoning questions, 4% higher on â€œandâ€ questions, and 3% higher on â€œorâ€ questions. The third category includes multi-hop questions such as â€œAre there salt shakers or skateboards in the picture?â€, for which the generated program is shown below. &lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;img = open_image(&quot;Image13.jpg&quot;) salt_shakers_exist = query(img, &quot;Are there any salt shakers?&quot;) skateboards_exist = query(img, &quot;Are there any skateboards?&quot;) if salt_shakers_exist == &quot;yes&quot; or skateboards_exist == &quot;yes&quot;: answer = &quot;yes&quot; else: answer = &quot;no&quot; &lt;/font>;&lt;/pre>; &lt;br />; &lt;p>; In COVR, we find that CodeVQA&#39;s gain over the baseline is higher when the number of input images is larger, as shown in the table below. This trend indicates that breaking the problem down into single-image questions is beneficial. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;margin-left: auto;å³è¾¹è·ï¼šè‡ªåŠ¨ï¼› text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Number of images&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;3&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>;&amp;nbsp; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;91.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;51.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;47.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;75.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.2 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present CodeVQA, a framework for few-shot visual question answering that relies on code generation to perform multi-step visual reasoning. Exciting directions for future work include expanding the set of modules used and creating a similar framework for visual tasks beyond VQA. We note that care should be taken when considering whether to deploy a system such as CodeVQA, since vision-language models like the ones used in our visual functions &lt;a href=&quot;https://arxiv.org/abs/2108.02818&quot;>;have been shown to exhibit social biases&lt;/a>;. At the same time, compared to monolithic models, CodeVQA offers additional interpretability (through the Python program) and controllability (by modifying the prompts or visual functions), which are useful in production systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was a collaboration between &lt;a href=&quot;https://bair.berkeley.edu/baircommons.html&quot;>;UC Berkeley&#39;s Artificial Intelligence Research lab&lt;/a>; (BAIR) and Google Research, and was conducted by Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6926843365781180400/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/modular-visual-question-answering-via.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/modular-visual-question-answering-via.html&quot; rel=&quot;alternate&quot; title=&quot;Modular visual question answering via code generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-561916049750681872&lt;/id>;&lt;published>;2023-07-06T12:50:00.000-07:00&lt;/published>;&lt;updated>;2023-07-06T12:50:04.339-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Pic2Word: Mapping pictures to words for zero-shot composed image retrieval&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kuniaki Saito, Student Researcher, Google Research, Cloud AI Team, and Kihyuk Sohn, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s1100/Pic2Word.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Image retrieval plays a crucial role in search engines. Typically, their users rely on either image or text as a query to retrieve a desired target image. However, text-based retrieval has its limitations, as describing the target image accurately using words can be challenging. For instance, when searching for a fashion item, users may want an item whose specific attribute, eg, the color of a logo or the logo itself, is different from what they find in a website. Yet searching for the item in an existing search engine is not trivial since precisely describing the fashion item by text can be challenging. To address this fact, &lt;a href=&quot;https://arxiv.org/pdf/1812.07119.pdf&quot;>;composed image retrieval&lt;/a>; (CIR) retrieves images based on a query that combines both an image and a text sample that provides instructions on how to modify the image to fit the intended retrieval target. Thus, CIR allows precise retrieval of the target image by combining image and text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, CIR methods require large amounts of labeled data, ie, triplets of a 1) query image, 2) description, and 3) target image. Collecting such labeled data is costly, and models trained on this data are often tailored to a specific use case, limiting their ability to generalize to different datasets. &lt;/p>; &lt;p>; To address these challenges, in â€œ&lt;a href=&quot;https://arxiv.org/abs/2302.03084&quot;>;Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval&lt;/a>;â€, we propose a task called zero-shot CIR (ZS-CIR). In ZS-CIR, we aim to build a single CIR model that performs a variety of CIR tasks, such as &lt;a href=&quot;https://www.zheyuanliu.me/CIRR/&quot;>;object composition,&lt;/a>; &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;attribute editing&lt;/a>;, or domain conversion, without requiring labeled triplet data. Instead, we propose to train a retrieval model using large-scale image-caption pairs and unlabeled images, which are considerably easier to collect than supervised CIR datasets at scale. To encourage reproducibility and further advance this space, we also &lt;a href=&quot;https://github.com/google-research/composed_image_retrieval&quot;>;release the code&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s1062/image9.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;342&quot; data-original-width=&quot;1062&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Description of existing composed image retrieval model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCWi9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s949/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;476&quot; data-original-width=&quot;949&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCWi9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We train a composed image retrieval model using image-caption data only. Our model retrieves images aligned with the composition of the query image and text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Method overview&lt;/h2>; &lt;p>; We propose to leverage the language capabilities of the language encoder in the &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;contrastive language-image pre-trained model&lt;/a>; (CLIP), which excels at generating semantically meaningful language embeddings for a wide range of textual concepts and attributes. To that end, we use a lightweight mapping sub-module in CLIP that is designed to map an input picture (eg, a photo of a cat) from the image embedding space to a word token (eg, â€œcatâ€) in the textual input space. The whole network is optimized with the vision-language contrastive loss to again ensure the visual and text embedding spaces are as close as possible given a pair of an image and its textual description. Then, the query image can be treated as if it is a word. This enables the flexible and seamless composition of query image features and text descriptions by the language encoder. We call our method Pic2Word and provide an overview of its training process in the figure below. We want the mapped token &lt;em>;s&lt;/em>; to represent the input image in the form of word token. Then, we train the mapping network to reconstruct the image embedding in the language embedding, &lt;em>;p&lt;/em>;. Specifically, we optimize the contrastive loss proposed in CLIP computed between the visual embedding &lt;em>;v&lt;/em>; and the textual embedding &lt;em>;p&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYHXfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s611/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;611&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYHXfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Training of the mapping network (&lt;em>;f&lt;sub>;M&lt;/sub>;&lt;/em>;) using unlabeled images only. We optimize only the mapping network with a frozen visual and text encoder.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Given the trained mapping network, we can regard an image as a word token and pair it with the text description to flexibly compose the joint image-text query as shown in the figure below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s827/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-width=&quot;827&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With the trained mapping network, we regard the image as a word token and pair it with the text description to flexibly compose the joint image-text query.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We conduct a variety of experiments to evaluate Pic2Word&#39;s performance on a variety of CIR tasks. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Domain conversion&lt;/h3>; &lt;p>; We first evaluate the capability of compositionality of the proposed method on domain conversion â€” given an image and the desired new image domain (eg, sculpture, origami, cartoon, toy), the output of the system should be an image with the same content but in the new desired image domain or style. As illustrated below, we evaluate the ability to compose the category information and domain description given as an image and text, respectively. We evaluate the conversion from real images to four domains using &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; and &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>;. &lt;/p>; &lt;p>; To compare with approaches that do not require supervised training data, we pick three approaches: (i) &lt;em>;image only&lt;/em>; performs retrieval only with visual embedding, (ii) &lt;em>;text only &lt;/em>;employs only text embedding, and (iii) &lt;em>;image + text &lt;/em>;averages the visual and text embedding to compose the query. The comparison with (iii) shows the importance of composing image and text using a language encoder. We also compare with &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;Combiner&lt;/a>;, which trains the CIR model on &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; or &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRR&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s965/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;513&quot; data-original-width=&quot;965&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We aim to convert the domain of the input query image into the one described with text, eg, origami.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As shown in figure below, our proposed approach outperforms baselines by a large margin. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s754/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;754&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results (&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;recall&lt;/a>;@10, ie, the percentage of relevant instances in the first 10 images retrieved.) on composed image retrieval for domain conversion.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Fashion attribute composition&lt;/h3>; &lt;p>; Next, we evaluate the composition of fashion attributes, such as the color of cloth, logo, and length of sleeve, using the &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; dataset. The figure below illustrates the desired output given the query. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s818/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;309&quot; data-original-width=&quot;818&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of CIR for fashion attributes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In the figure below, we present a comparison with baselines, including supervised baselines that utilized triplets for training the CIR model: (i) &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;CB&lt;/a>; uses the same architecture as our approach, (ii) &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRPLANT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2203.08101.pdf&quot;>;ALTEMIS&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2007.00145.pdf&quot;>;MAAF&lt;/a>; use a smaller backbone, such as ResNet50. Comparison to these approaches will give us the understanding on how well our zero-shot approach performs on this task. &lt;/p>; &lt;p>; Although CB outperforms our approach, our method performs better than supervised baselines with smaller backbones. This result suggests that by utilizing a robust CLIP model, we can train a highly effective CIR model without requiring annotated triplets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s807/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;498&quot; data-original-width=&quot;807&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results (&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;recall&lt;/a>;@10, ie, the percentage of relevant instances in the first 10 images retrieved.) on composed image retrieval for Fashion-IQ dataset (higher is better). Light blue bars train the model using triplets. Note that our approach performs on par with these supervised baselines with shallow (smaller) backbones.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Qualitative results&lt;/h3>; &lt;p>; We show several examples in the figure below. Compared to a baseline method that does not require supervised training data (text + image feature averaging), our approach does a better job of correctly retrieving the target image. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s1999/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Qualitative results on diverse query images and text description.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; In this article, we introduce Pic2Word, a method for mapping pictures to words for ZS-CIR. We propose to convert the image into a word token to achieve a CIR model using only an image-caption dataset. Through a variety of experiments, we verify the effectiveness of the trained model on diverse CIR tasks, indicating that training on an image-caption dataset can build a powerful CIR model. One potential future research direction is utilizing caption data to train the mapping network, although we use only image data in the present work. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Also thanks to Zizhao Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/561916049750681872/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/pic2word-mapping-pictures-to-words-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/pic2word-mapping-pictures-to-words-for.html&quot; rel=&quot;alternate&quot; title=&quot;Pic2Word: Mapping pictures to words for zero-shot composed image retrieval&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s72-c/Pic2Word.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-667019077470952746&lt;/id>;&lt;published>;2023-06-29T14:08:00.000-07:00&lt;/published>;&lt;updated>;2023-06-29T14:08:24.386-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Announcing the first Machine Unlearning Challenge&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Fabian Pedregosa and Eleni Triantafillou, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s900/Unlearning.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Deep learning has recently driven tremendous progress in a wide array of applications, ranging from &lt;a href=&quot;https://imagen.research.google/&quot;>;realistic image generation&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;impressive retrieval systems&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/ai/bard-google-ai-search-updates/&quot;>;language models that can hold human-like conversations&lt;/a>;. While this progress is very exciting, the widespread use of deep neural network models requires caution: as guided by Google&#39;s AI &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Principles&lt;/a>;, we seek to develop AI technologies responsibly by understanding and mitigating potential risks, such as the propagation and amplification of unfair biases and protecting user privacy. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Fully erasing the influence of the data requested to be deleted is challenging since, aside from simply deleting it from databases where it&#39;s stored, it also requires erasing the influence of that data on other artifacts such as trained machine learning models. Moreover, recent research [&lt;a href=&quot;https://arxiv.org/abs/1610.05820&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;2&lt;/a>;] has shown that in some cases it may be possible to infer with high accuracy whether an example was used to train a machine learning model using &lt;a href=&quot;https://en.wikipedia.org/wiki/Adversarial_machine_learning#Model_extraction&quot;>;membership inference attacks&lt;/a>; (MIAs). This can raise privacy concerns, as it implies that even if an individual&#39;s data is deleted from a database, it may still be possible to infer whether that individual&#39;s data was used to train a model. &lt;/p>; &lt;p>; Given the above, &lt;em>;machine unlearning&lt;/em>; is an emergent subfield of machine learning that aims to remove the influence of a specific subset of training examples â€” the &quot;forget set&quot; â€” from a trained model. Furthermore, an ideal unlearning algorithm would remove the influence of certain examples &lt;em>;while maintaining&lt;/em>; other beneficial properties, such as the accuracy on the rest of the train set and generalization to held-out examples. A straightforward way to produce this unlearned model is to retrain the model on an adjusted training set that excludes the samples from the forget set. However, this is not always a viable option, as retraining deep models can be computationally expensive. An ideal unlearning algorithm would instead use the already-trained model as a starting point and efficiently make adjustments to remove the influence of the requested data. &lt;/p>; &lt;p>; Today we&#39;re thrilled to announce that we&#39;ve teamed up with a broad group of academic and industrial researchers to organize the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>;. The competition considers a realistic scenario in which after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. The competition will be hosted on &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>;, and submissions will be automatically scored in terms of both forgetting quality and model utility. We hope that this competition will help advance the state of the art in machine unlearning and encourage the development of efficient, effective and ethical unlearning algorithms. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Machine unlearning applications&lt;/h2>; &lt;p>; Machine unlearning has applications beyond protecting user privacy. For instance, one can use unlearning to erase inaccurate or outdated information from trained models (eg, due to errors in labeling or changes in the environment) or remove harmful, manipulated, or outlier data. &lt;/p>; &lt;p>; The field of machine unlearning is related to other areas of machine learning such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1802.07569&quot;>;life-long learning&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Fairness_(machine_learning)&quot;>;fairness&lt;/a>;. Differential privacy aims to guarantee that no particular training example has too large an influence on the trained model; a stronger goal compared to that of unlearning, which only requires erasing the influence of the designated forget set. Life-long learning research aims to design models that can learn continuously while maintaining previously-acquired skills. As work on unlearning progresses, it may also open additional ways to boost fairness in models, by correcting unfair biases or disparate treatment of members belonging to different groups (eg, demographics, age groups, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s720/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Anatomy of unlearning.&lt;/b>; An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the &quot;forget set&quot;). From the model, forget set, and retain set, the unlearning algorithm produces an updated model. An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Challenges of machine unlearning&lt;/h2>; &lt;p>; The problem of unlearning is complex and multifaceted as it involves several conflicting objectives: forgetting the requested data, maintaining the model&#39;s utility (eg, accuracy on retained and held-out data), and efficiency. Because of this, existing unlearning algorithms make different trade-offs. For example, full retraining achieves successful forgetting without damaging model utility, but with poor efficiency, while &lt;a href=&quot;https://arxiv.org/abs/2007.02923&quot;>;adding noise&lt;/a>; to the weights achieves forgetting at the expense of utility. &lt;/p>; &lt;p>; Furthermore, the evaluation of forgetting algorithms in the literature has so far been highly inconsistent. While some &lt;a href=&quot;https://arxiv.org/abs/1911.04933&quot;>;works&lt;/a>; report the classification accuracy on the samples to unlearn, &lt;a href=&quot;https://proceedings.mlr.press/v119/wu20b.html&quot;>;others&lt;/a>; report distance to the fully retrained model, and yet others use the error rate of membership inference attacks as a metric for forgetting quality [&lt;a href=&quot;https://arxiv.org/abs/2302.09880&quot;>;4&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2010.10981&quot;>;5&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.02205&quot;>;6&lt;/a>;]. &lt;/p>; &lt;p>; We believe that the inconsistency of evaluation metrics and the lack of a standardized protocol is a serious impediment to progress in the field â€” we are unable to make direct comparisons between different unlearning methods in the literature. This leaves us with a myopic view of the relative merits and drawbacks of different approaches, as well as open challenges and opportunities for developing improved algorithms. To address the issue of inconsistent evaluation and to advance the state of the art in the field of machine unlearning, we&#39;ve teamed up with a broad group of academic and industrial researchers to organize the first unlearning challenge. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Announcing the first Machine Unlearning Challenge&lt;/h2>; &lt;p>; We are pleased to announce the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>;, which will be held as part of the &lt;a href=&quot;https://neurips.cc/Conferences/2023/CompetitionTrack&quot;>;NeurIPS 2023 Competition Track.&lt;/a>; The goal of the competition is twofold. First, by unifying and standardizing the evaluation metrics for unlearning, we hope to identify the strengths and weaknesses of different algorithms through apples-to-apples comparisons. Second, by opening this competition to everyone, we hope to foster novel solutions and shed light on open challenges and opportunities. &lt;/p>; &lt;p>; The competition will be hosted on &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; and run between mid-July 2023 and mid-September 2023. As part of the competition, today we&#39;re announcing the availability of the &lt;a href=&quot;https://github.com/unlearning-challenge/starting-kit&quot;>;starting kit&lt;/a>;. This starting kit provides a foundation for participants to build and test their unlearning models on a toy dataset. &lt;/p>; &lt;p>; The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. For this, we will make available as part of the starting kit a dataset of synthetic faces (samples shown below) and we&#39;ll also use several real-face datasets for evaluation of submissions. The participants are asked to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We will also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s2000/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;2000&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Excerpt images from the &lt;a href=&quot;https://github.com/microsoft/FaceSynthetics&quot;>;Face Synthetics&lt;/a>; dataset together with age annotations. The competition considers the scenario in which an age predictor has been trained on face images like the above, and, after training, a certain subset of the training images must be forgotten.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For evaluating forgetting, we will use tools inspired by MIAs, such as &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;LiRA&lt;/a>;. MIAs were first developed in the privacy and security literature and their goal is to infer which examples were part of the training set. Intuitively, if unlearning is successful, the unlearned model contains no traces of the forgotten examples, causing MIAs to fail: the attacker would be &lt;em>;unable&lt;/em>; to infer that the forget set was, in fact, part of the original training set. In addition, we will also use statistical tests to quantify how different the distribution of unlearned models (produced by a particular submitted unlearning algorithm) is compared to the distribution of models retrained from scratch. For an ideal unlearning algorithm, these two will be indistinguishable. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Machine unlearning is a powerful tool that has the potential to address several open problems in machine learning. As research in this area continues, we hope to see new methods that are more efficient, effective, and responsible. We are thrilled to have the opportunity via this competition to spark interest in this field, and we are looking forward to sharing our insights and findings with the community. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. We are writing this blog post on behalf of the organization team of the Unlearning Competition: Eleni Triantafillou*, Fabian Pedregosa* (*equal contribution), Meghdad Kurmanji, Kairan Zhao, Gintare Karolina Dziugaite, Peter Triantafillou, Ioannis Mitliagkas, Vincent Dumoulin, Lisheng Sun Hosoya, Peter Kairouz, Julio CS Jacques Junior, Jun Wan, Sergio Escalera and Isabelle Guyon.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/667019077470952746/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/announcing-first-machine-unlearning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/announcing-first-machine-unlearning.html&quot; rel=&quot;alternate&quot; title=&quot;Announcing the first Machine Unlearning Challenge&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s72-c/Unlearning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9161751123508054082&lt;/id>;&lt;published>;2023-06-29T12:10:00.004-07:00&lt;/published>;&lt;updated>;2023-07-18T14:55:41.583-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;On-device diffusion plugins for conditioned text-to-image generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao and Tingbo Hou, Software Engineers, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s800/MediaPipeDiffusion-hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown great success in text-to-image generation, achieving high image quality, improved inference performance, and expanding our creative inspiration. Nevertheless, it is still challenging to efficiently control the generation, especially with conditions that are difficult to describe with text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we announce &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; diffusion plugins, which enable controllable text-to-image generation to be run on-device. Expanding upon our &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;prior work&lt;/a>; on GPU inference for on-device large generative models, we introduce new low-cost solutions for controllable text-to-image generation that can be plugged into existing diffusion models and their Low-Rank Adaptation (&lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;>;LoRA&lt;/a>;) variants. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s1080/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;780&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Text-to-image generation with control plugins running on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Background&lt;/h2>; &lt;p>; With diffusion models, image generation is modeled as an iterative denoising process. Starting from a noise image, at each step, the diffusion model gradually denoises the image to reveal an image of the target concept. Research shows that leveraging language understanding via text prompts can greatly improve image generation. For text-to-image generation, the text embedding is connected to the model via cross-attention layers. Yet, some information is difficult to describe by text prompts, eg, the position and pose of an object. To address this problem, researchers add additional models into the diffusion to inject control information from a condition image. &lt;/p>; &lt;p>; Common approaches for controlled text-to-image generation include &lt;a href=&quot;https://arxiv.org/abs/2211.12572&quot;>;Plug-and-Play&lt;/a>;, &lt;a href=&quot;https://github.com/lllyasviel/ControlNet&quot;>;ControlNet&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2302.08453&quot;>;T2I Adapter&lt;/a>;. Plug-and-Play applies a widely used denoising diffusion implicit model (&lt;a href=&quot;https://arxiv.org/abs/2010.02502&quot;>;DDIM&lt;/a>;) inversion approach that reverses the generation process starting from an input image to derive an initial noise input, and then employs a copy of the diffusion model (860M parameters for Stable Diffusion 1.5) to encode the condition from an input image. Plug-and-Play extracts spatial features with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;self-attention&lt;/a>; from the copied diffusion, and injects them into the text-to-image diffusion. ControlNet creates a trainable copy of the encoder of a diffusion model, which connects via a convolution layer with zero-initialized parameters to encode conditioning information that is conveyed to the decoder layers. However, as a result, the size is large, half that of the diffusion model (430M parameters for Stable Diffusion 1.5). T2I Adapter is a smaller network (77M parameters) and achieves similar effects in controllable generation. T2I Adapter only takes the condition image as input, and its output is shared across all diffusion iterations. Yet, the adapter model is not designed for portable devices. &lt;/p>; &lt;br />; &lt;h2>;The MediaPipe diffusion plugins&lt;/h2>; &lt;p>; To make conditioned generation efficient, customizable, and scalable, we design the MediaPipe diffusion plugin as a separate network that is: &lt;/p>; &lt;ul>; &lt;li>;&lt;i>;Plugable&lt;/i>;: It can be easily connected to a pre-trained base model. &lt;/li>;&lt;li>;&lt;i>;Trained from scratch&lt;/i>;: It does not use pre-trained weights from the base model. &lt;/li>;&lt;li>;&lt;i>;Portable&lt;/i>;: It runs outside the base model on mobile devices, with negligible cost compared to the base model inference. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Parameter Size&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Plugable&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;From Scratch&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Portable&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Plug-and-Play &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;860M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âœ”ï¸ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âŒ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âŒ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;430M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âœ”ï¸ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âŒ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âŒ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;T2I Adapter &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;77M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âœ”ï¸ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âœ”ï¸ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âŒ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âœ”ï¸ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âœ”ï¸ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;âœ”ï¸ &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Plug-and-Play, ControlNet, T2I Adapter, and the MediaPipe diffusion plugin.&lt;br />;* The number varies depending on the particulars of the diffusion model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The MediaPipe diffusion plugin is a portable on-device model for text-to-image generation. It extracts multiscale features from a conditioning image, which are added to the encoder of a diffusion model at corresponding levels. When connecting to a text-to-image diffusion model, the plugin model can provide an extra conditioning signal to the image generation. We design the plugin network to be a lightweight model with only 6M parameters. It uses depth-wise convolutions and inverted bottlenecks from &lt;a href=&quot;https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html&quot;>;MobileNetv2&lt;/a>; for fast inference on mobile devices. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s1724/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;628&quot; data-original-width=&quot;1724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the MediaPipe diffusion model plugin. The plugin is a separate network, whose output can be plugged into a pre-trained text-to-image generation model. Features extracted by the plugin are applied to the associated downsampling layer of the diffusion model (blue).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Unlike ControlNet, we inject the same control features in all diffusion iterations. That is, we only run the plugin once for one image generation, which saves computation. We illustrate some intermediate results of a diffusion process below. The control is effective at every diffusion step and enables controlled generation even at early steps. More iterations improve the alignment of the image with the text prompt and generate more detail. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s1280/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the generation process using the MediaPipe diffusion plugin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Examples&lt;/h2>; &lt;p>; In this work, we developed plugins for a diffusion-based text-to-image generation model with MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmark&lt;/a>;, MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmark&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_map&quot;>;depth maps&lt;/a>;, and &lt;a href=&quot;https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html&quot;>;Canny edge&lt;/a>;. For each task, we select about 100K images from a &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;web-scale image-text dataset&lt;/a>;, and compute control signals using corresponding MediaPipe solutions. We use refined captions from &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; for training the plugins. &lt;/p>; &lt;br />; &lt;h3>;Face Landmark&lt;/h3>; &lt;p>; The MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmarker&lt;/a>; task computes 478 landmarks (with attention) of a human face. We use the &lt;a href=&quot;https://github.com/google/mediapipe/blob/26a7ca5c64cd885978677931a7218d33cd7d1dec/mediapipe/python/solutions/drawing_utils.py&quot;>;drawing utils&lt;/a>; in MediaPipe to render a face, including face contour, mouth, eyes, eyebrows, and irises, with different colors. The following table shows randomly generated samples by conditioning on face mesh and prompts. As a comparison, both ControlNet and Plugin can control text-to-image generation with given conditions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s1346/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1018&quot; data-original-width=&quot;1346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Face-landmark plugin for text-to-image generation, compared with ControlNet.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Holistic Landmark&lt;/h3>; &lt;p>; MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmarker&lt;/a>; task includes landmarks of body pose, hands, and face mesh. Below, we generate various stylized images by conditioning on the holistic features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s1351/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;970&quot; data-original-width=&quot;1351&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Holistic-landmark plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Depth&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s1344/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depth-plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Canny Edge&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s1345/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;955&quot; data-original-width=&quot;1345&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Canny-edge plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We conduct a quantitative study of the &lt;em>;face landmark&lt;/em>; plugin to demonstrate the model&#39;s performance. The evaluation dataset contains 5K human images. We compare the generation quality as measured by the widely used metrics, &lt;a href=&quot;https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance&quot;>;FrÃ©chet Inception Distance&lt;/a>; (FID) and &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>; scores. The base model is a pre-trained text-to-image diffusion model. We use &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>; v1.5 here. &lt;/p>; &lt;p>; As shown in the following table, both ControlNet and the MediaPipe diffusion plugin produce much better sample quality than the base model, in terms of FID and CLIP scores. Unlike ControlNet, which needs to run at every diffusion step, the MediaPipe plugin only runs once for each image generated. We measured the performance of the three models on a server machine (with Nvidia V100 GPU) and a mobile phone (Galaxy S23). On the server, we run all three models with 50 diffusion steps, and on mobile, we run 20 diffusion steps using the &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/image_generator&quot;>;MediaPipe image generation app&lt;/a>;. Compared with ControlNet, the MediaPipe plugin shows a clear advantage in inference efficiency while preserving the sample quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Model&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;FIDâ†“&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;CLIPâ†‘&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Inference Time (s)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Nvidia V100&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;10.32 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.26 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.5 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.51 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.31 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.4 (+48%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;18.2 (+58.3%) &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.30 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 (+0.2%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.8 (+2.6%) &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quantitative comparison on FID, CLIP, and inference time.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We test the performance of the plugin on a wide range of mobile devices from mid-tier to high-end. We list the results on some representative devices in the following table, covering both Android and iOS. &lt;/p>; &lt;p>; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Device&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;7&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Android&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;iOS&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 4&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 6&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 7&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 12 Pro&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 13 Pro&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;strong>;Time&lt;/strong>; (ms) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;128 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;68 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;48 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;73 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;63 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Inference time (ms) of the plugin on different mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we present MediaPipe, a portable plugin for conditioned text-to-image generation. It injects features extracted from a condition image to a diffusion model, and consequently controls the image generation. Portable plugins can be connected to pre-trained diffusion models running on servers or devices. By running text-to-image generation and plugins fully on-device, we enable more flexible applications of generative AI. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank all team members who contributed to this work: Raman Sarokin and Juhyun Lee for the GPU inference solution; Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, and Matthias Grundmann for leadership. Special thanks to Jiuqiang Tang&lt;/em>;&lt;i>;, Joe Zou and Lu wang,&lt;/i>;&lt;em>;&amp;nbsp;who made this technology and all the demos running on-device.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/9161751123508054082/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/on-device-diffusion-plugins-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/06/on-device-diffusion-plugins-for.html&quot; rel=&quot;alternate&quot; title=&quot;On-device diffusion plugins for conditioned text-to-image generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s72-c/MediaPipeDiffusion-hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;