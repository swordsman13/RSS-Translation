<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-10-02T00:52:00.082-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Publications"></category><category term="Research"></category><category term="Machine Perception"></category><category term="TensorFlow"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Computational Photography"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="MOOC"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="accessibility"></category><category term="optimization"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="ACL"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="Android"></category><category term="TPU"></category><category term="Awards"></category><category term="ICML"></category><category term="ML"></category><category term="Structured Data"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="Responsible AI"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="RAI-HCT Highlights"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1286&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-7111364624265266582&lt;/id>;&lt;发布>;2023-10-02T00:51:00.000-07:00&lt;/发布>;&lt;更新>;2023-10- 02T00：51：28.775-07：00 &lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http： //www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICCV&quot;>;&lt; /category>;&lt;title type=&quot;text&quot;>;Google 在 ICCV 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 项目经理 Shaina Mehta&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVnC8dDc8c44GFT7cAZh8PfC8_Mpt4h1rhl-uNMNGoGlNCAZVsT51z89pMqEcVgwa7UPUuvXlr07PpOJlxomCAyRRTOEssXQxDwm 4SX8J4JC_63fKWKywHLuqPHBLRZLl4yYIC311eAXC4r47i1zeZoPg2OXhjxuBmzVXCFn5MrJtH7QhZtMLKzzFvXyvx/s320/ICCV%20hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google 很荣幸成为 &lt;a href=&quot;&lt;a href=&quot;https://iccv2023.thecvf.com/iccv.2023.sponsors-93.php&quot;>;白金赞助商&lt;/a>; https://iccv2023.thecvf.com/&quot;>;国际计算机视觉会议&lt;/a>;（ICCV 2023），这是一项重要的年度会议，本周将于法国巴黎举行。作为计算机视觉研究领域的领导者，Google 在今年的会议上表现强劲，共收到 60 篇论文，并积极参与 27 个&lt;a href=&quot;https://iccv2023.thecvf.com/list.of.accepted.workshops-90 .php&quot;>;研讨会&lt;/a>;和&lt;a href=&quot;https://iccv2023.thecvf.com/list.of.accepted.tutorials-91.php&quot;>;教程&lt;/a>;。 Google 还很荣幸成为&lt;a href=&quot;https://www.latinxinai.org/iccv-2023&quot;>;简历中的拉丁语&lt;/a>;研讨会的白金赞助商。我们期待分享我们广泛的计算机视觉研究成果，并扩大我们与更广泛的研究界的合作伙伴关系。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 参加 ICCV 2023？我们希望您能够参观 Google 展位，与积极追求计算机视觉最新创新的研究人员交谈，并查看一些预定的展位活动（例如下面列出的演示和问答环节）。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter 帐户，了解有关 ICCV 2023 上 Google 展位活动的更多信息。&lt;/p>; &lt;p>; 请看下面了解有关在 ICCV 2023 上展示的 Google 研究的更多信息（Google 隶属关系以&lt;strong>;粗体&lt;/strong>;显示）。 &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;董事会及组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 总主席：&lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;br />; 财务主席：&lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>; &lt; br />; 劳资关系主席：&lt;strong>;&lt;em>;Rahul Sukthankar&lt;/em>;&lt;/strong>; &lt;br />; 宣传和社交媒体联合主席：&lt;strong>;&lt;em>;龚博清&lt;/em>;&lt;/strong >; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展台活动&lt;/h2>; &lt;div style= &quot;margin-left: 20px;&quot;>; &lt;p>; 标题：ImagenThings：即时个性化图像到图像生成&lt;br />; 演讲者：&lt;strong>;&lt;em>;Xuhui Jia&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Suraj Kothawade&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 4 日星期三中午 12:30 &lt;/p>; &lt;p>; 标题：Open Images V7 (&lt;a href=&quot;https://storage .googleapis.com/openimages/web_v7/2022_pointillism_arxiv.pdf&quot;>;论文&lt;/a>;、&lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;>;数据集&lt;/a>;、 &lt;a href=&quot;https://blog.research.google/2022/10/open-images-v7-now-featuring-point.html&quot;>;博客文章&lt;/a>;）&lt;br />;演示者：&lt;strong>; &lt;em>;Rodrigo Benenson&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jasper Uijlings&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jordi Pont-Tuset&lt;/em>;&lt;/strong>; &lt; br />; 欧洲中部夏令时间 10 月 4 日星期三下午 3:30 &lt;/p>; &lt;p>; 标题：AI4Design (&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2023/papers/Han_SVDiff_Compact_Parameter_Space_for_Diffusion_Fine-Tuning_ICCV_2023_paper.pdf &quot;>;论文&lt;/a>;) &lt;br />; 演讲者：&lt;strong>;&lt;em>;Andrew Marmon&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Peggy Chi&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;CK Ng&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 5 日星期四上午 10:30 &lt;/p>; &lt;p>; 标题：前言：数据驱动的少样本超体积先验高分辨率人脸合成&lt;br />; 演讲者：&lt;strong>;&lt;em>;Marcel Bühler&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kripasindhu Sarkar&lt;/em>;&lt;/strong>; &lt;br />; 星期四， 10 月 5 日中午 12:30（欧洲中部夏令时间） &lt;/p>; &lt;p>; 标题：哎哟！合成和合成图像的视觉和语言基准 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 5 日星期四下午 1:00 &lt;/ p>; &lt;p>; 标题：Fact Check Explorer 中的图像搜索（&lt;a href=&quot;https://blog.google/products/news/new-features-coming-to-fact-check-explorer/&quot;>;博客文章&lt; /a>;) &lt;br />; 演讲者：&lt;strong>;&lt;em>;Yair Alon&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Avneesh Sud&lt;/em>;&lt;/strong>; &lt;br />; 十月星期四欧洲中部夏令时间 5 日下午 3:30 &lt;/p>; &lt;p>; 标题：UnLoc：视频本地化任务的统一框架（&lt;a href=&quot;https://arxiv.org/pdf/2308.11062.pdf&quot;>;论文&lt;/a >;) &lt;br />; 演讲者：&lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Xuehan Xiong&lt;/em>;&lt;/strong>; &lt;br />; 10 月 6 日，星期五上午 10:30 CEST &lt;/p>; &lt;p>; 标题：逆问题的即时调整潜在扩散模型 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Hyungjin Chung&lt;/em>;&lt;/strong>; &lt;br />; 星期五，10 月 6 日中午 12:30 CEST &lt;/p>; &lt;p>; 标题：现实世界应用的神经隐式表示 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;、&lt;strong >;&lt;em>;Fabian Manhardt&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Marie-Julie Rakotosaona&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 6 日星期五下午 3:30 &lt;/p >; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;已接受论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px ;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.14383.pdf&quot;>;采用轻量级 ToF 传感器的单目密集 SLAM 的多模态神经辐射场&lt;/a>; &lt;br / >; &lt;em>;刘新阳&lt;/em>;、&lt;em>;李一进&lt;/em>;、&lt;em>;滕彦斌&lt;/em>;、&lt;em>;包胡军&lt;/em>;、&lt;em>;张国锋&lt;/em>; 、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;崔兆鹏&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// arxiv.org/pdf/2309.05569.pdf&quot;>;ITI-GEN：包容性文本到图像生成&lt;/a>; &lt;br />; &lt;em>;Cheng Chang&lt;/em>;、&lt;em>;Xuanbai Chen&lt;/em>;、 &lt;em>;柴思琪&lt;/em>;、&lt;em>;陈亨利吴&lt;/em>;、&lt;strong>;&lt;em>;Dmitry Lagun&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thabo Beeler&lt;/em>; &lt;/strong>;, &lt;em>;Fernando De la Torre&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.16201.pdf&quot;>;ASIC：在中对齐稀疏-狂野图像集&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Kamal Gupta&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Varun Jampani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Carlos Esteves&lt;/em>;&lt;/strong>;、&lt;em>;Abhinav Shrivastava&lt;/em>;、&lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Noah Snavely&lt;/em>; em>;&lt;/strong>;，&lt;strong>;&lt;em>;阿布舍克·卡尔&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.06833.pdf&quot;>; VQ3D：在 ImageNet 上学习 3D 感知生成模型&lt;/a>; &lt;br />; &lt;em>;Kyle Sargent&lt;/em>;、&lt;em>;Jing Yu Koh&lt;/em>;、&lt;strong>;&lt;em>;Han Zhu&lt;/em>; em>;&lt;/strong>;,&lt;strong>; &lt;em>;张慧文&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Charles Herrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pratul Srinivasan&lt; /em>;&lt;/strong>;、&lt;em>;吴家军&lt;/em>;、&lt;strong>;&lt;em>;孙德清&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// arxiv.org/pdf/2302.11154.pdf&quot;>;开放域视觉实体识别：识别数百万个维基百科实体&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;胡何翔&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;鸾易&lt;/em>;&lt;/strong>;、&lt;em>;杨晨&lt;/em>;*、&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;曼达尔·乔希&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kristina Toutanova&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;Ming-Wei Chang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15343.pdf&quot;>;语言图像预训练的 Sigmoid 损失&lt;/ a>; &lt;br />; &lt;strong>;&lt;em>;翟晓华&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;巴兹尔·穆斯塔法&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;亚历山大·科列斯尼科夫&lt;/em>; em>;&lt;/strong>;，&lt;strong>;&lt;em>;卢卡斯·拜尔&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05422.pdf&quot;>;一次性追踪所有地点的所有内容&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;王倩倩&lt;/em>;&lt;/strong>;、&lt;em>;张彦宇&lt;/em>;、&lt;em>;蔡若金&lt;/ em>;、&lt;strong>;&lt;em>;李正琪&lt;/em>;&lt;/strong>;、&lt;em>;Bharath Hariharan&lt;/em>;、&lt;strong>;&lt;em>;Aleksander Holynski&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Noah Snavely&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06706.pdf&quot;>;Zip-NeRF：基于网格的抗锯齿神经辐射场&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;乔纳森·T.巴伦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;本·米尔登霍尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Dor Verbin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pratul P. Srinivasan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Peter Hedman&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.07090.pdf&quot;>;Delta 去噪分数&lt;/a>; &lt;br />; &lt;em>;Amir Hertz&lt;/em>;*, &lt;strong >;&lt;em>;Kfir Aberman&lt;/em>;&lt;/strong>;、&lt;em>;丹尼尔·科恩-奥尔&lt;/em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13508 .pdf&quot;>;DreamBooth3D：主题驱动的文本到 3D 生成&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Amit Raj&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Srinivas Kaza&lt;/a>; &lt;em>;Srinivas Kaza&lt;/a>; &lt;em>;Amit Raj&lt;/em>;&lt;/strong>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;本·普尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迈克尔·尼迈耶&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;纳塔尼尔·鲁伊斯&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;本·米尔登霍尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Shiran Zada&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kfir Aberman &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迈克尔·鲁宾斯坦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔纳森·巴伦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;元祯李&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;瓦伦·贾帕尼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09224。 pdf&quot;>;百科全书式 VQA：有关细粒度类别详细属性的视觉问题&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Thomas Mensink&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jasper Uijlings&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;路易斯·卡斯特雷洪&lt;/em>;&lt;/strong>;、&lt;em>;阿鲁什·戈埃尔&lt;/em>;*、&lt;em>;费利佩·卡达尔&lt;/em>;*、&lt;strong>; &lt;em>;Howard Zhou&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;沙飞&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;Vittorio Ferrar&lt;/em>;i&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.05916.pdf&quot;>;GECCO：几何条件点扩散模型&lt;/a>; &lt;br />; &lt;em>;Michał J. Tyszkiewicz&lt;/em>;、&lt;em>;Pascal Fua&lt;/em>;、&lt;strong>;&lt;em>;Eduard Trulls&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11489.pdf&quot;>;从不成对多视图之间的语义对齐中学习以实现自我中心视频识别&lt;/a>; &lt;br />; &lt;em>;Qitong Wang&lt;/em >;、&lt;strong>;&lt;em>;赵龙&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;袁良哲&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘婷&lt;/em>;&lt;/ strong>;, &lt;em>;Xi Peng&lt;/​​em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17806.pdf&quot;>;用于逆向渲染的神经微面场&lt;/a>; &lt; br />; &lt;em>;Alexander Mai&lt;/em>;、&lt;strong>;&lt;em>;Dor Verbin&lt;/em>;&lt;/strong>;、&lt;em>;Falko Kuester&lt;/em>;、&lt;em>;Sara Fridovich-Keil&lt;/em>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09346.pdf&quot;>;Rosetta 神经元：挖掘模型动物园中的公共单元&lt;/a>; &lt;br />; &lt;em>; Amil Dravid&lt;/em>;、&lt;em>;Yossi Gandelsman&lt;/em>;、&lt;em>;Alexei A. Efros&lt;/em>;、&lt;strong>;&lt;em>;Assaf Shocher&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12066.pdf&quot;>;教 CLIP 数到十&lt;/a>; &lt;br />; &lt;em>;Roni Paiss&lt;/em>;*, &lt;strong>; &lt;em>;Ariel Ephrat&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Omer Tov&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shiran Zada&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;因巴尔·莫塞里&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;米哈尔·伊拉尼&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;塔利·德克尔&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.12048.pdf&quot;>;Vox-E：3D 对象的文本引导体素编辑&lt;/a>; &lt;br />; &lt;em>;Etai Sella &lt;/em>;、&lt;em>;Gal Fiebelman&lt;/em>;、&lt;strong>;&lt;em>;Peter Hedman&lt;/em>;&lt;/strong>;、&lt;em>;Hadar Averbuch-Elor&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.12074.pdf&quot;>;CC3D：组合 3D 场景的布局条件生成&lt;/a>; &lt;br />; &lt;em>;Sherwin Bahmani&lt;/em>;，&lt;em >;Jeong Joon Park&lt;/em>;、&lt;em>;Despoina Paschalidou&lt;/em>;、&lt;em>;颜星光&lt;/em>;、&lt;em>;Gordon Wetzstein&lt;/em>;、&lt;em>;Leonidas Guibas&lt;/em>;、&lt; strong>;&lt;em>;Andrea Tagliasacchi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11607.pdf&quot;>;深入研究单目运动感知匹配3D 对象跟踪&lt;/a>; &lt;br />; &lt;em>;黄冠志&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi- Hsuan Tsai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01172.pdf&quot;>;用于 3D 感知图像生成的生成多平面神经辐射&lt;/a >; &lt;br />; &lt;em>;Amandeep Kumar&lt;/em>;、&lt;em>;Ankan Kumar Bhunia&lt;/em>;、&lt;em>;Sanath Narayan&lt;/em>;、&lt;em>;Hisham Cholakkal&lt;/em>;、&lt;em>;Rao穆罕默德·安维尔&lt;/em>;、&lt;em>;萨尔曼·汗&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;em>;Fahad Shahbaz Khan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.07313.pdf&quot;>;M2T：两次屏蔽变形金刚以加快解码速度&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Fabian Mentzer&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;Eirikur Agustsson&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Michael Tschannen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= &quot;https://arxiv.org/pdf/2304.02859.pdf&quot;>;MULLER：用于视觉的多层拉普拉斯调整器&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;屠正中&lt;/em>;&lt;/strong>;，&lt;strong >; &lt;em>;佩曼·米兰法&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;侯赛因·塔莱比&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2303.11305.pdf&quot;>;SVDiff：用于扩散微调的紧凑参数空间&lt;/a>; &lt;br />; &lt;em>;韩利公&lt;/em>;*，&lt;strong>;&lt;em>;李银晓&lt;/em>;&lt; /strong>;, &lt;strong>;&lt;em>;张涵&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;佩曼·米兰法&lt;/em>;&lt;/strong>;, &lt;em>;迪米特里斯·梅塔克萨斯&lt;/em>;, &lt;strong>; >;&lt;em>;Feng Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.08996.pdf&quot;>;利用迭代扩散模型实现真实人脸恢复Beyond&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;赵阳&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;侯廷波&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;余- 苏川&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;贾旭辉&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;李彦东&lt;/em>;&lt;/strong>;，&lt;strong>; &lt; em>;Matthias Grundmann&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08998.pdf&quot;>;利用视觉和语言模型进行统一视觉关系检测&lt;/ a>; &lt;br />; &lt;strong>;&lt;em>;赵龙&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;袁良哲&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;龚伯庆&lt;/ em>;&lt;/strong>;、&lt;strong>;&lt;em>;崔银&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Florian Schroff&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;明轩杨&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Hartwig Adam&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘婷&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03757.pdf&quot;>;3D 运动放大：可视化时变辐射场中的细微运动&lt;/a>; &lt;br />; &lt;em>;Brandon Y. Feng&lt;/ em>;、&lt;em>;哈迪·阿尔扎耶&lt;/em>;、&lt;strong>;&lt;em>;迈克尔·鲁宾斯坦&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;威廉·T.弗里曼&lt;/em>;&lt;/strong>;、&lt; em>;Jia-Bin Huang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.06954.pdf&quot;>;全局特征是图像检索和重新排名所需的全部&lt;/ a>; &lt;br />; &lt;em>;邵世豪&lt;/em>;、&lt;strong>;&lt;em>;陈凯峰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Arjun Karpur&lt;/em>;&lt;/strong>;、 &lt;em>;崔庆华&lt;/em>;、&lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;曹丙一&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.15827.pdf&quot;>;在基于提示的持续学习中引入语言指导&lt;/a>; &lt;br />; &lt;em>;Muhammad Gul Zain Ali Khan&lt;/em>;， &lt;em>;Muhammad Ferjad Naeem&lt;/em>;、&lt;em>;Luc Van Gool&lt;/em>;、&lt;em>;Didier Stricker&lt;/em>;、&lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;、&lt; em>;Muhammad Zeshan Afzal&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01789.pdf&quot;>;用于图像去模糊的多尺度结构引导扩散&lt;/a>; &lt;br / >; &lt;em>;任孟伟&lt;/em>;*、&lt;strong>;&lt;em>;毛里西奥·德尔布拉西奥&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;侯赛因·塔莱比&lt;/em>;&lt;/strong>;、&lt;em>;Guido Gerig&lt;/em>;，&lt;strong>;&lt;em>;Peyman Milanfar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.09711.pdf&quot;>;稳健挑战性条件下的单目深度估计&lt;/a>; &lt;br />; &lt;em>;Stefano Gasperini&lt;/em>;、&lt;em>;Nils Morbitzer&lt;/em>;、&lt;em>;HyunJun Jung&lt;/em>;、&lt;em>;Nassir Navab&lt; /em>;,&lt;strong>; &lt;em>;Federico Tombari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.11751.pdf&quot;>;基于分数扩散模型作为逆向成像的原则性先验&lt;/a>; &lt;br />; &lt;em>;Berthy T. Feng&lt;/​​em>;*，&lt;strong>; &lt;em>;Jamie Smith&lt;/em>;&lt;/strong>;，&lt;strong>;&lt; em>;Michael Rubinstein&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张惠文&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Katherine L. Bouman&lt;/em>;&lt;/strong>;、&lt; strong>; &lt;em>;William T. Freeman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2309.01858.pdf&quot;>;迈向通用图像嵌入：A大规模数据集和通用图像表示的挑战&lt;/a>; &lt;br />; &lt;em>;Nikolaos-Antonios Ypsilantis&lt;/em>;，&lt;strong>;&lt;em>;Kaifeng Chen&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Bingyi Cao&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mario Lipovsky&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Pelin Dogan-Schonberger&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Grzegorz Makosa&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Boris Bluntschli&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mojtaba Seyedhosseini&lt;/em>;&lt;/strong>; ，&lt;em>;Ondrej Chum&lt;/em>;，&lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.06383 .pdf&quot;>;U-RED：部分点云的无监督 3D 形状检索和变形&lt;/a>; &lt;br />; &lt;em>;Yan Di&lt;/em>;、&lt;em>;Chenyangguang Zhu&lt;/em>;、&lt;em>;Ruida张&lt;/em>;、&lt;strong>;&lt;em>;Fabian Manhardt&lt;/em>;&lt;/strong>;、&lt;em>;苏永志&lt;/em>;、&lt;em>;Jason Rambach&lt;/em>;、&lt;em>;Didier Stricker&lt;/em>; em>;、&lt;em>;纪向阳&lt;/em>;、&lt;strong>;&lt;em>;费德里科·汤巴里&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf /2303.17606.pdf&quot;>;AvatarCraft：通过参数化形状和姿势控制将文本转换为神经人类头像&lt;/a>; &lt;br />; &lt;em>;Ruixiang Jiang&lt;/em>;、&lt;em>;Can Wang&lt;/em>;、&lt;em>; >;张静波&lt;/em>;、&lt;strong>;&lt;em>;柴梦雷&lt;/em>;&lt;/strong>;、&lt;em>;何明明&lt;/em>;、&lt;em>;陈东东&lt;/em>;、&lt;em>;廖靖&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14700.pdf&quot;>;通过改进的 AR 模型学习多功能 3D 形状生成&lt;/a>; &lt;br />; &lt;em >;Simian Luo&lt;/em>;、&lt;em>;钱学林&lt;/em>;、&lt;em>;付彦伟&lt;/em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt;em>;泰英&lt;/em>;、&lt;em>;张振宇&lt;/em>;、&lt;em>;王成杰&lt;/em>;、&lt;em>;薛向阳&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// arxiv.org/pdf/2308.11198.pdf&quot;>;稀疏视图中手-物体交互的新颖视图合成和姿势估计&lt;/a>; &lt;br />; &lt;em>;Wentian Qu&lt;/em>;，&lt;em>;Zhaopeng Cui&lt; /em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt;em>;孟晨宇&lt;/em>;、&lt;em>;马翠霞&lt;/em>;、&lt;em>;邓晓明&lt;/em>; , &lt;em>;王红安&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05534.pdf&quot;>;PreSTU：场景文本理解预训练&lt;/a >; &lt;br />; &lt;em>;Jihyung Kil&lt;/em>;*、&lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陈曦&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;胡鹤翔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;塞巴斯蒂安·古德曼&lt;/em>;&lt;/strong>;、&lt;em>;赵伟伦&lt;/em>;、&lt;strong>;&lt; em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://v&quot;>;可变形物体的密集对应的隐式形状表示的自监督学习&lt;/a>; &lt; br />; &lt;em>;张宝文&lt;/em>;、&lt;em>;李嘉禾&lt;/em>;、&lt;em>;邓小明&lt;/em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、 &lt;em>;马翠霞&lt;/em>;、&lt;em>;王红安&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.06948.pdf&quot;>;自我调节提示：不忘初心的基础模型适应&lt;/a>; &lt;br />; &lt;em>;Muhammad Uzair Khattak&lt;/em>;、&lt;em>;Syed Talal Wasi&lt;/em>;、&lt;em>;Muzammal Nasee&lt;/em>;、&lt;em>;Salman Kha&lt;/em>;、&lt;strong>;&lt;em>;严明轩&lt;/em>;&lt;/strong>;、&lt;em>;Fahad Shahbaz Khan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2308.11015.pdf&quot;>;Spectral Graphmer：基于频谱图的变换器，使用多视图彩色图像进行以自我为中心的双手重建&lt;/a>; &lt;br />; &lt;em>;Tze Ho Elden Tse&lt;/em >;*、&lt;strong>;&lt;em>;Franziska Mueller&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;沉正阳&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;唐丹航&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;窦明松&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;张银达&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;Sasa Petrovic&lt;/em>;&lt;/strong>;、&lt;em>;Hyung Jin Chang&lt;/em>;、&lt;strong>;&lt;em>;乔纳森·泰勒&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Bardia Doosti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.12411.pdf&quot;>;在 3D 室内合成不同的人体动作场景&lt;/a>; &lt;br />; &lt;em>;赵凯峰&lt;/em>;、&lt;em>;张艳&lt;/em>;、&lt;em>;王少飞&lt;/em>;、&lt;strong>;&lt;em>;Thabo Beeler&lt;/em>; >;&lt;/strong>;, &lt;em>;Siyu Tang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06419.pdf&quot;>;通过 3D 模型估计未知物体进行跟踪视频中&lt;/a>; &lt;br />; &lt;em>;Denys Rozumnyi&lt;/em>;、&lt;em>;Jiri Matas&lt;/em>;、&lt;em>;Marc Pollefeys&lt;/em>;、&lt;strong>;&lt;em>;Vittorio Ferrari&lt;/ em>;&lt;/strong>;、&lt;em>;Martin R. Oswald&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11062.pdf&quot;>;UnLoc：统一框架视频本地化任务&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;沉彦&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;熊学涵&lt;/em>;&lt;/strong>;、&lt;strong>;&lt; em>;Arsha Nagrani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;王忠浩&lt;/em>;&lt;/strong>;*、&lt;strong >;&lt;em>;葛维娜&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;大卫·罗斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;科迪莉亚·施密德&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06708.pdf&quot;>;动词在行动：提高视频语言模型中的动词理解&lt;/a>; &lt;br />; &lt;em>;Liliane Momeni &lt;/em>;、&lt;strong>;&lt;em>;玛蒂尔德·卡隆&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿尔莎·纳格拉尼&lt;/em>;&lt;/strong>;、&lt;em>;安德鲁·齐瑟曼&lt;/em>;、&lt;强>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2309.06703v1.pdf&quot;>;VLSlice：交互式视觉和-语言切片发现&lt;/a>; &lt;br />; &lt;em>;Eric Slyman&lt;/em>;、&lt;strong>;&lt;em>;Minsuk Kahng&lt;/em>;&lt;/strong>;、&lt;em>;Stefan Lee&lt;/em>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09012.pdf&quot;>;是的，我们可以：基于局部特征的视觉定位的约束近似最近邻&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;Dror Aiger&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德烈·阿劳霍&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;西蒙·林宁&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05922.pdf&quot;>;视听掩码自动编码器&lt;/a>; &lt;br />; &lt;em>;Mariana-Iuliana Georgescu&lt;/em>;*, &lt;强>;&lt;em>;爱德华多·丰塞卡&lt;/em>;&lt;/strong>;、&lt;em>;拉杜·图多尔·伊内斯库&lt;/em>;、&lt;strong>;&lt;em>;马里奥·卢西奇&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;科迪莉亚·施密德&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;阿努拉格·阿纳布&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.11386 .pdf&quot;>;CLR：用于持续学习的通道式轻量级重编程&lt;/a>; &lt;br />; &lt;em>;葛云浩&lt;/em>;、&lt;em>;李岳成&lt;/em>;、&lt;em>;倪硕&lt;/em>; >;、&lt;strong>;&lt;em>;赵家平&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Laurent Itti&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05410.pdf&quot;>;LU-NeRF：通过同步本地未姿势 NeRF 进行场景和姿势估计&lt;/a>; &lt;br />; &lt;em>;泽州&lt;/em>; &lt;em>;程&lt;/em>;*,&lt;strong>; &lt;em>;卡洛斯&lt;/em>; &lt;em>;埃斯特维斯&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Varun&lt;/em>; &lt;em>;Jampani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Abhishek&lt;/em>; &lt;em>;Kar&lt;/em>;&lt;/strong>;、&lt;em>;Subhransu&lt;/em>; &lt;em>;Maji&lt;/em>;、&lt;strong>;&lt;em>;Ameesh&lt;/em>; &lt;em>;Makadia&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/2304.10075.pdf&quot;>;实时抗锯齿神经渲染的多尺度表示&lt;/a>; &lt;br />; &lt;em>;Dongting&lt;/em>; &lt;em>;Hu&lt;/em>;，&lt;em>;Zhenkai&lt; /em>; &lt;em>;张&lt;/em>;、&lt;strong>;&lt;em>;廷波&lt;/em>; &lt;em>;侯&lt;/em>;&lt;/strong>;、&lt;em>;铜梁&lt;/em>; &lt;em>;刘&lt;/em>; em>;, &lt;em>;欢&lt;/em>; &lt;em>;福&lt;/em>;, &lt;em>;明明&lt;/em>; &lt;em>;宫&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //arxiv.org/pdf/2304.10532.pdf&quot;>;Nerfbusters：从随意捕获的 NeRF 中移除幽灵文物&lt;/a>; &lt;br />; &lt;em>;Frederik Warburg&lt;/em>;、&lt;em>;Ethan Weber&lt;/em>;、 &lt;em>;Matthew Tancik&lt;/em>;、&lt;strong>;&lt;em>;Aleksander Holynski&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Angjoo Kanazawa&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05407.pdf&quot;>;在没有先验知识的情况下分割已知物体和看不见的未知物体&lt;/a>; &lt;br />; &lt;em>;Stefano Gasperini&lt;/em>;，&lt;em>;阿尔瓦罗·马科斯-拉米罗&lt;/em>;、&lt;em>;迈克尔·施密特&lt;/em>;、&lt;em>;纳西尔·纳瓦布&lt;/em>;、&lt;em>;本杰明·布萨姆&lt;/em>;、&lt;strong>;&lt;em>;费德里科·汤巴里&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14340.pdf&quot;>;SparseFusion：融合多模态稀疏表示用于多传感器 3D 对象检测&lt;/a >; &lt;br />; &lt;em>;谢一辰&lt;/em>;、&lt;em>;徐晨风&lt;/em>;、&lt;strong>;&lt;em>;Marie-Julie Rakotosaona&lt;/em>;&lt;/strong>;、&lt;em>;Patrick Rim&lt; /em>;、&lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;、&lt;em>;Kurt Keutzer&lt;/em>;、&lt;em>;富冢正义&lt;/em>;、&lt;em>;魏战&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15446.pdf&quot;>;SwiftFormer：基于 Transformer 的实时移动视觉应用的高效附加注意力&lt;/a>; &lt;br />; &lt;em>;Abdelrahman Shaker&lt;/em>;、&lt;em>;Muhammad Maa&lt;/em>;、&lt;em>;Hanoona Rashee&lt;/em>;、&lt;em>;Salman Kha&lt;/em>;、&lt;strong>;&lt;em>;严明轩&lt;/em>;&lt;/strong>;，&lt;em>;Fahad Shahbaz Kha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12948.pdf&quot;>;敏捷建模：来自在几分钟内从概念到分类器&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Otilia Stretcu&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Edward Vendrow&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Kenji Hata&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Krishnamurthy Viswanathan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;维托里奥·法拉利、&lt;/em>;&lt;/strong>; &lt;strong >;&lt;em>;Sasan Tavakkol&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;周文磊&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Aditya Avinash&lt;/em>;&lt;/strong>;、&lt;强>;&lt;em>;罗恩明&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尼尔戈登奥尔德林&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;穆罕默德侯赛因巴泰尼&lt;/em>;&lt;/strong>; ,&lt;strong>;&lt;em>;加布里埃尔·伯杰&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;安德鲁·邦纳&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;卢春达&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;哈维尔·A·雷伊&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;朱利亚·德萨尔沃&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;兰杰·克里希纳&lt;/em>; >;&lt;/strong>;、&lt;strong>;&lt;em>;Ariel Fuxman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09011.pdf&quot;>;CAD -Estate：RGB 视频中的大型 CAD 模型注释&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Kevis-Kokitsi Maninis&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Stefan Popov&lt;/em >;&lt;/strong>;、&lt;em>;马蒂亚斯·尼斯纳&lt;/em>;、&lt;strong>;&lt;em>;维托里奥·法拉利&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/2306.01209.pdf&quot;>;恶劣天气下的人数统计&lt;/a>; &lt;br />; &lt;em>;黄志凯&lt;/em>;、&lt;em>;陈伟霆&lt;/em>;、&lt;em>;袁-Chun Jiang&lt;/em>;、&lt;em>;Sy-Yen Kuo&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://arxiv.org/pdf/2304.06025.pdf&quot;>;DreamPose：具有稳定扩散的时尚视频合成&lt;/a>; &lt;br />; &lt;em>;Johanna Karras&lt;/em>;，&lt;strong>;&lt;em>;Aleksander Holynski&lt; /em>;&lt;/strong>;、&lt;em>;王庭春&lt;/em>;、&lt;em>;Ira Kemelmacher-Shlizerman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2301.09637.pdf&quot;>;InfiniCity：无限规模城市综合&lt;/a>; &lt;br />; &lt;em>;Chieh Hubert Lin&lt;/em>;、&lt;em>;Hsin-Ying Lee&lt;/em>;、&lt;em>;Willi Menapace&lt;/em>;、&lt;em>;Menglei Chai&lt;/em>;、&lt;em>;Aliaksandr Siarohin&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;em>;Sergey Tulyakov &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://faculty.ucmerced.edu/mhyang/papers/iccv2023_sampling.pdf&quot;>;采样：场景自适应分层多平面图像表示，用于从单幅图像&lt;/a>; &lt;br />; &lt;em>;周小宇&lt;/em>;、&lt;em>;林志伟&lt;/em>;、&lt;em>;单晓军&lt;/em>;、&lt;em>;王永涛&lt;/em>;、 &lt;strong>;&lt;em>;孙德清&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;http: //sifeiliu.net/NDLM_ICCV23tutorial/&quot;>;利用噪声和无标签数据学习无法分类的大型模型&lt;/a>; &lt;br />; &lt;em>;Sifei Liu&lt;/em>;，&lt;em>;洪旭尹&lt;/em>;，&lt; em>;Shalini De Mello&lt;/em>;、&lt;em>;Pavlo Molchanov&lt;/em>;、&lt;em>;Jose M. Alvarez&lt;/em>;、&lt;em>;Jan Kautz&lt;/em>;、&lt;em>;小龙&lt;/em>; &lt;em>;王&lt;/em>;、&lt;em>;Anima Anandkumar&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;em>;Trevor Darrell&lt;/em>; &lt;br / >; 演讲者：&lt;strong>;&lt;em>;Varun Jampani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.latinxinai.org/iccv-2023&quot;>;人工智能中的拉丁语&lt;/a>; &lt;br />; 白金赞助商&lt;br />; 小组成员：&lt;strong>;&lt;em>;Daniel Castro Chin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Andre Araujo&lt;/em>;&lt;/strong>; &lt;br />; 特邀演讲嘉宾：&lt;strong>; &lt;em>;Irfan Essa&lt;/em>;&lt;/strong>; &lt;br />; 志愿者：&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; >;&lt;em>;袁良哲&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pedro Velez&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vincent Etter&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://sg2rl.github.io/index.html&quot;>;场景图和图表示学习&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Federico Tombari&lt; /em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://web.northeastern.edu/smilelab/amfg2023/&quot;>;人脸和手势分析与建模国际研讨会&lt;/a>; &lt; br />; 演讲者：&lt;strong>;&lt;em>;Todd Zickler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://3dv-in-ecommerce.github.io/&quot;>;3D电子商务中的愿景和建模挑战&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://bigmac- vision.github.io/&quot;>;BigMAC：计算机视觉的大模型适应&lt;/a>; &lt;br />;组织者：&lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://iccv23-arow.github.io/&quot;>;现实世界中的对抗鲁棒性（AROW）&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;白宇桐&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://geonet-challenge.github.io/ICCV2023/&quot;>;GeoNet：第一届跨地域鲁棒计算机视觉研讨会&lt;/a>; &lt;br />; 演讲者: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Tarun Kalluri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= “https://gkioxari.github.io/Tutorials/iccv2023/&quot;>;Quo Vadis，计算机视觉？ &lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;比尔·弗里曼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/ view/vshh/home&quot;>;去 NeRF 还是不去 NeRF：人类大脑的视图合成挑战&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>; &lt;br / >; 组织者：&lt;strong>;&lt;em>;Stefanos Zafeiriou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/nivt-iccv2023/&quot; >;视觉变形金刚的新想法&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;br />;组织者：&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://lsfsl.net/limit23/&quot;>;非常有限的图像表示学习：自我、合成和公式监督的潜力&lt;/a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Manel Baradad Jurjo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/rcv2023 /&quot;>;面向计算机视觉的资源高效深度学习&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>; &lt;br />;组织者：&lt;strong>;&lt;em>;余家辉&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rishabh Tiwari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://cvaad-workshop.github.io/&quot;>;Computer Vision Aided Architectural Design &lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Noah Snavely&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://av4d.org/&quot;>;AV4D: Visual Learning of Sounds in Spaces&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;David Harwath&lt;/em >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://wvlar.github.io/iccv23/&quot;>;Vision-and-Language Algorithmic Reasoning&lt;/a>; &lt;br />; Speaker: &lt;strong >;&lt;em>;François Chollet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://v&quot;>;Neural Fields for Autonomous Driving and Robotics&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Jon Barron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://campworkshop.org/&quot;>;International Challenge on Compositional and Multimodal Perception &lt;/a>; &lt; br />; Organizer: &lt;strong>;&lt;em>;Ranjay Krishna&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://opensun3d.github.io/&quot;>;Open-Vocabulary 3D Scene Understanding (OpenSUN3D)&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Thomas Funkhouser&lt;/em>;&lt;/strong>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Francis Engelmann&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Johanna Wald&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Leonidas Guibas&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mono3d-iccv-workshop&quot;>;Frontiers of Monocular 3D Perception: Geometric Foundation Models&lt;/a>; &lt; br />; Speaker: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/perdream/home &quot;>;PerDream: PERception, Decision Making and REAsoning Through Multimodal Foundational Modeling&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://cmp.felk.cvut.cz/sixd/workshop_2023/&quot;>;Recovering 6D Object Pose&lt;/a>; &lt;br />; Speaker: &lt;strong>;Fabian Manhardt&lt;/strong>;,&lt;strong>; Martin Sundermeyer&lt; /strong>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Martin Sundermeyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view /wicviccv2023/&quot;>;Women in Computer Vision (WiCV)&lt;/a>; &lt;br />; Panelist: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://languagefor3dscenes.github.io/ICCV2023/&quot;>;Language for 3D Scenes&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://ai3dcc.github.io/&quot;>;AI for 3D Content Creation&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Kai-Hung Chang&lt;/em>;&lt;/strong >; &lt;br />; Organizer: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cv4metaverse /&quot;>;Computer Vision for Metaverse&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Jon Barron&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Funkhouser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.datacomp.ai/workshop.html#first&quot;>;Towards the Next Generation of Computer Vision Datasets&lt;/a>; &lt;br />; Speaker: &lt;strong>; &lt;em>;Tom Duerig&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style -span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http:// blog.research.google/feeds/7111364624265266582/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google /2023/10/google-at-iccv-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www. blogger.com/feeds/8474926331452026626/posts/default/7111364624265266582&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/7111364624265266582&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-at-iccv-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICCV 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161 &lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https:/ /img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEhVnC8dDc8c44GFT7cAZh8PfC8_Mpt4h1rhl-uNMNGoGlNCAZVsT51z89pMqEcVgwa7UPUuvXlr07PpOJlxomCAyRRTOEssXQxDwm4SX8J4JC_63fKWKywHLuqPHBLRZLl4yYIC311eAXC4r47i1zeZoPg2OXhjxuBmzVXCFn5MrJtH7QhZtMLKzzFvXyvx/s72-c/ICCV%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media :thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3765645246777916540&lt;/id>;&lt;published>;2023-09 -28T13:01:00.000-07:00&lt;/published>;&lt;updated>;2023-09-28T13:01:53.849-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;Computational Photography&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme= &quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;DynIBaR: Space-time view synthesis from videos of dynamic scenes&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zhengqi Li and Noah Snavely, Research Scientists, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhGX53UfU9eWiSTRWdkrUWNln3KGyagkwfUi_38zEihOJ2qLkmQ-3yNsuJJ7SkJ-BTLlVrxJlyoEYl7-tAer6v4MnIAw49TWLGWa8cFgl_c_2WUJqw1O3J8nVhU-VtVwO5Z-bq7rH6pZj7APe5yDZCUSZyeDO39shlGFkVsSQDd1ZWYUT0eDmeJ9JLoWlA3/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; A mobile phone&#39;s camera is a powerful tool for capturing everyday moments. However, capturing a dynamic scene using a single camera is fundamentally limited. For instance, if we wanted to adjust the camera motion or timing of a recorded video (eg, to freeze time while sweeping the camera around to highlight a dramatic moment), we would typically need an expensive Hollywood setup with a synchronized camera rig. Would it be possible to achieve similar effects solely from a video captured using a mobile phone&#39;s camera, without a Hollywood budget? &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.11082&quot;>;DynIBaR: Neural Dynamic Image-Based Rendering&lt;/a>;”, a &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/Awards&quot;>;best paper honorable mention&lt;/a>; at &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>;, we describe a new method that generates photorealistic free-viewpoint renderings from a single video of a complex, dynamic scene. Neural Dynamic Image-Based Rendering (DynIBaR) can be used to generate a range of video effects, such as “&lt;a href=&quot;https://en.wikipedia.org/wiki/Bullet_time&quot;>;bullet time&lt;/a>;” effects (where time is paused and the camera is moved at a normal speed around a scene), video stabilization, depth of field, and slow motion, from a single video taken with a phone&#39;s camera. We demonstrate that DynIBaR significantly advances video rendering of complex moving scenes, opening the door to new kinds of video editing applications. We have also released the &lt;a href=&quot;https://github.com/google/dynibar&quot;>;code&lt;/a>; on the DynIBaR &lt;a href=&quot;https://dynibar.github.io/&quot;>;project page&lt;/a>;, so you can try it out yourself. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an in-the-wild video of a complex, dynamic scene, DynIBaR can freeze time while allowing the camera to continue to move freely through the scene.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; The last few years have seen tremendous progress in computer vision techniques that use &lt;a href=&quot;https://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot;>;neural radiance fields&lt;/a>; (NeRFs) to reconstruct and render static (non-moving) 3D scenes. However, most of the videos people capture with their mobile devices depict moving&lt;em>; &lt;/em>;objects, such as people, pets, and cars. These moving scenes lead to a much more challenging 4D (3D + time) scene reconstruction&lt;em>; &lt;/em>;problem that cannot be solved using standard view synthesis methods. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Standard view synthesis methods output blurry, inaccurate renderings when applied to videos of dynamic scenes.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Other recent methods tackle view synthesis for dynamic scenes using space-time neural radiance fields (ie, &lt;em>;&lt;a href=&quot;https://www.cs.cornell.edu/~zl548/NSFF/&quot;>;Dynamic NeRFs&lt;/a>;&lt;/em>;), but such approaches still exhibit inherent limitations that prevent their application to casually captured, in-the-wild videos. In particular, they struggle to render high-quality novel views from videos featuring long time duration, uncontrolled camera paths and complex object motion. &lt;/p>; &lt;p>; The key pitfall is that they store a complicated, moving scene in a single data structure. In particular, they encode scenes in the weights of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) neural network. MLPs can approximate any function — in this case, a function that maps a 4D space-time point (&lt;em>;x&lt;/em>;, &lt;em>;y&lt;/em>;, &lt;em>;z&lt;/em>;, &lt;em>;t&lt;/em>;) to an RGB color and density that we can use in rendering images of a scene. However, the capacity of this MLP (defined by the number of parameters in its neural network) must increase according to the video length and scene complexity, and thus, training such models on in-the-wild videos can be computationally intractable. As a result, we get blurry, inaccurate renderings like those produced by &lt;a href=&quot;https://free-view-video.github.io/&quot;>;DVS&lt;/a>; and &lt;a href=&quot;https://www.cs.cornell.edu/~zl548/NSFF/&quot;>;NSFF&lt;/a>; (shown below). DynIBaR avoids creating such large scene models by adopting a different rendering paradigm. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-0.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DynIBaR (&lt;strong>;bottom row&lt;/strong>;) significantly improves rendering quality compared to prior dynamic view synthesis methods (&lt;strong>;top row&lt;/strong>;) for videos of complex dynamic scenes. Prior methods produce blurry renderings because they need to store the entire moving scene in an MLP data structure.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Image-based rendering (IBR)&lt;/h2>; &lt;p>; A key insight behind DynIBaR is that we don&#39;t actually need to store all of the scene contents in a video in a giant MLP. Instead, we directly use pixel data from nearby input video frames to render new views. DynIBaR builds on an &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Image-based_modeling_and_rendering&quot;>;image-based rendering&lt;/a>;&lt;/em>; (IBR) method called &lt;a href=&quot;https://ibrnet.github.io/&quot;>;IBRNet&lt;/a>; that was designed for view synthesis for static scenes. IBR methods recognize that a new target view of a scene should be very similar to nearby source images, and therefore synthesize the target by dynamically selecting and warping pixels from the nearby source frames, rather than reconstructing the whole scene in advance. IBRNet, in particular, learns to blend nearby images together to recreate new views of a scene within a volumetric rendering framework. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DynIBaR: Extending IBR to complex, dynamic videos&lt;/h2>; &lt;p>; To extend IBR to dynamic scenes, we need to take scene motion into account during rendering. Therefore, as part of reconstructing an input video, we solve for the motion&lt;em>; &lt;/em>;of every 3D point, where we represent scene motion using a motion trajectory field encoded by an MLP. Unlike prior dynamic NeRF methods that store the entire scene appearance and geometry in an MLP, we only store motion, a signal that is more smooth and sparse, and use the input video frames to determine everything else needed to render new views. &lt;/p>; &lt;p>; We optimize DynIBaR for a given video by taking each input video frame, rendering rays to form a 2D image using volume rendering (as in &lt;a href=&quot;https://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot;>;NeRF&lt;/a>;), and comparing that rendered image to the input frame. That is, our optimized representation should be able to perfectly reconstruct the input video. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVxjezPKVF8ofpfJP7WtIN3-wOMHJPXISABUaoBbnS9wMtko3lTDTfMfntgTTkcNfG8DxFP63DobaehzDY2QA3OcNf-tJW6JJk1ghWO2oma_XWYD2FcKeylPDYPUhqKax8FBCy9qxZ3RzizzQluECi6mV2lkuOkd3r16wRaG5mRcpFt_JCSDyUt6Q8BTUb/s660/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;660&quot; height=&quot;465&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVxjezPKVF8ofpfJP7WtIN3-wOMHJPXISABUaoBbnS9wMtko3lTDTfMfntgTTkcNfG8DxFP63DobaehzDY2QA3OcNf-tJW6JJk1ghWO2oma_XWYD2FcKeylPDYPUhqKax8FBCy9qxZ3RzizzQluECi6mV2lkuOkd3r16wRaG5mRcpFt_JCSDyUt6Q8BTUb/w640-h465/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We illustrate how DynIBaR renders images of dynamic scenes. For simplicity, we show a 2D world, as seen from above. (&lt;strong>;a&lt;/strong>;) A set of input source views (&lt;strong>;triangular&lt;/strong>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Viewing_frustum&quot;>;camera frusta&lt;/a>;) observe a cube moving through the scene (&lt;strong>;animated square&lt;/strong>;). Each camera is labeled with its timestamp (&lt;em>;t&lt;/em>;-2, &lt;em>;t&lt;/em>;-1, etc). (&lt;strong>;b&lt;/strong>;) To render a view from camera at time &lt;em>;t&lt;/em>;, DynIBaR shoots a virtual ray through each pixel (&lt;strong>;blue line&lt;/strong>;), and computes colors and opacities for sample points along that ray. To compute those properties, DyniBaR projects those samples into other views via multi-view geometry, but first, we must compensate for the estimated motion of each point (&lt;strong>;dashed red line&lt;/strong>;). (&lt;strong>;c&lt;/strong>;) Using this estimated motion, DynIBaR moves each point in 3D to the relevant time before projecting it into the corresponding source camera, to sample colors for use in rendering. DynIBaR optimizes the motion of each scene point as part of learning how to synthesize new views of the scene.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; However, reconstructing and deriving new views for a complex, moving scene is a highly ill-posed problem, since there are many solutions that can explain the input video — for instance, it might create disconnected 3D representations for each time step. Therefore, optimizing DynIBaR to reconstruct the input video alone is insufficient. To obtain high-quality results, we also introduce several other techniques, including a method called &lt;em>;cross-time rendering&lt;/em>;. Cross-time rendering refers to the use of the state of our 4D representation at one time instant to render images from a different time instant, which encourages the 4D representation to be coherent over time. To further improve rendering fidelity, we automatically factorize the scene into two components, a static one and a dynamic one, modeled by time-invariant and time-varying scene representations respectively. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Creating video effects&lt;/h2>; &lt;p>; DynIBaR enables various video effects. We show several examples below. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Video stabilization&lt;/h3>; &lt;p>; We use a shaky, handheld input video to compare DynIBaR&#39;s video stabilization performance to existing 2D video stabilization and dynamic NeRF methods, including &lt;a href=&quot;https://alex04072000.github.io/FuSta/&quot;>;FuSta&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3363550&quot;>;DIFRINT&lt;/a>;, &lt;a href=&quot;https://hypernerf.github.io/&quot;>;HyperNeRF&lt;/a>;, and &lt;a href=&quot;https://www.cs.cornell.edu/~zl548/NSFF/&quot;>;NSFF&lt;/a>;. We demonstrate that DynIBaR produces smoother outputs with higher rendering fidelity and fewer artifacts (eg, flickering or blurry results). In particular, FuSta yields residual camera shake, DIFRINT produces flicker around object boundaries, and HyperNeRF and NSFF produce blurry results. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-4.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Simultaneous view synthesis and slow motion&lt;/h3>; &lt;p>; DynIBaR can perform view synthesis in both space and time simultaneously, producing smooth 3D cinematic effects. Below, we demonstrate that DynIBaR can take video inputs and produce smooth 5X slow-motion videos rendered using novel camera paths. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Video bokeh&lt;/h3>; &lt;p>; DynIBaR can also generate high-quality &lt;a href=&quot;https://en.wikipedia.org/wiki/Bokeh&quot;>;video bokeh&lt;/a>; by synthesizing videos with dynamically changing &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_of_field&quot;>;depth of field&lt;/a>;. Given an all-in-focus input video, DynIBar can generate high-quality output videos with varying out-of-focus regions that call attention to moving (eg, the running person and dog) and static content (eg, trees and buildings) in the scene. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; DynIBaR is a leap forward in our ability to render complex moving scenes from new camera paths. While it currently involves per-video optimization, we envision faster versions that can be deployed on in-the-wild videos to enable new kinds of effects for consumer video editing using mobile devices. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;DynIBaR is the result of a collaboration between researchers at Google Research and Cornell University. The key contributors to the work presented in this post include Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3765645246777916540/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/dynibar-space-time-view-synthesis-from.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3765645246777916540&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3765645246777916540&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/dynibar-space-time-view-synthesis-from.html&quot; rel=&quot;alternate&quot; title=&quot;DynIBaR: Space-time view synthesis from videos of dynamic scenes&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGX53UfU9eWiSTRWdkrUWNln3KGyagkwfUi_38zEihOJ2qLkmQ-3yNsuJJ7SkJ-BTLlVrxJlyoEYl7-tAer6v4MnIAw49TWLGWa8cFgl_c_2WUJqw1O3J8nVhU-VtVwO5Z-bq7rH6pZj7APe5yDZCUSZyeDO39shlGFkVsSQDd1ZWYUT0eDmeJ9JLoWlA3/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2311847718846675808&lt;/id>;&lt;published>;2023-09-28T11:16:00.002-07:00&lt;/published>;&lt;updated>;2023-09-28T11:22:44.163-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Re-weighted gradient descent via distributionally robust optimization&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Ramnath Kumar, Pre-Doctoral Researcher, and Arun Sai Suggala, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONjsYNtXi5AX4ZZ6qPfdY2Gwt2W5xZy1PF1m9ZaxucZtRZ5ZzmsNzHNNWkCKcpX1_n15DkZCR59ivF_uDCUUmbdijaAuiK3tlC9HahtKc1s1r6pQWwbwMhnuNK3Guu5G5QigWU6p4yaJXCBLvIosDHXwtyhxMVfplzK4wTXiwCwOGo1B2aFidcgtaZfN_/s320/hero%20RGD.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>;Deep neural networks (DNNs) have become essential for solving a wide range of tasks, from standard supervised learning (&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.pdf&quot;>;image classification using ViT&lt;/a>;) to &lt;a href=&quot;https://arxiv.org/pdf/2201.11775.pdf&quot;>;meta-learning&lt;/a>;. The most commonly-used paradigm for learning DNNs is&lt;em>; &lt;a href=&quot;http://web.mit.edu/6.962/www/www_spring_2001/emin/slt.pdf&quot;>;empirical risk minimization&lt;/a>; (ERM)&lt;/em>;, which aims to identify a network that minimizes the average &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss&quot;>;loss&lt;/a>; on training data points. Several algorithms, including &lt;a href=&quot;https://www2.isye.gatech.edu/~nemirovs/Nemirovskii_Yudin_1983.pdf&quot;>;stochastic gradient descent&lt;/a>; (SGD), &lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;>;Adam&lt;/a>;, and &lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;>;Adagrad&lt;/a>;, have been proposed for solving ERM. However, a drawback of ERM is that it weights all the samples equally, often ignoring the rare and more difficult samples, and focusing on the easier and abundant samples. This leads to suboptimal performance on unseen data, especially when the training data is scarce.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To overcome this challenge, recent works have developed data re-weighting techniques for improving ERM performance. However, these approaches focus on specific learning tasks (such as &lt;a href=&quot;https://arxiv.org/pdf/1708.02002.pdf&quot;>;classification&lt;/a>;) and/or require learning an &lt;a href=&quot;https://arxiv.org/pdf/1902.07379.pdf&quot;>;additional meta model&lt;/a>; that predicts the weights of each data point. The presence of an additional model significantly increases the complexity of training and makes them unwieldy in practice.&lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2306.09222&quot;>;Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization&lt;/a>;” we introduce a variant of the classical &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent#&quot;>;SGD algorithm&lt;/a>; that re-weights data points during each optimization step based on their difficulty. Stochastic Re-weighted Gradient Descent (RGD) is a lightweight algorithm that comes with a simple closed-form expression, and can be applied to solve any learning task using just two lines of code. At any stage of the learning process, RGD simply reweights a data point as the exponential of its loss. We empirically demonstrate that the RGD reweighting algorithm improves the performance of numerous learning algorithms across various tasks, ranging from supervised learning to meta learning. Notably, we show improvements over state-of-the-art methods on &lt;a href=&quot;https://arxiv.org/abs/2007.01434&quot;>;DomainBed&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;Tabular classification&lt;/a>;. Moreover, the RGD algorithm also boosts performance for &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; using the &lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>;GLUE&lt;/a>; benchmarks and &lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.pdf&quot;>;ViT on ImageNet-1K&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Distributionally robust optimization&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/1610.03425.pdf&quot;>;Distributionally robust optimization&lt;/a>; (DRO) is an approach that assumes a “worst-case” data distribution shift may occur, which can harm a model&#39;s performance. If a model has focussed on identifying few spurious features for prediction, these “worst-case” data distribution shifts could lead to the misclassification of samples and, thus, a performance drop. DRO optimizes the loss for samples in that “worst-case” distribution, making the model robust to perturbations (eg, removing a small fraction of points from a dataset, minor up/down weighting of data points, etc.) in the data distribution. In the context of classification, this forces the model to place less emphasis on noisy features and more emphasis on useful and predictive features. Consequently, models optimized using DRO tend to have &lt;a href=&quot;https://arxiv.org/pdf/1610.03425.pdf&quot;>;better generalization guarantees and stronger performance&lt;/a>; on unseen samples. &lt;/p>; &lt;p>; Inspired by these results, we develop the RGD algorithm as a technique for solving the DRO objective. Specifically, we focus on &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;Kullback–Leibler divergence&lt;/a>;-based DRO, where one adds perturbations to create distributions that are close to the original data distribution in the KL divergence metric, enabling a model to perform well over all possible perturbations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhAuznJBUTcLrt6urFhWEkWCLviEMHf3KsvQAGd1JZSusTYW1kbTYg7mz-mmHUQtmYCHuclpML5JqBhCHB0OmYChuI62CJBB5nJdefCX0idWac3WKaJgb9UfvljskRgcYPrLnakAdnaNFRsWI8nQ3cPTy4pSdABX8k4tg3UrXaFbcT3I9pLuKxbl9UpQ80/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1236&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhAuznJBUTcLrt6urFhWEkWCLviEMHf3KsvQAGd1JZSusTYW1kbTYg7mz-mmHUQtmYCHuclpML5JqBhCHB0OmYChuI62CJBB5nJdefCX0idWac3WKaJgb9UfvljskRgcYPrLnakAdnaNFRsWI8nQ3cPTy4pSdABX8k4tg3UrXaFbcT3I9pLuKxbl9UpQ80/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Figure illustrating DRO. In contrast to &lt;a href=&quot;http://web.mit.edu/6.962/www/www_spring_2001/emin/slt.pdf&quot;>;ERM&lt;/a>;, which learns a model that minimizes expected loss over original data distribution, DRO learns a model that performs well on several perturbed versions of the original data distribution.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Stochastic re-weighted gradient descent&lt;/h2>; &lt;p>; Consider a random subset of samples (called a mini-batch), where each data point has an associated &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss&quot;>;loss&lt;/a>; &lt;i>;L&lt;sub>;i&lt;/sub>;&lt;/i>;. Traditional algorithms like SGD give equal importance to all the samples in the mini-batch, and update the parameters of the model by descending along the averaged gradients of the loss of those samples. With RGD, we reweight each sample in the mini-batch and give more importance to points that the model identifies as more difficult. To be precise, we use the loss as a proxy to calculate the difficulty of a point, and reweight it by the exponential of its loss. Finally, we update the model parameters by descending along the weighted average of the gradients of the samples. &lt;/p>; &lt;p>; Due to stability considerations, in our experiments we clip and scale the loss before computing its exponential. Specifically, we clip the loss at some threshold &lt;em>;T&lt;/em>;, and multiply it with a scalar that is inversely proportional to the threshold. An important aspect of RGD is its simplicity as it doesn&#39;t rely on a meta model to compute the weights of data points. Furthermore, it can be implemented with two lines of code, and combined with any popular optimizers (such as &lt;a href=&quot;https://www2.isye.gatech.edu/~nemirovs/Nemirovskii_Yudin_1983.pdf&quot;>;SGD&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;>;Adam&lt;/a>;, and &lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;>;Adagrad&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg71xwsL38tNMVHtrY7NqXEzpFmHH-o6E9lVSsVVTks8JnrU1uHBg9CA8ZaIBpX-bi0F1w-LBZAcnco0JXxfdOup-6Qn4fk_DC0MAVT1ytwoi8TCeYfNqBPAGwBe6raUde9O7FibdDNjz4_7pMA3poNl-o_6bZM0r3VbWfzSacWhY019yvTobeDrmSSh1sb/s1973/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1541&quot; data-original-width=&quot;1973&quot; height=&quot;500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg71xwsL38tNMVHtrY7NqXEzpFmHH-o6E9lVSsVVTks8JnrU1uHBg9CA8ZaIBpX-bi0F1w-LBZAcnco0JXxfdOup-6Qn4fk_DC0MAVT1ytwoi8TCeYfNqBPAGwBe6raUde9O7FibdDNjz4_7pMA3poNl-o_6bZM0r3VbWfzSacWhY019yvTobeDrmSSh1sb/w640-h500/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Figure illustrating the intuitive idea behind RGD in a binary classification setting. Feature 1 and Feature 2 are the features available to the model for predicting the label of a data point. RGD upweights the data points with high losses that have been misclassified by the model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We present empirical results comparing RGD with state-of-the-art techniques on standard supervised learning and domain adaptation (refer to the &lt;a href=&quot;https://arxiv.org/abs/2306.09222&quot;>;paper&lt;/a>; for results on meta learning). In all our experiments, we tune the clipping level and the learning rate of the optimizer using a held-out validation set. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Supervised learning&lt;/h3>; &lt;p>; We evaluate RGD on several supervised learning tasks, including language, vision, and tabular classification. For the task of language classification, we apply RGD to the &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; model trained on the &lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>;General Language Understanding Evaluation&lt;/a>; (&lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>;GLUE&lt;/a>;) benchmark and show that RGD outperforms the BERT baseline by +1.94% with a standard deviation of 0.42%. To evaluate RGD&#39;s performance on vision classification, we apply RGD to the ViT-S model trained on the ImageNet-1K dataset, and show that RGD outperforms the ViT-S baseline by +1.01% with a standard deviation of 0.23%. Moreover, we perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;hypothesis tests&lt;/a>; to confirm that these results are statistically significant with a p-value that is less than 0.05.&lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioW7ubyangDExeb1OJ8vWgXuU9VnDkeSwOqY1pmRHq22Pyym63a8_WHjd6W3yiXLUDPgPEXQkS31aOnkwgpCOgzhBRzNLyV0OkHlZskXpqOOmCV-xZyZ6NQpuC85jen-YxDfhF-usexsm-lZDlGnoenyoEwS9slSplJOHmoPeySmwvi4Qr7c_KBko4PjaK/s1999/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1334&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioW7ubyangDExeb1OJ8vWgXuU9VnDkeSwOqY1pmRHq22Pyym63a8_WHjd6W3yiXLUDPgPEXQkS31aOnkwgpCOgzhBRzNLyV0OkHlZskXpqOOmCV-xZyZ6NQpuC85jen-YxDfhF-usexsm-lZDlGnoenyoEwS9slSplJOHmoPeySmwvi4Qr7c_KBko4PjaK/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RGD&#39;s performance on language and vision classification using GLUE and Imagenet-1K benchmarks. Note that MNLI, QQP, QNLI, SST-2, MRPC, RTE and COLA are diverse datasets which comprise the GLUE benchmark.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For tabular classification, we use &lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;MET&lt;/a>; as our baseline, and consider various binary and multi-class datasets from &lt;a href=&quot;https://archive.ics.uci.edu/&quot;>;UC Irvine&#39;s machine learning repository&lt;/a>;. We show that applying RGD to the &lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;MET&lt;/a>; framework improves its performance by 1.51% and 1.27% on binary and multi-class tabular classification, respectively, achieving state-of-the-art performance in this domain. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYobZ13Q2uashBWpbo58LiuXs2OO0UGKorkJ5VMm0rUUlJS92R6_kBrjRbAPAFRVj88xBEptVFuDSHtlbzjNQKNpfKI0lOOp_KJI_rXdxka8qddjeP5MuQATCRDCbnBAkfEYxqUK44LQ8cTZEUHDThR4aVqZ5_rdfx6931HvMVuY4OPh4G_Z96l6sjWuw9/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYobZ13Q2uashBWpbo58LiuXs2OO0UGKorkJ5VMm0rUUlJS92R6_kBrjRbAPAFRVj88xBEptVFuDSHtlbzjNQKNpfKI0lOOp_KJI_rXdxka8qddjeP5MuQATCRDCbnBAkfEYxqUK44LQ8cTZEUHDThR4aVqZ5_rdfx6931HvMVuY4OPh4G_Z96l6sjWuw9/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0Uh_DCijOhzBjSzNyJoi92YCp2jD3n2k-l9aLuUWA8geS7QfkgzxZFj8WOseCbOfJu6hZxNFpAFDNFmBLSL7o0_bFHV6O0iZ6GPzderDKYpnW8QTYPRLLFGO0R6XzCITFBUVJFsMXfLUWK4qEffp70g4tEPloFdirGRpgClA0blEOZMPvf7TmxaKS9Vyw/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0Uh_DCijOhzBjSzNyJoi92YCp2jD3n2k-l9aLuUWA8geS7QfkgzxZFj8WOseCbOfJu6hZxNFpAFDNFmBLSL7o0_bFHV6O0iZ6GPzderDKYpnW8QTYPRLLFGO0R6XzCITFBUVJFsMXfLUWK4qEffp70g4tEPloFdirGRpgClA0blEOZMPvf7TmxaKS9Vyw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of RGD for classification of various tabular datasets.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Domain generalization&lt;/h3>; &lt;p>; To evaluate RGD&#39;s generalization capabilities, we use the standard &lt;a href=&quot;https://arxiv.org/abs/2007.01434&quot;>;DomainBed&lt;/a>; benchmark, which is commonly used to study a model&#39;s out-of-domain performance. We apply RGD to &lt;a href=&quot;https://arxiv.org/abs/2210.01360&quot;>;FRR&lt;/a>;, a recent approach that improved out-of-domain benchmarks, and show that RGD with FRR performs an average of 0.7% better than the FRR baseline. Furthermore, we confirm with &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;hypothesis tests&lt;/a>; that most benchmark results (except for Office Home) are statistically significant with a p-value less than 0.05. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhv_KdUdYK72dakGtm0fkZJGsJqe3jRCL1EXAdpC98gHDcHpXirdr2qiMMQfLxFyUrEi5NLrL9Rrx3sqx8CImE00srUKH51MYvsvXzSWImhDBvjokiPE7Oz76CZHr4Ty3RRHX6IW4BcpQRQhbilp1-nw6D0BqPxNuvpx1vaB5Auv2Gtb-y_INiFNTCJmaV9/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhv_KdUdYK72dakGtm0fkZJGsJqe3jRCL1EXAdpC98gHDcHpXirdr2qiMMQfLxFyUrEi5NLrL9Rrx3sqx8CImE00srUKH51MYvsvXzSWImhDBvjokiPE7Oz76CZHr4Ty3RRHX6IW4BcpQRQhbilp1-nw6D0BqPxNuvpx1vaB5Auv2Gtb-y_INiFNTCJmaV9/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of RGD on DomainBed benchmark for distributional shifts. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Class imbalance and fairness&lt;/h3>; &lt;p>; To demonstrate that models learned using RGD perform well despite class imbalance, where certain classes in the dataset are underrepresented, we compare RGD&#39;s performance with ERM on &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/html/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.html&quot;>;long-tailed CIFAR-10&lt;/a>;. We report that RGD improves the accuracy of baseline ERM by an average of 2.55% with a standard deviation of 0.23%. Furthermore, we perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;hypothesis tests&lt;/a>; and confirm that these results are statistically significant with a p-value of less than 0.05. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBesU8uGmy9DLXZMZyC94wiLHs_swM94ZcUxhyphenhyphen6IbKHwzpDjwH-L8q2E-8iRQ9TmDC2QCWSNqZKm8pFKynE_4kcU1u3ERg_s35uKaRvc2PqvNMOPTBBcMu4ciXtkwudlY7wFXR0DHvu0lUccgcRNi5ZQTHZdaQyKXl7AXwl9sUBSjoLpdUHxztgUzVWgHw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1356&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBesU8uGmy9DLXZMZyC94wiLHs_swM94ZcUxhyphenhyphen6IbKHwzpDjwH-L8q2E-8iRQ9TmDC2QCWSNqZKm8pFKynE_4kcU1u3ERg_s35uKaRvc2PqvNMOPTBBcMu4ciXtkwudlY7wFXR0DHvu0lUccgcRNi5ZQTHZdaQyKXl7AXwl9sUBSjoLpdUHxztgUzVWgHw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of RGD on the long-tailed Cifar-10 benchmark for class imbalance domain. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Limitations &lt;/h2>; &lt;p>; The RGD algorithm was developed using popular research datasets, which were already curated to remove corruptions (eg, noise and incorrect labels). Therefore, RGD may not provide performance improvements in scenarios where training data has a high volume of corruptions. A potential approach to handle such scenarios is to apply an &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/375663.375668&quot;>;outlier removal technique&lt;/a>; to the RGD algorithm. This outlier removal technique should be capable of filtering out outliers from the mini-batch and sending the remaining points to our algorithm. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; RGD has been shown to be effective on a variety of tasks, including out-of-domain generalization, tabular representation learning, and class imbalance. It is simple to implement and can be seamlessly integrated into existing algorithms with just two lines of code change. Overall, RGD is a promising technique for boosting the performance of DNNs, and could help push the boundaries in various domains. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The paper described in this blog post was written by Ramnath Kumar, Arun Sai Suggala, Dheeraj Nagaraj and Kushal Majmundar. We extend our sincere gratitude to the anonymous reviewers, Prateek Jain, Pradeep Shenoy, Anshul Nasery, Lovish Madaan, and the numerous dedicated members of the machine learning and optimization team at Google Research India for their invaluable feedback and contributions to this work.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2311847718846675808/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/re-weighted-gradient-descent-via.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2311847718846675808&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2311847718846675808&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/re-weighted-gradient-descent-via.html&quot; rel=&quot;alternate&quot; title=&quot;Re-weighted gradient descent via distributionally robust optimization&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONjsYNtXi5AX4ZZ6qPfdY2Gwt2W5xZy1PF1m9ZaxucZtRZ5ZzmsNzHNNWkCKcpX1_n15DkZCR59ivF_uDCUUmbdijaAuiK3tlC9HahtKc1s1r6pQWwbwMhnuNK3Guu5G5QigWU6p4yaJXCBLvIosDHXwtyhxMVfplzK4wTXiwCwOGo1B2aFidcgtaZfN_/s72-c/hero%20RGD.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-109886617977245321&lt;/id>;&lt;published>;2023-09-26T07:10:00.000-07:00&lt;/published>;&lt;updated>;2023-09-26T07:10:39.655-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Neural Networks&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google Research embarks on effort to map a mouse brain&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Michał Januszewski, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFf6_wflZxOT_rmYJu0VCRvigHGMvE2QXwYJeh6F7GHmxnEtg7_bEraidqQ8uii_-N5EtBr2LfOGV_ccWS2g1Qcjssq2HmmGCqbaL_ij-UroaO6JtJVhyNbxK5PN57OfVlmkfXGSg3DMFawUlj1btghs5Nb9tmvQVSZH3pDoLK0SoKlhMh028164YArWcc/s320/connectome%20600x580.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; The human brain is perhaps the most computationally complex machine in existence, consisting of &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_brain#:~:text=brainstem.%5B37%5D-,Microanatomy,-%5Bedit%5D&quot;>;networks of billions of cells&lt;/a>;. Researchers currently don&#39;t understand the full picture of how glitches in its network machinery contribute to mental illnesses and other diseases, such as dementia. However, the emerging &lt;a href=&quot;https://en.wikipedia.org/wiki/Connectomics&quot;>;connectomics&lt;/a>; field, which aims to precisely map the connections between every cell in the brain, could help solve that problem. While maps have only been created for simpler organisms, technological advances for mapping even larger brains can enable us to understand how the human brain works, and how to treat brain diseases. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we&#39;re &lt;a href=&quot;https://www.ninds.nih.gov/news-events/highlights-announcements/nih-brain-initiative-launches-projects-develop-innovative-technologies-map-brain-incredible-detail&quot;>;excited to announce&lt;/a>; that the &lt;a href=&quot;https://research.google/teams/connectomics/&quot;>;Connectomics team&lt;/a>; at Google Research and our collaborators are launching a &lt;a href=&quot;https://reporter.nih.gov/project-details/10665380&quot;>;$33 million project&lt;/a>; to expand the frontiers of connectomics over the next five years. Supported by the &lt;a href=&quot;https://braininitiative.nih.gov/&quot;>;Brain Research Through Advancing Innovative Neurotechnologies&lt;/a>; (BRAIN) Initiative at the &lt;a href=&quot;https://www.nih.gov/&quot;>;National Institutes of Health&lt;/a>; (NIH) and led by researchers at &lt;a href=&quot;https://lichtmanlab.fas.harvard.edu/&quot;>;Harvard University&lt;/a>;, we&#39;ll be working alongside a multidisciplinary team of experts from the &lt;a href=&quot;https://alleninstitute.org/division/brain-science/&quot;>;Allen Institute&lt;/a>;, &lt;a href=&quot;https://fietelab.mit.edu/&quot;>;MIT&lt;/a>;, &lt;a href=&quot;https://www2.mrc-lmb.cam.ac.uk/group-leaders/h-to-m/gregory-jefferis/&quot;>;Cambridge University&lt;/a>;, &lt;a href=&quot;https://pni.princeton.edu/&quot;>;Princeton University&lt;/a>; and &lt;a href=&quot;https://www.jhuapl.edu/&quot;>;Johns Hopkins University&lt;/a>;, with advisers from &lt;a href=&quot;https://www.janelia.org/lab/hess-lab&quot;>;HHMI&#39;s Janelia Research Campus&lt;/a>;. Our project goal is to tackle an immense challenge in neuroscience: mapping a tiny fraction (2-3%) of the mouse brain. We will specifically target the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5690575/&quot;>;hippocampal&lt;/a>; region, which is responsible for encoding memories, attention and spatial navigation. This project is one of 11 funded by the NIH&#39;s $150 million &lt;a href=&quot;https://braininitiative.nih.gov/funding-opportunies/brain-initiative-connectivity-across-scales-brain-connects-comprehensive-0&quot;>;BRAIN Initiative Connectivity Across Scales&lt;/a>; (BRAIN CONNECTS) program. Google Research is contributing computational and analytical resources to this effort, and will not receive any funding from the NIH. Our project asks a critical question: Can we scale and speed up our technologies enough to map the whole connectome of a mouse brain? &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The modern era of connectomics&lt;/h2>; &lt;p>; This effort to map the connectome of a small part of the mouse brain builds on a decade of innovation in the field, including many advances initiated by the Connectomics team at Google Research. We hope to accomplish something similar to the early days of the &lt;a href=&quot;https://www.genome.gov/human-genome-project&quot;>;Human Genome Project&lt;/a>;, when scientists worked for years to sequence a small portion of the human genome as they refined technologies that would enable them to complete the rest of the genome. &lt;/p>; &lt;p>; In 2021, we and collaborators at Harvard successfully &lt;a href=&quot;https://blog.research.google/2021/06/a-browsable-petascale-reconstruction-of.html&quot;>;mapped one cubic millimeter of the human brain&lt;/a>;, which we released as the &lt;a href=&quot;https://h01-release.storage.googleapis.com/landing.html&quot;>;H01&lt;/a>; dataset, a resource for studying the human brain and scaling connectomics technologies. But mapping the entire human brain connectome would require gathering and analyzing as much as a zettabyte of data (one billion terabytes), which is beyond the current capabilities of existing technologies. &lt;/p>; &lt;p>; Analyzing a mouse connectome is the next best thing. It is small enough to be technically feasible and could potentially deliver insights relevant to our own minds; neuroscientists already use mice to study human brain function and dysfunction. By working together to map 10–15 cubic mm of the mouse brain, we hope to develop new approaches that will allow us to map the entire remainder of the mouse brain, and the human brain thereafter. &lt;/p>; &lt;p>; &lt;/p>;&lt;table>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ3vCYyxz8BDdHUzAEYteV7DWtaSXT7q3ajif8uJXF-Aqfw4-fNrAaVxXU4xnevMe74eCKHtw_OGG3wC7wZUCn4hcN3snTPgqFfD9HrYqft0SoyZ1bN2cGFFEjGoGEU_GV3go90LahMLyCGrPFGVRaniC-ngxMgNiQoGM_03aln2kTqIDGUqXPMZcgH1-W/s960/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ3vCYyxz8BDdHUzAEYteV7DWtaSXT7q3ajif8uJXF-Aqfw4-fNrAaVxXU4xnevMe74eCKHtw_OGG3wC7wZUCn4hcN3snTPgqFfD9HrYqft0SoyZ1bN2cGFFEjGoGEU_GV3go90LahMLyCGrPFGVRaniC-ngxMgNiQoGM_03aln2kTqIDGUqXPMZcgH1-W/w640-h360/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Neuroscientists have been working for decades to map increasingly larger and more complicated connectomes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;One of biology&#39;s largest datasets&lt;/h2>; &lt;p>; In this connectomics project, we will map the connectome of the &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-76447-9_1&quot;>;hippocampal formation&lt;/a>; of the mouse brain, which converts short-term memories into long-term memories and helps the mouse navigate in space. The mouse hippocampal formation is the largest area of any brain we&#39;ve attempted to understand in this way. Through mapping this region of the mouse brain, we will create one of the largest datasets in biology, combining about 25,000 terabytes, or 25 petabytes of brain data. For reference, there are about 250 billion stars in our Milky Way Galaxy. If each of those stars was a single byte, it would take 100,000 Milky Way Galaxies to match the 25 petabytes of data that the project will collect when mapping a small region of the mouse brain. &lt;/p>; &lt;p>; To illustrate the hippocampal project&#39;s scale, we calculated the number of Pixel phones (shown as stacks of Pixels below) needed to store the image data from the completed connectome projects that mapped the roundworm and fruit fly brains, as well as for the mouse hippocampal region and entire mouse brain projects, which are just getting started. &lt;/p>; &lt;p>; Then, we compared the heights of each Pixel stack to familiar objects and landmarks. It would take a stack of 100 Pixels, as tall as a four-year-old girl, to store the image data for the fruit fly brain, the largest completed project thus far. In contrast, the mouse hippocampal connectome effort will require storage equivalent to more than 48,800 Pixels, reaching as high as the Empire State Building. The animation below shows how the mouse hippocampal project will surpass the scale of previous connectome projects. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitPA5h7cSox988b0Xr2cEjrzf0word6JAtcNjtU1S7QF4vsULGgdqSR1KjCh0jibAR-keO1kIkIX0ooyGgZaC9wU61NDijkDpPgnz2eqswz_J_alrpb2FColxo17BnxOVytsuq9gIwrWmzmz52tzsE4qUu8sVnPoobeIMBToilH58YpWtNwxCLi5Bi_ASV/s1600/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitPA5h7cSox988b0Xr2cEjrzf0word6JAtcNjtU1S7QF4vsULGgdqSR1KjCh0jibAR-keO1kIkIX0ooyGgZaC9wU61NDijkDpPgnz2eqswz_J_alrpb2FColxo17BnxOVytsuq9gIwrWmzmz52tzsE4qUu8sVnPoobeIMBToilH58YpWtNwxCLi5Bi_ASV/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We are partnering with several collaborators to build a connectome (a map of the connections between brain cells) for the hippocampal region of a mouse brain. This project will create the largest connectomic dataset ever, surpassing the scale of previous projects that mapped the smaller roundworm and fruit fly brains. We hope this effort will lead to the development of new approaches that will allow us to later map an entire mouse brain. This animation shows how the field of connectomics is scaling up by calculating the number of Pixel phones needed to store the data from various projects. It would take just two Pixels, the height of an olive, to store the roundworm connectome data, while it would take a stack of Pixels the size of Mount Everest to store the data from an entire mouse connectome.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Understanding the connectome of the mouse hippocampal formation could help illuminate the way our own brains work. For instance, we may find common features between this circuitry in the mouse brain and human brains that explain how we know where we are, how our brains associate memories with specific locations, and what goes wrong in people who can&#39;t properly form new spatial memories. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Opening the petabyte pipeline&lt;/h2>; &lt;p>; Over the last decade, our team has worked to develop tools for managing massive connectomic datasets, and extracting scientific value from them. But a mouse brain has 1,000 times more neurons than the brain of the &lt;em>;Drosophila&lt;/em>; fruit fly, an organism for which &lt;a href=&quot;https://blog.research.google/2020/01/releasing-drosophila-hemibrain.html&quot;>;we helped build a connectome for a large part of the brain&lt;/a>;. Starting the mouse brain connectome will challenge us to improve existing technologies to enable us to map more data faster than ever before. &lt;/p>; &lt;p>; We&#39;ll continue to refine our &lt;a href=&quot;https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html&quot;>;flood-filling networks&lt;/a>;, which use deep learning to trace, or “segment”, each neuron&#39;s path through three-dimensional brain volumes made from electron microscope data. We&#39;ll also extend the capabilities of our self-supervised learning technology, &lt;a href=&quot;https://blog.research.google/2022/11/multi-layered-mapping-of-brain-tissue.html&quot;>;SegCLR&lt;/a>;, which allows us to automatically extract key insights from segmented volumes, such as identifying cell type (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Pyramidal_cell#:~:text=Pyramidal%20cells%2C%20or%20pyramidal%20neurons,cortex%20and%20the%20corticospinal%20tract.&quot;>;pyramidal neuron&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Basket_cell&quot;>;basket neuron&lt;/a>;, etc.) and parts of each neuron (eg, axon, dendrite, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiBHvnYysuO3m0CehlpRcx_XgxxTDUhKsr4BPEW0ypOVs_q2uh8RvJQAu323SXu9IbWVmxPGTsCEqtpaOvbdzAnUv2TLlqFnPEbfOKE1XOIMSyS9OEtlcM3KCIsWOdZgDQ1AqkG2kX6YbrFAMrn8g8FTiLlxA6ASLPz_f2AvlkqcfYzLwBEVkNA7R3OWRD/s720/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiBHvnYysuO3m0CehlpRcx_XgxxTDUhKsr4BPEW0ypOVs_q2uh8RvJQAu323SXu9IbWVmxPGTsCEqtpaOvbdzAnUv2TLlqFnPEbfOKE1XOIMSyS9OEtlcM3KCIsWOdZgDQ1AqkG2kX6YbrFAMrn8g8FTiLlxA6ASLPz_f2AvlkqcfYzLwBEVkNA7R3OWRD/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A flood filling network traces a neuron through three-dimensional brain space.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We will also continue to enhance the scalability and performance of our core connectomics infrastructure, such as &lt;a href=&quot;https://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html&quot;>;TensorStore for storage&lt;/a>; and &lt;a href=&quot;https://github.com/google/neuroglancer&quot;>;Neuroglancer for visualization&lt;/a>;, in order to enable all of our computational pipelines and human analysis workflows to operate at these new scales of data. We&#39;re eager to get to work to discover what peering into a mouse&#39;s mind might tell us about our own. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The mouse connectomics project described in this blog post will be supported in part by the NIH BRAIN Initiative under award number 1UM1NS132250. Google Research is contributing computational and analytical resources to the mouse connectome project, and will not receive funding from the NIH. Many people were involved in the development of the technologies that make this project possible. We thank our long-term academic collaborators in the Lichtman Lab (Harvard University), HHMI Janelia, and the Denk Lab (Max Planck Institute for Biological Intelligence), and acknowledge core contributions from the Connectomics Team at Google. We also thank John Guilyard for creating the illustrative animation in this post, and Elise Kleeman, and Erika Check Hayden for their support. Thanks to Lizzie Dorfman, Michael Brenner, Jay Yagnik and Jeff Dean for their support, coordination and leadership.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/109886617977245321/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/google-research-embarks-on-effort-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/109886617977245321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/109886617977245321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/google-research-embarks-on-effort-to.html&quot; rel=&quot;alternate&quot; title=&quot;Google Research embarks on effort to map a mouse brain&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFf6_wflZxOT_rmYJu0VCRvigHGMvE2QXwYJeh6F7GHmxnEtg7_bEraidqQ8uii_-N5EtBr2LfOGV_ccWS2g1Qcjssq2HmmGCqbaL_ij-UroaO6JtJVhyNbxK5PN57OfVlmkfXGSg3DMFawUlj1btghs5Nb9tmvQVSZH3pDoLK0SoKlhMh028164YArWcc/s72-c/connectome%20600x580.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2762083676649407668&lt;/id>;&lt;published>;2023-09-21T14:25:00.000-07:00&lt;/published>;&lt;updated>;2023-09-21T14:25:35.521-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Cheng-Yu Hsieh, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7i39GKWT7_noICLVOMuf2x7v7wM21MXu9deGmnVlsieFv9m_rJOb5ytwiI_A9T3N4N1vZCVprlgbs7dePmQ1qjkcz-mT0nkeyLq-LmkF4oJB-ofCmrv81jqXMHPHe8Tl1dQSbDAPS9gADUUByZHBygkr-jlwbfIx3FM14n5cAbE7ZFVm84K6UDQLfeArm/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Large language models (LLMs) have enabled a new data-efficient learning paradigm wherein they can be used to solve unseen new tasks via &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;zero-shot or few-shot prompting&lt;/a>;. However, LLMs are challenging to deploy for real-world applications due to their sheer size. For instance, serving a single 175 billion LLM requires at least 350GB of GPU memory using &lt;a href=&quot;https://arxiv.org/abs/2201.12023&quot;>;specialized infrastructure&lt;/a>;, not to mention that today&#39;s state-of-the-art LLMs are composed of over &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;500 billion parameters&lt;/a>;. Such computational requirements are inaccessible for many research teams, especially for applications that require low latency performance. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To circumvent these deployment challenges, practitioners often choose to deploy smaller specialized models instead. These smaller models are trained using one of two common paradigms: &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;fine-tuning&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;distillation&lt;/a>;. Fine-tuning updates a pre-trained smaller model (eg, &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>;) using downstream manually-annotated data. Distillation trains the same smaller models with labels generated by a larger LLM. Unfortunately, to achieve comparable performance to LLMs, fine-tuning methods require human-generated labels, which are expensive and tedious to obtain, while distillation requires large amounts of unlabeled data, which can also be hard to collect. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.02301&quot;>;Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes&lt;/a>;”, presented at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL2023&lt;/a>;, we set out to tackle this trade-off between model size and training data collection cost. We introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs&#39; performance. We demonstrate that the distilling step-by-step mechanism enables a 770M parameter T5 model to outperform the few-shot prompted 540B PaLM model using only 80% of examples in a benchmark dataset, which demonstrates a more than 700x model size reduction with much less training data required by standard approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s1570/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width=&quot;1570&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;While LLMs offer strong zero and few-shot performance, they are challenging to serve in practice. On the other hand, traditional ways of training small task-specific models require a large amount of training data. Distilling step-by-step provides a new paradigm that reduces both the deployed model size as well as the number of data required for training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Distilling step-by-step&lt;/h2>; &lt;p>; The key idea of distilling step-by-step is to extract informative &lt;em>;natural language&lt;/em>; &lt;em>;rationales (ie, &lt;/em>;intermediate reasoning steps)&lt;em>; &lt;/em>;from LLMs, which can in turn be used to train small models in a more data-efficient way. Specifically, natural language rationales explain the connections between the input questions and their corresponding outputs. For example, when asked, “&lt;em>;Jesse&#39;s room is 11 feet long and 15 feet wide. If she already has 16 square feet of carpet, how much more carpet does she need to cover the whole floor?&lt;/em>;”, an LLM can be prompted by the few-shot &lt;a href=&quot;https://blog.research.google/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; (CoT) prompting technique to provide intermediate rationales, such as, “&lt;em>;Area = length * width. Jesse&#39;s room has 11 * 15 square feet.&lt;/em>;” That better explains the connection from the input to the final answer, “&lt;em>;(11 * 15 ) - 16&lt;/em>;”. These rationales can contain relevant task knowledge, such as “&lt;em>;Area = length * width”&lt;/em>;, that may originally require many data for small models to learn. We utilize these extracted rationales as additional, richer supervision to train small models, in addition to the standard task labels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;932&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview on distilling step-by-step: First, we utilize CoT prompting to extract rationales from an LLM. We then use the generated rationales to train small task-specific models within a multi-task learning framework, where we prepend task prefixes to the input examples and train the model to output differently based on the given task prefix.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Distilling step-by-step consists of two main stages. In the first stage, we leverage few-shot CoT prompting to extract rationales from LLMs. Specifically, given a task, we prepare few-shot exemplars in the LLM input prompt where each example is composed of a triplet containing: (1) input, (2) rationale, and (3) output. Given the prompt, an LLM is able to mimic the triplet demonstration to generate the rationale for any new input. For instance, in a &lt;a href=&quot;https://arxiv.org/abs/1811.00937&quot;>;commonsense question answering task&lt;/a>;, given the input question “Sammy wanted to go to where the people are. Where might he go? Answer Choices: (a) populated areas, (b) race track, (c) desert, (d) apartment, (e) roadblock”, distilling step-by-step provides the correct answer to the question, “(a) populated areas”, paired with the rationale that provides better connection from the question to the answer, “The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of people.” By providing CoT examples paired with rationales in the prompt, the &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning ability&lt;/a>; allows LLMs to output corresponding rationales for future unseen inputs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1175&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use the few-shot CoT prompting, which contains both an example rationale (&lt;strong>;highlighted in green&lt;/strong>;) and a label (&lt;strong>;highlighted in blue&lt;/strong>;), to elicit rationales from an LLM on new input examples. The example is from a commonsense question answering task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; After the rationales are extracted, in the second stage, we incorporate the rationales in training small models by framing the training process as a multi-task problem. Specifically, we train the small model with a novel &lt;em>;rationale generation task&lt;/em>; in addition to the standard &lt;em>;&lt;a href=&quot;https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html?m=1&quot;>;label prediction task&lt;/a>;&lt;/em>;. The rationale generation task enables the model to learn to generate the intermediate reasoning steps for the prediction, and guides the model to better predict the resultant label. We prepend &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;task prefixes&lt;/a>; (ie, [label] and [rationale] for label prediction and rationale generation, respectively) to the input examples for the model to differentiate the two tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental setup&lt;/h2>; &lt;p>; In the experiments, we consider a &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;540B PaLM&lt;/a>; model as the LLM. For task-specific downstream models, we use &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5 models&lt;/a>;. For CoT prompting, we use the &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;original CoT prompts&lt;/a>; when available and curate our own examples for new datasets. We conduct the experiments on four benchmark datasets across three different NLP tasks: &lt;a href=&quot;https://arxiv.org/abs/1812.01193&quot;>;e-SNLI&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1910.14599&quot;>;ANLI&lt;/a>; for&lt;a href=&quot;https://arxiv.org/abs/1508.05326&quot;>; natural language inference&lt;/a>;; &lt;a href=&quot;https://arxiv.org/abs/1811.00937&quot;>;CQA&lt;/a>; for commonsense question answering; and &lt;a href=&quot;https://arxiv.org/abs/2103.07191&quot;>;SVAMP&lt;/a>; for &lt;a href=&quot;https://aclanthology.org/N16-1136/&quot;>;arithmetic math word problems&lt;/a>;. We include two sets of baseline methods. For comparison to &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;few-shot prompted LLMs&lt;/a>;, we compare to &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;few-shot CoT prompting&lt;/a>; with a &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;540B PaLM&lt;/a>; model. In the &lt;a href=&quot;https://arxiv.org/abs/2305.02301&quot;>;paper&lt;/a>;, we also compare standard task-specific model training to both &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;standard fine-tuning&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;standard distillation&lt;/a>;. In this blogpost, we will focus on the comparisons to standard fine-tuning for illustration purposes. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Less training data&lt;/h3>; &lt;p>; Compared to &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;standard fine-tuning&lt;/a>;, the distilling step-by-step method achieves better performance using much less training data. For instance, on the e-SNLI dataset, we achieve better performance than standard fine-tuning when using only 12.5% of the full dataset (shown in the upper left quadrant below). Similarly, we achieve a dataset size reduction of 75%, 25% and 20% on ANLI, CQA, and SVAMP. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1477&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Distilling step-by-step compared to standard fine-tuning using 220M T5 models on varying sizes of human-labeled datasets. On all datasets, distilling step-by-step is able to outperform standard fine-tuning, trained on the full dataset, by using much less training examples.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Smaller deployed model size&lt;/h3>; &lt;p>; Compared to &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;few-shot CoT prompted LLMs&lt;/a>;, distilling step-by-step achieves better performance using much smaller model sizes. For instance, on the e-SNLI dataset, we achieve better performance than 540B PaLM by using a 220M T5 model. On ANLI, we achieve better performance than 540B PaLM by using a 770M T5 model, which is over 700X smaller. Note that on ANLI, the same 770M T5 model struggles to match PaLM&#39;s performance using standard fine-tuning. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1384&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We perform distilling step-by-step and standard fine-tuning on varying sizes of T5 models and compare their performance to LLM baselines, ie, Few-shot CoT and PINTO Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, eg, over 700× smaller models on ANLI. Standard fine-tuning fails to match LLM&#39;s performance using the same model size.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Distilling step-by-step outperforms few-shot LLMs with smaller models using less data&lt;/h3>; &lt;p>; Finally, we explore the smallest model sizes and the least amount of data for distilling step-by-step to outperform PaLM&#39;s few-shot performance. For instance, on ANLI, we surpass the performance of the 540B PaLM using a 770M T5 model. This smaller model only uses 80% of the full dataset. Meanwhile, we observe that standard fine-tuning cannot catch up with PaLM&#39;s performance even using 100% of the full dataset. This suggests that distilling step-by-step simultaneously reduces the model size as well as the amount of data required to outperform LLMs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1400&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show the minimum size of T5 models and the least amount of human-labeled examples required for distilling step-by-step to outperform LLM&#39;s few-shot CoT by a coarse-grained search. Distilling step-by-step is able to outperform few-shot CoT using not only much smaller models, but it also achieves so with much less training examples compared to standard fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We propose distilling step-by-step, a novel mechanism that extracts rationales from LLMs as informative supervision in training small, task-specific models. We show that distilling step-by-step reduces both the training dataset required to curate task-specific smaller models and the model size required to achieve, and even surpass, a few-shot prompted LLM&#39;s performance. Overall, distilling step-by-step presents a resource-efficient paradigm that tackles the trade-off between model size and training data required. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Availability on Google Cloud Platform&lt;/h2>; &lt;p>; Distilling step-by-step is available for private preview on &lt;a href=&quot;https://cloud.google.com/vertex-ai&quot;>;Vertex AI&lt;/a>;. If you are interested in trying it out, please contact &lt;a href=&quot;mailto:vertex-llm-tuning-preview@google.com&quot;>;vertex-llm-tuning-preview@google.com&lt;/a>; with your Google Cloud Project number and a summary of your use case. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Thanks to Xiang Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2762083676649407668/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2762083676649407668&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2762083676649407668&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html&quot; rel=&quot;alternate&quot; title=&quot;Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7i39GKWT7_noICLVOMuf2x7v7wM21MXu9deGmnVlsieFv9m_rJOb5ytwiI_A9T3N4N1vZCVprlgbs7dePmQ1qjkcz-mT0nkeyLq-LmkF4oJB-ofCmrv81jqXMHPHe8Tl1dQSbDAPS9gADUUByZHBygkr-jlwbfIx3FM14n5cAbE7ZFVm84K6UDQLfeArm/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7378908661087185403&lt;/id>;&lt;published>;2023-09-15T10:39:00.002-07:00&lt;/published>;&lt;updated>;2023-09-15T10:43:29.705-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MediaPipe FaceStylizer: On-device real-time few-shot face stylization&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Haolin Jia, Software Engineer, and Qifei Wang, Senior Software Engineer, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihspgvRDlnUmToZlZzCcRWCFIt9TRnLH9lepBq6ojBLxZ0nFEPBS7bQgOHY0klCfkewtZy5KSGnKYialzlh4xNSFP5Th4mvWJWcJ8XvNkCAJK9T7f_xHSK-H0uPW0paxcTG3nhQtP7eY7iKSVjX-Oaca182KHKiuGzj2C4yWT_kenY-Ys7LtS7i93RCtcP/s828/FaceStylizer-Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, we have witnessed rising interest across consumers and researchers in integrated augmented reality (AR) experiences using real-time face feature generation and editing functions in mobile applications, including short videos, virtual reality, and gaming. As a result, there is a growing demand for lightweight, yet high-quality face generation and editing models, which are often based on &lt;a href=&quot;https://arxiv.org/pdf/1406.2661.pdf&quot;>;generative adversarial network&lt;/a>; (GAN) techniques. However, the majority of GAN models suffer from high computational complexity and the need for a large training dataset. In addition, it is also important to employ GAN models responsibly. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this post, we introduce &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe FaceStylizer&lt;/a>;, an efficient design for few-shot face stylization that addresses the aforementioned model complexity and data efficiency challenges while being guided by Google&#39;s responsible &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. The model consists of a face generator and a face encoder used as &lt;a href=&quot;https://arxiv.org/pdf/2101.05278.pdf&quot;>;GAN inversion&lt;/a>; to map the image into latent code for the generator. We introduce a mobile-friendly synthesis network for the face generator with an auxiliary head that converts features to &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; at each level of the generator to generate high quality images from coarse to fine granularities. We also carefully designed the &lt;a href=&quot;https://developers.google.com/machine-learning/gan/loss&quot;>;loss functions&lt;/a>; for the aforementioned auxiliary heads and combined them with the common GAN loss functions to distill the student generator from the teacher &lt;a href=&quot;https://github.com/NVlabs/stylegan&quot;>;StyleGAN&lt;/a>; model, resulting in a lightweight model that maintains high generation quality. The proposed solution is available in open source through &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>;. Users can fine-tune the generator to learn a style from one or a few images using MediaPipe Model Maker, and deploy to on-device face stylization applications with the customized model using &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe FaceStylizer&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Few-shot on-device face stylization&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;An end-to-end pipeline&lt;/h3>; &lt;p>; Our goal is to build a pipeline to support users to adapt the MediaPipe FaceStylizer to different styles by fine-tuning the model with a few examples. To enable such a face stylization pipeline, we built the pipeline with a GAN inversion encoder and efficient face generator model (see below). The encoder and generator pipeline can then be adapted to different styles via a few-shot learning process. The user first sends a single or a few similar samples of the style images to MediaPipe ModelMaker to fine-tune the model. The fine-tuning process freezes the encoder module and only fine-tunes the generator. The training process samples multiple latent codes close to the encoding output of the input style images as the input to the generator. The generator is then trained to reconstruct an image of a person&#39;s face in the style of the input style image by optimizing a joint adversarial loss function that also accounts for style and content. With such a fine-tuning process, the MediaPipe FaceStylizer can adapt to the customized style, which approximates the user&#39;s input. It can then be applied to stylize test images of real human faces. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Generator: BlazeStyleGAN&lt;/h3>; &lt;p>; The &lt;a href=&quot;https://arxiv.org/abs/1812.04948&quot;>;StyleGAN&lt;/a>; model family has been widely adopted for face generation and various face editing tasks. To support efficient on-device face generation, we based the design of our generator on StyleGAN. This generator, which we call &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.html&quot;>;BlazeStyleGAN&lt;/a>;, is similar to StyleGAN in that it also contains a mapping network and synthesis network. However, since the synthesis network of StyleGAN is the major contributor to the model&#39;s high computation complexity, we designed and employed a more efficient synthesis network. The improved efficiency and generation quality is achieved by: &lt;/p>; &lt;ol>; &lt;li>;Reducing the latent feature dimension in the synthesis network to a quarter of the resolution of the counterpart layers in the teacher StyleGAN, &lt;/li>; &lt;li>; Designing multiple auxiliary heads to transform the downscaled feature to the image domain to form a coarse-to-fine image pyramid to evaluate the perceptual quality of the reconstruction, and &lt;/li>; &lt;li>; Skipping all but the final auxiliary head at inference time. &lt;/li>; &lt;/ol>; &lt;p>; With the newly designed architecture, we train the BlazeStyleGAN model by distilling it from a teacher StyleGAN model. We use a multi-scale perceptual loss and adversarial loss in the distillation to transfer the high fidelity generation capability from the teacher model to the student BlazeStyleGAN model and also to mitigate the artifacts from the teacher model. &lt;/p>; &lt;p>; More details of the model architecture and training scheme can be found in &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.pdf&quot;>;our paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3TpxAGbwALgLPFX9ocKbOTjYkRKvx28IY1jWHzM5mdgc3xTBFpn2MfbW1WpJPgjewSY2-zxRyo9FJEjxcx0gzaJxGf2bJGSuVkUUUTM0Eob60tjUuwaF-RUsGERGzl0o5LYkn7G0oFXB1UORri9RhnZ0FIvfbh3F5PFdt7IbVeeNw-yO6Ua8MB0Y2V5A/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;488&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3TpxAGbwALgLPFX9ocKbOTjYkRKvx28IY1jWHzM5mdgc3xTBFpn2MfbW1WpJPgjewSY2-zxRyo9FJEjxcx0gzaJxGf2bJGSuVkUUUTM0Eob60tjUuwaF-RUsGERGzl0o5LYkn7G0oFXB1UORri9RhnZ0FIvfbh3F5PFdt7IbVeeNw-yO6Ua8MB0Y2V5A/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual comparison between face samples generated by StyleGAN and BlazeStyleGAN. The images on the first row are generated by the teacher StyleGAN. The images on the second row are generated by the student BlazeStyleGAN. The face generated by BlazeStyleGAN has similar visual quality to the image generated by the teacher model. Some results demonstrate the student BlazeStyleGAN suppresses the artifacts from the teacher model in the distillation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In the above figure, we demonstrate some sample results of our BlazeStyleGAN. By comparing with the face image generated by the teacher StyleGAN model (top row), the images generated by the student BlazeStyleGAN (bottom row) maintain high visual quality and further reduce artifacts produced by the teacher due to the loss function design in our distillation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;An encoder for efficient GAN inversion&lt;/h3>; &lt;p>; To support image-to-image stylization, we also introduced an efficient GAN inversion as the encoder to map input images to the latent space of the generator. The encoder is defined by a &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;>;MobileNet V2&lt;/a>; backbone and trained with natural face images. The loss is defined as a combination of image perceptual quality loss, which measures the content difference, style similarity and embedding distance, as well as the &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization&quot;>;L1 loss&lt;/a>; between the input images and reconstructed images. &lt;/p>; &lt;br />; &lt;h2>;On-device performance&lt;/h2>; &lt;p>; We documented model complexities in terms of parameter numbers and computing &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPs&lt;/a>; in the following table. Compared to the teacher StyleGAN (33.2M parameters), BlazeStyleGAN (generator) significantly reduces the model complexity, with only 2.01M parameters and 1.28G FLOPs for output resolution 256x256. Compared to StyleGAN-1024 (generating image size of 1024x1024), the BlazeStyleGAN-1024 can reduce both model size and computation complexity by 95% with no notable quality difference and can even suppress the artifacts from the teacher StyleGAN model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Model&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Image Size&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;#Params (M)&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;FLOPs (G)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;StyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1024 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;33.17 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;74.3 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1024 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.07 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4.70 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;512 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.05 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.57 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;256 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.01 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.28 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Encoder&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;256 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.44 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.60 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model complexity measured by parameter numbers and FLOPs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We benchmarked the inference time of the MediaPipe FaceStylizer on various high-end mobile devices and demonstrated the results in the table below. From the results, both BlazeStyleGAN-256 and BlazeStyleGAN-512 achieved real-time performance on all GPU devices. It can run in less than 10 ms runtime on a high-end phone&#39;s GPU. BlazeStyleGAN-256 can also achieve real-time performance on the iOS devices&#39; CPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;strong>;Model&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;BlazeStyleGAN-256 (ms)&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Encoder-256 (ms)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 11&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.14 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.48 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 12&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.25 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 13 Pro&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.22 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.41 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Pixel 6&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.24 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.23 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Samsung Galaxy S10&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;17.01 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.70 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Samsung Galaxy S20&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;8.95 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;8.20 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Latency benchmark of the BlazeStyleGAN, face encoder, and the end-to-end pipeline on various mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Fairness evaluation&lt;/h2>; &lt;p>; The model has been trained with a high diversity dataset of human faces. The model is expected to be fair to different human faces. The fairness evaluation demonstrates the model performs good and balanced in terms of human gender, skin-tone, and ages. &lt;/p>; &lt;br />; &lt;h2>;Face stylization visualization&lt;/h2>; &lt;p>; Some face stylization results are demonstrated in the following figure. The images in the top row (in orange boxes) represent the style images used to fine-tune the model. The images in the left column (in the green boxes) are the natural face images used for testing. The 2x4 matrix of images represents the output of the MediaPipe FaceStylizer which is blending outputs between the natural faces on the left-most column and the corresponding face styles on the top row. The results demonstrate that our solution can achieve high-quality face stylization for several popular styles. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpKZWwOnXcr79JiDtlbXO4sckZWVIpWtneSu3J4OgtIs6o6JtNpF8nbPkPpZqphMOP73LxX3MYS08OEEMQn3dAvmIG BFYBgQiL-bpMHWRKDl8WF2rO-aBca1uWr4y6p5sl8tvH1FgXMfXUyV-1c4UhuhPQ6QfTodu_tUdfkF-U257joD7tkkAAzv6AAoY/s940/image3 .jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;940&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpKZWwOnXcr79JiDtlbXO4sckZWVIpWtneSu3J4OgtIs6o6JtNpF8nbPkPpZqphMOP73LxX3MYS08OEEMQn3dAvmIGBFYBgQiL-bpMHWRKDl8WF2rO- aBca1uWr4y6p5sl8tvH1FgXMfXUyV-1c4UhuhPQ6QfTodu_tUdfkF-U257joD7tkkAAzv6AAoY/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MediaPipe FaceStylizer 的示例结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MediaPipe 解决方案&lt;/h2>; &lt;p>; MediaPipe FaceStylizer 将于 &lt;a href=&quot;https://developers.google.com 向公众用户发布/mediapipe/solutions&quot;>;MediaPipe 解决方案&lt;/a>;。用户可以利用 &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe Model Maker&lt;/a>; 使用自己的风格图像训练自定义的面部风格化模型。训练后，导出的 &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TFLite&lt;/a>; 模型文件包可以跨平台（Android、iOS、Web、Python 等）部署到应用程序中。 ）只需几行代码即可使用 &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe Tasks FaceStylizer API&lt;/a>;。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是通过 Google 多个团队的协作完成的。我们要感谢 Omer Tov、Yang Zhu、Andrey Vakunov、Fei Deng、Ariel Ephrat、Inbar Mosseri、Lu Wang、Chuo-Ling Chang、Tingbo Hou 和 Matthias Grundmann 的贡献。&lt;/em>; &lt;/p>;&lt; /content>;&lt;link href=&quot;http://blog.research.google/feeds/7378908661087185403/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/mediapipe-facestylizer-on-device-real.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7378908661087185403&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/7378908661087185403&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2023/09/mediapipe-facestylizer-on-device-real.html&quot; rel=&quot;alternate&quot; title=&quot;MediaPipe FaceStylizer：设备上实时少镜头脸部风格化&quot; type=&quot;text/html&quot;/>;&lt;author >;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16 “rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>;&lt;/gd :image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihspgvRDlnUmToZlZzCcRWCFIt9TRnLH9lepBq6ojBLxZ0nFEPBS7bQgOHY0klCfkewtZy5KSGnKYialzlh4xNSFP5Th4 mvWJWcJ8XvNkCAJK9T7f_xHSK-H0uPW0paxcTG3nhQtP7eY7iKSVjX-Oaca182KHKiuGzj2C4yWT_kenY-Ys7LtS7i93RCtcP/s72-c/FaceStylizer-Hero .png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;条目>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-118669777610020383&lt;/id>;&lt;发布>;2023-09-14T12:39:00.035-07:00&lt;/发布>;&lt;更新>;2023-09 -14T18：23：00.311-07：00 &lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“可访问性”>;&lt;/类别>;&lt;类别方案=“http” ://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;使用图神经网络进行设备上内容蒸馏&lt;/stitle>;&lt;content type= &quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院研究工程师 Gabriel Barcik 和 Duc-Hieu Tran&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEjAbKikU6UWOjYuGg9JWDI3s8QKhXHtUEl2STZt_TNmwR4Y_B65GkYs--uMmYUYVVBdbTrtAVLRmlEGc1fTgeMS2E1kaNLmUJk6xWoj0qm0axNj2OcMzzPTZ68ygM0f7 ZOo_8qoUXjTGGTO74-LZ9gvt9eK8lZjRDE0HWJNMJWYM_A2ppRGl5v8QVrxBEg7/s971/ScreenGNN.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 在当今的数字时代，智能手机和桌面网络浏览器是访问新闻和信息的主要工具。然而，网站混乱的激增——包括复杂的布局、导航元素和无关的链接——严重损害了阅读体验和文章导航。对于有无障碍要求的个人来说，这个问题尤其严重。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了改善用户体验并使阅读更方便，&lt;a href=&quot;https://www.android.com/google-features-on -android/december-2022/#accessibility&quot;>;Android&lt;/a>; 和 &lt;a href=&quot;https://blog.google/outreach-initiatives/education/chromebook-updates-2023/&quot;>;Chrome&lt;/a>; 用户可以利用阅读模式功能，该功能通过处理网页来增强可访问性，以允许可定制的对比度、可调整的文本大小、更清晰的字体以及启用文本到语音的实用程序。此外，Android 的阅读模式还可以从应用程序中提取内容。扩展阅读模式以涵盖广泛的内容并提高其性能，同时仍然在用户设备上本地运行而不向外部传输数据，提出了独特的挑战。 &lt;/p>; &lt;p>; 为了在不损害隐私的情况下扩大阅读模式功能，我们开发了一种新颖的设备上内容蒸馏模型。与早期尝试使用 &lt;a href=&quot;https://github.com/chromium/dom-distiller&quot;>;DOM Distiller&lt;/a>;（一种仅限于新闻文章的启发式方法）不同，我们的模型在各种类型的质量和多功能性方面都表现出色的内容。我们确保文章内容不会脱离当地环境的限制。我们的设备上内容蒸馏模型可以将长篇内容顺利地转换为简单且可定制的布局，以提供更愉快的阅读之旅，同时也优于领先的替代方法。在这里，我们探讨了这项研究的细节，重点介绍了我们的方法、方法和结果。 &lt;/p>; &lt;br />; &lt;h2>;图神经网络&lt;/h2>; &lt;p>; 我们不依赖于难以维护和扩展到各种文章布局的复杂启发式方法，而是将此任务视为完全&lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;>;监督学习&lt;/a>;问题。这种数据驱动的方法使模型能够更好地泛化不同的布局，而不受启发式的约束和脆弱性。以前优化阅读体验的工作依赖于 HTML 或文档的解析、过滤和建模对象模型&lt;/a>; (DOM)，一种由用户的 Web 浏览器从站点 HTML 自动生成的编程接口，它表示文档的结构并允许对其进行操作。 &lt;/p>; &lt;p>; 新的阅读模式模型依赖于&lt;a href=&quot;https://developer.chrome.com/blog/full-accessibility-tree/#what-is-the-accessibility-tree&quot;>;可访问性树&lt;/a>;，它提供了 DOM 的简化且更易于访问的表示。辅助功能树是从 DOM 树自动生成的，并由辅助技术利用，以允许残障人士与 Web 内容进行交互。这些可以在 Chrome Web 浏览器和 Android 上通过为 WebView 提供的 &lt;a href=&quot;https://developer.android.com/reference/android/view/accessibility/AccessibilityNodeInfo&quot;>;AccessibilityNodeInfo&lt;/a>; 对象来使用和本机应用程序内容。 &lt;/p>; &lt;p>; 我们首先手动收集和注释可访问性树。该项目使用的 Android 数据集包含大约 10k 个标记示例，而 Chrome 数据集包含大约 100k 个标记示例。我们开发了一种新颖的工具，它使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_neural_network&quot;>;图神经网络&lt;/a>;（GNN）使用多类从可访问性树中提取基本内容监督学习方法。数据集由从网络上采样的长篇文章组成，并标有标题、段落、图像、发布日期等类别。 &lt;/p>; &lt;p>; GNN 是处理树状数据结构的自然选择，因为传统模型通常需要详细的、手工制作的特征来理解这些树中的布局和链接，而 GNN 可以自然地学习这些连接。为了说明这一点，请考虑家谱的类比。在这样的树中，每个节点代表一个家庭成员，连接表示家庭关系。如果要使用传统模型预测某些特征，可能需要“具有某种特征的直系亲属数量”等特征。然而，对于 GNN，这种手动特征设计就变得多余了。通过直接将树结构输入模型，GNN 利用消息传递机制，每个节点与其邻居进行通信。随着时间的推移，信息在网络上共享和积累，使模型能够自然地辨别复杂的关系。 &lt;/p>; &lt;p>; 回到可访问性树的背景，这意味着 GNN 可以通过理解和利用树内的固有结构和关系来有效地提取内容。此功能使他们能够根据树内的信息流识别并可能省略非必要部分，从而确保更准确的内容提炼。 &lt;/p>; &lt;p>; 我们的架构很大程度上遵循使用 &lt;a href=&quot;https:/ 的&lt;a href=&quot;https://arxiv.org/abs/1806.01261&quot;>;编码-处理-解码&lt;/a>;范例/arxiv.org/abs/1704.01212&quot;>;消息传递神经网络&lt;/a>;用于对文本节点进行分类。整体设计如下图所示。文章的树表示是模型的输入。我们根据边界框信息、文本信息和可访问性角色计算轻量级特征。然后，GNN 使用消息传递神经网络通过树的边缘传播每个节点的潜在表示。此传播过程允许附近的节点、容器和文本元素相互共享上下文信息，从而增强模型对页面结构和内容的理解。然后，每个节点根据收到的消息更新其当前状态，为节点分类提供更明智的基础。经过固定数量的消息传递步骤后，现在上下文化的节点潜在表示被解码为基本或非基本类。这种方法使模型能够利用树中的固有关系和代表每个节点的手工特征，从而丰富最终的分类。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2RnSYjrCvHDVNxzOHlR3duUISGpF-xRz_Xv3fGUn-Fg0FLfPrkBBnYgVq8kMpbCQ525gHfFWd5L2vMwHw BSNBQiYUY7Q9ZAG-CJBR8SPU9-SQDuJ9k49Rd9Vg9b9TTenlIIgcALirLbBa6mMHyl5enrhystjwi0xOxy8gWedhGgB5SyQHi-e8YgSYjR/s1600 /image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2RnSYjrCvHDVNxzOHlR3duUISGpF-xRz_Xv3fGUn-Fg0FLfPrkBBnYgVq8kMpbCQ525gHfFWd5L2vMwHwbSlNBQiYUY7Q9ZAg-cJbR 8SpU9-SQDuJ9k49Rd9Vg9b9TTenlIIgcALirLbBa6mMHyl5enrhystjwi0xOxy8gWedhGgB5SyQHi-e8YgSYjR/s16000/image1.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在移动设备上处理文章的算法的视觉演示。图神经网络 (GNN) 用于从文章中提取基本内容。 1. 从应用程序中提取文章的树表示。 2. 为每个节点计算轻量级特征，表示为向量。 3. 消息传递神经网络通过树的边缘传播信息并更新每个节点表示。 4.包含文本内容的叶节点被分类为必要内容或非必要内容。 5. 基于 GNN 输出构建了应用程序的整洁版本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们刻意限制模型使用的特征集以增加其广泛的泛化性跨语言并加快用户设备上的推理延迟。这是一个独特的挑战，因为我们需要创建一个可以保护隐私的设备上轻量级模型。 &lt;/p>; &lt;p>; 我们最终的轻量级 Android 模型有 64k 个参数，大小为 334kB，中位延迟为 800ms，而 Chrome 模型有 241k 个参数，大小为 928kB，中位延迟为 378ms。通过采用此类设备内处理，我们确保用户数据永远不会离开设备，从而强化我们的&lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;负责任的做法&lt;/a>;以及对用户隐私的承诺。模型中使用的特征可以分为中间节点特征、叶节点文本特征和元素位置特征。我们执行了&lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_engineering&quot;>;功能工程&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_selection&quot;>;功能选择&lt;/a>;来优化模型性能和模型大小的特征集。最终模型被转换为 &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; 格式，以作为 Android 或 Chrome 上的设备上模型进行部署。 &lt;/p>; &lt;br />; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们在单个 GPU 中对 GNN 进行了大约 50 个周期的训练。 Android模型在网页和原生应用测试集上的表现如下：&lt;/p>;&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEiuC7wLsxGXfTVYqPvr7WMVSHtkC64DQwTlnbMrjZG-_EtJ7UPQO0O3vAiPPStbFESxgc4yronwPMS4MOK1o7b_qiun_zQnXzx6pdHp8mvGXzIlOf2508bsfyaRAaWo DA0th3vFLL_IQuyETrsbM3jNAunGaKkuwjCNn7CX0TfrZZdO21zlrj6XxsPEAhJz/s1339/AndroidQuality.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot; 487“数据原始宽度=“1339”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuC7wLsxGXfTVYqPvr7WMVSHtkC64DQwTlnbMrjZG-_EtJ7UPQO0O3vAiPPStbFESxgc4yronwPMS4MOK1o 7b_qiun_zQnXzx6pdHp8mvGXzIlOf2508bsfyaRAaWoDA0th3vFLL_IQuyETrsbM3jNAunGaKkuwjCNn7CX0TfrZZdO21zlrj6XxsPEAhJz/s16000/AndroidQuality.png&quot;/>;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;该表显示了 Android 中网页和本机应用的内容蒸馏指标。我们报告&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;精确度、召回率&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/F-score三个类别的“F1 分数”&lt;/a>;：非必要内容、标题和正文，包括宏观平均值和每个类别中实例数的加权平均值。节点度量以可访问性树节点的粒度评估分类性能，这类似于段落级别。相比之下，单词度量在单个单词级别评估分类，这意味着节点中的每个单词都获得相同的分类。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;tablealign=&quot;center &quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;13&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;Android 质量 &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;font-size: large; text-align: center;&quot;>;&lt;strong>;网页&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;font-size: large; text-align: center;&quot;>;&lt;strong>;原生应用&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#e8eaed&quot;>; &lt;td style=&quot;font-size: normal;&quot;>;&lt;strong>;&lt;em>;节点指标&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt; /td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;精度&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;回忆&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/ em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;精度&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;回忆&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot; text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;非必要&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp ;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9842 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9842 &lt;/td>;&lt;td>; ;&quot;>;0.9846 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9844 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/ td>; &lt;td style=&quot;text-align: center;&quot;>;0.9744 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9350 &lt;/td >;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9543 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;标题&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9187 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9784 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9476 &lt;/td>;&lt;td>; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9183 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align:中心;&quot;>;0.8568 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8865 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>; &lt;em>;主文本&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9223 &lt;/td>;&lt;td>;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9172 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9172 &lt;/td>;&lt;td>; &quot;>;0.9197 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8443 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;0.9424 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8907 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;宏观平均&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>; 0.9417 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9600 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td style=&quot;text-align: center;&quot;>;0.9506 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9124 &lt;/td>;&lt;td >;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9114 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align : center;&quot;>;0.9105 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;加权平均值&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style= &quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>; &quot;>;0.9392 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9353 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;0.9363 &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#e8eaed&quot;>; &lt;td>;&lt;strong>;&lt;em>;字词指标&lt;/em>;&lt;/strong >; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;精度&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;回想&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text -align: center;&quot;>;&lt;em>;F1 分数&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;精度&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;召回率&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1 分数&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;标题 + 主要文本&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9510&lt;/strong>; &lt;/td >;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9683&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/ td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9595&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center ;&quot;>;&lt;strong>;0.9473&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9507&lt;/strong>; &lt; /td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9490&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt; /table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container “样式=”margin-left：自动; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;该表显示了 Android 中网页和本机应用程序的内容蒸馏指标。我们报告&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;精确度、召回率&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot; >;F1-score&lt;/a>; 三个类别：非必要内容、标题和正文，包括宏观平均和每个类别中实例数的加权平均。节点指标以可访问性的粒度评估分类性能树节点，类似于段落级别。相比之下，单词度量在单个单词级别评估分类，这意味着节点中的每个单词都会获得相同的分类。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; -->; &lt;p>; 在评估经常访问的网页文章的结果质量时，主要文本（本质上是段落）的 F1 分数超过 0.9 对应于 88% 的这些文章正在处理而没有遗漏任何段落。此外，在超过 95% 的情况下，蒸馏对读者来说是有价值的。简而言之，绝大多数读者会认为提炼出来的内容既相关又准确，错误或遗漏的情况很少发生。 &lt;/p>; &lt;p>; Chrome 内容蒸馏与其他模型（例如 &lt;a href=&quot;https://github.com/chromium/dom-distiller&quot;>;DOM Distiller&lt;/a>; 或 &lt;a href=&quot; https://github.com/mozilla/readability&quot;>;一组英语页面上的 Mozilla 可读性&lt;/a>;如下表所示。我们重用机器翻译的指标来比较这些模型的质量。参考文本来自真实的主要内容，来自模型的文本作为假设文本。结果表明，与其他基于 DOM 的方法相比，我们的模型具有出色的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM3va2Pt2otiRp6UesiiUqxZygl24Pxv3EpTYAKcS_fIJEDq5I6DMq3Lcc_DzOEg0VsE41SvIMg9ISgrdAP27yVT9 StyysA89xcqSLyj7QWYi-Hc4eye-3c3WNLA9SWIWqtt-cbcMJsPX13UJrPDoMuqTUTJdyp-sW1GqgxmAADmrsze4kCyXHV8dPnEZj/s1339/ChromeQuality .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM3va2Pt2otiRp6UesiiUqxZygl24Pxv3EpTYAKcS_fIJEDq5I6DMq3Lcc_DzOEg0VsE41SvIMg9ISgrdAP27yVT9StyysA89xcqSLyj7Q WYi-Hc4eye-3c3WNlA9SWIWqtt-cbcMJsPX13UJrPDoMuqTUTJdyp-sW1GqgxmAADmrsze4kCyXHV8dPnEZj/s16000/ChromeQuality.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;该表展示了 DOM-Distiller、Mozilla Readability 和新 Chrome 模型之间的比较。我们报告基于文本的指标，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLUE&lt;/a>;、&lt;a href=&quot;https://aclanthology.org/W15- 3049.pdf&quot;>;CHRF&lt;/a>; 和 &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE&lt;/a>;，通过比较从每个模型中提取的主体文本评分者使用我们的注释政策手动标记的真实文本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot; 0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;7&quot; style=&quot;font-size: x- large; text-align: center;&quot;>;网页上的 Chrome 模型比较 &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td style=&quot;font-size: normal; text-align: center;&quot; >;&lt;strong>;&lt;em>;公制/型号&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em >;DOM Distiller&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Mozilla 可读性&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;我们的 Chrome 模型&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td 样式=&quot;text-align: center;&quot;>;&lt;em>;BLEU&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;78.97 &lt; /td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;79.16 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td 样式=&quot;text-align: center;&quot;>;94.59 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;CHRF&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.92 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align:中心;&quot;>;0.92 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td 样式=&quot;text-align: center;&quot;>;&lt;em>;ROUGE1&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.10 &lt; /td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.62 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td 样式=&quot;text-align: center;&quot;>;95.13 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGE2&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;81.84 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align:中心;&quot;>;82.66 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;94.81 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td 样式=&quot;text-align: center;&quot;>;&lt;em>;ROUGE3&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;80.21 &lt; /td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;81.45 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td 样式=&quot;text-align: center;&quot;>;94.60 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGEL&lt;/em>; &lt;/td>;&lt;td>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;83.58 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align:中心;&quot;>;84.02 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;95.04 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td 样式=&quot;text-align: center;&quot;>;&lt;em>;ROUGEL-SUM&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>; 83.46 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.03 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td style=&quot;text-align: center;&quot;>;95.04 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div >; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;表格呈现了 DOM-Distiller、Mozilla Readability 和新版 Chrome 之间的比较模型。我们报告基于文本的指标，例如 &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLUE&lt;/a>;、&lt;a href=&quot;https://aclanthology.org/ W15-3049.pdf&quot;>;CHRF&lt;/a>; 和 &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE&lt;/a>;，通过比较从中提取的主体文本每个模型都由评估者使用我们的注释策略手动标记为真实文本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;p>; Chrome 内容蒸馏模型的 F1 分数不同广泛使用的语言的测试集上的标题和主要文本内容表明，Chrome 模型尤其能够支持多种语言。&lt;/p>;&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing =&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6Z1lggR6AGKL0sxRWLmKEN50R4- OC4_JFrieWniKARBKxSSSo1jlry-Q7yRkysHBX1JpLOy0lEGhWNf3UlmEErjJoT_BixDzAUDqHwuKb-XngC0d_6-ynB5cXlRtNUjvneK1M8fzxH1va8jggc5cfROR8DwaTEx-hW0taoeGPtNML3qQ8Trxee m_J7UTM/s1339/ChromeLanguages.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;195&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEg6Z1lggR6AGKL0sxRWLmKEN50R4-OC4_JFrieWniKArBKxSSSo1jlry-Q7yRkysHBX1JpLOy0lEGhWNf3UlmEErjJoT_BixDzAUDqHwuKb-XngC0d_6-ynB5cXlRtNUjvneK1M8 fzxH1va8jggc5cfROR8DwaTEx-hW0taoeGPtNML3qQ8Trxeem_J7UTM/s16000/ChromeLanguages.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;该表显示了 Chrome 模型的标题和主要文本类别的 F1 分数的每种语言。语言代码对应于以下语言：德语、英语、西班牙语、法语、意大利语、波斯语、日语、韩语、葡萄牙语、越南语、简体中文和繁体中文。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot;类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;27&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;不同语言的 Chrome 模型&lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td style=&quot;font-size: normal; text-align: left;&quot;>;&lt;strong>;&lt;em>;F1-score&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;de&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;en&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;es&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;fr&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;it&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;fa&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ja&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ko&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;pt&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;vi&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;zh-Hans&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;zh-Hant&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;average&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;headline&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.89 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.96 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;main text&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.84 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.87 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.88 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents per language of F1-scores of the Chrome model for the headline and main text classes. The language codes correspond to the following languages: German, English, Spanish, French, Italian, Persian, Japanese, Korean, Portuguese, Vietnamese, simplified Chinese and traditional Chinese.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The digital age demands both streamlined content presentation and an unwavering commitment to user privacy. Our research highlights the effectiveness of Reading Mode in platforms like Android and Chrome, offering an innovative, data-driven approach to content parsing through Graph Neural Networks. Crucially, our lightweight on-device model ensures that content distillation occurs without compromising user data, with all processes executed locally. This not only enhances the reading experience but also reinforces our dedication to user privacy. As we navigate the evolving landscape of digital content consumption, our findings underscore the paramount importance of prioritizing the user in both experience and security. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This project is the result of joint work with Manuel Tragut, Mihai Popa, Abodunrinwa Toki, Abhanshu Sharma, Matt Sharifi, David Petrou and Blaise Aguera y Arcas. We sincerely thank our collaborators Gang Li and Yang Li. We are very grateful to Tom Small for assisting us in preparing the post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/118669777610020383/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/on-device-content-distillation-with.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/118669777610020383&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/118669777610020383&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/on-device-content-distillation-with.html&quot; rel=&quot;alternate&quot; title=&quot;On-device content distillation with graph neural networks&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAbKikU6UWOjYuGg9JWDI3s8QKhXHtUEl2STZt_TNmwR4Y_B65GkYs--uMmYUYVVBdbTrtAVLRmlEGc1fTgeMS2E1kaNLmUJk6xWoj0qm0axNj2OcMzzPTZ68ygM0f7ZOo_8qoUXjTGGTO74-LZ9gvt9eK8lZjRDE0HWJNMJWYM_A2ppRGl5v8QVrxBEg7/s72-c/ScreenGNN.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6191878611070084392&lt;/id>;&lt;published>;2023-09-12T14:22:00.000-07:00&lt;/published>;&lt;updated>;2023-09-12T14:22:55.668-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Publications&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;World scale inverse reinforcement learning in Google Maps&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Matt Barnes, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgA5ZpGidOzCqTYTLV8bj62lAG7yfVa0cson-09oo7hqGA9ayl7h6koU96mxkBOqP_NyqiKamaoFrSAHtBlY7UH7XMnoq5Hn1H0hoiC5Uk0mMOkunNi6-j08iSEmUXTYEmp1YuFEgWLRJtieseqhQseMpy6e8KY5wElwNKDcy99GjCO-j04G0TVaJb0Pcr/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Routing in Google Maps remains one of our most helpful and frequently used features. Determining the best route from A to B requires making complex trade-offs between factors including the &lt;a href=&quot;https://www.deepmind.com/blog/traffic-prediction-with-advanced-graph-neural-networks&quot;>;estimated time of arrival&lt;/a>; (ETA), tolls, directness, surface conditions (eg, paved, unpaved roads), and user preferences, which vary across transportation mode and local geography. Often, the most natural visibility we have into travelers&#39; preferences is by analyzing real-world travel patterns. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Learning preferences from observed sequential decision making behavior is a classic application of &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning#Inverse_reinforcement_learning&quot;>;inverse reinforcement learning&lt;/a>; (IRL). Given a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;>;Markov decision process&lt;/a>; (MDP) — a formalization of the road network — and a set of demonstration trajectories (the traveled routes), the goal of IRL is to recover the users&#39; latent reward function. Although &lt;a href=&quot;https://link.springer.com/article/10.1007/s10462-021-10108-x&quot;>;past research&lt;/a>; has created increasingly general IRL solutions, these have not been successfully scaled to world-sized MDPs. Scaling IRL algorithms is challenging because they typically require solving an RL subroutine &lt;em>;at every update step&lt;/em>;. At first glance, even attempting to fit a world-scale MDP into memory to compute a single gradient step appears infeasible due to the large number of road segments and limited high bandwidth memory. When applying IRL to routing, one needs to consider all reasonable routes between each demonstration&#39;s origin and destination. This implies that any attempt to break the world-scale MDP into smaller components cannot consider components smaller than a metropolitan area. &lt;/p>; &lt;p>; To this end, in &quot;&lt;a href=&quot;https://arxiv.org/abs/2305.11290&quot;>;Massively Scalable Inverse Reinforcement Learning in Google Maps&lt;/a>;&quot;, we share the result of a multi-year collaboration among Google Research, Maps, and Google DeepMind to surpass this IRL scalability limitation. We revisit classic algorithms in this space, and introduce advances in graph compression and parallelization, along with a new IRL algorithm called Receding Horizon Inverse Planning (RHIP) that provides fine-grained control over performance trade-offs. The final RHIP policy achieves a 16–24% relative improvement in global route match rate, ie, the percentage of de-identified traveled routes that exactly match the suggested route in Google Maps. To the best of our knowledge, this represents the largest instance of IRL in a real world setting to date. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIjGlEhwPBRCKI-_LLSIF_TRSB7FGohWSwbvTh0K-3wovXgGmWKBYFXBIKhdVOkqHPLq5F4ZLO0tZVXyyprnXDJ2UqzWmDLQV4RwXDqiMjAmbCTkcey7JauVHhhcakVEveWA4U4qnzokEli9E_PblSodxtsRO7hCRnmJBRvU-zRcxNCP_9pG4-Kuff2twW/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1040&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIjGlEhwPBRCKI-_LLSIF_TRSB7FGohWSwbvTh0K-3wovXgGmWKBYFXBIKhdVOkqHPLq5F4ZLO0tZVXyyprnXDJ2UqzWmDLQV4RwXDqiMjAmbCTkcey7JauVHhhcakVEveWA4U4qnzokEli9E_PblSodxtsRO7hCRnmJBRvU-zRcxNCP_9pG4-Kuff2twW/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Google Maps improvements in route match rate relative to the existing baseline, when using the RHIP inverse reinforcement learning policy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The benefits of IRL&lt;/h2>; &lt;p>; A subtle but crucial detail about the routing problem is that it is &lt;em>;goal conditioned&lt;/em>;, meaning that every destination state induces a slightly different MDP (specifically, the destination is a terminal, zero-reward state). IRL approaches are well suited for these types of problems because the learned reward function transfers across MDPs, and only the destination state is modified. This is in contrast to approaches that directly learn a policy, which typically require an extra factor of &lt;em>;S&lt;/em>; parameters, where &lt;em>;S&lt;/em>; is the number of MDP states. &lt;/p>; &lt;p>; Once the reward function is learned via IRL, we take advantage of a powerful inference-time trick. First, we evaluate the entire graph&#39;s rewards once in an &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/static-vs-dynamic-inference/video-lecture&quot;>;offline batch setting&lt;/a>;. This computation is performed entirely on servers without access to individual trips, and operates only over batches of road segments in the graph. Then, we save the results to an in-memory database and use a fast online &lt;a href=&quot;https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm&quot;>;graph search algorithm&lt;/a>; to find the highest reward path for routing requests between any origin and destination. This circumvents the need to perform online inference of a deeply parameterized model or policy, and vastly improves serving costs and latency. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHgaS1N-fsKIzQTMZEelXDZ8TVZboZvSZ3Z9ZLPlYR48NtGRabWXer_1r38dLm4cCIF8lBuapSVBPWzkVdBkTG7fdFuOwbry0Shdg7_sR19FdVOichywmraMPVUvBLl3XYS2B0rY1JdroZxIqlWB0rjl-mIGV3gASY6IuxhEQgjVdAqVvZJL1IYJ3HU3zc/s791/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;406&quot; data-original-width=&quot;791&quot; height=&quot;328&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHgaS1N-fsKIzQTMZEelXDZ8TVZboZvSZ3Z9ZLPlYR48NtGRabWXer_1r38dLm4cCIF8lBuapSVBPWzkVdBkTG7fdFuOwbry0Shdg7_sR19FdVOichywmraMPVUvBLl3XYS2B0rY1JdroZxIqlWB0rjl-mIGV3gASY6IuxhEQgjVdAqVvZJL1IYJ3HU3zc/w640-h328/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Reward model deployment using batch inference and fast online planners.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Receding Horizon Inverse Planning&lt;/h2>; &lt;p>; To scale IRL to the world MDP, we compress the graph and shard the global MDP using a sparse &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;Mixture of Experts&lt;/a>; (MoE) based on geographic regions. We then apply classic IRL algorithms to solve the local MDPs, estimate the loss, and send gradients back to the MoE. The worldwide reward graph is computed by decompressing the final MoE reward model. To provide more control over performance characteristics, we introduce a new generalized IRL algorithm called Receding Horizon Inverse Planning (RHIP). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHUdaeXJURVEV2f--mytRfbx5k3yftkMKBpuu43GKhyMaAa-fJC3O4QfTxZjz3hkGjUsCBiO6LLcYRsGeyrpOgIUzDDPW1lOwlr47YXKSLUwJuXxMELxRV9SKcr4BDoy5_2HzcFZX-Wb7m3PcQhWvx1efvG5gkXG_HfrhqrqfZ_xrNyBmSado-Wf_si3wH/s1617/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;526&quot; data-original-width=&quot;1617&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHUdaeXJURVEV2f--mytRfbx5k3yftkMKBpuu43GKhyMaAa-fJC3O4QfTxZjz3hkGjUsCBiO6LLcYRsGeyrpOgIUzDDPW1lOwlr47YXKSLUwJuXxMELxRV9SKcr4BDoy5_2HzcFZX-Wb7m3PcQhWvx1efvG5gkXG_HfrhqrqfZ_xrNyBmSado-Wf_si3wH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;IRL reward model training using MoE parallelization, graph compression, and RHIP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; RHIP is inspired by people&#39;s tendency to perform extensive local planning (&quot;What am I doing for the next hour?&quot;) and approximate long-term planning (&quot;What will my life look like in 5 years?&quot;). To take advantage of this insight, RHIP uses robust yet expensive stochastic policies in the local region surrounding the demonstration path, and switches to cheaper deterministic planners beyond some horizon. Adjusting the horizon &lt;em>;H&lt;/em>; allows controlling computational costs, and often allows the discovery of the performance sweet spot. Interestingly, RHIP generalizes many classic IRL algorithms and provides the novel insight that they can be viewed along a stochastic vs. deterministic spectrum (specifically, for &lt;em>;H&lt;/em>;=∞ it reduces to &lt;a href=&quot;https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf&quot;>;MaxEnt&lt;/a>;, for &lt;em>;H&lt;/em>;=1 it reduces to &lt;a href=&quot;https://www.ijcai.org/Proceedings/07/Papers/416.pdf&quot;>;BIRL&lt;/a>;, and for &lt;em>;H&lt;/em>;=0 it reduces to &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1143844.1143936&quot;>;MMP&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVip8f74bfUV6t4yy3-smAgmUday76QLz1AyzFRIzsudZ-MA7vfQACQbm2_6OhbqQ_wA8ZY-aGtwlEb2bZiJs-Ltp98wTlxA92ndfccuowYt5lUa6Ve7ZIoANax2HWMF45mVXQDuQcNHfqZRLebAgOYBdUSlHi-7P6TMTszN5Pwe0jYvVUEHIjYPrigDTm/s1039/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;422&quot; data-original-width=&quot;1039&quot; height=&quot;260&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVip8f74bfUV6t4yy3-smAgmUday76QLz1AyzFRIzsudZ-MA7vfQACQbm2_6OhbqQ_wA8ZY-aGtwlEb2bZiJs-Ltp98wTlxA92ndfccuowYt5lUa6Ve7ZIoANax2HWMF45mVXQDuQcNHfqZRLebAgOYBdUSlHi-7P6TMTszN5Pwe0jYvVUEHIjYPrigDTm/w640-h260/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given a demonstration from s&lt;sub>;o&lt;/sub>; to s&lt;sub>;d&lt;/sub>;, (1) RHIP follows a robust yet expensive stochastic policy in the local region surrounding the demonstration (&lt;strong>;blue region&lt;/strong>;). (2) Beyond some horizon H, RHIP switches to following a cheaper deterministic planner (&lt;strong>;red lines&lt;/strong>;). Adjusting the horizon enables fine-grained control over performance and computational costs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Routing wins&lt;/h2>; &lt;p>; The RHIP policy provides a 15.9% and 24.1% lift in global route match rate for driving and two-wheelers (eg, scooters, motorcycles, mopeds) relative to the well-tuned Maps baseline, respectively. We&#39;re especially excited about the benefits to more sustainable transportation modes, where factors beyond journey time play a substantial role. By tuning RHIP&#39;s horizon &lt;em>;H&lt;/em>;, we&#39;re able to achieve a policy that is both more accurate than all other IRL policies and 70% faster than MaxEnt. &lt;/p>; &lt;p>; Our 360M parameter reward model provides intuitive wins for Google Maps users in live &lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;>;A/B experiments&lt;/a>;. Examining road segments with a large absolute difference between the learned rewards and the baseline rewards can help improve certain Google Maps routes. For example: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhghO43O0wC_DVBWbvclvbXruodtRbalSFRwKCmiNI1ws5NmlvDI6MwPhtLUkPCR5gfjiQ72lFRyI9WUFFX2VwPBNMC2KJxJjX49yrAhfNvU9_YFGQ2Z27K8VROnu9B4K7YNCmptiuoezWZZViqMMYonhmiYlgRnttMbpP2T44XLZE1kvm1bhSxfwgR-zNJ/s1245/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;426&quot; data-original-width=&quot;1245&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhghO43O0wC_DVBWbvclvbXruodtRbalSFRwKCmiNI1ws5NmlvDI6MwPhtLUkPCR5gfjiQ72lFRyI9WUFFX2VwPBNMC2KJxJjX49yrAhfNvU9_YFGQ2Z27K8VROnu9B4K7YNCmptiuoezWZZViqMMYonhmiYlgRnttMbpP2T44XLZE1kvm1bhSxfwgR-zNJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Nottingham, UK. The preferred route (&lt;strong>;blue&lt;/strong>;) was previously marked as private property due to the presence of a large gate, which indicated to our systems that the road may be closed at times and would not be ideal for drivers. As a result, Google Maps routed drivers through a longer, alternate detour instead (&lt;strong>;red&lt;/strong>;). However, because real-world driving patterns showed that users regularly take the preferred route without an issue (as the gate is almost never closed), IRL now learns to route drivers along the preferred route by placing a large positive reward on this road segment.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Increasing performance via increased scale – both in terms of dataset size and model complexity – has proven to be a persistent trend in machine learning. Similar gains for inverse reinforcement learning problems have historically remained elusive, largely due to the challenges with handling practically sized MDPs. By introducing scalability advancements to classic IRL algorithms, we&#39;re now able to train reward models on problems with hundreds of millions of states, demonstration trajectories, and model parameters, respectively. To the best of our knowledge, this is the largest instance of IRL in a real-world setting to date. See the &lt;a href=&quot;https://arxiv.org/abs/2305.11290&quot;>;paper&lt;/a>; to learn more about this work. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Contributors to the project include Matthew Abueg, Oliver Lange, Matt Deeds, Jason Trader, Denali Molitor, Markus Wulfmeier, Shawn O&#39;Banion, Ryan Epp, Renaud Hartert, Rui Song, Thomas Sharp, Rémi Robert, Zoltan Szego, Beth Luan, Brit Larabee and Agnieszka Madurska.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;We&#39;d also like to extend our thanks to Arno Eigenwillig, Jacob Moorman, Jonathan Spencer, Remi Munos, Michael Bloesch and Arun Ahuja for valuable discussions and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6191878611070084392/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/world-scale-inverse-reinforcement.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6191878611070084392&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6191878611070084392&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/world-scale-inverse-reinforcement.html&quot; rel=&quot;alternate&quot; title=&quot;World scale inverse reinforcement learning in Google Maps&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgA5ZpGidOzCqTYTLV8bj62lAG7yfVa0cson-09oo7hqGA9ayl7h6koU96mxkBOqP_NyqiKamaoFrSAHtBlY7UH7XMnoq5Hn1H0hoiC5Uk0mMOkunNi6-j08iSEmUXTYEmp1YuFEgWLRJtieseqhQseMpy6e8KY5wElwNKDcy99GjCO-j04G0TVaJb0Pcr/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-237108111388574068&lt;/id>;&lt;published>;2023-09-08T15:59:00.002-07:00&lt;/published>;&lt;updated>;2023-09-11T10:00:14.121-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Differentially private median and more&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Edith Cohen and Uri Stemmer, Research Scientists, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiY4hQkG4Ofm6plcTF56kw-L1RI613kh8ztTMZvJgmn2VpFF84K1TIfBymU6p_xVE0q-SRafCvZtrzdCgNz4qqsyqVTevfEffmxqhQ_Jg6rJQgo5vm3zoOkQ6JnxeMNQ4Y7LrOufGJB8lTnJoKSYZYke2qZi0rvO9PUB5POaYdou4O7lE2-IO7hD0__aJcy/s1554/dpmedian.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;Differential privacy&lt;/a>; (DP) is a rigorous mathematical definition of privacy. DP algorithms are randomized to protect user data by ensuring that the probability of any particular output is nearly unchanged when a data point is added or removed. Therefore, the output of a DP algorithm does not disclose the presence of any one data point. There has been significant progress in both foundational research and adoption of differential privacy with contributions such as the &lt;a href=&quot;https://privacysandbox.com/&quot;>;Privacy Sandbox&lt;/a>; and &lt;a href=&quot;https://opensource.googleblog.com/2020/06/expanding-our-differential-privacy.html&quot;>;Google Open Source Library&lt;/a>;. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ML and data analytics algorithms can often be described as performing multiple basic computation steps on the same dataset. When each such step is differentially private, so is the output, but with multiple steps the overall privacy guarantee &lt;em>;deteriorates,&lt;/em>; a phenomenon known as the &lt;em>;cost of composition&lt;/em>;. &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy#Composability&quot;>;Composition theorems&lt;/a>; bound the increase in privacy loss with the number &lt;em>;k&lt;/em>; of computations: In the general case, the privacy loss increases with the square root of &lt;em>;k&lt;/em>;. This means that we need much stricter privacy guarantees for each step in order to meet our overall privacy guarantee goal. But in that case, we lose utility. One way to improve the privacy vs. utility trade-off is to identify when the use cases admit a tighter privacy analysis than what follows from composition theorems. &lt;/p>; &lt;p>; Good candidates for such improvement are when each step is applied to a disjoint part (slice) of the dataset. When the slices are selected in a data-independent way, each point affects only one of the &lt;em>;k&lt;/em>; outputs and the privacy guarantees do not deteriorate with &lt;em>;k&lt;/em>;. However, there are applications in which we need to select the slices adaptively (that is, in a way that depends on the output of prior steps). In these cases, a change of a single data point may cascade — changing multiple slices and thus increasing composition cost. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;Õptimal Differentially Private Learning of Thresholds and Quasi-Concave Optimization&lt;/a>;”, presented at &lt;a href=&quot;http://acm-stoc.org/stoc2023/&quot;>;STOC 2023&lt;/a>;, we describe a new paradigm that allows for slices to be selected adaptively and yet avoids composition cost. We show that DP algorithms for multiple fundamental aggregation and learning tasks can be expressed in this Reorder-Slice-Compute (RSC) paradigm, gaining significant improvements in utility. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The Reorder-Slice-Compute (RSC) paradigm&lt;/h2>; &lt;p>; An algorithm &lt;em>;A&lt;/em>; falls in the RSC paradigm if it can be expressed in the following general form (see visualization below). The input is a sensitive set &lt;em>;D&lt;/em>; of data points. The algorithm then performs a sequence of &lt;em>;k&lt;/em>; steps as follows: &lt;/p>; &lt;ol>; &lt;li>;Select an ordering over data points, a slice size &lt;em>;m&lt;/em>;, and a DP algorithm &lt;em>;M&lt;/em>;. The selection may depend on the output of &lt;em>;A&lt;/em>; in prior steps (and hence is adaptive). &lt;/li>;&lt;li>;Slice out the (approximately) top &lt;em>;m&lt;/em>; data points according to the order from the dataset &lt;em>;D&lt;/em>;, apply &lt;em>;M&lt;/em>; to the slice, and output the result. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj63itU6U--NeU1FGMjUslYsM63Ut85OAGE0bilRt8hy0-bxugE6CvLlNmtf1OtRjnJRsxXcOAPBeoOS2eEIb74whQCRn4ayBbacPR1j4I46TQsZ1Au0b3NTgEwh-GtLs1pGyxItUHx_vMHg-tq2XyyrbZ6SUKi1gDvX9udLoRi98eW5peyOJGpnu6Euzb8/s666/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;478&quot; data-original-width=&quot;666&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj63itU6U--NeU1FGMjUslYsM63Ut85OAGE0bilRt8hy0-bxugE6CvLlNmtf1OtRjnJRsxXcOAPBeoOS2eEIb74whQCRn4ayBbacPR1j4I46TQsZ1Au0b3NTgEwh-GtLs1pGyxItUHx_vMHg-tq2XyyrbZ6SUKi1gDvX9udLoRi98eW5peyOJGpnu6Euzb8/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visualization of three Reorder-Slice-Compute (RSC) steps.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; If we analyze the overall privacy loss of an RSC algorithm using DP composition theorems, the privacy guarantee suffers from the expected composition cost, ie, it deteriorates with the square root of the number of steps &lt;em>;k&lt;/em>;. To eliminate this composition cost, we provide a novel analysis that removes the dependence on &lt;em>;k&lt;/em>; altogether: the overall privacy guarantee is close to that of a single step! The idea behind our tighter analysis is a novel technique that limits the potential cascade of affected steps when a single data point is modified (details &lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;in the paper&lt;/a>;). &lt;/p>; &lt;p>; Tighter privacy analysis means better utility. The effectiveness of DP algorithms is often stated in terms of the smallest input size (number of data points) that suffices in order to release a correct result that meets the privacy requirements. We describe several problems with algorithms that can be expressed in the RSC paradigm and for which our tighter analysis improved utility. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Private interval point&lt;/h2>; &lt;p>; We start with the following basic aggregation task. The input is a dataset &lt;em>;D&lt;/em>; of &lt;em>;n&lt;/em>; points from an ordered domain &lt;em>;X&lt;/em>; (think of the domain as the natural numbers between &lt;em>;1&lt;/em>; and&lt;em>; |X|&lt;/em>;). The goal is to return a point &lt;em>;y&lt;/em>; in &lt;em>;X&lt;/em>; that is in the &lt;em>;interval&lt;/em>; of &lt;em>;D&lt;/em>;, that is between the minimum and the maximum points in &lt;em>;D&lt;/em>;. &lt;/p>; &lt;p>; The solution to the interval point problem is trivial without the privacy requirement: simply return any point in the dataset &lt;em>;D&lt;/em>;. But this solution is not privacy-preserving as it discloses the presence of a particular datapoint in the input. We can also see that if there is only one point in the dataset, a privacy-preserving solution is not possible, as it must return that point. We can therefore ask the following fundamental question: What is the smallest input size &lt;em>;N&lt;/em>; for which we can solve the private interval point problem? &lt;/p>; &lt;p>; It is known that &lt;em>;N&lt;/em>; must increase with the domain size &lt;em>;|X|&lt;/em>; and that this dependence is at least the &lt;a href=&quot;https://en.wikipedia.org/wiki/Iterated_logarithm&quot;>;iterated log function&lt;/a>;&lt;em>; &lt;/em>;log&lt;em>;* |X|&lt;/em>; [&lt;a href=&quot;https://ieeexplore.ieee.org/document/7354419&quot;>;1&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3313276.3316312&quot;>;2&lt;/a>;]. On the other hand, the &lt;a href=&quot;https://proceedings.mlr.press/v125/kaplan20a.html&quot;>;best prior&lt;/a>; DP algorithm required the input size to be at least (log* &lt;em>;|X|&lt;/em>;)&lt;sup>;1.5&lt;/sup>;. To close this gap, we designed an RSC algorithm that requires only an order of log*&lt;em>; |X|&lt;/em>; points. &lt;/p>; &lt;p>; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Iterated_logarithm&quot;>;iterated log function&lt;/a>; is extremely slow growing: It is the number of times we need to take a logarithm of a value before we reach a value that is equal to or smaller than &lt;em>;1&lt;/em>;. How did this function naturally come out in the analysis? Each step of the RSC algorithm remapped the domain to a logarithm of its prior size. Therefore there were log* &lt;em>;|X|&lt;/em>; steps in total. The tighter RSC analysis eliminated a square root of the number of steps from the required input size. &lt;/p>; &lt;p>; Even though the interval point task seems very basic, it captures the essence of the difficulty of private solutions for common aggregation tasks. We next describe two of these tasks and express the required input size to these tasks in terms of &lt;em>;N&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Private approximate median&lt;/h2>; &lt;p>; One of these common aggregation tasks is &lt;em>;approximate median&lt;/em>;: The input is a dataset &lt;em>;D&lt;/em>; of &lt;em>;n&lt;/em>; points from an ordered domain &lt;em>;X&lt;/em>;. The goal is to return a point &lt;em>;y&lt;/em>; that is between the ⅓ and ⅔ quantiles of &lt;em>;D&lt;/em>;. That is, at least a third of the points in &lt;em>;D&lt;/em>; are smaller or equal to &lt;em>;y&lt;/em>; and at least a third of the points are larger or equal to &lt;em>;y&lt;/em>;. Note that returning an exact median is not possible with differential privacy, since it discloses the presence of a datapoint. Hence we consider the relaxed requirement of an approximate median (shown below). &lt;/p>; &lt;p>; We can compute an approximate median by finding an interval point: We slice out the &lt;em>;N&lt;/em>; smallest points and the &lt;em>;N&lt;/em>; largest points and then compute an interval point of the remaining points. The latter must be an approximate median. This works when the dataset size is at least &lt;em>;3N&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf26PttXtoA75NoqKmNacVcsoHkCQFld1jOhqiOuml8CHIMbBWbcWlHXn0r6hC-697XA8ey-GvVDshwQsvaQRYb8DwD6wRMnR0Nuo2rxMqszE8OW06pZ184dCz4s9NdKBfHF0mPhqjoA2l7lepeZ88Xru1Q3Zrxvm85W5onykpmwLeU5Gm3v2fYNYfQEWN/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;545&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf26PttXtoA75NoqKmNacVcsoHkCQFld1jOhqiOuml8CHIMbBWbcWlHXn0r6hC-697XA8ey-GvVDshwQsvaQRYb8DwD6wRMnR0Nuo2rxMqszE8OW06pZ184dCz4s9NdKBfHF0mPhqjoA2l7lepeZ88Xru1Q3Zrxvm85W5onykpmwLeU5Gm3v2fYNYfQEWN/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of a data D over domain X, the set of interval points, and the set of approximate medians.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Private learning of axis-aligned rectangles&lt;/h2>; &lt;p>; For the next task, the input is a set of &lt;em>;n&lt;/em>; labeled data points, where each point &lt;em>;x = (x&lt;sub>;1&lt;/sub>;,....,x&lt;sub>;d&lt;/sub>;)&lt;/em>; is a &lt;em>;d&lt;/em>;-dimensional vector over a domain &lt;em>;X&lt;/em>;. Displayed below, the goal is to learn values &lt;em>;a&lt;sub>;i &lt;/sub>;, b&lt;sub>;i&lt;/sub>;&lt;/em>; for the axes &lt;em>;i=1,...,d&lt;/em>; that define a &lt;em>;d&lt;/em>;-dimensional rectangle, so that for each example &lt;em>;x&lt;/em>; &lt;/p>; &lt;ul>; &lt;li>;If &lt;em>;x&lt;/em>; is positively labeled (shown as red plus signs below) then it lies within the rectangle, that is, for all axes &lt;em>;i&lt;/em>;, &lt;em>;x&lt;sub>;i&lt;/sub>;&lt;/em>; is in the interval &lt;em>;[a&lt;sub>;i &lt;/sub>;,b&lt;sub>;i&lt;/sub>;]&lt;/em>;, and &lt;/li>;&lt;li>;If &lt;em>;x&lt;/em>; is negatively labeled (shown as blue minus signs below) then it lies outside the rectangle, that is, for at least one axis &lt;em>;i&lt;/em>;, &lt;em>;x&lt;sub>;i&lt;/sub>;&lt;/em>; is outside the interval &lt;em>;[a&lt;sub>;i &lt;/sub>;,b&lt;sub>;i&lt;/sub>;]&lt;/em>;. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNdbuXJ043M9Z94-l2x2Df--GPDu3CCvfru3EBHSig4GxylqwV-uiEf84bK0oLmGIaC5h0zSyimqMQR3BJPKqbm3fmWTrOVoKA3MJusfKfEHVM0UlYpki5rWfx0z2niP0pckX3FX_0cyV-nTeP5V9J5_PPRLMe13gQM1FZAXlLQ_WBM4nySEXpAO1rQBHS/s1656/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1160&quot; data-original-width=&quot;1656&quot; height=&quot;280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNdbuXJ043M9Z94-l2x2Df--GPDu3CCvfru3EBHSig4GxylqwV-uiEf84bK0oLmGIaC5h0zSyimqMQR3BJPKqbm3fmWTrOVoKA3MJusfKfEHVM0UlYpki5rWfx0z2niP0pckX3FX_0cyV-nTeP5V9J5_PPRLMe13gQM1FZAXlLQ_WBM4nySEXpAO1rQBHS/w400-h280/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A set of 2-dimensional labeled points and a respective rectangle.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Any DP solution for this problem must be approximate in that the learned rectangle must be allowed to mislabel some data points, with some positively labeled points outside the rectangle or negatively labeled points inside it. This is because an exact solution could be very sensitive to the presence of a particular data point and would not be private. The goal is a DP solution that keeps this necessary number of mislabeled points small. &lt;/p>; &lt;p>; We first consider the one-dimensional case (&lt;em>;d = 1)&lt;/em>;. We are looking for an interval &lt;em>;[a,b]&lt;/em>; that covers all positive points and none of the negative points. We show that we can do this with at most &lt;em>;2N&lt;/em>; mislabeled points. We focus on the positively labeled points. In the first RSC step we slice out the &lt;em>;N&lt;/em>; smallest points and compute a private interval point as &lt;em>;a&lt;/em>;. We then slice out the &lt;em>;N&lt;/em>; largest points and compute a private interval point as &lt;em>;b&lt;/em>;. The solution&lt;em>; [a,b] &lt;/em>;correctly labels all negatively labeled points and mislabels at most &lt;em>;2N&lt;/em>; of the positively labeled points. Thus, at most ~&lt;em>;2N&lt;/em>; points are mislabeled in total. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0lIq2EkGGyGn15GSI5mEEAya4zPNNoAjoCBOkGj4Qljsk7M02T8s55E3FI9r_0Vfgx14N7PRpXFQN07bdqBgslKAxkkIbjhuV5LNdoMySNWRvvIwzWB9XyTn8ASLvvVQI0f1BWuFtOEbiG7ydKIbvkP3o3R1TgP9lDuXCFFT4wr8zEst5PNvbnlLlCg4z/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;441&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0lIq2EkGGyGn15GSI5mEEAya4zPNNoAjoCBOkGj4Qljsk7M02T8s55E3FI9r_0Vfgx14N7PRpXFQN07bdqBgslKAxkkIbjhuV5LNdoMySNWRvvIwzWB9XyTn8ASLvvVQI0f1BWuFtOEbiG7ydKIbvkP3o3R1TgP9lDuXCFFT4wr8zEst5PNvbnlLlCg4z/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration for &lt;em>;d = 1&lt;/em>;, we slice out &lt;em>;N&lt;/em>; left positive points and compute an interval point &lt;em>;a&lt;/em>;, slice out &lt;em>;N&lt;/em>; right positive points and compute an interval point &lt;em>;b&lt;/em>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With &lt;em>;d &amp;gt; 1&lt;/em>;, we iterate over the axes&lt;em>; i = 1,....,d&lt;/em>; and apply the above for the &lt;em>;i&lt;sup>;th &lt;/sup>;&lt;/em>;coordinates of input points to obtain the values &lt;em>;a&lt;sub>;i &lt;/sub>;, b&lt;sub>;i&lt;/sub>; &lt;/em>;. In each iteration, we perform two RSC steps and slice out &lt;em>;2N&lt;/em>; positively labeled points. In total, we slice out &lt;em>;2dN&lt;/em>; points and all remaining points were correctly labeled. That is, all negatively-labeled points are outside the final &lt;em>;d&lt;/em>;-dimensional rectangle and all positively-labeled points, except perhaps ~&lt;em>;2dN&lt;/em>;, lie inside the rectangle. Note that this algorithm uses the full flexibility of RSC in that the points are ordered differently by each axis. Since we perform &lt;em>;d&lt;/em>; steps, the RSC analysis shaves off a factor of square root of &lt;em>;d&lt;/em>; from the number of mislabeled points. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training ML models with adaptive selection of training examples&lt;/h2>; &lt;p>; The training efficiency or performance of ML models can sometimes be improved by selecting training examples in a way that depends on the current state of the model, eg, self-paced &lt;a href=&quot;https://www.frontiersin.org/research-topics/36658/curriculum-learning-and-related-applications&quot;>;curriculum learning&lt;/a>; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Active_learning_(machine_learning)&quot;>;active learning&lt;/a>;. &lt;/p>; &lt;p>; The most common method for private training of ML models is &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>;, where noise is added to the gradient update from each minibatch of training examples. Privacy analysis with DP-SGD typically assumes that training examples are &lt;em>;randomly&lt;/em>; partitioned into minibatches. But if we impose a data-dependent selection order on training examples, and further modify the selection criteria &lt;em>;k&lt;/em>; times during training, then analysis through DP composition results in deterioration of the privacy guarantees of a magnitude equal to the square root of &lt;em>;k&lt;/em>;. &lt;/p>; &lt;p>; Fortunately, example selection with DP-SGD can be naturally expressed in the RSC paradigm: each selection criteria reorders the training examples and each minibatch is a slice (for which we compute a noisy gradient). With RSC analysis, there is no privacy deterioration with &lt;em>;k&lt;/em>;, which brings DP-SGD training with example selection into the practical domain. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The RSC paradigm was introduced in order to tackle an open problem that is primarily of theoretical significance, but turns out to be a versatile tool with the potential to enhance data efficiency in production environments. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The &lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;work&lt;/a>; described here was done jointly with &lt;a href=&quot;https://people.eecs.berkeley.edu/~xinlyu/&quot;>;Xin Lyu&lt;/a>;, Jelani Nelson, and Tamas Sarlos.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/237108111388574068/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/differentially-private-median-and-more.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/237108111388574068&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/237108111388574068&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/differentially-private-median-and-more.html&quot; rel=&quot;alternate&quot; title=&quot;Differentially private median and more&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiY4hQkG4Ofm6plcTF56kw-L1RI613kh8ztTMZvJgmn2VpFF84K1TIfBymU6p_xVE0q-SRafCvZtrzdCgNz4qqsyqVTevfEffmxqhQ_Jg6rJQgo5vm3zoOkQ6JnxeMNQ4Y7LrOufGJB8lTnJoKSYZYke2qZi0rvO9PUB5POaYdou4O7lE2-IO7hD0__aJcy/s72-c/dpmedian.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5967099570177693866&lt;/id>;&lt;published>;2023-09-07T15:03:00.000-07:00&lt;/published>;&lt;updated>;2023-09-07T15:03:10.510-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TPU&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A novel computational fluid dynamics framework for turbulent flow research&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shantanu Shahane, Software Engineer, and Matthias Ihme, Research Scientist, Athena Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s990/image1.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Turbulence is ubiquitous in environmental and engineering fluid flows, and is encountered routinely in everyday life. A better understanding of these turbulent processes could provide valuable insights across a variety of research areas — improving the prediction of cloud formation by atmospheric transport and the spreading of wildfires by turbulent energy exchange, understanding sedimentation of deposits in rivers, and improving the efficiency of combustion in aircraft engines to reduce emissions, to name a few. However, despite its importance, our current understanding and our ability to reliably predict such flows remains limited. This is mainly attributed to the highly chaotic nature and the enormous spatial and temporal scales these fluid flows occupy, ranging from energetic, large-scale movements on the order of several meters on the high-end, where energy is injected into the fluid flow, all the way down to micrometers (μm) on the low-end, where the turbulence is dissipated into heat by viscous friction. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; A powerful tool to understand these turbulent flows is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Direct_numerical_simulation&quot;>;direct numerical simulation&lt;/a>; (DNS), which provides a detailed representation of the unsteady three-dimensional flow-field without making any approximations or simplifications. More specifically, this approach utilizes a discrete grid with small enough grid spacing to capture the underlying continuous equations that govern the dynamics of the system (in this case, variable-density &lt;a href=&quot;https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations&quot;>;Navier-Stokes equations&lt;/a>;, which govern all fluid flow dynamics). When the grid spacing is small enough, the discrete grid points are enough to represent the true (continuous) equations without the loss of accuracy. While this is attractive, such simulations require tremendous computational resources in order to capture the correct fluid-flow behaviors across such a wide range of spatial scales. &lt;/p>; &lt;p>; The actual span in spatial resolution to which direct numerical calculations must be applied depends on the task and is determined by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Reynolds_number&quot;>;Reynolds number&lt;/a>;, which compares inertial to viscous forces. Typically, the Reynolds number can range between 10&lt;sup>;2&lt;/sup>; up to 10&lt;sup>;7 &lt;/sup>;(even larger for atmospheric or interstellar problems). In 3D, the grid size for the resolution required scales roughly with the Reynolds number to the power of 4.5! Because of this strong scaling dependency, simulating such flows is generally limited to flow regimes with moderate Reynolds numbers, and typically requires access to &lt;a href=&quot;https://www.top500.org/lists/top500/2023/06/&quot;>;high-performance computing systems&lt;/a>; with millions of CPU/GPU cores. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0010465522000108?via%3Dihub&quot;>;A TensorFlow simulation framework for scientific computing of fluid flows on tensor processing units&lt;/a>;”, we introduce a new simulation framework that enables the computation of fluid flows with TPUs. By leveraging latest advances on &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>; software and TPU-hardware architecture, this software tool allows detailed large-scale simulations of turbulent flows at unprecedented scale, pushing the boundaries of scientific discovery and turbulence analysis. We demonstrate that this framework scales efficiently to accommodate the scale of the problem or, alternatively, improved run times, which is remarkable since most large-scale distributed computation frameworks exhibit reduced efficiency with scaling. The software is available as an open-source project on &lt;a href=&quot;https://github.com/google-research/swirl-lm&quot;>;GitHub&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Large-scale scientific computation with accelerators&lt;/h2>; &lt;p>; The software solves variable-density Navier-Stokes equations on TPU architectures using the TensorFlow framework. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;>;single-instruction, multiple-data&lt;/a>; (SIMD) approach is adopted for parallelization of the TPU solver implementation. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite_difference_method&quot;>;finite difference&lt;/a>; operators on a &lt;a href=&quot;https://www.cfd-online.com/Wiki/Collocated_grid&quot;>;colocated&lt;/a>; structured mesh are cast as filters of the convolution function of TensorFlow, leveraging TPU&#39;s matrix multiply unit (MXU). The framework takes advantage of the low-latency high-bandwidth inter-chips interconnect (ICI) between the TPU accelerators. In addition, by leveraging the single-precision floating-point computations and highly optimized executable through the accelerated linear algebra (XLA) compiler, it&#39;s possible to perform large-scale simulations with excellent scaling on TPU hardware architectures. &lt;/p>; &lt;p>; This research effort demonstrates that the &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;>;graph&lt;/a>;-based TensorFlow in combination with new types of ML special purpose hardware, can be used as a programming paradigm to solve partial differential equations representing multiphysics flows. The latter is achieved by augmenting the Navier-Stokes equations with physical models to account for chemical reactions, heat-transfer, and density changes to enable, for example, simulations of &lt;a href=&quot;https://arxiv.org/abs/2301.04698&quot;>;cloud formation&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;wildfires&lt;/a>;. &lt;/p>; &lt;p>; It&#39;s worth noting that this framework is the first open-source &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_fluid_dynamics&quot;>;computational fluid dynamics&lt;/a>; (CFD) framework for high-performance, large-scale simulations to fully leverage the cloud accelerators that have become common (and become a commodity) with the advancement of machine learning (ML) in recent years. While our work focuses on using TPU accelerators, the code can be easily adjusted for other accelerators, such as GPU clusters. &lt;/p>; &lt;p>; This framework demonstrates a way to greatly reduce the cost and turn-around time associated with running large-scale scientific CFD simulations and enables even greater iteration speed in fields, such as climate and weather research. Since the framework is implemented using TensorFlow, an ML language, it also enables the ready integration with ML methods and allows the exploration of ML approaches on CFD problems. With the general accessibility of TPU and GPU hardware, this approach lowers the barrier for researchers to contribute to our understanding of large-scale turbulent systems. &lt;/p>; &lt;br />; &lt;h2>;Framework validation and homogeneous isotropic turbulence&lt;/h2>; &lt;p>; Beyond demonstrating the performance and the scaling capabilities, it is also critical to validate the correctness of this framework to ensure that when it is used for CFD problems, we get reasonable results. For this purpose, researchers typically use idealized benchmark problems during CFD solver development, many of which we adopted in our work (more details in the &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0010465522000108?via%3Dihub&quot;>;paper&lt;/a>;). &lt;/p>; &lt;p>; One such benchmark for turbulence analysis is &lt;a href=&quot;https://en.wikipedia.org/wiki/Homogeneous_isotropic_turbulence&quot;>;homogeneous isotropic turbulence&lt;/a>;&lt;em>; &lt;/em>;(HIT), which is a canonical and well studied flow in which the statistical properties, such as kinetic energy, are invariant under translations and rotations of the coordinate axes. By pushing the resolution to the limits of the current state of the art, we were able to perform direct numerical simulations with more than eight billion degrees of freedom — equivalent to a three-dimensional mesh with 2,048 grid points along each of the three directions. We used 512 TPU-v4 cores, distributing the computation of the grid points along the x, y, and z axes to a distribution of [2,2,128] cores, respectively, optimized for the performance on TPU. The wall clock time per timestep was around 425 milliseconds and the flow was simulated for a total of 400,000 timesteps. 50 TB data, which includes the velocity and density fields, is stored for 400 timesteps (every 1,000th step). To our knowledge, this is one of the largest turbulent flow simulations of its kind conducted to date. &lt;/p>; &lt;p>; Due to the complex, chaotic nature of the turbulent flow field, which extends across several magnitudes of resolution, simulating the system in high resolution is necessary. Because we employ a fine-resolution grid with eight billion points, we are able to accurately resolve the field. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s990/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;894&quot; data-original-width=&quot;990&quot; height=&quot;361&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/w400-h361/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Contours of &lt;em>;x&lt;/em>;-component of velocity along the &lt;em>;z&lt;/em>; midplane. The high resolution of the simulation is critical to accurately represent the turbulent field.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The turbulent kinetic energy and dissipation rates are two statistical quantities commonly used to analyze a turbulent flow. The temporal decay of these properties in a turbulent field without additional energy injection is due to viscous dissipation and the decay asymptotes follow the expected analytical &lt;a href=&quot;https://en.wikipedia.org/wiki/Energy_cascade&quot;>;power law&lt;/a>;. This is in agreement with the theoretical asymptotes and observations reported in the literature and thus, validates our framework. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7fkt8YvPwlGZcapjmEL3eQgZo1o1CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s562/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;562&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7fkt8YvPwlGZcapjmEL3eQgZo1o1CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Solid line: Temporal evolution of turbulent kinetic energy (&lt;em>;k&lt;/em>;). Dashed line: Analytical power laws for decaying homogeneous isotropic turbulence (&lt;em>;n&lt;/em>;=1.3) (&lt;em>;Ⲧ&lt;sub>;l&lt;/sub>;&lt;/em>;: &lt;a href=&quot;https://physics.stackexchange.com/questions/79184/what-are-the-length-and-time-scales-in-turbulence&quot;>;eddy turnover time&lt;/a>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4DsMuf2OR-sIgJeAGmleM2lY2kAijlsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s565/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;565&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4DsMuf2OR-sIgJeAGmleM2lY2kAijlsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Solid line: Temporal evolution of dissipation rate (&lt;em>;ε&lt;/em>;). Dashed line: Analytical power laws for decaying homogeneous isotropic turbulence (n=1.3).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The energy spectrum of a turbulent flow represents the energy content across &lt;a href=&quot;https://en.wikipedia.org/wiki/Wavenumber&quot;>;wavenumber&lt;/a>;, where the wavenumber &lt;i>;k&lt;/i>; is proportional to the inverse wavelength &lt;i>;λ&lt;/i>; (ie, &lt;i>;k&lt;/i>; ∝ 1/&lt;i>;λ&lt;/i>;). Generally, the spectrum can be qualitatively divided into three ranges: source range, inertial range and viscous dissipative range (from left to right on the wavenumber axis, below). The lowest wavenumbers in the source range correspond to the largest turbulent eddies, which have the most energy content. These large eddies transfer energy to turbulence in the intermediate wavenumbers (inertial range), which is statistically isotropic (ie, essentially uniform in all directions). The smallest eddies, corresponding to the largest wavenumbers, are dissipated into thermal energy by the viscosity of the fluid. By virtue of the fine grid having 2,048 points in each of the three spatial directions, we are able to resolve the flow field up to the length scale at which viscous dissipation takes place. This direct numerical simulation approach is the most accurate as it does not require any closure model to approximate the energy cascade below the grid size. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW8Epx5sVTF0DnW-5NYdIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N-k6DcBbh912IOG/s569/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;569&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW8Epx5sVTF0DnW-5NYdIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N-k6DcBbh912IOG/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Spectrum of turbulent kinetic energy at different time instances. The spectrum is normalized by the instantaneous integral length (&lt;em>;l&lt;/em>;) and the turbulent kinetic energy (&lt;em>;k&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A new era for turbulent flows research&lt;/h2>; &lt;p>; More recently, we extended this framework to predict &lt;a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;wildfires&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2301.04698&quot;>;atmospheric flows&lt;/a>;, which is relevant for climate-risk assessment. Apart from enabling high-fidelity simulations of complex turbulent flows, this simulation framework also provides capabilities for &lt;a href=&quot;https://www.osti.gov/servlets/purl/1478744&quot;>;scientific machine learning&lt;/a>; (SciML) — for example, downsampling from a fine to a coarse grid (&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_order_reduction&quot;>;model reduction&lt;/a>;) or building models that run at lower resolution while still capturing the correct dynamic behaviors. It could also provide avenues for further scientific discovery, such as building ML-based models to better parameterize microphysics of turbulent flows, including physical relationships between temperature, pressure, vapor fraction, etc., and could improve upon various control tasks, eg, to reduce the energy consumption of buildings or find more efficient propeller shapes. While attractive, a main bottleneck in SciML has been the availability of data for training. To explore this, we have been working with groups at Stanford and Kaggle to make the data from our high-resolution HIT simulation available through a community-hosted web-platform, &lt;a href=&quot;https://blastnet.github.io&quot;>;BLASTNet&lt;/a>;, to provide broad access to high-fidelity data to the research community via a network-of-datasets approach. We hope that the availability of these emerging high-fidelity simulation tools in conjunction with community-driven datasets will lead to significant advances in various areas of fluid mechanics. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Qing Wang, Yi-Fan Chen, and John Anderson for consulting and advice, Tyler Russell and Carla Bromberg for program management. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5967099570177693866/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html&quot; rel=&quot;alternate&quot; title=&quot;A novel computational fluid dynamics framework for turbulent flow research&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s72-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8763480087701216145&lt;/id>;&lt;published>;2023-09-06T12:47:00.001-07:00&lt;/published>;&lt;updated>;2023-09-12T16:01:52.707-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;TSMixer: An all-MLP architecture for time series forecasting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Si-An Chen, Student Researcher, Cloud AI Team, and Chun-Liang Li, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4BgCqd24e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;span style=&quot;font-size: small;&quot;>;&lt;i>;&lt;b>;Update — 2023/09/12:&lt;/b>; This post has been updated as the work described is now published in &lt;a href=&quot;https://www.jmlr.org/tmlr/&quot;>;&lt;em>;Transactions on Machine Learning Research&lt;/em>;&lt;/a>;.&lt;/i>;&lt;/span>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;Time series&lt;/a>; forecasting is critical to various real-world applications, from &lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy&quot;>;demand forecasting&lt;/a>; to &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/29606177/&quot;>;pandemic spread prediction&lt;/a>;. In &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;multivariate time series forecasting&lt;/a>; (forecasting multiple variants at the same time), one can split existing methods into two categories: univariate models and multivariate models. Univariate models focus on inter-series interactions or temporal patterns that encompass trends and seasonal patterns on a time series with a single variable. Examples of such trends and seasonal patterns might be the way mortgage rates increase due to inflation, and how traffic peaks during rush hour. In addition to inter-series patterns, multivariate models process intra-series features, known as cross-variate information, which is especially useful when one series is an advanced indicator of another series. For example, a rise in body weight may cause an increase in blood pressure, and increasing the price of a product may lead to a decrease in sales. Multivariate models have &lt;a href=&quot;https://research.google/pubs/pub49697/&quot;>;recently become popular solutions&lt;/a>; for multivariate forecasting as practitioners believe their capability of handling cross-variate information may lead to better performance. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In recent years, deep learning Transformer-based architectures have become a popular choice for multivariate forecasting models due to their superior performance on sequence tasks. However, advanced multivariate models &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;perform surprisingly worse&lt;/a>; than simple univariate linear models on commonly-used &lt;a href=&quot;https://github.com/thuml/Autoformer&quot;>;long-term forecasting benchmarks&lt;/a>;, such as &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;Electricity Transformer Temperature&lt;/a>; (ETT), &lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption&quot;>;Electricity&lt;/a>;, &lt;a href=&quot;https://pems.dot.ca.gov/&quot;>;Traffic&lt;/a>;, and &lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;Weather&lt;/a>;. These results raise two questions: &lt;/p>; &lt;ul>; &lt;li>;Does cross-variate information benefit time series forecasting? &lt;/li>;&lt;li>;When cross-variate information is not beneficial, can multivariate models still perform as well as univariate models? &lt;/li>; &lt;/ul>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.06053&quot;>;TSMixer: An All-MLP Architecture for Time Series Forecasting&lt;/a>;”, published in the &lt;em>;&lt;a href=&quot;https://www.jmlr.org/tmlr/&quot;>;Transactions on Machine Learning Research&lt;/a>;&lt;/em>; (TMLR), we analyze the advantages of univariate linear models and reveal their effectiveness. Insights from this analysis lead us to develop Time-Series Mixer (TSMixer), an advanced multivariate model that leverages linear model characteristics and performs well on long-term forecasting benchmarks. To the best of our knowledge, TSMixer is the first multivariate model that performs as well as state-of-the-art univariate models on long-term forecasting benchmarks, where we show that cross-variate information is less beneficial. To demonstrate the importance of cross-variate information, we evaluate a more challenging real-world application, &lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy&quot;>;M5&lt;/a>;. Finally, empirical results show that TSMixer outperforms state-of-the-art models, such as &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2201.12740&quot;>;Fedformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TSMixer architecture&lt;/h2>; &lt;p>; A key difference between linear models and Transformers is how they capture temporal patterns. On one hand, linear models apply fixed and time-step-dependent weights to capture static temporal patterns, and are unable to process cross-variate information. On the other hand, Transformers use &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;attention mechanisms&lt;/a>; that apply dynamic and data-dependent weights at each time step, capturing dynamic temporal patterns and enabling them to process cross-variate information. &lt;/p>; &lt;p>; In our analysis, we show that under common assumptions of temporal patterns, linear models have naïve solutions to perfectly recover the time series or place bounds on the error, which means they are great solutions for learning static temporal patterns of univariate time series more effectively. In contrast, it is non-trivial to find similar solutions for attention mechanisms, as the weights applied to each time step are dynamic. Consequently, we develop a new architecture by replacing Transformer attention layers with linear layers. The resulting TSMixer model, which is similar to the computer vision &lt;a href=&quot;https://arxiv.org/abs/2105.01601&quot;>;MLP-Mixer&lt;/a>; method, alternates between applications of the multi-layer perceptron in different directions, which we call &lt;em>;time-mixing&lt;/em>; and &lt;em>;feature-mixing,&lt;/em>; respectively. The TSMixer architecture efficiently captures both temporal patterns and cross-variate information, as shown in the figure below. The residual designs ensure that TSMixer retains the capacity of temporal linear models while still being able to exploit cross-variate information. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY3lV1paiioZ2nw4n3mY4JJx8Lo74kiLsL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY3lV1paiioZ2nw4n3mY4JJx8Lo74kiLsL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transformer block and TSMixer block architectures. TSMixer replaces the multi-head attention layer with time-mixing, a linear model applied on the time dimension.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQiznbfrqB0cRopGe4GCKupIQZnbNt8XutFC5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/s1646/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1646&quot; height=&quot;195&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQiznbfrqB0cRopGe4GCKupIQZnbNt8XutFC5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/w400-h195/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison between data-dependent (attention mechanisms) and time-step-dependent (linear models). This is an example of forecasting the next time step by learning the weights of the previous three time steps.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation on long-term forecasting benchmarks&lt;/h2>; &lt;p>; We evaluate TSMixer using seven popular &lt;a href=&quot;https://github.com/thuml/Autoformer&quot;>;long-term forecasting datasets&lt;/a>; (&lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTm1&lt;/a>;, &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTm2&lt;/a>;, &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTh1&lt;/a>;, &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTh2&lt;/a>;, &lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption&quot;>;Electricity&lt;/a>;, &lt;a href=&quot;https://pems.dot.ca.gov/&quot;>;Traffic&lt;/a>;, and &lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;Weather&lt;/a>;), where &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;recent research&lt;/a>; has shown that univariate linear models outperform advanced multivariate models with large margins. We compare TSMixer with state-of-the-art multivariate models (&lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2201.12740&quot;>;FEDformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2012.07436&quot;>;Informer&lt;/a>;), and univariate models, including &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;linear models&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;. The figure below shows the average improvement of &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;mean squared error&lt;/a>; (MSE) by TSMixer compared with others. The average is calculated across datasets and multiple forecasting horizons. We demonstrate that TSMixer significantly outperforms other multivariate models and performs on par with state-of-the-art univariate models. These results show that multivariate models are capable of performing as well as univariate models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQKHbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s1200/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQKHbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The average MSE improvement of TSMixer compared with other baselines. The red bars show multivariate methods and the blue bars show univariate methods. TSMixer achieves significant improvement over other multivariate models and achieves comparable results to univariate models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Ablation study&lt;/h2>; &lt;p>; We performed an ablation study to compare TSMixer with TMix-Only, a TSMixer variant that consists of time mixing layers only. The results show that TMix-Only performs almost the same as TSMixer, which means the additional feature mixing layers do not improve the performance and confirms that cross-variate information is less beneficial on popular benchmarks. The results validate the superior univariate model performance shown in &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;previous research&lt;/a>;. However, existing long-term forecasting benchmarks are not well representative of the need for cross-variate information in some real-world applications where time series may be intermittent or sparse, hence temporal patterns may not be sufficient for forecasting. Therefore, it may be inappropriate to evaluate multivariate forecasting models solely on these benchmarks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation on M5: Effectiveness of cross-variate information&lt;/h2>; &lt;p>; To further demonstrate the benefit of multivariate models, we evaluate TSMixer on the challenging M5 benchmark, a large-scale retail dataset containing crucial cross-variate interactions. M5 contains the information of 30,490 products collected over 5 years. Each product description includes time series data, like daily sales, sell price, promotional event information, and static (non-time-series) features, such as store location and product category. The goal is to forecast the daily sales of each product for the next 28 days, evaluated using the &lt;a href=&quot;https://mofc.unic.ac.cy/m5-competition/&quot;>;weighted root mean square scaled error&lt;/a>; (WRMSSE) from the M5 competition. The complicated nature of retail makes it more challenging to forecast solely using univariate models that focus on temporal patterns, so multivariate models with cross-variate information and even auxiliary features are more essential. &lt;/p>; &lt;p>; First, we compare TSMixer to other methods only considering the historical data, such as daily sales and historical sell prices. The results show that multivariate models outperforms univariate models significantly, indicating the usefulness of cross-variate information. And among all compared methods, TSMixer effectively leverages the cross-variate information and achieves the best performance. &lt;/p>; &lt;p>; Additionally, to leverage more information, such as static features (eg, store location, product category) and future time series (eg, a promotional event scheduled in coming days) provided in M5, we propose a principle design to extend TSMixer. The extended TSMixer aligns different types of features into the same length, and then applies multiple mixing layers to the concatenated features to make predictions. The extended TSMixer architecture outperforms models popular in industrial applications, including &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;, showcasing its strong potential for real-world impact. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQgaVscfd-K5hQniB7N3iM6tc22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/s1950/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1950&quot; data-original-width=&quot;1760&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQgaVscfd-K5hQniB7N3iM6tc22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/w578-h640/image2.png&quot; width=&quot;578&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The architecture of the extended TSMixer. In the first stage (align stage), it aligns the different types of features into the same length before concatenating them. In the second stage (mixing stage) it applies multiple mixing layers conditioned with static features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKPJQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbKZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_hA46iTaDBpogHn7iVJ615O/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKPJQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbKZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_hA46iTaDBpogHn7iVJ615O/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The WRMSSE on M5. The first three methods (&lt;strong>;blue&lt;/strong>;) are univariate models. The middle three methods (&lt;strong>;orange&lt;/strong>;) are multivariate models that consider only historical features. The last three methods (&lt;strong>;red&lt;/strong>;) are multivariate models that consider historical, future, and static features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present TSMixer, an advanced multivariate model that leverages linear model characteristics and performs as well as state-of-the-art univariate models on long-term forecasting benchmarks. TSMixer creates new possibilities for the development of time series forecasting architectures by providing insights into the importance of cross-variate and auxiliary information in real-world scenarios. The empirical results highlight the need to consider more realistic benchmarks for multivariate forecasting models in future research. We hope that this work will inspire further exploration in the field of time series forecasting, and lead to the development of more powerful and effective models that can be applied to real-world applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, and Tomas Pfister.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8763480087701216145/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8763480087701216145&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8763480087701216145&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html&quot; rel=&quot;alternate&quot; title=&quot;TSMixer: An all-MLP architecture for time series forecasting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4BgCqd24e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6223088894812086013&lt;/id>;&lt;published>;2023-08-31T10:14:00.000-07:00&lt;/published>;&lt;updated>;2023-08-31T10:14:27.378-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;WeatherBench 2: A benchmark for the next generation of data-driven weather models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Stephan Rasp, Research Scientist, and Carla Bromberg, Program Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFdjdsi2hvZyKcB0ODholetAjBMQ43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s800/weatherbenchgif.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In 1950, weather forecasting started its digital revolution when researchers used the first programmable, general-purpose computer &lt;a href=&quot;https://en.wikipedia.org/wiki/ENIAC#:~:text=ENIAC%20(%2F%CB%88%C9%9Bni,of%20them%20in%20one%20computer.&quot;>;ENIAC&lt;/a>; to solve mathematical equations describing how weather evolves. In the more than 70 years since, continuous advancements in computing power and improvements to the model formulations have led to steady gains in weather forecast skill: a 7-day forecast today is about as accurate as a 5-day forecast in 2000 and a 3-day forecast in 1980. While improving forecast accuracy at the pace of approximately one day per decade may not seem like a big deal, every day improved is important in far reaching use cases, such as for logistics planning, disaster management, agriculture and energy production. This &lt;a href=&quot;https://www.nature.com/articles/nature14956&quot;>;“quiet” revolution&lt;/a>; has been tremendously valuable to society, saving lives and providing economic value across many sectors. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Now we are seeing the start of yet another revolution in weather forecasting, this time fueled by advances in machine learning (ML). Rather than hard-coding approximations of the physical equations, the idea is to have algorithms learn how weather evolves from looking at large volumes of past weather data. Early attempts at doing so go back to &lt;a href=&quot;https://gmd.copernicus.org/articles/11/3999/2018/&quot;>;2018&lt;/a>; but the pace picked up considerably in the last two years when several large ML models demonstrated weather forecasting skill comparable to the best physics-based models. Google&#39;s MetNet [&lt;a href=&quot;https://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;2&lt;/a>;], for instance, demonstrated state-of-the-art capabilities for forecasting regional weather one day ahead. For global prediction, Google DeepMind created &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast&lt;/a>;, a graph neural network to make 10 day predictions at a horizontal resolution of 25 km, competitive with the best physics-based models in many skill metrics. &lt;/p>; &lt;p>; Apart from potentially providing more accurate forecasts, one key advantage of such ML methods is that, once trained, they can create forecasts in a matter of minutes on inexpensive hardware. In contrast, traditional weather forecasts require large super-computers that run for hours every day. Clearly, ML represents a tremendous opportunity for the weather forecasting community. This has also been recognized by leading weather forecasting centers, such as the &lt;a href=&quot;https://www.ecmwf.int/&quot;>;European Centre for Medium-Range Weather Forecasts&#39;&lt;/a>; (ECMWF) &lt;a href=&quot;https://www.ecmwf.int/en/elibrary/81207-machine-learning-ecmwf-roadmap-next-10-years&quot;>;machine learning roadmap&lt;/a>; or the &lt;a href=&quot;https://www.noaa.gov/&quot;>;National Oceanic and Atmospheric Administration&#39;s&lt;/a>; (NOAA) &lt;a href=&quot;https://sciencecouncil.noaa.gov/wp-content/uploads/2023/04/2020-AI-Strategy.pdf&quot;>;artificial intelligence strategy&lt;/a>;. &lt;/p>; &lt;p>; To ensure that ML models are trusted and optimized for the right goal, forecast evaluation is crucial. Evaluating weather forecasts isn&#39;t straightforward, however, because weather is an incredibly multi-faceted problem. Different end-users are interested in different properties of forecasts, for example, renewable energy producers care about wind speeds and solar radiation, while crisis response teams are concerned about the track of a potential cyclone or an impending heat wave. In other words, there is no single metric to determine what a “good” weather forecast is, and the evaluation has to reflect the multi-faceted nature of weather and its downstream applications. Furthermore, differences in the exact evaluation setup — eg, which resolution and ground truth data is used — can make it difficult to compare models. Having a way to compare novel and established methods in a fair and reproducible manner is crucial to measure progress in the field. &lt;/p>; &lt;p>; To this end, we are announcing &lt;a href=&quot;https://arxiv.org/abs/2308.15560&quot;>;WeatherBench 2&lt;/a>; (WB2), a benchmark for the next generation of data-driven, global weather models. WB2 is an update to the &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020MS002203&quot;>;original benchmark&lt;/a>; published in 2020, which was based on initial, lower-resolution ML models. The goal of WB2 is to accelerate the progress of data-driven weather models by providing a trusted, reproducible framework for evaluating and comparing different methodologies. The &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;official website&lt;/a>; contains scores from several state-of-the-art models (at the time of writing, these are &lt;a href=&quot;https://arxiv.org/abs/2202.07575&quot;>;Keisler (2022)&lt;/a>;, an early graph neural network, Google DeepMind&#39;s &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast&lt;/a>; and Huawei&#39;s &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06185-3&quot;>;Pangu-Weather&lt;/a>;, a transformer-based ML model). In addition, forecasts from ECMWF&#39;s high-resolution and ensemble forecasting systems are included, which represent some of the best traditional weather forecasting models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3hPB82GvFv2luh4j_BkavTXp9I0P61xnFiySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s1960/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;372&quot; data-original-width=&quot;1960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3hPB82GvFv2luh4j_BkavTXp9I0P61xnFiySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Making evaluation easier&lt;/h2>; &lt;p>; The key component of WB2 is an &lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;open-source evaluation framework&lt;/a>; that allows users to evaluate their forecasts in the same manner as other baselines. Weather forecast data at high-resolutions can be quite large, making even evaluation a computational challenge. For this reason, we built our evaluation code on &lt;a href=&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>;, which allows users to split computations into smaller chunks and evaluate them in a distributed fashion, for example using &lt;a href=&quot;https://cloud.google.com/dataflow&quot;>;DataFlow&lt;/a>; on Google Cloud. The code comes with a &lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/evaluation.html&quot;>;quick-start guide&lt;/a>; to help people get up to speed. &lt;/p>; &lt;p>; Additionally, we &lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/data-guide.html&quot;>;provide&lt;/a>; most of the ground-truth and baseline data on Google Cloud Storage in cloud-optimized &lt;a href=&quot;https://zarr.dev/&quot;>;Zarr&lt;/a>; format at different resolutions, for example, a comprehensive copy of the &lt;a href=&quot;https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.3803&quot;>;ERA5&lt;/a>; dataset used to train most ML models. This is part of a larger Google effort to provide &lt;a href=&quot;https://github.com/google-research/arco-era5&quot;>;analysis-ready, cloud-optimized weather and climate datasets&lt;/a>; to the research community and &lt;a href=&quot;https://github.com/google-research/arco-era5#roadmap&quot;>;beyond&lt;/a>;. Since downloading these data from the respective archives and converting them can be time-consuming and compute-intensive, we hope that this should considerably lower the entry barrier for the community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Assessing forecast skill&lt;/h2>; &lt;p>; Together with our collaborators from &lt;a href=&quot;https://www.ecmwf.int/&quot;>;ECMWF&lt;/a>;, we defined a set of headline scores that best capture the quality of global weather forecasts. As the figure below shows, several of the ML-based forecasts have lower errors than the &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts&quot;>;state-of-the-art physical models&lt;/a>; on deterministic metrics. This holds for a range of variables and regions, and underlines the competitiveness and promise of ML-based approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9XtxfFzMCqVOuSFzlg-7pAfYMQYN7YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9XtxfFzMCqVOuSFzlg-7pAfYMQYN7YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;This scorecard shows the skill of different models compared to ECMWF&#39;s &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/changes-ecmwf-model&quot;>;Integrated Forecasting System&lt;/a>; (IFS), one of the best physics-based weather forecasts, for several variables. IFS forecasts are evaluated against IFS analysis. All other models are evaluated against ERA5. The order of ML models reflects publication date.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Toward reliable probabilistic forecasts&lt;/h2>; &lt;p>; However, a single forecast often isn&#39;t enough. Weather is inherently chaotic because of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Butterfly_effect&quot;>;butterfly effect&lt;/a>;. For this reason, operational weather centers now run ~50 slightly perturbed realizations of their model, called an ensemble, to estimate the forecast probability distribution across various scenarios. This is important, for example, if one wants to know the likelihood of extreme weather. &lt;/p>; &lt;p>; Creating reliable probabilistic forecasts will be one of the next key challenges for global ML models. Regional ML models, such as Google&#39;s &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;MetNet&lt;/a>; already estimate probabilities. To anticipate this next generation of global models, WB2 already provides probabilistic metrics and baselines, among them &lt;a href=&quot;https://www.ecmwf.int/en/about/media-centre/focus/2017/fact-sheet-ensemble-weather-forecasting&quot;>;ECMWF&#39;s IFS ensemble&lt;/a>;, to accelerate research in this direction. &lt;/p>; &lt;p>; As mentioned above, weather forecasting has many aspects, and while the headline metrics try to capture the most important aspects of forecast skill, they are by no means sufficient. One example is forecast realism. Currently, many ML forecast models tend to “hedge their bets” in the face of the intrinsic uncertainty of the atmosphere. In other words, they tend to predict smoothed out fields that give lower average error but do not represent a realistic, physically consistent state of the atmosphere. An example of this can be seen in the animation below. The two data-driven models, Pangu-Weather and GraphCast (bottom), predict the large-scale evolution of the atmosphere remarkably well. However, they also have less small-scale structure compared to the ground truth or the physical forecasting model IFS HRES (top). In WB2 we include a range of these case studies and also a spectral metric that quantifies such blurring. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-ZkxF_5hdQslK1PKIgB0HwKeCdYUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MscBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s1200/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1020&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-ZkxF_5hdQslK1PKIgB0HwKeCdYUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MscBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Forecasts of a front passing through the continental United States initialized on January 3, 2020. Maps show temperature at a pressure level of 850 &lt;a href=&quot;https://sites.research.google/weatherbench/faq/&quot;>;hPa&lt;/a>; (roughly equivalent to an altitude of 1.5km) and &lt;a href=&quot;https://sites.research.google/weatherbench/faq/&quot;>;geopotential&lt;/a>; at a pressure level of 500 hPa (roughly 5.5 km) in contours. ERA5 is the corresponding ground-truth analysis, IFS HRES is ECMWF&#39;s physics-based forecasting model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; WeatherBench 2 will continue to evolve alongside ML model development. The &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;official website&lt;/a>; will be updated with the latest state-of-the-art models. (To submit a model, please follow &lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/submit.html&quot;>;these instructions&lt;/a>;). We also invite the community to provide feedback and suggestions for improvements through issues and pull requests on the &lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;WB2 GitHub page&lt;/a>;. &lt;/p>; &lt;p>; Designing evaluation well and targeting the right metrics is crucial in order to make sure ML weather models benefit society as quickly as possible. WeatherBench 2 as it is now is just the starting point. We plan to extend it in the future to address key issues for the future of ML-based weather forecasting. Specifically, we would like to add station observations and better precipitation datasets. Furthermore, we will explore the inclusion of nowcasting and subseasonal-to-seasonal predictions to the benchmark. &lt;/p>; &lt;p>; We hope that WeatherBench 2 can aid researchers and end-users as weather forecasting continues to evolve. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;WeatherBench 2 is the result of collaboration across many different teams at Google and external collaborators at ECMWF. From ECMWF, we would like to thank Matthew Chantry, Zied Ben Bouallegue and Peter Dueben. From Google, we would like to thank the core contributors to the project: Stephan Rasp, Stephan Hoyer, Peter Battaglia, Alex Merose, Ian Langmore, Tyler Russell, Alvaro Sanchez, Antonio Lobato, Laurence Chiu, Rob Carver, Vivian Yang, Shreya Agrawal, Thomas Turnbull, Jason Hickey, Carla Bromberg, Jared Sisk, Luke Barrington, Aaron Bell, and Fei Sha. We also would like to thank Kunal Shah, Rahul Mahrsee, Aniket Rawat, and Satish Kumar. Thanks to John Anderson for sponsoring WeatherBench 2. Furthermore, we would like to thank Kaifeng Bi from the Pangu-Weather team and Ryan Keisler for their help in adding their models to WeatherBench 2.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6223088894812086013/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html&quot; rel=&quot;alternate&quot; title=&quot;WeatherBench 2: A benchmark for the next generation of data-driven weather models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFdjdsi2hvZyKcB0ODholetAjBMQ43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s72-c/weatherbenchgif.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1342410539466537463&lt;/id>;&lt;published>;2023-08-30T12:34:00.002-07:00&lt;/published>;&lt;updated>;2023-08-30T12:39:33.664-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Modeling and improving text stability in live captions&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Vikas Bahirwani, Research Scientist, and Susan Xu, Software Engineer, Google Augmented Reality&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45-muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U5-8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s2300/stabilizedcaptions.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Automatic speech recognition (ASR) technology has made conversations more accessible with live captions in remote conferencing software, mobile applications, and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3379337.3415817&quot;>;head-worn displays&lt;/a>;. However, to maintain real-time responsiveness, live caption systems often display interim predictions that are updated as new utterances are received. This can cause &lt;em>;text instability (&lt;/em>;a “flicker” where previously displayed text is updated, shown in the captions on the left in the video below), which can impair users&#39; reading experience due to distraction, fatigue, and difficulty following the conversation. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://research.google/pubs/pub52450/&quot;>;Modeling and Improving Text Stability in Live Captions&lt;/a>;”, presented at &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/98883&quot;>;ACM CHI 2023&lt;/a>;, we formalize this problem of text stability through a few key contributions. First, we quantify the text instability by employing a vision-based flicker metric that uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Luminance&quot;>;luminance&lt;/a>; contrast and &lt;a href=&quot;https://en.wikipedia.org/wiki/Discrete_Fourier_transform&quot;>;discrete Fourier transform&lt;/a>;. Second, we also introduce a stability algorithm to stabilize the rendering of live captions via tokenized alignment, semantic merging, and smooth animation. Finally, we conducted a user study (N=123) to understand viewers&#39; experience with live captioning&lt;strong>;. &lt;/strong>;Our statistical analysis demonstrates a strong correlation between our proposed flicker metric and viewers&#39; experience. Furthermore, it shows that our proposed stabilization techniques significantly improves viewers&#39; experience (eg, the captions on the right in the video above). &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/-hiAnT6QkmU?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;-hiAnT6QkmU&quot;>;&lt;/iframe>;&lt;/div>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Raw ASR captions vs. stabilized captions&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Metric&lt;/h2>; &lt;p>; Inspired by &lt;a href=&quot;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/5203/0000/Toward-perceptual-metrics-for-video-watermark-evaluation/10.1117/12.512550.full&quot;>;previous work&lt;/a>;, we propose a flicker-based metric to quantify text stability and objectively evaluate the performance of live captioning systems. Specifically, our goal is to quantify the flicker in a grayscale live caption video. We achieve this by comparing the difference in luminance between individual frames (frames in the figures below) that constitute the video. Large visual changes in luminance are obvious (eg, addition of the word “bright” in the figure on the bottom), but subtle changes (eg, update from “... this gold. Nice..” to “... this. Gold is nice”) may be difficult to discern for readers. However, converting the change in luminance to its constituting frequencies exposes both the obvious and subtle changes. &lt;/p>; &lt;p>; Thus, for each pair of contiguous frames, we convert the difference in luminance into its constituting frequencies using discrete Fourier transform. We then sum over each of the low and high frequencies to quantify the flicker in this pair. Finally, we average over all of the frame-pairs to get a per-video flicker. &lt;/p>; &lt;p>; For instance, we can see below that two identical frames (top) yield a flicker of 0, while two non-identical frames (bottom) yield a non-zero flicker. It is worth noting that higher values of the metric indicate high flicker in the video and thus, a worse user experience than lower values of the metric. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBfLLlGqieWGTJT4qRV_ZeUkUvQ3aYHkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s1399/noflicker.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;268&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBfLLlGqieWGTJT4qRV_ZeUkUvQ3aYHkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s16000/noflicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the flicker metric between two identical frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNeHPRxE1XIotPM7DZE-LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11-7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s1399/flicker.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;263&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNeHPRxE1XIotPM7DZE-LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11-7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s16000/flicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the flicker between two non-identical frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Stability algorithm &lt;/h2>; &lt;p>; To improve the stability of live captions, we propose an algorithm that takes as input already rendered sequence of tokens (eg, “Previous” in the figure below) and the new sequence of ASR predictions, and outputs an updated stabilized text (eg, “Updated text (with stabilization)” below). It considers both the natural language understanding (NLU) aspect as well as the ergonomic aspect (display, layout, etc.) of the user experience in deciding when and how to produce a stable updated text. Specifically, our algorithm performs tokenized alignment, semantic merging, and smooth animation to achieve this goal. In what follows, a token is defined as a word or punctuation produced by ASR. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45w4_GiltlNjsU3dz82vn9qhoTJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s1995/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;1995&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45w4_GiltlNjsU3dz82vn9qhoTJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show (a) the previously already rendered text, (b) the baseline layout of updated text without our merging algorithm, and (c) the updated text as generated by our stabilization algorithm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />;&lt;p>;&lt;br />;&lt;/p>;&lt;p>;&lt;br />;&lt;/p>; &lt;p>; Our algorithm address the challenge of producing stabilized updated text by first identifying three classes of changes (highlighted in red, green, and blue below): &lt;/p>; &lt;ol type=&quot;A&quot;>; &lt;li>;Red: Addition of tokens to the end of previously rendered captions (eg, &quot;How about&#39;&#39;). &lt;/li>; &lt;li>;Green: Addition / deletion of tokens, in the middle of already rendered captions. &lt;ul>; &lt;li>;B1: Addition of tokens (eg, &quot;I&#39;&#39; and &quot;friends&#39;&#39;). These may or may not affect the overall comprehension of the captions, but may lead to layout change. Such layout changes are not desired in live captions as they cause significant jitter and poorer user experience. Here “I” does not add to the comprehension but “friends” does. Thus, it is important to balance updates with stability specially for B1 type tokens. &lt;/li>; &lt;li>;B2: Removal of tokens, eg, &quot;in&#39;&#39; is removed in the updated sentence. &lt;/li>; &lt;/ul>; &lt;/li>; &lt;li>;Blue: Re-captioning of tokens: This includes token edits that may or may not have an impact on the overall comprehension of the captions. &lt;/li>;&lt;ul>; &lt;li>;C1: Proper nouns like &quot;disney land&#39;&#39; are updated to &quot;Disneyland&#39;&#39;. &lt;/li>; &lt;li>;C2: Grammatical shorthands like &quot;it&#39;s&#39;&#39; are updated to &quot;It was&#39;&#39;. &lt;/li>; &lt;/ul>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0wEYXbx4wXxWiW-r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-04qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLhC8Db2aLqq/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0wEYXbx4wXxWiW-r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-04qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLhC8Db2aLqq/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Classes of changes between previously displayed and updated text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Alignment, merging, and smoothing &lt;/h2>; &lt;p>; To maximize text stability, our goal is to align the old sequence with the new sequence using updates that make minimal changes to the existing layout while ensuring accurate and meaningful captions. To achieve this, we leverage a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm#:~:text=The%20Needleman%E2%80%93Wunsch%20algorithm%20is%20still%20widely%20used%20for%20optimal,alignments%20having%20the%20highest%20score.&quot;>;Needleman-Wunsch algorithm&lt;/a>; with dynamic programming to merge the two sequences depending on the class of tokens as defined above: &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;Case A tokens:&lt;/b>; We directly add case A tokens, and line breaks as needed to fit the updated captions. &lt;/li>;&lt;li>;&lt;b>;Case B tokens:&lt;/b>; Our preliminary studies showed that users preferred stability over accuracy for previously displayed captions. Thus, we only update case B tokens if the updates do not break an existing line layout. &lt;/li>;&lt;li>;&lt;b>;Case C tokens:&lt;/b>; We compare the semantic similarity of case C tokens by transforming original and updated sentences into sentence embeddings, measuring their dot-product, and updating them only if they are semantically different (similarity &amp;lt; 0.85) and the update will not cause new line breaks. &lt;/li>; &lt;/ul>; &lt;p>; Finally, we leverage animations to reduce visual jitter. We implement smooth scrolling and fading of newly added tokens to further stabilize the overall layout of the live captions. &lt;/p>; &lt;h2>;User evaluation&lt;/h2>; &lt;p>; We conducted a user study with 123 participants to (1) examine the correlation of our proposed flicker metric with viewers&#39; experience of the live captions, and (2) assess the effectiveness of our stabilization techniques. &lt;/p>; &lt;p>; We manually selected 20 videos in YouTube to obtain a broad coverage of topics including video conferences, documentaries, academic talks, tutorials, news, comedy, and more. For each video, we selected a 30-second clip with at least 90% speech. &lt;/p>; &lt;p>; We prepared four types of renderings of live captions to compare: &lt;/p>; &lt;ol>; &lt;li>;Raw ASR: raw speech-to-text results from a speech-to-text API. &lt;/li>;&lt;li>;Raw ASR + thresholding: only display interim speech-to-text result if its confidence score is higher than 0.85. &lt;/li>;&lt;li>;Stabilized captions: captions using our algorithm described above with alignment and merging. &lt;/li>;&lt;li>;Stabilized and smooth captions: stabilized captions with smooth animation (scrolling + fading) to assess whether softened display experience helps improve the user experience. &lt;/li>; &lt;/ol>; &lt;p>; We collected user ratings by asking the participants to watch the recorded live captions and rate their assessments of comfort, distraction, ease of reading, ease of following the video, fatigue, and whether the captions impaired their experience. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Correlation between flicker metric and user experience&lt;/h3>; &lt;p>; We calculated &lt;a href=&quot;https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot;>;Spearman&#39;s coefficient&lt;/a>; between the flicker metric and each of the behavioral measurements (values range from -1 to 1, where negative values indicate a negative relationship between the two variables, positive values indicate a positive relationship, and zero indicates no relationship). Shown below, our study demonstrates statistically significant (𝑝 &amp;lt; 0.001) correlations between our flicker metric and users&#39; ratings. The absolute values of the coefficient are around 0.3, indicating a moderate relationship. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;Behavioral Measurement&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;&lt;b>;Correlation to Flickering Metric*&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Comfort &lt;/td>; &lt;td align=&quot;center&quot;>; -0.29 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Distraction &lt;/td>; &lt;td align=&quot;center&quot;>; 0.33 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Easy to read &lt;/td>; &lt;td align=&quot;center&quot;>; -0.31 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Easy to follow videos &lt;/td>; &lt;td align=&quot;center&quot;>; -0.29 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Fatigue &lt;/td>; &lt;td align=&quot;center&quot;>; 0.36 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Impaired Experience &lt;/td>; &lt;td align=&quot;center&quot;>; 0.31 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Spearman correlation tests of our proposed flickering metric. *&lt;em>;p&lt;/em>; &amp;lt; 0.001.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Stabilization of live captions&lt;/h3>; &lt;p>; Our proposed technique (stabilized smooth captions) received consistently better ratings, significant as measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&quot;>;Mann-Whitney U test&lt;/a>; (&lt;em>;p&lt;/em>; &amp;lt; 0.01 in the figure below), in five out of six aforementioned survey statements. That is, users considered the stabilized captions with smoothing to be more comfortable and easier to read, while feeling less distraction, fatigue, and impairment to their experience than other types of rendering. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7yYjqjaNsJ-5K9TsJBMyKDEmy8kS92ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s960/graph.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;387&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7yYjqjaNsJ-5K9TsJBMyKDEmy8kS92ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s16000/graph.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User ratings from 1 (Strongly Disagree) – 7 (Strongly Agree) on survey statements. (**: p&amp;lt;0.01, ***: p&amp;lt;0.001; ****: p&amp;lt;0.0001; ns: non-significant)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future direction&lt;/h2>; &lt;p>; Text instability in live captioning significantly impairs users&#39; reading experience. This work proposes a vision-based metric to model caption stability that statistically significantly correlates with users&#39; experience, and an algorithm to stabilize the rendering of live captions. Our proposed solution can be potentially integrated into existing ASR systems to enhance the usability of live captions for a variety of users, including those with translation needs or those with hearing accessibility needs. &lt;/p>; &lt;p>; Our work represents a substantial step towards measuring and improving text stability. This can be evolved to include language-based metrics that focus on the consistency of the words and phrases used in live captions over time. These metrics may provide a reflection of user discomfort as it relates to language comprehension and understanding in real-world scenarios. We are also interested in conducting &lt;a href=&quot;https://en.wikipedia.org/wiki/Eye_tracking&quot;>;eye-tracking&lt;/a>; studies (eg, videos shown below) to track viewers&#39; gaze patterns, such as eye fixation and saccades, allowing us to better understand the types of errors that are most distracting and how to improve text stability for those. &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowfullscreen=&quot;&quot; youtube-src-id=&quot;1E2jGUscWyc&quot; width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/1E2jGUscWyc?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of tracking a viewer&#39;s gaze when reading raw ASR captions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowfullscreen=&quot;&quot; youtube-src-id=&quot;YfotwPIznL8&quot; width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/YfotwPIznL8?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of tracking a viewer&#39;s gaze when reading stabilized and smoothed captions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;p>; By improving text stability in live captions, we can create more effective communication tools and improve how people connect in everyday conversations in familiar or, through translation, unfamiliar languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Key contributors include Xingyu “Bruce” Liu, Jun Zhang, Leonardo Ferrer, Susan Xu, Vikas Bahirwani, Boris Smus, Alex Olwal, and Ruofei Du. We wish to extend our thanks to our colleagues who provided assistance, including Nishtha Bhatia, Max Spear, and Darcy Philippon. We would also like to thank Lin Li, Evan Parker, and CHI 2023 reviewers.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1342410539466537463/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text-stability.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1342410539466537463&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1342410539466537463&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text-stability.html&quot; rel=&quot;alternate&quot; title=&quot;Modeling and improving text stability in live captions&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45-muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U5-8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s72-c/stabilizedcaptions.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8352967842060136321&lt;/id>;&lt;published>;2023-08-29T12:57:00.001-07:00&lt;/published>;&lt;updated>;2023-08-30T17:14:40.708-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SayTap: Language to quadrupedal locomotion&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yujin Tang and Wenhao Yu, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQqM01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s320/SayTap%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Simple and effective interaction between human and quadrupedal robots paves the way towards creating intelligent and capable helper robots, forging a future where technology enhances our lives in ways beyond our imagination. Key to such human-robot interaction systems is enabling quadrupedal robots to respond to natural language instructions. Recent developments in &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;large language models&lt;/a>; (LLMs) have demonstrated the potential to perform &lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;high-level planning&lt;/a>;. Yet, it remains a challenge for LLMs to comprehend low-level commands, such as joint angle targets or motor torques, especially for inherently unstable legged robots, necessitating high-frequency control signals. Consequently, most &lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;existing&lt;/a>; &lt;a href=&quot;https://code-as-policies.github.io/&quot;>;work&lt;/a>; presumes the provision of high-level APIs for LLMs to dictate robot behavior, inherently limiting the system&#39;s expressive capabilities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://saytap.github.io&quot;>;SayTap: Language to Quadrupedal Locomotion&lt;/a>;”, to be presented at &lt;a href=&quot;https://www.corl2023.org/&quot;>;CoRL 2023&lt;/a>;, we propose an approach that uses foot contact patterns (which refer to the sequence and manner in which a four-legged agent places its feet on the ground while moving) as an interface to bridge human commands in natural language and a locomotion controller that outputs low-level commands. This results in an interactive quadrupedal robot system that allows users to flexibly craft diverse locomotion behaviors (eg, a user can ask the robot to walk, run, jump or make other movements using simple language). We contribute an LLM prompt design, a reward function, and a method to expose the SayTap controller to the feasible distribution of contact patterns. We demonstrate that SayTap is a controller capable of achieving diverse locomotion patterns that can be transferred to real robot hardware. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;SayTap method&lt;/h2>; &lt;p>; The SayTap approach uses a contact pattern template, which is a 4 X &lt;i>;T&lt;/i>; matrix of 0s and 1s, with 0s representing an agent&#39;s feet in the air and 1s for feet on the ground. From top to bottom, each row in the matrix gives the foot contact patterns of the front left (FL), front right (FR), rear left (RL) and rear right (RR) feet. SayTap&#39;s control frequency is 50 Hz, so each 0 or 1 lasts 0.02 seconds. In this work, a desired foot contact pattern is defined by a cyclic sliding window of size L&lt;sub>;w&lt;/sub>; and of shape 4 X L&lt;sub>;w&lt;/sub>;. The sliding window extracts from the contact pattern template four foot ground contact flags, which indicate if a foot is on the ground or in the air between &lt;i>;t&lt;/i>; + 1 and &lt;i>;t&lt;/i>; + L&lt;sub>;w&lt;/sub>;. The figure below provides an overview of the SayTap method. &lt;/p>; &lt;p>; SayTap introduces these desired foot contact patterns as a new interface between natural language user commands and the locomotion controller. The locomotion controller is used to complete the main task (eg, following specified velocities) and to place the robot&#39;s feet on the ground at the specified time, such that the realized foot contact patterns are as close to the desired contact patterns as possible. To achieve this, the locomotion controller takes the desired foot contact pattern at each time step as its input in addition to the robot&#39;s proprioceptive sensory data (eg, joint positions and velocities) and task-related inputs (eg, user-specified velocity commands). We use &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_reinforcement_learning&quot;>;deep reinforcement learning&lt;/a>; to train the locomotion controller and represent it as a deep neural network. During controller training, a random generator samples the desired foot contact patterns, the policy is then optimized to output low-level robot actions to achieve the desired foot contact pattern. Then at test time a LLM translates user commands into foot contact patterns. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2aLrGlJ9CTOIkfI-G8jVZrYE083_U84CCOq2pGuewb4szwCvoRc8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/s935/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;935&quot; height=&quot;297&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2aLrGlJ9CTOIkfI-G8jVZrYE083_U84CCOq2pGuewb4szwCvoRc8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/w640-h297/image13.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap approach overview.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/saytap_blog.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap uses foot contact patterns (eg, 0 and 1 sequences for each foot in the inset, where 0s are foot in the air and 1s are foot on the ground) as an interface that bridges natural language user commands and low-level control commands. With a reinforcement learning-based locomotion controller that is trained to realize the desired contact patterns, SayTap allows a quadrupedal robot to take both simple and direct instructions (eg, “Trot forward slowly.”) as well as vague user commands (eg, “Good news, we are going to a picnic this weekend!”) and react accordingly.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We demonstrate that the LLM is capable of accurately mapping user commands into foot contact pattern templates in specified formats when given properly designed prompts, even in cases when the commands are unstructured or vague. In training, we use a random pattern generator to produce contact pattern templates that are of various pattern lengths &lt;i>;T&lt;/i>;, foot-ground contact ratios within a cycle based on a given gait type &lt;i>;G&lt;/i>;, so that the locomotion controller gets to learn on a wide distribution of movements leading to better generalization. See the &lt;a href=&quot;https://arxiv.org/abs/2306.07580&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; With a simple prompt that contains only three in-context examples of commonly seen foot contact patterns, an LLM can translate various human commands accurately into contact patterns and even generalize to those that do not explicitly specify how the robot should react. &lt;/p>; &lt;p>; SayTap prompts are concise and consist of four components: (1) general instruction that describes the tasks the LLM should accomplish; (2) gait definition that reminds the LLM of basic knowledge about quadrupedal gaits and how they can be related to emotions; (3) output format definition; and (4) examples that give the LLM chances to learn in-context. We also specify five velocities that allow a robot to move forward or backward, fast or slow, or remain still. &lt;/p>; &lt;span style=&quot;font-size: small;&quot;>; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;&lt;font color=&quot;#0000ff&quot;>; General instruction block&lt;/font>; You are a dog foot contact pattern expert. Your job is to give a velocity and a foot contact pattern based on the input. You will always give the output in the correct format no matter what the input is. &lt;font color=&quot;#0000ff&quot;>;Gait definition block&lt;/font>; The following are description about gaits: 1. Trotting is a gait where two diagonally opposite legs strike the ground at the same time. 2. Pacing is a gait where the two legs on the left/right side of the body strike the ground at the same time. 3. Bounding is a gait where the two front/rear legs strike the ground at the same time. It has a longer suspension phase where all feet are off the ground, for example, for at least 25% of the cycle length. This gait also gives a happy feeling. &lt;font color=&quot;#0000ff&quot;>;Output format definition block&lt;/font>; The following are rules for describing the velocity and foot contact patterns: 1. You should first output the velocity, then the foot contact pattern. 2. There are five velocities to choose from: [-1.0, -0.5, 0.0, 0.5, 1.0]. 3. A pattern has 4 lines, each of which represents the foot contact pattern of a leg. 4. Each line has a label. &quot;FL&quot; is front left leg, &quot;FR&quot; is front right leg, &quot;RL&quot; is rear left leg, and &quot;RR&quot; is rear right leg. 5. In each line, &quot;0&quot; represents foot in the air, &quot;1&quot; represents foot on the ground. &lt;font color=&quot;#0000ff&quot;>;Example block&lt;/font>; Input: Trot slowly Output: 0.5 FL: 11111111111111111000000000 FR: 00000000011111111111111111 RL: 00000000011111111111111111 RR: 11111111111111111000000000 Input: Bound in place Output: 0.0 FL: 11111111111100000000000000 FR: 11111111111100000000000000 RL: 00000011111111111100000000 RR: 00000011111111111100000000 Input: Pace backward fast Output: -1.0 FL: 11111111100001111111110000 FR: 00001111111110000111111111 RL: 11111111100001111111110000 RR: 00001111111110000111111111 Input: &lt;/pre>;&lt;/span>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap prompt to the LLM. Texts in blue are used for illustration and are not input to LLM.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Following simple and direct commands&lt;/h3>; &lt;p>; We demonstrate in the videos below that the SayTap system can successfully perform tasks where the commands are direct and clear. Although some commands are not covered by the three in-context examples, we are able to guide the LLM to express its internal knowledge from the pre-training phase via the “Gait definition block” (see the second block in our prompt above) in the prompt. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Following unstructured or vague commands&lt;/h3>; &lt;p>; But what is more interesting is SayTap&#39;s ability to process unstructured and vague instructions. With only a little hint in the prompt to connect certain gaits with general impressions of emotions, the robot bounds up and down when hearing exciting messages, like “We are going to a picnic!” Furthermore, it also presents the scenes accurately (eg, moving quickly with its feet barely touching the ground when told the ground is very hot). &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We present SayTap, an interactive system for quadrupedal robots that allows users to flexibly craft diverse locomotion behaviors. SayTap introduces desired foot contact patterns as a new interface between natural language and the low-level controller. This new interface is straightforward and flexible, moreover, it allows a robot to follow both direct instructions and commands that do not explicitly state how the robot should react. &lt;/p>; &lt;p>; One interesting direction for future work is to test if commands that imply a specific feeling will allow the LLM to output a desired gait. In the gait definition block shown in the results section above, we provide a sentence that connects a happy mood with bounding gaits. We believe that providing more information can augment the LLM&#39;s interpretations (eg, implied feelings). In our evaluation, the connection between a happy feeling and a bounding gait led the robot to act vividly when following vague human commands. Another interesting direction for future work is to introduce multi-modal inputs, such as videos and audio. Foot contact patterns translated from those signals will, in theory, still work with our pipeline and will unlock many more interesting use cases. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, Aleksandra Faust and Tatsuya Harada conducted this research. This work was conceived and performed while the team was in Google Research and will be continued at Google DeepMind. The authors would like to thank Tingnan Zhang, Linda Luu, Kuang-Huei Lee, Vincent Vanhoucke and Douglas Eck for their valuable discussions and technical support in the experiments.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8352967842060136321/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html&quot; rel=&quot;alternate&quot; title=&quot;SayTap: Language to quadrupedal locomotion&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQqM01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s72-c/SayTap%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-781660063956467509&lt;/id>;&lt;published>;2023-08-28T09:59:00.003-07:00&lt;/published>;&lt;updated>;2023-08-29T01:37:19.871-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dahun Kim and Weicheng Kuo, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVuIj5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s1100/RO-ViT-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The ability to detect objects in the visual world is crucial for computer vision and machine intelligence, enabling applications like adaptive autonomous agents and versatile shopping systems. However, modern object detectors are limited by the manual annotations of their training data, resulting in a vocabulary size significantly smaller than the vast array of objects encountered in reality. To overcome this, the &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection task&lt;/a>; (OVD) has emerged, utilizing image-text pairs for training and incorporating new category names at test time by associating them with the image content. By treating categories as text embeddings, open-vocabulary detectors can predict a wide range of unseen objects. Various techniques such as &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;image-text pre-training&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;knowledge distillation&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.09106&quot;>;pseudo labeling&lt;/a>;, and frozen models, often employing &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural network&lt;/a>; (CNN) backbones, have been proposed. With the growing popularity of &lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;>;vision transformers&lt;/a>; (ViTs), it is important to explore their potential for building proficient open-vocabulary detectors. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The existing approaches assume the availability of pre-trained &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;vision-language models&lt;/a>; (VLMs) and focus on fine-tuning or &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;distillation&lt;/a>; from these models to address the disparity between image-level pre-training and object-level fine-tuning. However, as VLMs are primarily designed for image-level tasks like classification and retrieval, they do not fully leverage the concept of objects or regions during the pre-training phase. Thus, it could be beneficial for open-vocabulary detection if we build locality information into the image-text pre-training. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.07011&quot;>;RO-ViT: Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers&lt;/a>;”, presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we introduce a simple method to pre-train vision transformers in a region-aware manner to improve open-vocabulary detection. In vision transformers, positional embeddings are added to image patches to encode information about the spatial position of each patch within the image. Standard pre-training typically uses full-image positional embeddings, which does not generalize well to detection tasks. Thus, we propose a new positional embedding scheme, called “cropped positional embedding”, that better aligns with the use of region crops in detection fine-tuning. In addition, we replace the &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function#Neural_networks&quot;>;softmax cross entropy loss&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;>;focal loss&lt;/a>; in contrastive image-text learning, allowing us to learn from more challenging and informative examples. Finally, we leverage recent advances in novel object proposals to enhance open-vocabulary detection fine-tuning, which is motivated by the observation that existing methods often miss novel objects during the proposal stage due to overfitting to foreground categories. We are also releasing the code &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm/rovit&quot;>;here&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Region-aware image-text pre-training&lt;/h2>; &lt;p>; Existing VLMs are trained to match an image as a whole to a text description. However, we observe there is a mismatch between the way the positional embeddings are used in the existing contrastive pre-training approaches and open-vocabulary detection. The positional embeddings are important to transformers as they provide the information of where each element in the set comes from. This information is often useful for downstream recognition and localization tasks. Pre-training approaches typically apply full-image positional embeddings during training, and use the same positional embeddings for downstream tasks, eg, zero-shot recognition. However, the recognition occurs at region-level for open-vocabulary detection fine-tuning, which requires the full-image positional embeddings to generalize to regions that they never see during the pre-training. &lt;/p>; &lt;p>; To address this, we propose &lt;em>;cropped positional embeddings&lt;/em>; (CPE). With CPE, we upsample positional embeddings from the image size typical for pre-training, eg, 224x224 pixels, to that typical for detection tasks, eg, 1024x1024 pixels. Then we randomly crop and resize a region, and use it as the image-level positional embeddings during pre-training. The position, scale, and aspect ratio of the crop is randomly sampled. Intuitively, this causes the model to view an image not as a full image in itself, but as a region crop from some larger unknown image. This better matches the downstream use case of detection where recognition occurs at region- rather than image-level. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZqTJq4heQruWCeaQsM5uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s952/RO-ViT-img6.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;952&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZqTJq4heQruWCeaQsM5uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s16000/RO-ViT-img6.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;For the pre-training, we propose&amp;nbsp;&lt;em>;cropped positional embedding&lt;/em>;&amp;nbsp;(CPE) which randomly crops and resizes a region of positional embeddings instead of using the whole-image positional embedding (PE). In addition, we use focal loss instead of the common softmax cross entropy loss for contrastive learning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also find it beneficial to learn from hard examples with a focal loss. Focal loss enables finer control over how hard examples are weighted than what the softmax cross entropy loss can provide. We adopt the focal loss and replace it with the softmax cross entropy loss in both image-to-text and text-to-image losses. Both CPE and focal loss introduce no extra parameters and minimal computation costs. &lt;/p>; &lt;br />; &lt;h2>;Open-vocabulary detector fine-tuning&lt;/h2>; &lt;p>; An open-vocabulary detector is trained with the detection labels of &#39;base&#39; categories, but needs to detect the union of &#39;base&#39; and &#39;novel&#39; (unlabeled) categories at test time. Despite the backbone features pre-trained from the vast open-vocabulary data, the added detector layers (neck and heads) are newly trained with the downstream detection dataset. Existing approaches often miss novel/unlabeled objects in the object proposal stage because the proposals tend to classify them as background. To remedy this, we leverage recent advances in a novel object proposal method and adopt the localization quality-based objectness (ie, &lt;a href=&quot;https://arxiv.org/abs/2108.06753&quot;>;centerness&lt;/a>; score) instead of object-or-not binary classification score, which is combined with the detection score. During training, we compute the detection scores for each detected region as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; between the region&#39;s embedding (computed via &lt;a href=&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks&quot;>;RoI-Align&lt;/a>; operation) and the text embeddings of the base categories. At test time, we append the text embeddings of novel categories, and the detection score is now computed with the union of the base and novel categories. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw7MCZE0IsOhdkt8ny6ipDfFi4BxNOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s682/RO-ViT-img4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;682&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw7MCZE0IsOhdkt8ny6ipDfFi4BxNOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s16000/RO-ViT-img4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The pre-trained ViT backbone is transferred to the downstream open-vocabulary detection by replacing the global average pooling with detector heads. The&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks#:~:text=new%20method%20called-,ROIAlign,-%2C%20which%20can%20represent&quot; style=&quot;text-align: left;&quot;>;RoI-Align&lt;/a>;&amp;nbsp;embeddings are matched with the cached category embeddings to obtain the VLM score, which is combined with the detection score into the open-vocabulary detection score.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate RO-ViT on the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; open-vocabulary detection benchmark. At the system-level, our best model achieves 33.6 box&amp;nbsp;&lt;a href=&quot;https://blog.paperspace.com/mean-average-precision/&quot;>;average precision&lt;/a>;&amp;nbsp;on rare categories (&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;AP&lt;sub>;r&lt;/sub>;&lt;/a>;) and 32.1 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;mask AP&lt;sub>;r&lt;/sub>;&lt;/a>;, which outperforms&amp;nbsp;the best existing ViT-based approach OWL-ViT by 8.0 AP&lt;sub>;r&lt;/sub>;&amp;nbsp;and the best CNN-based approach ViLD-Ens by 5.8 mask AP&lt;sub>;r&lt;/sub>;. It also exceeds the performance of many other approaches based on knowledge distillation, pre-training, or joint training with weak supervision.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel-0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQHrll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;426&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel-0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQHrll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RO-ViT outperforms both the state-of-the-art (SOTA)&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2205.06230&quot; style=&quot;text-align: left;&quot;>;ViT-based&lt;/a>;&amp;nbsp;and&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot; style=&quot;text-align: left;&quot;>;CNN-based&lt;/a>;&amp;nbsp;methods on LVIS open-vocabulary detection benchmark. We show mask AP on rare categories (AP&lt;sub>;r&lt;/sub>;) , except for SOTA ViT-based (OwL-ViT) where we show box AP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Apart from evaluating region-level representation through open-vocabulary detection, we evaluate the image-level representation of RO-ViT in image-text retrieval through the &lt;a href=&quot;https://arxiv.org/abs/1504.00325&quot;>;MS-COCO&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1505.04870&quot;>;Flickr30K&lt;/a>; benchmarks. Our model with 303M ViT outperforms the state-of-the-art CoCa model with 1B ViT on MS COCO, and is on par on Flickr30K. This shows that our pre-training method not only improves the region-level representation but also the global image-level representation for retrieval. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKVrwp3_wXzf8KGqaY50rK3fLcWtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s1692/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;940&quot; data-original-width=&quot;1692&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKVrwp3_wXzf8KGqaY50rK3fLcWtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show zero-shot image-text retrieval on MS COCO and Flickr30K benchmarks, and compare with dual-encoder methods. We report recall@1 (top-1 recall) on image-to-text (I2T) and text-to-image (T2I) retrieval tasks. RO-ViT outperforms the state-of-the-art&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot; style=&quot;text-align: left;&quot;>;CoCa&lt;/a>;&amp;nbsp;with the same backbone.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFfI7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT-03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s1476/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;362&quot; data-original-width=&quot;1476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFfI7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT-03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RO-ViT open-vocabulary detection on LVIS. We only show the novel categories for clarity. RO-ViT detects many novel categories that it has never seen during detection training: “fishbowl”, “sombrero”, “persimmon”, “gargoyle”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Visualization of positional embeddings&lt;/h2>; &lt;p>; We visualize and compare the learned positional embeddings of RO-ViT with the baseline. Each tile is the cosine similarity between positional embeddings of one patch and all other patches. For example, the tile in the top-left corner (marked in red) visualizes the similarity between the positional embedding of the location (row=1, column=1) and those positional embeddings of all other locations in 2D. The brightness of the patch indicates how close the learned positional embeddings of different locations are. RO-ViT forms more distinct clusters at different patch locations showing symmetrical global patterns around the center patch. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwStwANdQkmi6f2w62aXNhkEu3daexyXV5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s1686/RO-ViT-img1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1686&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwStwANdQkmi6f2w62aXNhkEu3daexyXV5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s16000/RO-ViT-img1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Each tile shows the cosine similarity between the positional embedding of the patch (at the indicated row-column position) and the positional embeddings of all other patches. ViT-B/16 backbone is used.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present RO-ViT, a contrastive image-text pre-training framework to bridge the gap between image-level pre-training and open-vocabulary detection fine-tuning. Our methods are simple, scalable, and easy to apply to any contrastive backbones with minimal computation overhead and no increase in parameters. RO-ViT achieves the state-of-the-art on LVIS open-vocabulary detection benchmark and on the image-text retrieval benchmarks, showing the learned representation is not only beneficial at region-level but also highly effective at the image-level. We hope this study can help the research on open-vocabulary detection from the perspective of image-text pre-training which can benefit both region-level and image-level tasks. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Dahun Kim, Anelia Angelova, and Weicheng Kuo conducted this work and are now at Google DeepMind. We would like to thank our colleagues at Google Research for their advice and helpful discussions. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/781660063956467509/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVuIj5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s72-c/RO-ViT-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3964459643854458124&lt;/id>;&lt;published>;2023-08-25T10:38:00.003-07:00&lt;/published>;&lt;updated>;2023-08-25T10:46:59.205-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Perception Fairness&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Susanna Ricco and Utsav Prabhu, co-leads, Perception Fairness Team, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MOHdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJe8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1-inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Google&#39;s &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI research&lt;/a>; is built on a foundation of collaboration — between teams with diverse backgrounds and expertise, between researchers and product developers, and ultimately with the community at large. The Perception Fairness team drives progress by combining deep subject-matter expertise in both computer vision and machine learning (ML) fairness with direct connections to the researchers building the perception systems that power products across Google and beyond. Together, we are working to intentionally design our systems to be inclusive from the ground up, guided by &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google&#39;s AI Principles&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf-_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkYpL7MqBpdeqIJm9H5sani/s948/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;214&quot; data-original-width=&quot;948&quot; height=&quot;144&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf-_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkYpL7MqBpdeqIJm9H5sani/w640-h144/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Perception Fairness research spans the design, development, and deployment of advanced multimodal models including the latest foundation and generative models powering Google&#39;s products.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our team&#39;s mission is to advance the frontiers of fairness and inclusion in multimodal ML systems, especially related to &lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;>;foundation&lt;/a>; models and &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>;. This encompasses core technology components including classification, localization, captioning, retrieval, visual question answering, text-to-image or text-to-video generation, and generative image and video editing. We believe that fairness and inclusion can and should be top-line performance goals for these applications. Our research is focused on unlocking novel analyses and mitigations that enable us to proactively design for these objectives throughout the development cycle. We answer core questions, such as: How can we use ML to responsibly and faithfully model human perception of demographic, cultural, and social identities in order to promote fairness and inclusion? What kinds of system biases (eg, underperforming on images of people with certain skin tones) can we measure and how can we use these metrics to design better algorithms? How can we build more inclusive algorithms and systems and react quickly when failures occur? &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Measuring representation of people in media&lt;/h2>; &lt;p>; ML systems that can edit, curate or create images or videos can affect anyone exposed to their outputs, shaping or reinforcing the beliefs of viewers around the world. Research to reduce representational harms, such as reinforcing stereotypes or denigrating or erasing groups of people, requires a deep understanding of both the content and the &lt;a href=&quot;https://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html&quot;>;societal context&lt;/a>;. It hinges on how different observers perceive themselves, their communities, or how others are represented. There&#39;s considerable debate in the field regarding which social categories should be studied with computational tools and how to do so responsibly. Our research focuses on working toward scalable solutions that are informed by sociology and social psychology, are aligned with human perception, embrace the subjective nature of the problem, and enable nuanced measurement and mitigation. One example is our research on &lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;differences in human perception and annotation of skin tone in images&lt;/a>; using the &lt;a href=&quot;skintone.google&quot;>;Monk Skin Tone scale&lt;/a>;. &lt;/p>; &lt;p>; Our tools are also used to study representation in large-scale content collections. Through our Media Understanding for Social Exploration (MUSE) project, we&#39;ve partnered with academic researchers, nonprofit organizations, and major consumer brands to understand patterns in mainstream media and advertising content. We first published this work in 2017, with a co-authored study analyzing &lt;a href=&quot;https://about.google/intl/ALL_au/main/gender-equality-films/&quot;>;gender equity in Hollywood movies&lt;/a>;. Since then, we&#39;ve increased the scale and depth of our analyses. In 2019, we released findings based on &lt;a href=&quot;https://www.thinkwithgoogle.com/feature/diversity-inclusion/&quot;>;over 2.7 million YouTube advertisements&lt;/a>;. In the &lt;a href=&quot;https://blog.google/technology/ai/using-ai-to-study-12-years-of-representation-in-tv/&quot;>;latest study&lt;/a>;, we examine representation across intersections of perceived gender presentation, perceived age, and skin tone in over twelve years of popular US television shows. These studies provide insights for content creators and advertisers and further inform our own research. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFuL3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPtS8ONz02Nrgn4If7YOukbw0txEfy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/s800/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;800&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFuL3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPtS8ONz02Nrgn4If7YOukbw0txEfy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/w640-h360/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration (not actual data) of computational signals that can be analyzed at scale to reveal representational patterns in media collections. [Video Collection / Getty Images]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Moving forward, we&#39;re expanding the ML fairness concepts on which we focus and the domains in which they are responsibly applied. Looking beyond photorealistic images of people, we are working to develop tools that model the representation of communities and cultures in illustrations, abstract depictions of humanoid characters, and even images with no people in them at all. Finally, we need to reason about not just who is depicted, but how they are portrayed — what narrative is communicated through the surrounding image content, the accompanying text, and the broader cultural context. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Analyzing bias properties of perceptual systems&lt;/h2>; &lt;p>; Building advanced ML systems is complex, with multiple stakeholders informing various criteria that decide product behavior. Overall quality has historically been defined and measured using summary statistics (like overall accuracy) over a test dataset as a proxy for user experience. But not all users experience products in the same way. &lt;br />; &lt;/p>; &lt;p>; Perception Fairness enables practical measurement of nuanced system behavior beyond summary statistics, and makes these metrics core to the system quality that directly informs product behaviors and launch decisions. This is often much harder than it seems. Distilling complex bias issues (eg, disparities in performance across intersectional subgroups or instances of stereotype reinforcement) to a small number of metrics without losing important nuance is extremely challenging. Another challenge is balancing the interplay between fairness metrics and other product metrics (eg, user satisfaction, accuracy, latency), which are often phrased as conflicting despite being compatible. It is common for researchers to describe their work as optimizing an &quot;accuracy-fairness&quot; tradeoff when in reality widespread user satisfaction is aligned with meeting fairness and inclusion objectives. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu2yPn_wK23Z8PdYkeEmycLRJ0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s640/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu2yPn_wK23Z8PdYkeEmycLRJ0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We built and released the &lt;a href=&quot;https://storage.googleapis.com/openimages/web/extended.html#miap&quot;>;MIAP dataset&lt;/a>; as part of &lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;>;Open Images&lt;/a>;, leveraging our research on perception of socially relevant concepts and detection of biased behavior in complex systems to create a resource that furthers ML fairness research in computer vision. Original photo credits — left: &lt;a href=&quot;https://www.flickr.com/photos/boston_public_library/8242010414/&quot;>;Boston Public Library&lt;/a>;; middle: &lt;a href=&quot;https://www.flickr.com/photos/jenrobinson/20183915655/&quot;>;jen robinson&lt;/a>;; right: &lt;a href=&quot;https://www.flickr.com/photos/mrgarin/2484859086/&quot;>;Garin Fons&lt;/a>;; all used with permission under the &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;>;CC- BY 2.0 license&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To these ends, our team focuses on two broad research directions. First, democratizing access to well-understood and widely-applicable fairness analysis tooling, engaging partner organizations in adopting them into product workflows, and informing leadership across the company in interpreting results. This work includes developing broad benchmarks, &lt;a href=&quot;https://ai.googleblog.com/2021/06/a-step-toward-more-inclusive-people.html&quot;>;curating widely-useful high-quality test datasets&lt;/a>; and tooling centered around techniques such as sliced analysis and counterfactual testing — often building on the core representation signals work described earlier. Second, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3461702.3462557&quot;>;advancing novel approaches&lt;/a>; towards fairness analytics — including partnering with &lt;a href=&quot;https://ai.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;product efforts&lt;/a>; that may result in breakthrough findings or &lt;a href=&quot;https://services.google.com/fh/files/blogs/bsr-google-cr-api-hria-executive-summary.pdf&quot;>;inform launch strategy&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Advancing AI responsibly&lt;/h2>; &lt;p>; Our work does not stop with analyzing model behavior. Rather, we use this as a jumping-off point for identifying algorithmic improvements in collaboration with other researchers and engineers on product teams. Over the past year we&#39;ve launched upgraded components that power Search and &lt;a href=&quot;https://blog.google/products/photos/google-photos-memories-view/&quot;>;Memories&lt;/a>; features in Google Photos, leading to more consistent performance and drastically improving robustness through added layers that keep mistakes from cascading through the system. We are working on improving ranking algorithms in Google Images to diversify representation. We updated algorithms that may reinforce historical stereotypes, using additional signals responsibly, such that it&#39;s more likely for &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;everyone to see themselves reflected in Search results and find what they&#39;re looking for&lt;/a>;. &lt;/p>; &lt;p>; This work naturally carries over to the world of generative AI, where &lt;a href=&quot;https://blog.google/technology/research/how-ai-creates-photorealistic-images-from-text/&quot;>;models can create collections of images or videos seeded from image and text prompts&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;can answer questions about images and videos&lt;/a>;. We&#39;re excited about the potential of these technologies to &lt;a href=&quot;https://blog.google/products/shopping/ai-virtual-try-on-google-shopping/&quot;>;deliver new experiences to users&lt;/a>; and as tools to further our own research. To enable this, we&#39;re collaborating across the research and responsible AI communities to develop guardrails that mitigate failure modes. We&#39;re leveraging our tools for understanding representation to power scalable benchmarks that can be combined with human feedback, and investing in research from pre-training through deployment to steer the models to generate higher quality, more inclusive, and more controllable output. We want these models to inspire people, producing diverse outputs, translating concepts without relying on tropes or stereotypes, and providing consistent behaviors and responses across counterfactual variations of prompts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Opportunities and ongoing work&lt;/h2>; &lt;p>; Despite over a decade of focused work, the field of perception fairness technologies still seems like a nascent and fast-growing space, rife with opportunities for breakthrough techniques. We continue to see opportunities to contribute technical advances backed by interdisciplinary scholarship. The gap between what we can measure in images versus the underlying aspects of human identity and expression is large — closing this gap will require increasingly complex media analytics solutions. Data metrics that indicate true representation, situated in the appropriate context and heeding a diversity of viewpoints, remains an open challenge for us. Can we reach a point where we can reliably identify depictions of nuanced stereotypes, continually update them to reflect an ever-changing society, and discern situations in which they could be offensive? Algorithmic advances driven by human feedback point a promising path forward. &lt;/p>; &lt;p>; Recent focus on AI safety and ethics in the context of modern large model development has spurred new ways of thinking about measuring systemic biases. We are exploring multiple avenues to use these models — along with recent developments in concept-based explainability methods, causal inference methods, and cutting-edge UX research — to quantify and minimize undesired biased behaviors. We look forward to tackling the challenges ahead and developing technology that is built for everybody. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank every member of the Perception Fairness team, and all of our collaborators.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3964459643854458124/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Perception Fairness&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MOHdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJe8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1-inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6350776943545232437&lt;/id>;&lt;published>;2023-08-24T15:10:00.000-07:00&lt;/published>;&lt;updated>;2023-08-24T15:10:57.268-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;How to compare a noisy quantum processor to a classical computer&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sergio Boixo and Vadim Smelyanskiy, Principal Scientists, Google Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qsEcLuEDPnWZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; A full-scale error-corrected quantum computer will be able to solve some problems that are impossible for classical computers, but building such a device is a huge endeavor. We are proud of the &lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;milestones&lt;/a>; that we have achieved toward a fully error-corrected quantum computer, but that large-scale computer is still some number of years away. Meanwhile, we are using our current noisy quantum processors as flexible platforms for quantum experiments. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In contrast to an error-corrected quantum &lt;em>;computer&lt;/em>;, experiments in noisy quantum &lt;em>;processors&lt;/em>; are currently limited to a few thousand quantum operations or gates, before noise degrades the quantum state. In 2019 we implemented a specific computational task called &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1666-5&quot;>;random circuit sampling&lt;/a>; on our quantum processor &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;and showed&lt;/a>; for the first time that it outperformed state-of-the-art classical supercomputing. &lt;/p>; &lt;p>; Although they have not yet reached beyond-classical capabilities, we have also used our processors to observe novel physical phenomena, such as &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04257-w&quot;>;time crystals&lt;/a>; and &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;Majorana edge modes&lt;/a>;, and have made new experimental discoveries, such as robust &lt;a href=&quot;https://ai.googleblog.com/2022/12/formation-of-robust-bound-states-of.html&quot;>;bound states&lt;/a>; of interacting photons and the &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;noise-resilience&lt;/a>; of Majorana edge modes of Floquet evolutions. &lt;/p>; &lt;p>; We expect that even in this intermediate, noisy regime, we will find applications for the quantum processors in which useful quantum experiments can be performed much faster than can be calculated on classical supercomputers — we call these &quot;computational applications&quot; of the quantum processors. No one has yet demonstrated such a beyond-classical computational application. So as we aim to achieve this milestone, the question is: What is the best way to compare a quantum experiment run on such a quantum processor to the computational cost of a classical application? &lt;/p>; &lt;p>; We already know how to compare an error-corrected quantum algorithm to a classical algorithm. In that case, the field of &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_complexity&quot;>;computational complexity&lt;/a>; tells us that we can compare their respective computational costs — that is, the number of operations required to accomplish the task. But with our current experimental quantum processors, the situation is not so well defined. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;Effective quantum volume, fidelity and computational cost of noisy quantum processing experiments&lt;/a>;”, we provide a framework for measuring the computational cost of a quantum experiment, introducing the experiment&#39;s “effective quantum volume”, which is the number of quantum operations or gates that contribute to a measurement outcome. We apply this framework to evaluate the computational cost of three recent experiments: our &lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;random circuit sampling&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;experiment&lt;/a>;, our&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>; experiment measuring quantities known as “out of time order correlators” (OTOCs)&lt;/a>;, and a &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;recent experiment on a Floquet evolution&lt;/a>; related to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Ising_model&quot;>;Ising model&lt;/a>;. We are particularly excited about OTOCs because they provide a direct way to experimentally measure the effective quantum volume of a circuit (a sequence of quantum gates or operations), which is itself a computationally difficult task for a classical computer to estimate precisely. OTOCs are also important in &lt;a href=&quot;https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance&quot;>;nuclear magnetic resonance&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Electron_paramagnetic_resonance&quot;>;electron spin resonance spectroscopy&lt;/a>;. Therefore, we believe that OTOC experiments are a promising candidate for a first-ever computational application of quantum processors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd6VsOOMzDaexufiJs_8oyo1G37h2lr4Y1Y28zD7lZ3OIB_EL_LsY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/s1280/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;1280&quot; height=&quot;297&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd6VsOOMzDaexufiJs_8oyo1G37h2lr4Y1Y28zD7lZ3OIB_EL_LsY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/w640-h297/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Plot of computational cost and impact of some recent quantum experiments. While some (eg, &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;QC-QMC 2022&lt;/a>;) have had high impact and others (eg, &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;RCS 2023&lt;/a>;) have had high computational cost, none have yet been both useful and hard enough to be considered a “computational application.” We hypothesize that our future OTOC experiment could be the first to pass this threshold. Other experiments plotted are referenced in the text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Random circuit sampling: Evaluating the computational cost of a noisy circuit&lt;/h2>; &lt;p>; When it comes to running a quantum circuit on a noisy quantum processor, there are two competing considerations. On one hand, we aim to do something that is difficult to achieve classically. The computational cost — the number of operations required to accomplish the task on a classical computer — depends on the quantum circuit&#39;s &lt;em>;effective quantum volume&lt;/em>;: the larger the volume, the higher the computational cost, and the more a quantum processor can outperform a classical one. &lt;/p>; &lt;p>; But on the other hand, on a noisy processor, each quantum gate can introduce an error to the calculation. The more operations, the higher the error, and the lower the fidelity of the quantum circuit in measuring a quantity of interest. Under this consideration, we might prefer simpler circuits with a smaller effective volume, but these are easily simulated by classical computers. The balance of these competing considerations, which we want to maximize, is called the &quot;computational resource&quot;, shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1wOEPS4TwK55cMxJmg19mQD7ZCtuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/s973/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;653&quot; data-original-width=&quot;973&quot; height=&quot;430&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1wOEPS4TwK55cMxJmg19mQD7ZCtuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/w640-h430/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Graph of the tradeoff between quantum volume and noise in a quantum circuit, captured in a quantity called the “computational resource.” For a noisy quantum circuit, this will initially increase with the computational cost, but eventually, noise will overrun the circuit and cause it to decrease. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; We can see how these competing considerations play out in a simple &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;“hello world” program&lt;/a>; for quantum processors, known as random circuit sampling (RCS), which was the first demonstration of a quantum processor outperforming a classical computer. Any error in any gate is likely to make this experiment fail. Inevitably, this is a hard experiment to achieve with significant fidelity, and thus it also serves as a benchmark of system fidelity. But it also corresponds to the highest known computational cost achievable by a quantum processor. We recently reported the &lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;most powerful RCS&lt;/a>; experiment performed to date, with a low measured experimental fidelity of 1.7x10&lt;sup>;-3&lt;/sup>;, and a high theoretical computational cost of ~10&lt;sup>;23&lt;/sup>;. These quantum circuits had 700 two-qubit gates. We estimate that this experiment would take ~47 years to simulate in the world&#39;s largest supercomputer. While this checks one of the two boxes needed for a computational application — it outperforms a classical supercomputer — it is not a particularly useful application &lt;em>;per se&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;OTOCs and Floquet evolution: The effective quantum volume of a local observable&lt;/h2>; &lt;p>; There are many open questions in quantum &lt;a href=&quot;https://en.wikipedia.org/wiki/Many-body_problem&quot;>;many-body physics&lt;/a>; that are classically intractable, so running some of these experiments on our quantum processor has great potential. We typically think of these experiments a bit differently than we do the RCS experiment. Rather than measuring the quantum state of all qubits at the end of the experiment, we are usually concerned with more specific, local physical observables. Because not every operation in the circuit necessarily impacts the observable, a local observable&#39;s effective quantum volume might be smaller than that of the full circuit needed to run the experiment. &lt;/p>; &lt;p>; We can understand this by applying the concept of a light cone from &lt;a href=&quot;https://en.wikipedia.org/wiki/Special_relativity&quot;>;relativity&lt;/a>;, which determines which events in space-time can be causally connected: some events cannot possibly influence one another because information takes time to propagate between them. We say that two such events are outside their respective light cones. In a quantum experiment, we replace the light cone with something called a “butterfly cone,” where the growth of the cone is determined by the butterfly speed — the speed with which information spreads throughout the system. (This speed is characterized by measuring OTOCs, discussed later.) The effective quantum volume of a local observable is essentially the volume of the butterfly cone, including only the quantum operations that are causally connected to the observable. So, the faster information spreads in a system, the larger the effective volume and therefore the harder it is to simulate classically. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY-WpDN34wUfig-rtH1NC7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/s1392/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1392&quot; data-original-width=&quot;1092&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY-WpDN34wUfig-rtH1NC7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/w502-h640/image2.png&quot; width=&quot;502&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A depiction of the effective volume V&lt;sub>;eff&lt;/sub>; of the gates contributing to the local observable B. A related quantity called the effective area A&lt;sub>;eff&lt;/sub>; is represented by the cross-section of the plane and the cone. The perimeter of the base corresponds to the front of information travel that moves with the butterfly velocity v&lt;sub>;B&lt;/sub>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; We apply this framework to a recent &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;experiment&lt;/a>; implementing a so-called Floquet Ising model, a physical model related to the time crystal and Majorana experiments. From the data of this experiment, one can directly estimate an effective fidelity of 0.37 for the largest circuits. With the measured gate error rate of ~1%, this gives an estimated effective volume of ~100. This is much smaller than the light cone, which included two thousand gates on 127 qubits. So, the butterfly velocity of this experiment is quite small. Indeed, we argue that the effective volume covers only ~28 qubits, not 127, using numerical simulations that obtain a larger precision than the experiment. This small effective volume has also been &lt;a href=&quot;https://arxiv.org/abs/2306.17839&quot;>;corroborated&lt;/a>; with the OTOC technique. Although this was a deep circuit, the estimated computational cost is 5x10&lt;sup>;11&lt;/sup>;, almost one trillion times less than the recent RCS experiment. Correspondingly, this experiment can be &lt;a href=&quot;https://arxiv.org/abs/2306.15970&quot;>;simulated&lt;/a>; in less than a second per data point on a single A100 GPU. So, while this is certainly a useful application, it does not fulfill the second requirement of a computational application: substantially outperforming a classical simulation. &lt;/p>; &lt;p>; Information scrambling experiments with OTOCs are a promising avenue for a computational application. OTOCs can tell us important physical information about a system, such as the butterfly velocity, which is critical for precisely measuring the effective quantum volume of a circuit. OTOC experiments with fast entangling gates offer a potential path for a first beyond-classical demonstration of a computational application with a quantum processor. Indeed, in our &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>;experiment&lt;/a>; from 2021 we achieved an effective fidelity of F&lt;sub>;eff &lt;/sub>;~ 0.06 with an experimental signal-to-noise ratio of ~1, corresponding to an effective volume of ~250 gates and a computational cost of 2x10&lt;sup>;12&lt;/sup>;. &lt;/p>; &lt;p>; While these early OTOC experiments are not sufficiently complex to outperform classical simulations, there is a deep physical reason why OTOC experiments are good candidates for the first demonstration of a computational application. Most of the interesting quantum phenomena accessible to near-term quantum processors that are hard to simulate classically correspond to a quantum circuit exploring many, many quantum energy levels. Such evolutions are typically chaotic and standard time-order correlators (TOC) decay very quickly to a purely random average in this regime. There is no experimental signal left. This does not happen for &lt;a href=&quot;https://arxiv.org/abs/2101.08870&quot;>;OTOC measurements&lt;/a>;, which allows us to grow complexity at will, only limited by the error per gate. We anticipate that a reduction of the error rate by half would double the computational cost, pushing this experiment to the beyond-classical regime. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Using the effective quantum volume framework we have developed, we have determined the computational cost of our RCS and OTOC experiments, as well as a recent Floquet evolution experiment. While none of these meet the requirements yet for a computational application, we expect that with improved error rates, an OTOC experiment will be the first beyond-classical, useful application of a quantum processor. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6350776943545232437/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html&quot; rel=&quot;alternate&quot; title=&quot;How to compare a noisy quantum processor to a classical computer&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qsEcLuEDPnWZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3589347607411181063&lt;/id>;&lt;published>;2023-08-24T12:33:00.002-07:00&lt;/published>;&lt;updated>;2023-08-24T12:33:37.720-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Teaching language models to reason algorithmically&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hattie Zhou, Graduate Student at MILA, Hanie Sedghi, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Large language models (LLMs), such as &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, have shown impressive progress in recent years, which have been driven by &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;scaling up models&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;>;training data sizes&lt;/a>;. Nonetheless, a long standing debate has been whether LLMs can reason symbolically (ie, manipulating symbols based on logical rules). For example, LLMs are able to perform simple arithmetic operations when numbers are small, but struggle to perform with large numbers. This suggests that LLMs have not learned the underlying rules needed to perform these arithmetic operations. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While neural networks have powerful &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&quot;>;pattern matching capabilities&lt;/a>;, they are prone to overfitting to spurious statistical patterns in the data. This does not hinder good performance when the training data is large and diverse and the evaluation is in-distribution. However, for tasks that require rule-based reasoning (such as addition), LLMs struggle with out-of-distribution generalization as spurious correlations in the training data are often much easier to exploit than the true rule-based solution. As a result, despite significant progress in a variety of natural language processing tasks, performance on simple arithmetic tasks like addition has remained a challenge. Even with modest improvement of &lt;a href=&quot;https://openai.com/research/gpt-4&quot;>;GPT-4&lt;/a>; on the &lt;a href=&quot;https://arxiv.org/abs/2103.03874&quot;>;MATH&lt;/a>; dataset, &lt;a href=&quot;https://arxiv.org/abs/2303.12712&quot;>;errors are still largely due to arithmetic and calculation mistakes&lt;/a>;. Thus, an important question is whether LLMs are capable of algorithmic reasoning, which involves solving a task by applying a set of abstract rules that define the algorithm. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;Teaching Algorithmic Reasoning via In-Context Learning&lt;/a>;”, we describe an approach that leverages &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; to enable algorithmic reasoning capabilities in LLMs. In-context learning refers to a model&#39;s ability to perform a task after seeing a few examples of it within the context of the model. The task is specified to the model using a prompt, without the need for weight updates. We also present a novel algorithmic prompting technique that enables general purpose language models to achieve strong &lt;a href=&quot;https://arxiv.org/abs/2207.04901&quot;>;generalization&lt;/a>; on arithmetic problems that are more difficult than those seen in the prompt. Finally, we demonstrate that a model can reliably execute algorithms on out-of-distribution examples with an appropriate choice of prompting strategy. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/s1200/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;1200&quot; height=&quot;89&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/w640-h89/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;By providing algorithmic prompts, we can teach a model the rules of arithmetic via in-context learning. In this example, the LLM (word predictor) outputs the correct answer when prompted with an easy addition question (eg, 267+197), but fails when asked a similar addition question with longer digits. However, when the more difficult question is appended with an algorithmic prompt for addition (blue box with white +&lt;strong>; &lt;/strong>;shown below the word predictor), the model is able to answer correctly. Moreover, the model is capable of simulating the multiplication algorithm (&lt;strong>;X&lt;/strong>;) by composing a series of addition calculations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Teaching an algorithm as a skill&lt;/h2>; &lt;p>; In order to teach a model an algorithm as a skill, we develop algorithmic prompting, which builds upon other rationale-augmented approaches (eg, &lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpad&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>;). Algorithmic prompting extracts algorithmic reasoning abilities from LLMs, and has two notable distinctions compared to other prompting approaches: (1) it solves tasks by outputting the steps needed for an algorithmic solution, and (2) it explains each algorithmic step with sufficient detail so there is no room for misinterpretation by the LLM. &lt;/p>; &lt;p>; To gain intuition for algorithmic prompting, let&#39;s consider the task of two-number addition. In a scratchpad-style prompt, we process each digit from right to left and keep track of the carry value (ie, we add a 1 to the next digit if the current digit is greater than 9) at each step. However, the rule of carry is ambiguous after seeing only a few examples of carry values. We find that including explicit equations to describe the rule of carry helps the model focus on the relevant details and interpret the prompt more accurately. We use this insight to develop an algorithmic prompt for two-number addition, where we provide explicit equations for each step of computation and describe various indexing operations in non-ambiguous formats. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/s1584/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1250&quot; data-original-width=&quot;1584&quot; height=&quot;505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/w640-h505/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of various prompt strategies for addition.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Using only three prompt examples of addition with answer length up to five digits, we evaluate performance on additions of up to 19 digits. Accuracy is measured over 2,000 total examples sampled uniformly over the length of the answer. As shown below, the use of algorithmic prompts maintains high accuracy for questions significantly longer than what&#39;s seen in the prompt, which demonstrates that the model is indeed solving the task by executing an input-agnostic algorithm. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s1600/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;548&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Test accuracy on addition questions of increasing length for different prompting methods. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Leveraging algorithmic skills as tool use&lt;/h2>; &lt;p>; To evaluate if the model can leverage algorithmic reasoning in a broader reasoning process, we evaluate performance using grade school math word problems (&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;GSM8k&lt;/a>;). We specifically attempt to replace addition calculations from GSM8k with an algorithmic solution. &lt;/p>; &lt;p>; Motivated by context length limitations and possible interference between different algorithms, we explore a strategy where differently-prompted models interact with one another to solve complex tasks. In the context of GSM8k, we have one model that specializes in informal mathematical reasoning using &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;chain-of-thought prompting&lt;/a>;, and a second model that specializes in addition using &lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;algorithmic prompting&lt;/a>;. The informal mathematical reasoning model is prompted to output specialized tokens in order to call on the addition-prompted model to perform the arithmetic steps. We extract the queries between tokens, send them to the addition-model and return the answer to the first model, after which the first model continues its output. We evaluate our approach using a difficult problem from the GSM8k (GSM8k-Hard), where we randomly select 50 addition-only questions and increase the numerical values in the questions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/s1268/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;330&quot; data-original-width=&quot;1268&quot; height=&quot;167&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/w640-h167/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example from the GSM8k-Hard dataset. The chain-of-thought prompt is augmented with brackets to indicate when an algorithmic call should be performed.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We find that using separate contexts and models with specialized prompts is an effective way to tackle GSM8k-Hard. Below, we observe that the performance of the model with algorithmic call for addition is 2.3x the chain-of-thought baseline. Finally, this strategy presents an example of solving complex tasks by facilitating interactions between LLMs specialized to different skills via in-context learning. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/s716/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;192&quot; data-original-width=&quot;716&quot; height=&quot;108&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/w400-h108/image2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Chain-of-thought (CoT) performance on GSM8k-Hard with or without algorithmic call.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present an approach that leverages &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; and a novel algorithmic prompting technique to unlock algorithmic reasoning abilities in LLMs. Our results suggest that it may be possible to transform longer context into better reasoning performance by providing more detailed explanations. Thus, these findings point to the ability of using or otherwise simulating long contexts and generating more informative rationales as promising research directions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our co-authors Behnam Neyshabur, Azade Nova, Hugo Larochelle and Aaron Courville for their valuable contributions to the paper and great feedback on the blog. We thank Tom Small for creating the animations in this post. This work was done during Hattie Zhou&#39;s internship at Google Research.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3589347607411181063/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html&quot; rel=&quot;alternate&quot; title=&quot;Teaching language models to reason algorithmically&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6641308922483151364&lt;/id>;&lt;published>;2023-08-22T11:47:00.004-07:00&lt;/published>;&lt;updated>;2023-08-24T10:34:33.888-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Language to rewards for robotic skill synthesis&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Wenhao Yu and Fei Xia, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s320/Language%20to%20reward.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Empowering end-users to interactively teach robots to perform novel tasks is a crucial capability for their successful integration into real-world applications. For example, a user may want to teach a robot dog to perform a new trick, or teach a manipulator robot how to organize a lunch box based on user preferences. The recent advancements in &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;large language models&lt;/a>; (LLMs) pre-trained on extensive internet data have shown a promising path towards achieving this goal. Indeed, researchers have explored diverse ways of leveraging LLMs for robotics, from &lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;step-by-step planning&lt;/a>; and &lt;a href=&quot;https://interactive-language.github.io/&quot;>;goal-oriented dialogue&lt;/a>; to &lt;a href=&quot;https://ai.googleblog.com/2022/11/robots-that-write-their-own-code.html&quot;>;robot-code-writing agents&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While these methods impart new modes of compositional generalization, they focus on using language to link together new behaviors from an &lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;existing library of control primitives&lt;/a>; that are either manually engineered or learned &lt;em>;a priori&lt;/em>;. Despite having internal knowledge about robot motions, LLMs struggle to directly output low-level robot commands due to the limited availability of relevant training data. As a result, the expression of these methods are bottlenecked by the breadth of the available primitives, the design of which often requires extensive expert knowledge or massive data collection. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2306.08647&quot;>;Language to Rewards for Robotic Skill Synthesis&lt;/a>;”, we propose an approach to enable users to teach robots novel actions through natural language input. To do so, we leverage reward functions as an interface that bridges the gap between language and low-level robot actions. We posit that reward functions provide an ideal interface for such tasks given their richness in semantics, modularity, and interpretability. They also provide a direct connection to low-level policies through black-box optimization or reinforcement learning (RL). We developed a language-to-reward system that leverages LLMs to translate natural language user instructions into reward-specifying code and then applies &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; to find optimal low-level robot actions that maximize the generated reward function. We demonstrate our language-to-reward system on a variety of robotic control tasks in simulation using a quadruped robot and a dexterous manipulator robot. We further validate our method on a physical robot manipulator. &lt;/p>; &lt;p>; The language-to-reward system consists of two core components: (1) a Reward Translator, and (2) a Motion Controller&lt;em>;. &lt;/em>;The&lt;em>; &lt;/em>;Reward Translator maps natural language instruction from users to reward functions represented as &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;python code&lt;/a>;. The Motion Controller optimizes the given reward function using &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;receding horizon optimization&lt;/a>; to find the optimal low-level robot actions, such as the amount of torque that should be applied to each robot motor. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s720/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLMs cannot directly generate low-level robotic actions due to lack of data in pre-training dataset. We propose to use reward functions to bridge the gap between language and low-level robot actions, and enable novel complex robot motions from natural language instructions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Reward Translator: Translating user instructions to reward functions&lt;/h2>; &lt;p>; The Reward Translator module was built with the goal of mapping natural language user instructions to reward functions. Reward tuning is highly domain-specific and requires expert knowledge, so it was not surprising to us when we found that LLMs trained on generic language datasets are unable to directly generate a reward function for a specific hardware. To address this, we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;in-context learning&lt;/a>; ability of LLMs. Furthermore, we split the Reward Translator into two sub-modules: &lt;em>;Motion Descriptor&lt;/em>; and &lt;em>;Reward Coder&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Motion Descriptor&lt;/h3>; &lt;p>; First, we design a Motion Descriptor that interprets input from a user and expands it into a natural language description of the desired robot motion following a predefined template. This Motion Descriptor turns potentially ambiguous or vague user instructions into more specific and descriptive robot motions, making the reward coding task more stable. Moreover, users interact with the system through the motion description field, so this also provides a more interpretable interface for users compared to directly showing the reward function. &lt;/p>; &lt;p>; To create the Motion Descriptor, we use an LLM to translate the user input into a detailed description of the desired robot motion. We design &lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_motion_descriptor.txt&quot;>;prompts&lt;/a>; that guide the LLMs to output the motion description with the right amount of details and format. By translating a vague user instruction into a more detailed description, we are able to more reliably generate the reward function with our system. This idea can also be potentially applied more generally beyond robotics tasks, and is relevant to &lt;a href=&quot;https://innermonologue.github.io/&quot;>;Inner-Monologue&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought prompting&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Reward Coder &lt;/h3>; &lt;p>; In the second stage, we use the same LLM from Motion Descriptor for Reward Coder, which translates generated motion description into the reward function. Reward functions are represented using python code to benefit from the LLMs&#39; knowledge of reward, coding, and code structure. &lt;/p>; &lt;p>; Ideally, we would like to use an LLM to directly generate a reward function &lt;i>;R&lt;/i>; (&lt;i>;s&lt;/i>;, &lt;i>;t&lt;/i>;) that maps the robot state &lt;i>;s&lt;/i>; and time &lt;i>;t&lt;/i>; into a scalar reward value. However, generating the correct reward function from scratch is still a challenging problem for LLMs and correcting the errors requires the user to understand the generated code to provide the right feedback. As such, we pre-define a set of reward terms that are commonly used for the robot of interest and allow LLMs to composite different reward terms to formulate the final reward function. To achieve this, we design a &lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_reward_coder.txt&quot;>;prompt&lt;/a>; that specifies the reward terms and guide the LLM to generate the correct reward function for the task.&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s1293/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;1293&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The internal structure of the Reward Translator, which is tasked to map user inputs to reward functions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Motion Controller: Translating reward functions to robot actions&lt;/h2>; &lt;p>; The Motion Controller takes the reward function generated by the Reward Translator and synthesizes a controller that maps robot observation to low-level robot actions. To do this, we formulate the controller synthesis problem as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;>;Markov decision process&lt;/a>; (MDP), which can be solved using different strategies, including RL, &lt;a href=&quot;https://en.wikipedia.org/wiki/Trajectory_optimization&quot;>;offline trajectory optimization&lt;/a>;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;model predictive control&lt;/a>; (MPC). Specifically, we use an open-source implementation based on the &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; (MJPC). &lt;/p>; &lt;p>; MJPC has demonstrated the interactive creation of diverse behaviors, such as legged locomotion, grasping, and finger-gaiting, while supporting multiple planning algorithms, such as &lt;a href=&quot;https://homes.cs.washington.edu/~todorov/papers/TodorovACC05.pdf&quot;>;iterative linear–quadratic–Gaussian&lt;/a>; (iLQG) and &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;predictive sampling&lt;/a>;. More importantly, the frequent re-planning in MJPC empowers its robustness to uncertainties in the system and enables an interactive motion synthesis and correction system when combined with LLMs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Examples &lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Robot dog&lt;/h3>; &lt;p>; In the first example, we apply the language-to-reward system to a simulated quadruped robot and teach it to perform various skills. For each skill, the user will provide a concise instruction to the system, which will then synthesize the robot motion by using reward functions as an intermediate interface. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_quadruped_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Dexterous manipulator&lt;/h3>; &lt;p>; We then apply the language-to-reward system to a dexterous manipulator robot to perform a variety of manipulation tasks. The dexterous manipulator has 27 degrees of freedom, which is very challenging to control. Many of these tasks require manipulation skills beyond grasping, making it difficult for pre-designed primitives to work. We also include an example where the user can interactively instruct the robot to place an apple inside a drawer. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_manipulation_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Validation on real robots&lt;/h2>; &lt;p>; We also validate the language-to-reward method using a real-world manipulation robot to perform tasks such as picking up objects and opening a drawer. To perform the optimization in Motion Controller, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/ARTag&quot;>;AprilTag&lt;/a>;, a fiducial marker system, and &lt;a href=&quot;https://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot;>;F-VLM&lt;/a>;, an open-vocabulary object detection tool, to identify the position of the table and objects being manipulated. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;yes&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/real/l2r_demo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we describe a new paradigm for interfacing an LLM with a robot through reward functions, powered by a low-level model predictive control tool, MuJoCo MPC. Using reward functions as the interface enables LLMs to work in a semantic-rich space that plays to the strengths of LLMs, while ensuring the expressiveness of the resulting controller. To further improve the performance of the system, we propose to use a structured motion description template to better extract internal knowledge about robot motions from LLMs. We demonstrate our proposed system on two simulated robot platforms and one real robot for both locomotion and manipulation tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our co-authors Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, and Yuval Tassa for their help and support in various aspects of the project. We would also like to acknowledge Ken Caluwaerts, Kristian Hartikainen, Steven Bohez, Carolina Parada, Marc Toussaint, and the teams at Google DeepMind for their feedback and contributions.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6641308922483151364/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html&quot; rel=&quot;alternate&quot; title=&quot;Language to rewards for robotic skill synthesis&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s72-c/Language%20to%20reward.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6990000750711940626&lt;/id>;&lt;published>;2023-08-21T00:33:00.002-07:00&lt;/published>;&lt;updated>;2023-08-21T00:35:57.579-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Interspeech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at Interspeech 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJVLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s1075/Interspeech2023-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the 24th Annual &lt;a href=&quot;https://interspeech2023.org/&quot;>;Conference of the International Speech Communication Association&lt;/a>; (INTERSPEECH 2023) is being held in Dublin, Ireland, representing one of the world&#39;s most extensive conferences on research and technology of spoken language understanding and processing. Experts in speech-related research fields gather to take part in oral presentations and poster sessions and to build collaborations across the globe. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to be a &lt;a href=&quot;https://interspeech2023.org/sponsorship-exhibition/sponsors/&quot;>;Platinum Sponsor&lt;/a>; of INTERSPEECH 2023, where we will be showcasing more than 20 research publications and supporting a number of workshops and special sessions. We welcome in-person attendees to drop by the Google Research booth to meet our researchers and participate in Q&amp;amp;As and demonstrations of some of our latest speech technologies, which help to improve accessibility and provide convenience in communication for billions of users. In addition, online attendees are encouraged to visit our &lt;a href=&quot;http://topia.io/google-research&quot;>;virtual booth in Topia&lt;/a>; where you can get up-to-date information on research and opportunities at Google. Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions). You can also learn more about the Google research being presented at INTERSPEECH 2023 below (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; ISCA Board, Technical Committee Chair: &lt;strong>;&lt;em>;Bhuvana Ramabhadran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Area Chairs include: &lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Analysis of Speech and Audio Signals: &lt;strong>;&lt;em>;Richard Rose&lt;/em>;&lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Speech Synthesis and Spoken Language Generation: &lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Special Areas: &lt;strong>;&lt;em>;Tara Sainath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Satellite events&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/competition2023.html&quot;>;VoxCeleb Speaker Recognition Challenge 2023&lt;/a>; (VoxSRC-23)&lt;br />; Organizers include: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ssw2023.org/&quot;>;ISCA Speech Synthesis Workshop&lt;/a>; (SSW12)&lt;br />; Speakers include: &lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Keynote talk – ISCA Medalist&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/narayanan23_interspeech.pdf&quot;>;Bridging Speech Science and Technology — Now and Into the Future&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Shrikanth Narayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Survey Talk&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Speech Compression in the AI Era&lt;br />; Speaker: &lt;strong>;&lt;em>;Jan Skoglund&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Special session papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rose23_interspeech.pdf&quot;>;Cascaded Encoders for Fine-Tuning ASR Models on Overlapped Speech&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Richard Rose&lt;/b>;, &lt;b>;Oscar Chang&lt;/b>;, &lt;b>;Olivier Siohan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/erdogan23_interspeech.pdf&quot;>;TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Hakan Erdogan&lt;/b>;, &lt;b>;Scott Wisdom&lt;/b>;, Xuankai Chang*, &lt;b>;Zalán Borsos&lt;/b>;, &lt;b>;Marco Tagliasacchi&lt;/b>;, &lt;b>;Neil Zeghidour&lt;/b>;, &lt;b>;John R. Hershey&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/liang23d_interspeech.pdf&quot;>;DeePMOS: Deep Posterior Mean-Opinion-Score of Speech&lt;/a>;&lt;br />; &lt;em>;Xinyu Liang, Fredrik Cumlin,&lt;strong>; Christian Schüldt&lt;/strong>;, Saikat Chatterjee&lt;strong>;&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/baskar23_interspeech.pdf&quot;>;O-1: Self-Training with Oracle and 1-Best Hypothesis&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Murali Karthick Baskar&lt;/b>;, &lt;b>;Andrew Rosenberg&lt;/b>;, &lt;b>;Bhuvana Ramabhadran&lt;/b>;, &lt;b>;Kartik Audhkhasi&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/huo23b_interspeech.pdf&quot;>;Re-investigating the Efficient Transfer Learning of Speech Foundation Model Using Feature Fusion Methods&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Zhouyuan Huo&lt;/b>;, &lt;b>;Khe Chai Sim&lt;/b>;, &lt;b>;Dongseong Hwang&lt;/b>;, &lt;b>;Tsendsuren Munkhdalai&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;,&lt;b>; Pedro Moreno&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/camp23_interspeech.pdf&quot;>;MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Joshua Camp&lt;/b>;, &lt;b>;Tom Kenter&lt;/b>;, &lt;b>;Lev Finkelstein&lt;/b>;, &lt;b>;Rob Clark&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/gong23c_interspeech.pdf&quot;>;LanSER: Language-Model Supported Speech Emotion Recognition&lt;/a>;&lt;br />; &lt;em>;Taesik Gong,&lt;strong>; Josh Belanich&lt;/strong>;, &lt;strong>;Krishna Somandepalli&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Brian Eoff&lt;/strong>;, &lt;strong>;Brendan Jou&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23fa_interspeech.pdf&quot;>;Modular Domain Adaptation for Conformer-Based Streaming ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Qiujia Li&lt;/b>;, &lt;b>;Bo Li&lt;/b>;, &lt;b>;Dongseong Hwang&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;, &lt;b>;Pedro M. Mengibar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/panchapagesan23_interspeech.pdf&quot;>;On Training a Neural Residual Acoustic Echo Suppressor for Improved ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Sankaran Panchapagesan&lt;/b>;, &lt;b>;Turaj Zakizadeh Shabestary&lt;/b>;, &lt;b>;Arun Narayanan&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/eisenstein23_interspeech.pdf&quot;>;MD3: The Multi-dialect Dataset of Dialogues&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Jacob Eisenstein&lt;/b>;, &lt;b>;Vinodkumar Prabhakaran&lt;/b>;, &lt;b>;Clara Rivera&lt;/b>;, Dorottya Demszky, Devyani Sharma&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/wu23e_interspeech.pdf&quot;>;Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Zelin Wu&lt;/b>;, &lt;b>;Tsendsuren Munkhdalai&lt;/b>;, &lt;b>;Pat Rondon&lt;/b>;, &lt;b>;Golan Pundak&lt;/b>;, &lt;b>;Khe Chai Sim&lt;/b>;, &lt;b>;Christopher Li&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/blau23_interspeech.pdf&quot;>;Using Text Injection to Improve Recognition of Personal Identifiers in Speech&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yochai Blau&lt;/b>;, &lt;b>;Rohan Agrawal&lt;/b>;, &lt;b>;Lior Madmony&lt;/b>;, &lt;b>;Gary Wang&lt;/b>;, &lt;b>;Andrew Rosenberg&lt;/b>;, &lt;b>;Zhehuai Chen&lt;/b>;, &lt;b>;Zorik Gekhman&lt;/b>;, &lt;b>;Genady Beryozkin&lt;/b>;, &lt;b>;Parisa Haghani&lt;/b>;, &lt;b>;Bhuvana Ramabhadran&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23j_interspeech.pdf&quot;>;How to Estimate Model Transferability of Pre-trained Speech Models?&lt;/a>;&lt;br />; &lt;em>;Zih-Ching Chen, Chao-Han Huck Yang*, &lt;strong>;Bo Li&lt;/strong>;, &lt;strong>;Yu Zhang&lt;/strong>;, &lt;strong>;Nanxin Chen&lt;/strong>;, &lt;strong>;Shuo-yiin Chang&lt;/strong>;, &lt;strong>;Rohit Prabhavalkar, &lt;/strong>;Hung-yi Lee, &lt;strong>;Tara N. Sainath&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/peyser23_interspeech.pdf&quot;>;Improving Joint Speech-Text Representations Without Alignment&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Cal Peyser&lt;/b>;, &lt;b>;Zhong Meng&lt;/b>;, &lt;b>;Ke Hu&lt;/b>;, &lt;b>;Rohit Prabhavalkar&lt;/b>;, &lt;b>;Andrew Rosenberg&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;, Michael Picheny, Kyunghyun Cho &lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/bijwadia23_interspeech.pdf&quot;>;Text Injection for Capitalization and Turn-Taking Prediction in Speech Models&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shaan Bijwadia&lt;/b>;, &lt;b>;Shuo-yiin Chang&lt;/b>;, &lt;b>;Weiran Wang&lt;/b>;, &lt;b>;Zhong Meng&lt;/b>;, &lt;b>;Hao Zhang&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23_interspeech.pdf&quot;>;Streaming Parrotron for On-Device Speech-to-Speech Conversion&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;, &lt;b>;Fadi Biadsy&lt;/b>;, &lt;b>;Xia Zhang&lt;/b>;, &lt;b>;Liyang Jiang&lt;/b>;, &lt;b>;Phoenix Meadowlark&lt;/b>;, &lt;b>;Shivani Agrawal&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/huang23b_interspeech.pdf&quot;>;Semantic Segmentation with Bidirectional Language Models Improves Long-Form ASR&lt;/a>;&lt;br />; &lt;strong>;W. Ronny Huang&lt;/strong>;, &lt;strong>;Hao Zhang&lt;/strong>;, &lt;strong>;Shankar Kumar&lt;/strong>;, &lt;strong>;Shuo-yiin Chang&lt;/strong>;, &lt;strong>;Tara N. Sainath&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/taguchi23_interspeech.pdf&quot;>;Universal Automatic Phonetic Transcription into the International Phonetic Alphabet&lt;/a>;&lt;br />; &lt;em>;Chihiro Taguchi, Yusuke Sakai,&lt;strong>; Parisa Haghani&lt;/strong>;, David Chiang&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/hu23c_interspeech.pdf&quot;>;Mixture-of-Expert Conformer for Streaming Multilingual ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Ke Hu&lt;/b>;, &lt;b>;Bo Li&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;, &lt;b>;Yu Zhang&lt;/b>;, &lt;b>;Francoise Beaufays&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23c_interspeech.pdf&quot;>;Real Time Spectrogram Inversion on Mobile Phone&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;, &lt;b>;Marco Tagliasacchi&lt;/b>;, &lt;b>;Yunpeng Li&lt;/b>;, &lt;b>;Liyang Jiang&lt;/b>;, &lt;b>;Xia Zhang&lt;/b>;, &lt;b>;Fadi Biadsy&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23b_interspeech.pdf&quot;>;2-Bit Conformer Quantization for Automatic Speech Recognition&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;, &lt;b>;Phoenix Meadowlark&lt;/b>;, &lt;b>;Shaojin Ding&lt;/b>;, &lt;b>;David Qiu&lt;/b>;, &lt;b>;Jian Li&lt;/b>;, &lt;b>;David Rim&lt;/b>;, &lt;b>;Yanzhang He&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/koizumi23_interspeech.pdf&quot;>;LibriTTS-R: A Restored Multi-speaker Text-to-Speech Corpus&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yuma Koizumi&lt;/b>;, &lt;b>;Heiga Zen&lt;/b>;, &lt;b>;Shigeki Karita&lt;/b>;, &lt;b>;Yifan Ding&lt;/b>;, Kohei Yatabe, &lt;b>;Nobuyuki Morioka&lt;/b>;, &lt;b>;Michiel Bacchiani&lt;/b>;, &lt;b>;Yu Zhang&lt;/b>;, &lt;b>;Wei Han&lt;/b>;, &lt;b>;Ankur Bapna&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/yu23_interspeech.pdf&quot;>;PronScribe: Highly Accurate Multimodal Phonemic Transcription from Speech and Text&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yang Yu&lt;/b>;, Matthew Perez*, &lt;b>;Ankur Bapna&lt;/b>;, &lt;b>;Fadi Haik&lt;/b>;, &lt;b>;Siamak Tazari&lt;/b>;, &lt;b>;Yu Zhang&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/vashishth23_interspeech.pdf&quot;>;Label Aware Speech Representation Learning for Language Identification&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shikhar Vashishth&lt;/b>;, &lt;b>;Shikhar Bharadwaj&lt;/b>;, &lt;b>;Sriram Ganapathy&lt;/b>;, &lt;b>;Ankur Bapna&lt;/b>;, &lt;b>;Min Ma&lt;/b>;, &lt;b>;Wei Han&lt;/b>;, &lt;b>;Vera Axelrod&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6990000750711940626/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/google-at-interspeech-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/google-at-interspeech-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at Interspeech 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJVLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s72-c/Interspeech2023-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-362086873015740792&lt;/id>;&lt;published>;2023-08-18T11:28:00.004-07:00&lt;/published>;&lt;updated>;2023-08-18T11:30:51.167-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Autonomous visual information seeking with large language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ziniu Hu, Student Researcher, and Alireza Fathi, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s2500/AVIS.png&quot; style=&quot;display: none;&quot; />; &lt;p>; There has been great progress towards adapting large language models (LLMs) to accommodate multimodal inputs for tasks including &lt;a href=&quot;https://huggingface.co/docs/transformers/main/tasks/image_captioning#:~:text=Image%20captioning%20is%20the%20task,by%20describing%20images%20to%20them.&quot;>;image captioning&lt;/a>;, &lt;a href=&quot;https://huggingface.co/tasks/visual-question-answering&quot;>;visual question answering (VQA)&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open vocabulary recognition&lt;/a>;. Despite such achievements, current state-of-the-art visual language models (VLMs) perform inadequately on visual information seeking datasets, such as &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; and &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>;, where external knowledge is required to answer the questions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of visual information seeking queries where external knowledge is required to answer the question. Images are taken from the OK-VQA dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs//2306.08129&quot;>;AVIS: Autonomous Visual Information Seeking with Large Language Models&lt;/a>;”, we introduce a novel method that achieves state-of-the-art results on visual information seeking tasks. Our method integrates LLMs with three types of tools: (i) computer vision tools for extracting visual information from images, (ii) a web search tool for retrieving open world knowledge and facts, and (iii) an image search tool to glean relevant information from metadata associated with visually similar images. AVIS employs an LLM-powered planner to choose tools and queries at each step. It also uses an LLM-powered reasoner to analyze tool outputs and extract key information. A working memory component retains information throughout the process. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoSFcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s1999/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1758&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoSFcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of AVIS&#39;s generated workflow for answering a challenging visual information seeking question. The input image is taken from the Infoseek dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Comparison to previous work&lt;/h2>; &lt;p>; Recent studies (eg, &lt;a href=&quot;https://arxiv.org/abs/2304.09842&quot;>;Chameleon&lt;/a>;, &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>; and &lt;a href=&quot;https://multimodal-react.github.io/&quot;>;MM-ReAct&lt;/a>;) explored adding tools to LLMs for multimodal inputs. These systems follow a two-stage process: planning (breaking down questions into structured programs or instructions) and execution (using tools to gather information). Despite success in basic tasks, this approach often falters in complex real-world scenarios. &lt;/p>; &lt;p>; There has also been a surge of interest in applying LLMs as autonomous agents (eg, &lt;a href=&quot;https://openai.com/research/webgpt&quot;>;WebGPT&lt;/a>; and &lt;a href=&quot;https://react-lm.github.io/&quot;>;ReAct&lt;/a>;). These agents interact with their environment, adapt based on real-time feedback, and achieve goals. However, these methods do not restrict the tools that can be invoked at each stage, leading to an immense search space. Consequently, even the most advanced LLMs today can fall into infinite loops or propagate errors. AVIS tackles this via guided LLM use, influenced by human decisions from a user study. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Informing LLM decision making with a user study&lt;/h2>; &lt;p>; Many of the visual questions in datasets such as &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; and &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; pose a challenge even for humans, often requiring the assistance of various tools and APIs. An example question from the OK-VQA dataset is shown below. We conducted a user study to understand human decision-making when using external tools. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;889&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We conducted a user study to understand human decision-making when using external tools. Image is taken from the OK-VQA dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The users were equipped with an identical set of tools as our method, including &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PALI&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Search_engine&quot;>;web search&lt;/a>;. They received input images, questions, detected object crops, and buttons linked to image search results. These buttons offered diverse information about the detected object crops, such as knowledge graph entities, similar image captions, related product titles, and identical image captions. &lt;/p>; &lt;p>; We record user actions and outputs and use it as a guide for our system in two key ways. First, we construct a transition graph (shown below) by analyzing the sequence of decisions made by users. This graph defines distinct states and restricts the available set of actions at each state. For example, at the start state, the system can take only one of these three actions: PALI caption, PALI VQA, or object detection. Second, we use the examples of human decision-making to guide our planner and reasoner with relevant contextual instances to enhance the performance and effectiveness of our system. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/s1146/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;845&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/w472-h640/image7.png&quot; width=&quot;472&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS transition graph.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;General framework&lt;/h2>; &lt;p>; Our approach employs a dynamic decision-making strategy designed to respond to visual information-seeking queries. Our system has three primary components. First, we have a &lt;em>;planner&lt;/em>; to determine the subsequent action, including the appropriate API call and the query it needs to process. Second, we have a &lt;em>;working memory&lt;/em>; that retains information about the results obtained from API executions. Last, we have a &lt;em>;reasoner&lt;/em>;, whose role is to process the outputs from the API calls. It determines whether the obtained information is sufficient to produce the final response, or if additional data retrieval is required. &lt;/p>; &lt;p>; The planner undertakes a series of steps each time a decision is required regarding which tool to employ and what query to send to it. Based on the present state, the planner provides a range of potential subsequent actions. The potential action space may be so large that it makes the search space intractable. To address this issue, the planner refers to the transition graph to eliminate irrelevant actions. The planner also excludes the actions that have already been taken before and are stored in the working memory. &lt;/p>; &lt;p>; Next, the planner collects a set of relevant in-context examples that are assembled from the decisions previously made by humans during the user study. With these examples and the working memory that holds data collected from past tool interactions, the planner formulates a prompt. The prompt is then sent to the LLM, which returns a structured answer, determining the next tool to be activated and the query to be dispatched to it. This design allows the planner to be invoked multiple times throughout the process, thereby facilitating dynamic decision-making that gradually leads to answering the input query. &lt;/p>; &lt;p>; We employ a reasoner to analyze the output of the tool execution, extract the useful information and decide into which category the tool output falls: informative, uninformative, or final answer. Our method utilizes the LLM with appropriate prompting and in-context examples to perform the reasoning. If the reasoner concludes that it&#39;s ready to provide an answer, it will output the final response, thus concluding the task. If it determines that the tool output is uninformative, it will revert back to the planner to select another action based on the current state. If it finds the tool output to be useful, it will modify the state and transfer control back to the planner to make a new decision at the new state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1469&quot; data-original-width=&quot;1999&quot; height=&quot;470&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/w640-h470/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS employs a dynamic decision-making strategy to respond to visual information-seeking queries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate AVIS on &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; and &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; datasets. As shown below, even robust visual-language models, such as &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;, fail to yield high accuracy when fine-tuned on Infoseek. Our approach (AVIS), without fine-tuning, achieves 50.7% accuracy on the unseen entity split of this dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS visual question answering results on Infoseek dataset. AVIS achieves higher accuracy in comparison to previous baselines based on &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results on the OK-VQA dataset are shown below. AVIS with few-shot in-context examples achieves an accuracy of 60.2%, higher than most of the previous works. AVIS achieves lower but comparable accuracy in comparison to the PALI model fine-tuned on OK-VQA. This difference, compared to Infoseek where AVIS outperforms fine-tuned PALI, is due to the fact that most question-answer examples in OK-VQA rely on common sense knowledge rather than on fine-grained knowledge. Therefore, PaLI is able to encode such generic knowledge in the model parameters and doesn&#39;t require external knowledge. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual question answering results on A-OKVQA. AVIS achieves higher accuracy in comparison to previous works that use few-shot or zero-shot learning, including &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; and &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>;. AVIS also achieves higher accuracy than most of the previous works that are fine-tuned on OK-VQA dataset, including &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;REVEAL&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2206.01201&quot;>;ReVIVE&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.08614&quot;>;KAT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>;, and achieves results that are close to the fine-tuned &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present a novel approach that equips LLMs with the ability to use a variety of tools for answering knowledge-intensive visual questions. Our methodology, anchored in human decision-making data collected from a user study, employs a structured framework that uses an LLM-powered planner to dynamically decide on tool selection and query formation. An LLM-powered reasoner is tasked with processing and extracting key information from the output of the selected tool. Our method iteratively employs the planner and reasoner to leverage different tools until all necessary information required to answer the visual question is amassed. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A. Ross, Cordelia Schmid and Alireza Fathi.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/362086873015740792/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous-visual-information-seeking.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous-visual-information-seeking.html&quot; rel=&quot;alternate&quot; title=&quot;Autonomous visual information seeking with large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s72-c/AVIS.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4779594044050650231&lt;/id>;&lt;published>;2023-08-17T11:08:00.000-07:00&lt;/published>;&lt;updated>;2023-08-17T11:08:24.346-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Neural network pruning with combinatorial optimization&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hussein Hazimeh, Research Scientist, Athena Team, and Riade Benbaki, Graduate Student at MIT&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s320/CHITA%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Modern neural networks have achieved impressive performance across a variety of applications, such as &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;language, mathematical reasoning&lt;/a>;, and &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;vision&lt;/a>;. However, these networks often use large architectures that require lots of computational resources. This can make it impractical to serve such models to users, especially in resource-constrained environments like wearables and smartphones. A &lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;widely used approach&lt;/a>; to mitigate the inference costs of pre-trained networks is to prune them by removing some of their weights, in a way that doesn&#39;t significantly affect utility. In standard neural networks, each weight defines a connection between two neurons. So after weights are pruned, the input will propagate through a smaller set of connections and thus requires less computational resources. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s1600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Original network vs. a pruned network.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Pruning methods can be applied at different stages of the network&#39;s training process: post, during, or before training (ie, immediately after weight initialization). In this post, we focus on the post-training setting: given a pre-trained network, how can we determine which weights should be pruned? One popular method is &lt;a href=&quot;https://jmlr.org/papers/volume22/21-0366/21-0366.pdf&quot;>;magnitude pruning&lt;/a>;, which removes weights with the smallest magnitude. While efficient, this method doesn&#39;t directly consider the effect of removing weights on the network&#39;s performance. Another popular paradigm is &lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;optimization-based pruning&lt;/a>;, which removes weights based on how much their removal impacts the loss function. Although conceptually appealing, most existing optimization-based approaches seem to face a serious tradeoff between performance and computational requirements. Methods that make crude approximations (eg, assuming a diagonal &lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;>;Hessian matrix&lt;/a>;) can scale well, but have relatively low performance. On the other hand, while methods that make fewer approximations tend to perform better, they appear to be much less scalable. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2302.14623&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>;”, presented at &lt;a href=&quot;https://icml.cc/Conferences/2023/&quot;>;ICML 2023&lt;/a>;, we describe how we developed an optimization-based approach for pruning pre-trained neural networks at scale. CHITA (which stands for “Combinatorial Hessian-free Iterative Thresholding Algorithm”) outperforms existing pruning methods in terms of scalability and performance tradeoffs, and it does so by leveraging advances from several fields, including &lt;a href=&quot;https://en.wikipedia.org/wiki/High-dimensional_statistics&quot;>;high-dimensional statistics&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;combinatorial optimization&lt;/a>;, and neural network pruning. For example, CHITA can be 20x to 1000x faster than state-of-the-art methods for pruning &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; and improves accuracy by over 10% in many settings. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overview of contributions&lt;/h2>; &lt;p>; CHITA has two notable technical improvements over popular methods: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Efficient use of second-order information&lt;/strong>;: Pruning methods that use second-order information (ie, relating to &lt;a href=&quot;https://en.wikipedia.org/wiki/Second_derivative&quot;>;second derivatives&lt;/a>;) achieve the state of the art in many settings. In the literature, this information is typically used by computing the Hessian matrix or its inverse, an operation that is very difficult to scale because the Hessian size is quadratic with respect to the number of weights. Through careful reformulation, CHITA uses second-order information without having to compute or store the Hessian matrix explicitly, thus allowing for more scalability. &lt;/li>;&lt;li>;&lt;strong>;Combinatorial optimization&lt;/strong>;: Popular optimization-based methods use a simple optimization technique that prunes weights in isolation, ie, when deciding to prune a certain weight they don&#39;t take into account whether other weights have been pruned. This could lead to pruning important weights because weights deemed unimportant in isolation may become important when other weights are pruned. CHITA avoids this issue by using a more advanced, combinatorial optimization algorithm that takes into account how pruning one weight impacts others. &lt;/li>; &lt;/ul>; &lt;p>; In the sections below, we discuss CHITA&#39;s pruning formulation and algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A computation-friendly pruning formulation&lt;/h2>; &lt;p>; There are many possible pruning candidates, which are obtained by retaining only a subset of the weights from the original network. Let &lt;em>;k&lt;/em>; be a user-specified parameter that denotes the number of weights to retain. Pruning can be naturally formulated as a &lt;a href=&quot;https://arxiv.org/abs/1803.01454&quot;>;best-subset selection&lt;/a>; (BSS) problem: among all possible pruning candidates (ie, subsets of weights) with only &lt;em>;k&lt;/em>; weights retained, the candidate that has the smallest loss is selected. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s1600/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Pruning as a BSS problem: among all possible pruning candidates with the same total number of weights, the best candidate is defined as the one with the least loss. This illustration shows four candidates, but this number is generally much larger.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Solving the pruning BSS problem on the original loss function is generally computationally intractable. Thus, similar to previous work, such as &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf&quot;>;OBD&lt;/a>; and &lt;a href=&quot;https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&quot;>;OBS&lt;/a>;, we approximate the loss with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Quadratic_form&quot;>;quadratic function&lt;/a>; by using a second-order &lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor_series&quot;>;Taylor series&lt;/a>;, where the Hessian is estimated with the empirical &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;>;Fisher information matrix&lt;/a>;. While gradients can be typically computed efficiently, computing and storing the Hessian matrix is prohibitively expensive due to its sheer size. In the literature, it is common to deal with this challenge by making restrictive assumptions on the Hessian (eg, diagonal matrix) and also on the algorithm (eg, pruning weights in isolation). &lt;/p>; &lt;p>; CHITA uses an efficient reformulation of the pruning problem (BSS using the quadratic loss) that avoids explicitly computing the Hessian matrix, while still using all the information from this matrix. This is made possible by exploiting the low-&lt;a href=&quot;https://en.wikipedia.org/wiki/Rank_(linear_algebra)&quot;>;rank&lt;/a>; structure of the empirical Fisher information matrix. This reformulation can be viewed as a sparse &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;>;linear regression&lt;/a>; problem, where each regression coefficient corresponds to a certain weight in the neural network. After obtaining a solution to this regression problem, coefficients set to zero will correspond to weights that should be pruned. Our regression data matrix is (&lt;em>;n&lt;/em>; x &lt;em>;p&lt;/em>;), where &lt;em>;n&lt;/em>; is the batch (sub-sample) size and &lt;em>;p&lt;/em>; is the number of weights in the original network. Typically &lt;em>;n&lt;/em>; &amp;lt;&amp;lt; &lt;em>;p&lt;/em>;, so storing and operating with this data matrix is much more scalable than common pruning approaches that operate with the (&lt;em>;p&lt;/em>; x &lt;em>;p&lt;/em>;) Hessian. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s1601/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;CHITA reformulates the quadratic loss approximation, which requires an expensive Hessian matrix, as a linear regression (LR) problem. The LR&#39;s data matrix is linear in &lt;em>;p&lt;/em>;, which makes the reformulation more scalable than the original quadratic approximation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scalable optimization algorithms&lt;/h2>; &lt;p>; CHITA reduces pruning to a linear regression problem under the following sparsity constraint: at most &lt;em>;k&lt;/em>; regression coefficients can be nonzero. To obtain a solution to this problem, we consider a modification of the well-known &lt;a href=&quot;https://arxiv.org/pdf/0805.0510.pdf&quot;>;iterative hard thresholding&lt;/a>; (IHT) algorithm. IHT performs &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;>;gradient descent&lt;/a>; where after each update the following post-processing step is performed: all regression coefficients outside the Top-&lt;em>;k&lt;/em>; (ie, the &lt;em>;k&lt;/em>; coefficients with the largest magnitude) are set to zero. IHT typically delivers a good solution to the problem, and it does so iteratively exploring different pruning candidates and jointly optimizing over the weights. &lt;/p>; &lt;p>; Due to the scale of the problem, standard IHT with constant &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;>;learning rate&lt;/a>; can suffer from very slow convergence. For faster convergence, we developed a new &lt;a href=&quot;https://en.wikipedia.org/wiki/Line_search&quot;>;line-search&lt;/a>; method that exploits the problem structure to find a suitable learning rate, ie, one that leads to a sufficiently large decrease in the loss. We also employed several computational schemes to improve CHITA&#39;s efficiency and the quality of the second-order approximation, leading to an improved version that we call CHITA++. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We compare CHITA&#39;s run time and accuracy with several state-of-the-art pruning methods using different architectures, including &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;>;MobileNet&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Run time&lt;/strong>;: CHITA is much more scalable than comparable methods that perform joint optimization (as opposed to pruning weights in isolation). For example, CHITA&#39;s speed-up can reach over 1000x when pruning ResNet. &lt;/p>; &lt;p>; &lt;strong>;Post-pruning accuracy&lt;/strong>;:&lt;strong>; &lt;/strong>;Below, we compare the performance of CHITA and CHITA++ with magnitude pruning (MP), &lt;a href=&quot;https://arxiv.org/abs/2004.14340&quot;>;Woodfisher&lt;/a>; (WF), and &lt;a href=&quot;https://arxiv.org/abs/2203.04466&quot;>;Combinatorial Brain Surgeon&lt;/a>; (CBS), for pruning 70% of the model weights. Overall, we see good improvements from CHITA and CHITA++. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Post-pruning accuracy of various methods on ResNet20. Results are reported for pruning 70% of the model weights.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Post-pruning accuracy of various methods on MobileNet. Results are reported for pruning 70% of the model weights.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Next, we report results for pruning a larger network: ResNet50 (on this network, some of the methods listed in the ResNet20 figure couldn&#39;t scale). Here we compare with magnitude pruning and &lt;a href=&quot;https://arxiv.org/abs/2107.03356&quot;>;M-FAC&lt;/a>;. The figure below shows that CHITA achieves better test accuracy for a wide range of sparsity levels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/s1050/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;784&quot; data-original-width=&quot;1050&quot; height=&quot;478&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/w640-h478/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Test accuracy of pruned networks, obtained using different methods.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion, limitations, and future work&lt;/h2>; &lt;p>; We presented CHITA, an optimization-based approach for pruning pre-trained neural networks. CHITA offers scalability and competitive performance by efficiently using second-order information and drawing on ideas from combinatorial optimization and high-dimensional statistics. &lt;/p>; &lt;p>; CHITA is designed for &lt;em>;unstructured pruning&lt;/em>; in which any weight can be removed. In theory, unstructured pruning can significantly reduce computational requirements. However, realizing these reductions in practice requires special software (and possibly hardware) that support sparse computations. In contrast, &lt;em>;structured pruning&lt;/em>;, which removes whole structures like neurons, may offer improvements that are easier to attain on general-purpose software and hardware. It would be interesting to extend CHITA to structured pruning. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is part of a research collaboration between Google and MIT. Thanks to Rahul Mazumder, Natalia Ponomareva, Wenyu Chen, Xiang Meng, Zhe Zhao, and Sergei Vassilvitskii for their help in preparing this post and the paper. Also thanks to John Guilyard for creating the graphics in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4779594044050650231/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html&quot; rel=&quot;alternate&quot; title=&quot;Neural network pruning with combinatorial optimization&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s72-c/CHITA%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8689679451447865270&lt;/id>;&lt;published>;2023-08-15T12:59:00.001-07:00&lt;/published>;&lt;updated>;2023-08-15T13:00:54.887-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Recommender Systems&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Social Networks&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;STUDY: Socially aware temporally causal decoder recommender systems&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eltayeb Ahmed, Research Engineer, and Subhrajit Roy, Senior Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s837/study-hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Reading has many benefits for young students, such as &lt;a href=&quot;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/284286/reading_for_pleasure.pdf&quot;>;better linguistic and life skills&lt;/a>;, and reading for pleasure has been shown to correlate with &lt;a href=&quot;https://files.eric.ed.gov/fulltext/ED541404.pdf&quot;>;academic success&lt;/a>;. Furthermore students have reported &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/1361454042000312284&quot;>;improved emotional wellbeing&lt;/a>; from reading, as well as &lt;a href=&quot;https://eric.ed.gov/?id=ED496343&quot;>;better general knowledge and better understanding of other cultures&lt;/a>;. With the vast amount of reading material both online and off, finding age-appropriate, relevant and engaging content can be a challenging task, but helping students do so is a necessary step to engage them in reading. Effective recommendations that present students with relevant reading material helps keep students reading, and this is where machine learning (ML) can help. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ML has been widely used in building &lt;a href=&quot;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592-5&quot;>;recommender systems&lt;/a>; for various types of digital content, ranging from videos to books to e-commerce items. Recommender systems are used across a range of digital platforms to help surface relevant and engaging content to users. In these systems, ML models are trained to suggest items to each user individually based on user preferences, user engagement, and the items under recommendation. These data provide a strong learning signal for models to be able to recommend items that are likely to be of interest, thereby improving user experience. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2306.07946&quot;>;STUDY: Socially Aware Temporally Causal Decoder Recommender Systems&lt;/a>;”, we present a content recommender system for audiobooks in an educational setting taking into account the social nature of reading. We developed the STUDY algorithm in partnership with &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>;, an educational nonprofit, aimed at promoting reading in dyslexic students, that provides audiobooks to students through a school-wide subscription program. Leveraging the wide range of audiobooks in the Learning Ally library, our goal is to help students find the right content to help boost their reading experience and engagement. Motivated by the fact that what a person&#39;s peers are currently reading has significant effects on what they would find interesting to read, we jointly process the reading engagement history of students who are in the same classroom. This allows our model to benefit from live information about what is currently trending within the student&#39;s localized social group, in this case, their classroom. &lt;/p>; &lt;br />; &lt;h2>;Data&lt;/h2>; &lt;p>; &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; has a large digital library of curated audiobooks targeted at students, making it well-suited for building a social recommendation model to help improve student learning outcomes. We received two years of anonymized audiobook consumption data. All students, schools and groupings in the data were anonymized, only identified by a randomly generated ID not traceable back to real entities by Google. Furthermore all potentially identifiable metadata was only shared in an aggregated form, to protect students and institutions from being re-identified. The data consisted of time-stamped records of student&#39;s interactions with audiobooks. For each interaction we have an anonymized student ID (which includes the student&#39;s grade level and anonymized school ID), an audiobook identifier and a date. While many schools distribute students in a single grade across several classrooms, we leverage this metadata to make the simplifying assumption that all students in the same school and in the same grade level are in the same classroom. While this provides the foundation needed to build a better social recommender model, it&#39;s important to note that this does not enable us to re-identify individuals, class groups or schools. &lt;/p>; &lt;br />; &lt;h2>;The STUDY algorithm&lt;/h2>; &lt;p>; We framed the recommendation problem as a &lt;a href=&quot;https://arxiv.org/abs/2104.10584&quot;>;click-through rate&lt;/a>; prediction problem, where we model the conditional probability of a user interacting with each specific item conditioned on both 1) user and item characteristics and 2) the item interaction history sequence for the user at hand. &lt;a href=&quot;https://arxiv.org/abs/1905.06874&quot;>;Previous work&lt;/a>; suggests &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer&lt;/a>;-based models, a widely used model class developed by Google Research, are well suited for modeling this problem. When each user is processed individually this becomes an &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive sequence modeling problem&lt;/a>;. We use this conceptual framework to model our data and then extend this framework to create the STUDY approach. &lt;/p>; &lt;p>; While this approach for click-through rate prediction can model dependencies between past and future item preferences for an individual user and can learn patterns of similarity across users at train time, it cannot model dependencies across different users at inference time. To recognise the social nature of reading and remediate this shortcoming we developed the STUDY model, which concatenates multiple sequences of books read by each student into a single sequence that collects data from multiple students in a single classroom. &lt;/p>; &lt;p>; However, this data representation requires careful diligence if it is to be modeled by transformers. In transformers, the attention mask is the matrix that controls which inputs can be used to inform the predictions of which outputs. The pattern of using all prior tokens in a sequence to inform the prediction of an output leads to the upper triangular attention matrix traditionally found in causal decoders. However, since the sequence fed into the STUDY model is not temporally ordered, even though each of its constituent subsequences is, a standard &lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;>;causal decoder&lt;/a>; is no longer a good fit for this sequence. When trying to predict each token, the model is not allowed to attend to every token that precedes it in the sequence; some of these tokens might have timestamps that are later and contain information that would not be available at deployment time. &lt;br />; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tqRd-ZWDXOCRpHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/s1787/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1651&quot; data-original-width=&quot;1787&quot; height=&quot;370&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tqRd-ZWDXOCRpHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/w400-h370/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In this figure we show the attention mask typically used in causal decoders. Each column represents an output and each column represents an output. A value of 1 (shown as blue) for a matrix entry at a particular position denotes that the model can observe the input of that row when predicting the output of the corresponding column, whereas a value of 0 (shown as white) denotes the opposite.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The STUDY model builds on causal transformers by replacing the triangular matrix attention mask with a flexible attention mask with values based on timestamps to allow attention across different subsequences. Compared to a regular transformer, which would not allow attention across different subsequences and would have a triangular matrix mask within sequence, STUDY maintains a causal triangular attention matrix within a sequence and has flexible values across sequences with values that depend on timestamps. Hence, predictions at any output point in the sequence are informed by all input points that occurred in the past relative to the current time point, regardless of whether they appear before or after the current input in the sequence. This causal constraint is important because if it is not enforced at train time, the model could potentially learn to make predictions using information from the future, which would not be available for a real world deployment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s1104/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In (a) we show a sequential autoregressive transformer with causal attention that processes each user individually; in (b) we show an equivalent joint forward pass that results in the same computation as (a); and finally, in (c) we show that by introducing new nonzero values (shown in purple) to the attention mask we allow information to flow across users. We do this by allowing a prediction to condition on all interactions with an earlier timestamp, irrespective of whether the interaction came from the same user or not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s1035/Study.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1035&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s16000/Study.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In (a) we show a sequential autoregressive transformer with causal attention that processes each user individually; in (b) we show an equivalent joint forward pass that results in the same computation as (a); and finally, in (c) we show that by introducing new nonzero values (shown in purple) to the attention mask we allow information to flow across users. We do this by allowing a prediction to condition on all interactions with an earlier timestamp, irrespective of whether the interaction came from the same user or not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/s1104/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;548&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/w318-h640/image3.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In (a) we show a sequential autoregressive transformer with causal attention that processes each user individually; in (b) we show an equivalent joint forward pass that results in the same computation as (a); and finally, in (c) we show that by introducing new nonzero values (shown in purple) to the attention mask we allow information to flow across users. We do this by allowing a prediction to condition on all interactions with an earlier timestamp, irrespective of whether the interaction came from the same user or not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We used the Learning Ally dataset to train the STUDY model along with multiple baselines for comparison. We implemented an autoregressive click-through rate transformer decoder, which we refer to as “Individual”, a &lt;em>;k&lt;/em>;-nearest neighbor baseline (KNN), and a comparable social baseline, social attention memory network (SAMN). We used the data from the first school year for training and we used the data from the second school year for validation and testing. &lt;/p>; &lt;p>; We evaluated these models by measuring the percentage of the time the next item the user actually interacted with was in the model&#39;s top &lt;em>;n&lt;/em>; recommendations, ie, hits@&lt;em>;n,&lt;/em>; for different values of &lt;em>;n&lt;/em>;. In addition to evaluating the models on the entire test set we also report the models&#39; scores on two subsets of the test set that are more challenging than the whole data set. We observed that students will typically interact with an audiobook over multiple sessions, so simply recommending the last book read by the user would be a strong trivial recommendation. Hence, the first test subset, which we refer to as “non-continuation”, is where we only look at each model&#39;s performance on recommendations when the students interact with books that are different from the previous interaction. We also observe that students revisit books they have read in the past, so strong performance on the test set can be achieved by restricting the recommendations made for each student to only the books they have read in the past. Although there might be value in recommending old favorites to students, much value from recommender systems comes from surfacing content that is new and unknown to the user. To measure this we evaluate the models on the subset of the test set where the students interact with a title for the first time. We name this evaluation subset “novel”. &lt;/p>; &lt;p>; We find that STUDY outperforms all other tested models across almost every single slice we evaluated against. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/s1526/Study-img2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1486&quot; data-original-width=&quot;1526&quot; height=&quot;390&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/w400-h390/Study-img2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In this figure we compare the performance of four models, Study, Individual, KNN and SAMN. We measure the performance with hits@5, ie, how likely the model is to suggest the next title the user read within the model&#39;s top 5 recommendations. We evaluate the model on the entire test set (all) as well as the novel and non-continuation splits. We see STUDY consistently outperforms the other three models presented across all splits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Importance of appropriate grouping&lt;/h2>; &lt;p>; At the heart of the STUDY algorithm is organizing users into groups and doing joint inference over multiple users who are in the same group in a single forward pass of the model. We conducted an ablation study where we looked at the importance of the actual groupings used on the performance of the model. In our presented model we group together all students who are in the same grade level and school. We then experiment with groups defined by all students in the same grade level and district and also place all students in a single group with a random subset used for each forward pass. We also compare these models against the Individual model for reference. &lt;/p>; &lt;p>; We found that using groups that were more localized was more effective, with the school and grade level grouping outperforming the district and grade level grouping. This supports the hypothesis that the STUDY model is successful because of the social nature of activities such as reading — people&#39;s reading choices are likely to correlate with the reading choices of those around them. Both of these models outperformed the other two models (single group and Individual) where grade level is not used to group students. This suggests that data from users with similar reading levels and interests is beneficial for performance. &lt;/p>; &lt;br />; &lt;h2>;Future work&lt;/h2>; &lt;p>; This work is limited to modeling recommendations for user populations where the social connections are assumed to be homogenous. In the future it would be beneficial to model a user population where relationships are not homogeneous, ie, where categorically different types of relationships exist or where the relative strength or influence of different relationships is known. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work involved collaborative efforts from a multidisciplinary team of researchers, software engineers and educational subject matter experts. We thank our co-authors: Diana Mincu, Lauren Harrell, and Katherine Heller from Google. We also thank our colleagues at Learning Ally, Jeff Ho, Akshat Shah, Erin Walker, and Tyler Bastian, and our collaborators at Google, Marc Repnyek, Aki Estrella, Fernando Diaz, Scott Sanner, Emily Salkey and Lev Proleev.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8689679451447865270/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/study-socially-aware-temporally-causal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/study-socially-aware-temporally-causal.html&quot; rel=&quot;alternate&quot; title=&quot;STUDY: Socially aware temporally causal decoder recommender systems&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s72-c/study-hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3240052415427459327&lt;/id>;&lt;published>;2023-08-09T11:32:00.001-07:00&lt;/published>;&lt;updated>;2023-08-09T12:26:53.831-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Advances in document understanding&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sandeep Tata, Software Engineer, Google Research, Athena Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s320/Document%20understanding%20hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The last few years have seen rapid progress in systems that can automatically process complex business documents and turn them into structured objects. A system that can &lt;a href=&quot;https://ai.googleblog.com/2020/06/extracting-structured-data-from.html&quot;>;automatically extract data&lt;/a>; from documents, eg, receipts, insurance quotes, and financial statements, has the potential to dramatically improve the efficiency of business workflows by avoiding error-prone, manual work. Recent models, based on the &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; architecture, have shown &lt;a href=&quot;https://ai.googleblog.com/2022/04/formnet-beyond-sequential-modeling-for.html&quot;>;impressive gains in accuracy&lt;/a>;. Larger models, such as &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>;, are also being leveraged to further streamline these business workflows. However, the datasets used in academic literature fail to capture the challenges seen in real-world use cases. Consequently, academic benchmarks report strong model accuracy, but these same models do poorly when used for complex real-world applications. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;VRDU: A Benchmark for Visually-rich Document Understanding&lt;/a>;”, presented at &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>;, we announce the release of the new &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;Visually Rich Document Understanding&lt;/a>; (VRDU) dataset that aims to bridge this gap and help researchers better track progress on document understanding tasks. We list five requirements for a good document understanding benchmark, based on the kinds of real-world documents for which document understanding models are frequently used. Then, we describe how most datasets currently used by the research community fail to meet one or more of these requirements, while VRDU meets all of them. We are excited to announce the public release of the VRDU &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;dataset&lt;/a>; and &lt;a href=&quot;https://github.com/google-research-datasets/vrdu&quot;>;evaluation code&lt;/a>; under a &lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;Creative Commons license&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Benchmark requirements&lt;/h2>; &lt;p>; First, we compared state-of-the-art model accuracy (eg, with &lt;a href=&quot;https://arxiv.org/abs/2203.08411&quot;>;FormNet&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2012.14740&quot;>;LayoutLMv2&lt;/a>;) on real-world use cases to academic benchmarks (eg, &lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;, &lt;a href=&quot;https://openreview.net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;, &lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;). We observed that state-of-the-art models did not match academic benchmark results and delivered much lower accuracy in the real world. Next, we compared typical datasets for which document understanding models are frequently used with academic benchmarks and identified five dataset requirements that allow a dataset to better capture the complexity of real-world applications: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Rich Schema:&lt;/strong>; In practice, we see a wide variety of rich schemas for structured extraction. Entities have different data types (numeric, strings, dates, etc.) that may be required, optional, or repeated in a single document or may even be nested. Extraction tasks over simple flat schemas like (header, question, answer) do not reflect typical problems encountered in practice. &lt;/li>;&lt;li>;&lt;strong>;Layout-Rich Documents:&lt;/strong>; The documents should have complex layout elements. Challenges in practical settings come from the fact that documents may contain tables, key-value pairs, switch between single-column and double-column layout, have varying font-sizes for different sections, include pictures with captions and even footnotes. Contrast this with datasets where most documents are organized in sentences, paragraphs, and chapters with section headers — the kinds of documents that are typically the focus of classic natural language processing literature on &lt;a href=&quot;https://arxiv.org/abs/2007.14062&quot;>;long&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2004.08483&quot;>;inputs&lt;/a>;. &lt;/li>;&lt;li>;&lt;strong>;Diverse Templates:&lt;/strong>; A benchmark should include different structural layouts or templates. It is trivial for a high-capacity model to extract from a particular template by memorizing the structure. However, in practice, one needs to be able to generalize to new templates/layouts, an ability that the train-test split in a benchmark should measure. &lt;/li>;&lt;li>;&lt;strong>;High-Quality OCR&lt;/strong>;: Documents should have high-quality &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;Optical Character Recognition&lt;/a>; (OCR) results. Our aim with this benchmark is to focus on the VRDU task itself and to exclude the variability brought on by the choice of OCR engine. &lt;/li>;&lt;li>;&lt;strong>;Token-Level Annotation&lt;/strong>;: Documents should contain ground-truth annotations that can be mapped back to corresponding input text, so that each token can be annotated as part of the corresponding entity. This is in contrast with simply providing the text of the value to be extracted for the entity. This is key to generating clean training data where we do not have to worry about incidental matches to the given value. For instance, in some receipts, the &#39;total-before-tax&#39; field may have the same value as the &#39;total&#39; field if the tax amount is zero. Having token level annotations prevents us from generating training data where both instances of the matching value are marked as ground-truth for the &#39;total&#39; field, thus producing noisy examples. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s1600/Benchmark%20GIF.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s16000/Benchmark%20GIF.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;VRDU datasets and tasks&lt;/h2>; &lt;p>; The VRDU dataset is a combination of two publicly available datasets, &lt;a href=&quot;https://efile.fara.gov/ords/fara/f?p=1235:10&quot;>;Registration Forms&lt;/a>; and &lt;a href=&quot;https://publicfiles.fcc.gov/&quot;>;Ad-Buy forms&lt;/a>;. These datasets provide examples that are representative of real-world use cases, and satisfy the five benchmark requirements described above. &lt;/p>; &lt;p>; The Ad-buy Forms dataset consists of 641 documents with political advertisement details. Each document is either an invoice or receipt signed by a TV station and a campaign group. The documents use tables, multi-columns, and key-value pairs to record the advertisement information, such as the product name, broadcast dates, total price, and release date and time. &lt;/p>; &lt;p>; The Registration Forms dataset consists of 1,915 documents with information about foreign agents registering with the US government. Each document records essential information about foreign agents involved in activities that require public disclosure. Contents include the name of the registrant, the address of related bureaus, the purpose of activities, and other details. &lt;/p>; &lt;p>; We gathered a random sample of documents from the public &lt;a href=&quot;https://www.fcc.gov/&quot;>;Federal Communications Commission&lt;/a>; (FCC) and &lt;a href=&quot;https://www.justice.gov/nsd-fara&quot;>;Foreign Agents Registration Act&lt;/a>; (FARA) sites, and converted the images to text using &lt;a href=&quot;https://cloud.google.com/&quot;>;Google Cloud&#39;s&lt;/a>; &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;OCR&lt;/a>;. We discarded a small number of documents that were several pages long and the processing did not complete in under two minutes. This also allowed us to avoid sending very long documents for manual annotation — a task that can take over an hour for a single document. Then, we defined the schema and corresponding labeling instructions for a team of annotators experienced with document-labeling tasks. &lt;/p>; &lt;p>; The annotators were also provided with a few sample labeled documents that we labeled ourselves. The task required annotators to examine each document, draw a bounding box around every occurrence of an entity from the schema for each document, and associate that bounding box with the target entity. After the first round of labeling, a pool of experts were assigned to review the results. The corrected results are included in the published VRDU dataset. Please see the &lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;paper&lt;/a>; for more details on the labeling protocol and the schema for each dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s1600/Chart.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s16000/Chart.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Existing academic benchmarks (&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;, &lt;a href=&quot;https://openreview.net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;, &lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-NDA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-Charity&lt;/a>;, &lt;a href=&quot;https://wandb.ai/stacey/deepform_v1/reports/DeepForm-Understand-Structured-Documents-at-Scale--VmlldzoyODQ3Njg&quot;>;DeepForm&lt;/a>;) fall-short on one or more of the five requirements we identified for a good document understanding benchmark. VRDU satisfies all of them. See our &lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;paper&lt;/a>; for background on each of these datasets and a discussion on how they fail to meet one or more of the requirements.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We built four different model training sets with 10, 50, 100, and 200 samples respectively. Then, we evaluated the VRDU datasets using three tasks (described below): (1) Single Template Learning, (2) Mixed Template Learning, and (3) Unseen Template Learning. For each of these tasks, we included 300 documents in the testing set. We evaluate models using the &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1 score&lt;/a>; on the testing set. &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Single Template Learning&lt;/em>; (STL): This is the simplest scenario where the training, testing, and validation sets only contain a single template. This simple task is designed to evaluate a model&#39;s ability to deal with a fixed template. Naturally, we expect very high F1 scores (0.90+) for this task. &lt;/li>;&lt;li>;&lt;em>;Mixed Template Learning&lt;/em>; (MTL): This task is similar to the task that most related papers use: the training, testing, and validation sets all contain documents belonging to the same set of templates. We randomly sample documents from the datasets and construct the splits to make sure the distribution of each template is not changed during sampling. &lt;/li>;&lt;li>;&lt;em>;Unseen Template Learning&lt;/em>; (UTL): This is the most challenging setting, where we evaluate if the model can generalize to unseen templates. For example, in the Registration Forms dataset, we train the model with two of the three templates and test the model with the remaining one. The documents in the training, testing, and validation sets are drawn from disjoint sets of templates. To our knowledge, previous benchmarks and datasets do not explicitly provide such a task designed to evaluate the model&#39;s ability to generalize to templates not seen during training. &lt;/li>; &lt;/ul>; &lt;p>; The objective is to be able to evaluate models on their data efficiency. In our &lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;paper&lt;/a>;, we compared two recent models using the STL, MTL, and UTL tasks and made three observations. First, unlike with other benchmarks, VRDU is challenging and shows that models have plenty of room for improvements. Second, we show that few-shot performance for even state-of-the-art models is surprisingly low with even the best models resulting in less than an F1 score of 0.60. Third, we show that models struggle to deal with structured repeated fields and perform particularly poorly on them. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We release the new &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;Visually Rich Document Understanding&lt;/a>; (VRDU) dataset that helps researchers better track progress on document understanding tasks. We describe why VRDU better reflects practical challenges in this domain. We also present experiments showing that VRDU tasks are challenging, and recent models have substantial headroom for improvements compared to the datasets typically used in the literature with F1 scores of 0.90+ being typical. We hope the release of the VRDU dataset and evaluation code helps research teams advance the state of the art in document understanding. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Many thanks to Zilong Wang, Yichao Zhou, Wei Wei, and Chen-Yu Lee, who co-authored the paper along with Sandeep Tata. Thanks to Marc Najork, Riham Mansour and numerous partners across Google Research and the Cloud AI team for providing valuable insights. Thanks to John Guilyard for creating the animations in this post. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3240052415427459327/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/advances-in-document-understanding.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/advances-in-document-understanding.html&quot; rel=&quot;alternate&quot; title=&quot;Advances in document understanding&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s72-c/Document%20understanding%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-595171581401765238&lt;/id>;&lt;published>;2023-08-08T14:02:00.000-07:00&lt;/published>;&lt;updated>;2023-08-08T14:02:34.659-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AdaTape: Foundation model with adaptive computation and dynamic read-and-write&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Fuzhao Xue, Research Intern, and Mostafa Dehghani, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIqJhdQrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudIEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s2000/adatape.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;>;Adaptive computation&lt;/a>; refers to the ability of a machine learning system to adjust its behavior in response to changes in the environment. While conventional neural networks have a fixed function and computation capacity, ie, they spend the same number of FLOPs for processing different inputs, a model with adaptive and dynamic computation modulates the computational budget it dedicates to processing each input, depending on the complexity of the input. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Adaptive computation in neural networks is appealing for two key reasons. First, the mechanism that introduces adaptivity provides an &lt;a href=&quot;https://en.wikipedia.org/wiki/Inductive_bias&quot;>;inductive bias&lt;/a>; that can play a key role in solving some challenging tasks. For instance, enabling different numbers of computational steps for different inputs can be crucial in solving arithmetic problems that require modeling hierarchies of different depths. Second, it gives practitioners the ability to tune the cost of inference through greater flexibility offered by dynamic computation, as these models can be adjusted to spend more FLOPs processing a new input. &lt;/p>; &lt;p>; Neural networks can be made adaptive by using different functions or computation budgets for various inputs. A deep neural network can be thought of as a function that outputs a result based on both the input and its parameters. To implement adaptive function types, a subset of parameters are selectively activated based on the input, a process referred to as conditional computation. Adaptivity based on the function type has been explored in studies on &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;mixture-of-experts&lt;/a>;, where the sparsely activated parameters for each input sample are determined through routing. &lt;/p>; &lt;p>; Another area of research in adaptive computation involves dynamic computation budgets. Unlike in standard neural networks, such as &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, and &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;ViT&lt;/a>;, whose computation budget is fixed for different samples, &lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;recent research&lt;/a>; has demonstrated that adaptive computation budgets can improve performance on tasks where transformers fall short. Many of these works achieve adaptivity by using dynamic depth to allocate the computation budget. For example, the &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;>;Adaptive Computation Time&lt;/a>; (ACT) algorithm was proposed to provide an adaptive computational budget for recurrent neural networks. The &lt;a href=&quot;https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html&quot;>;Universal Transformer&lt;/a>; extends the ACT algorithm to transformers by making the computation budget dependent on the number of transformer layers used for each input example or token. Recent studies, like &lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;PonderNet&lt;/a>;, follow a similar approach while improving the dynamic halting mechanisms. &lt;/p>; &lt;p>; In the paper “&lt;a href=&quot;https://arxiv.org/abs/2301.13195&quot;>;Adaptive Computation with Elastic Input Sequence&lt;/a>;”, we introduce a new model that utilizes adaptive computation, called &lt;em>;AdaTape&lt;/em>;. This model is a Transformer-based architecture that uses a dynamic set of tokens to create elastic input sequences, providing a unique perspective on adaptivity in comparison to previous works. AdaTape uses an adaptive tape reading mechanism to determine a varying number of tape tokens that are added to each input based on input&#39;s complexity. AdaTape is very simple to implement, provides an effective knob to increase the accuracy when needed, but is also much more efficient compared to &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;other adaptive baselines&lt;/a>; because it directly injects adaptivity into the input sequence instead of the model depth. Finally, Adatape offers better performance on standard tasks, like image classification, as well as algorithmic tasks, while maintaining a favorable quality and cost tradeoff. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Adaptive computation transformer with elastic input sequence&lt;/h2>; &lt;p>; AdaTape uses both the adaptive function types and a dynamic computation budget. Specifically, for a batch of input sequences after tokenization (eg, a linear projection of non-overlapping patches from an image in the vision transformer), AdaTape uses a vector representing each input to dynamically select a variable-sized sequence of tape tokens. &lt;/p>; &lt;p>; AdaTape uses a bank of tokens, called a “tape bank”, to store all the candidate tape tokens that interact with the model through the adaptive tape reading mechanism. We explore two different methods for creating the tape bank: an input-driven bank and a learnable bank. &lt;/p>; &lt;p>; The general idea of the &lt;em>;input-driven &lt;/em>;bank is to extract a bank of tokens from the input while employing a different approach than the original model tokenizer for mapping the raw input to a sequence of input tokens. This enables dynamic, on-demand access to information from the input that is obtained using a different point of view, eg, a different image resolution or a different level of abstraction. &lt;/p>; &lt;p>; In some cases, tokenization in a different level of abstraction is not possible, thus an input-driven tape bank is not feasible, such as when it&#39;s difficult to further split each node in a &lt;a href=&quot;https://arxiv.org/abs/2012.09699&quot;>;graph transformer&lt;/a>;. To address this issue, AdaTape offers a more general approach for generating the tape bank by using a set of trainable vectors as tape tokens. This approach is referred to as the &lt;em>;learnable bank&lt;/em>; and can be viewed as an embedding layer where the model can dynamically retrieve tokens based on the complexity of the input example. The learnable bank enables AdaTape to generate a more flexible tape bank, providing it with the ability to dynamically adjust its computation budget based on the complexity of each input example, eg, more complex examples retrieve more tokens from the bank, which let the model not only use the knowledge stored in the bank, but also spend more FLOPs processing it, since the input is now larger. &lt;/p>; &lt;p>; Finally, the selected tape tokens are appended to the original input and fed to the following transformer layers. For each transformer layer, the same multi-head attention is used across all input and tape tokens. However, two different feed-forward networks (FFN) are used: one for all tokens from the original input and the other for all tape tokens. We observed slightly better quality by using separate feed-forward networks for input and tape tokens. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfdaEU1-w8kv7USzuc-VBwGumvHxfccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s1786/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;871&quot; data-original-width=&quot;1786&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfdaEU1-w8kv7USzuc-VBwGumvHxfccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of AdaTape. For different samples, we pick a variable number of different tokens from the tape bank. The tape bank can be driven from input, eg, by extracting some extra fine-grained information or it can be a set of trainable vectors. Adaptive tape reading is used to recursively select different sequences of tape tokens, with variable lengths, for different inputs. These tokens are then simply appended to inputs and fed to the transformer encoder.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;AdaTape provides helpful inductive bias&lt;/h2>; &lt;p>; We evaluate AdaTape on parity, a very challenging task for the standard Transformer, to study the effect of inductive biases in AdaTape. With the parity task, given a sequence 1s, 0s, and -1s, the model has to predict the evenness or oddness of the number of 1s in the sequence. Parity is the simplest non-counter-free or &lt;a href=&quot;https://arxiv.org/abs/2009.11264&quot;>;periodic regular language&lt;/a>;, but perhaps surprisingly, the task is unsolvable by the standard Transformer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3EoYw4UhzcKK0fGIscOJSv36zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/s866/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;432&quot; data-original-width=&quot;866&quot; height=&quot;319&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3EoYw4UhzcKK0fGIscOJSv36zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/w640-h319/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Evaluation on the parity task. The standard Transformer and Universal Transformer were unable to perform this task, both showing performance at the level of a random guessing baseline.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Despite being evaluated on short, simple sequences, both the standard Transformer and Universal Transformers were unable to perform the parity task as they are unable to maintain a counter within the model. However, AdaTape outperforms all baselines, as it incorporates a lightweight recurrence within its input selection mechanism, providing an inductive bias that enables the implicit maintenance of a counter, which is not possible in standard Transformers. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation on image classification&lt;/h2>; &lt;p>; We also evaluate AdaTape on the image classification task. To do so, we trained AdaTape on &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet-1K&lt;/a>; from scratch. The figure below shows the accuracy of AdaTape and the baseline methods, including &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>;, and the Universal Transformer ViT (UViT and U2T) versus their speed (measured as number of images, processed by each code, per second). In terms of quality and cost tradeoff, AdaTape performs much better than the alternative adaptive transformer baselines. In terms of efficiency, larger AdaTape models (in terms of parameter count) are faster than smaller baselines. Such results are consistent with the finding from &lt;a href=&quot;https://arxiv.org/abs/2110.12894&quot;>;previous work that&lt;/a>; shows that the adaptive model depth architectures are not well suited for many accelerators, like the TPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7ZIh6Rujxnd-H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H-Hxzek9ziXZsw5adSEMZU/s1473/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;657&quot; data-original-width=&quot;1473&quot; height=&quot;285&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7ZIh6Rujxnd-H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H-Hxzek9ziXZsw5adSEMZU/w640-h285/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We evaluate AdaTape by training on ImageNet from scratch. For &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>;, we not only report their results from the paper but also re-implement A-ViT by training from scratch, ie, A-ViT(Ours).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A study of AdaTape&#39;s behavior&lt;/h2>; &lt;p>; In addition to its performance on the parity task and ImageNet-1K, we also evaluated the token selection behavior of AdaTape with an input-driven bank on the &lt;a href=&quot;https://arxiv.org/abs/1707.02968&quot;>;JFT-300M&lt;/a>; validation set. To better understand the model&#39;s behavior, we visualized the token selection results on the &lt;em>;input-driven bank&lt;/em>; as heatmaps, where lighter colors mean that position is more frequently selected. The heatmaps reveal that AdaTape more frequently picks the central patches. This aligns with our prior knowledge, as central patches are typically more informative — especially in the context of datasets with natural images, where the main object is in the middle of the image. This result highlights the intelligence of AdaTape, as it can effectively identify and prioritize more informative patches to improve its performance. &lt;/p>; &lt;p>;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgkR2VcDoKTicWQ_hufXwrlAcPMoeYqcrCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/s839/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;839&quot; height=&quot;270&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgkR2VcDoKTicWQ_hufXwrlAcPMoeYqcrCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/w640-h270/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We visualize the tape token selection heatmap of AdaTape-B/32 (left) and AdaTape-B/16 (right). The hotter / lighter color means the patch at this position is more frequently selected.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; AdaTape is characterized by elastic sequence lengths generated by the adaptive tape reading mechanism. This also introduces a new inductive bias that enables AdaTape to have the potential to solve tasks that are challenging for both standard transformers and existing adaptive transformers. By conducting comprehensive experiments on image recognition benchmarks, we demonstrate that AdaTape outperforms standard transformers and adaptive architecture transformers when computation is held constant. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;One of the authors of this post, Mostafa Dehghani, is now at Google DeepMind. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/595171581401765238/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html&quot; rel=&quot;alternate&quot; title=&quot;AdaTape: Foundation model with adaptive computation and dynamic read-and-write&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIqJhdQrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudIEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s72-c/adatape.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;