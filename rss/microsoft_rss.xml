<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2024 年 8 月 29 日星期四 16:24:51 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.6.1</generator><item><title>人工智能创新：受大脑启发的设计，打造更强大、更可持续的技术</title><link/>https://www.microsoft.com/en-us/research/blog/innovations-in-ai-brain-inspired-design-for-more-capable-and-sustainable-technology/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Thu, 29 Aug 2024 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/blog/innovations-in-ai-brain-inspired-design-for-more-capable-and-sustainable-technology/ </guid><description><![CDATA[<p>研究人员及其合作者正在从大脑中汲取灵感，以开发更可持续的人工智能模型。 CircuitNet 和 CPG-PE 等项目通过模仿大脑的神经模式来提高性能和能源效率。</p><p>这篇文章《 <a href="https://www.microsoft.com/en-us/research/blog/innovations-in-ai-brain-inspired-design-for-more-capable-and-sustainable-technology/">人工智能创新：以大脑为灵感的设计，打造更强大、更可持续的技术》</a>首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1.jpg" alt="该图展示了生物神经网络中四种常见的神经连接模式：前馈、相互、横向和反馈。每个图案由代表神经元的圆圈和代表突触的箭头组成。 " class="wp-image-1080474" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Design-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>随着人工智能研究和技术发展的不断进步，还需要考虑管理大型数据集和执行困难计算所需的能源和基础设施资源。当我们向大自然寻找效率模型时，人脑脱颖而出，能够机智地处理复杂的任务。受此启发，微软的研究人员正在寻求了解大脑的高效过程，并将其复制到人工智能中。</p><p>在<a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">微软亚洲研究院<span class="sr-only">（在新选项卡中打开）</span></a> ，与<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.fudan.edu.cn/en/" target="_blank" rel="noreferrer noopener">复旦大学<span class="sr-only">（在新选项卡中打开）</span></a> 、 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://en.sjtu.edu.cn/" target="_blank" rel="noreferrer noopener">上海交通大学<span class="sr-only">（在新选项卡中打开）</span></a>和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.oist.jp/" target="_blank" rel="noreferrer noopener">冲绳工业大学<span class="sr-only">（在新选项卡中打开）</span></a>合作的三个著名项目正在进行中。其中一个介绍了一种模拟大脑学习和计算信息方式的神经网络；另一个提高了未来事件预测模型的准确性和效率；第三个提高人工智能在语言处理和模式预测方面的熟练程度。本博文中重点介绍的这些项目不仅旨在提高性能，而且还显着降低功耗，为更可持续的人工智能技术铺平道路。</p><h2 class="wp-block-heading" id="circuitnet-simulates-brain-like-neural-patterns"> CircuitNet 模拟类脑神经模式</h2><p>许多人工智能应用都依赖于人工神经网络，旨在模仿大脑复杂的神经模式。这些网络通常仅复制一种或两种类型的连接模式。相比之下，大脑使用各种神经连接模式传播信息，包括前馈兴奋和抑制、相互抑制、侧向抑制和反馈抑制（图 1）。这些网络包含紧密互连的局部区域，而遥远区域之间的连接较少。每个神经元形成数千个突触来执行其区域内的特定任务，而一些突触连接不同的功能簇——相互连接的神经元组，它们一起工作来执行特定的功能。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" width="854" height="225" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-1.png" alt="该图展示了生物神经网络中四种常见的神经连接模式：前馈、相互、横向和反馈。每个图案由代表神经元的圆圈和代表突触的箭头组成。 " class="wp-image-1080528" style="width:700px" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-1.png 854w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-1-300x79.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-1-768x202.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-1-240x63.png 240w" sizes="(max-width: 854px) 100vw, 854px" /><figcaption class="wp-element-caption">图 1：大脑中的四种神经连接模式。每个圆圈代表一个神经元，每个箭头代表一个突触。</figcaption></figure><p>受这种生物架构的启发，研究人员开发了<a href="https://www.microsoft.com/en-us/research/publication/circuitnet-a-generic-neural-network-to-realize-universal-circuit-motif-modeling/">CircuitNet</a> ，这是一种复制多种类型连接模式的神经网络。 CircuitNet 的设计结合了密集连接的本地节点和较少的远距离区域之间的连接，从而能够通过电路基序单元 (CMU)（有助于处理信息的小型重复连接模式）增强信号传输。这种结构（如图 2 所示）支持多轮信号处理，有可能改进人工智能系统处理复杂信息的方式。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="696" height="205" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-2.png" alt="说明 CircuitNet 架构的图表。左侧标有“模型输入”和“模型输出”的图表明 CircuitNet 可以处理各种输入形式并产生相应的输出。中间部分标记为“CircuitNet”，描述了几个称为电路主题单元（简称 CMU）的互连块，它们通过直接连接维持局部密集通信，并通过其输入和输出端口维持全局稀疏通信。右侧，单个 CMU 的详细视图揭示了紧密互连的神经元，展示了每个 CMU 如何模拟通用电路主题。" class="wp-image-1080522" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-2.png 696w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-2-300x88.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-2-240x71.png 240w" sizes="(max-width: 696px) 100vw, 696px" /><figcaption class="wp-element-caption">图 2.CircuitNet 的架构：通用神经网络执行各种任务，接受不同的输入，并生成相应的输出（左）。 CMU 将大多数连接保留在本地，而很少有长距离连接，从而提高了效率（中）。每个 CMU 都有密集互连的神经元来模拟通用电路模式（右）。</figcaption></figure><p>评估结果是有希望的。 CircuitNet 在函数逼近、强化学习、图像分类和时间序列预测方面优于几种流行的神经网络架构。它还实现了与其他神经网络相当或更好的性能，通常使用更少的参数，展示了其在各种机器学习任务中的有效性和强大的泛化能力。我们的下一步是在具有数十亿参数的大型模型上测试 CircuitNet 的性能。 </p><h2 class="wp-block-heading" id="spiking-neural-networks-a-new-framework-for-time-series-prediction">尖峰神经网络：时间序列预测的新框架</h2><p>尖峰神经网络 (SNN) 正在成为一种强大的人工神经网络，以其能源效率和在机器人、边缘计算和实时处理等领域的潜在应用而闻名。与连续处理信号的传统神经网络不同，SNN 仅在达到特定阈值时激活神经元，产生尖峰。这种方法模拟大脑处理信息和节省能量的方式。然而，SNN 不擅长根据历史数据预测未来事件，而这在交通和能源等领域是一个关键功能。</p><p>为了提高 SNN 的预测能力，研究人员提出了一种<a href="https://www.microsoft.com/en-us/research/publication/efficient-and-effective-time-series-forecasting-with-spiking-neural-networks/">SNN 框架</a>，旨在预测随时间变化的趋势，例如电力消耗或交通模式。这种方法利用了尖峰神经元处理时间信息的效率，并同步定期收集的时间序列数据和 SNN。两个编码层将时间序列数据转换为尖峰序列，允许 SNN 处理它们并做出准确的预测，如图 3 所示。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="746" height="331" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-3.png" alt="该图展示了基于 SNN 的时间序列预测的新框架。该图显示了从时间序列输入开始的过程，该输入由新颖的尖峰编码器编码为尖峰。然后将这些尖峰输入不同的 SNN 模型：(a) Spike-TCN、(b) Spike-RNN 和 (c) Spike-Transformer。最后，将学习到的特征输入到投影层进行预测。" class="wp-image-1080531" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-3.png 746w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-3-300x133.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-3-240x106.png 240w" sizes="(max-width: 746px) 100vw, 746px" /><figcaption class="wp-element-caption">图 3. 基于 SNN 的时间序列预测的新框架：使用新型尖峰编码器将时间序列数据编码为尖峰（中、下）。然后，尖峰由 SNN 模型（Spike-TCN、Spike-RNN 和 Spike-Transformer）进行处理以进行学习（上）。最后，将学习到的特征输入到投影层进行预测（右下）。</figcaption></figure><p>测试表明，这种 SNN 方法对于时间序列预测非常有效，通常可以匹配或优于传统方法，同时显着降低能耗。 SNN 成功捕获时间依赖性并对时间序列动态进行建模，提供与大脑处理信息的方式紧密结合的节能方法。我们计划根据大脑处理信息的方式继续探索进一步改进 SNN 的方法。</p><h2 class="wp-block-heading" id="refining-snn-sequence-prediction">细化 SNN 序列预测</h2><p>虽然 SNN 可以帮助模型预测未来事件，但研究表明，它对基于脉冲的通信的依赖使得直接应用人工神经网络中的许多技术变得具有挑战性。例如，SNN 很难有效地处理自然语言处理和时间序列分析中发现的节奏和周期性模式。作为回应，研究人员开发了一种名为<a href="https://www.microsoft.com/en-us/research/publication/advancing-spiking-neural-networks-for-sequential-modeling-with-central-pattern-generators/">CPG-PE</a>的 SNN 新方法，它结合了两种技术：</p><ol start="1" class="wp-block-list"><li>中枢模式发生器 (CPG)：脑干和脊髓中的神经网络，自主生成节律模式，控制运动、呼吸和咀嚼等功能</li></ol><ol start="2" class="wp-block-list"><li>位置编码 (PE)：帮助人工神经网络识别序列中元素的顺序和相对位置的过程</li></ol><p>通过集成这两种技术，CPG-PE 帮助 SNN 识别信号的位置和时间，提高它们处理基于时间的信息的能力。该过程如图 4 所示。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="980" height="289" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-4.png" alt="说明 CPG-PE 在 SNN 中的应用的图表。它显示了三个主要组成部分：标记为“X”的输入尖峰矩阵、涉及位置编码和线性变换以产生“X&#39;”的变换过程以及标记为“X_output”的尖峰神经元层的输出。输入矩阵“X”有多行对应于不同的通道或神经元，每行都包含时间步长的尖峰。变换过程将维度从 (D + 2N) 映射到 D。尖峰神经元层采用变换后的输入“X”并生成输出尖峰矩阵“X_output”。" class="wp-image-1080534" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-4.png 980w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-4-300x88.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-4-768x226.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Figure-4-240x71.png 240w" sizes="(max-width: 980px) 100vw, 980px" /><figcaption class="wp-element-caption">图 4：CPG-PE 在 SNN 中的应用。 X、X&#39; 和 X 输出是尖峰矩阵。</figcaption></figure><p>我们使用四个真实世界数据集来评估 CPG-PE：两个数据集涵盖交通模式，各一个数据集用于电力消耗和太阳能。结果表明，使用这种方法的 SNN <a href="https://www.microsoft.com/en-us/research/publication/advancing-spiking-neural-networks-for-sequential-modeling-with-central-pattern-generators/">明显优于没有位置编码</a>(PE) 的 SNN，如表 1 所示。此外，CPG-PE 可以轻松集成到任何为序列处理而设计的 SNN 中，使其适用于各种神经形态芯片和SNN 硬件。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1004" height="628" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Table-1.png" alt="表格显示了 Metr-la 和 Pems-bay 两个数据集上时间序列预测的实验结果，预测长度为 6、24、48 和 96。该表比较了各种模型的性能，包括不同配置的 SNN、RNN和变形金刚。报告 RSE 和 R^2 等性能指标。最佳 SNN 结果以粗体突出显示，向上箭头表示得分更高，代表更好的性能。" class="wp-image-1080537" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Table-1.png 1004w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Table-1-300x188.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Table-1-768x480.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/Brain-Inspired-Table-1-240x150.png 240w" sizes="(max-width: 1004px) 100vw, 1004px" /><figcaption class="wp-element-caption">表1：预测长度为6、24、48、96的两个基准的时间序列预测评估结果。“Metr-la”和“Pems-bay”是交通模式数据集。最佳 SNN 结果以粗体显示。向上箭头表示分数更高，代表更好的性能。 </figcaption></figure><h2 class="wp-block-heading" id="ongoing-ai-research-for-greater-capability-efficiency-and-sustainability">持续进行人工智能研究，以提高能力、效率和可持续性</h2><p>本博客中强调的创新展示了创造不仅能力更强而且效率更高的人工智能的潜力。展望未来，我们很高兴能够深化合作，继续将神经科学的见解应用于人工智能研究，继续致力于探索开发更可持续技术的方法。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p>这篇文章《 <a href="https://www.microsoft.com/en-us/research/blog/innovations-in-ai-brain-inspired-design-for-more-capable-and-sustainable-technology/">人工智能创新：以大脑为灵感的设计，打造更强大、更可持续的技术》</a>首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>;</content:encoded></item><item><title>研究重点：2024 年 8 月 26 日当周</title><link/>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-august-26-2024/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Wed, 28 Aug 2024 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1068642 </guid><description><![CDATA[<p>在 9 月 3 日的研究论坛上了解人工智能的下一步发展；  WizardArena 模拟人类注释的聊天机器人游戏； MInference 通过动态稀疏注意力加速长上下文 LLM 的预填充； Reef：快速简洁的非交互式零知识正则表达式证明。</p><p> 《<a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-august-26-2024/">研究焦点：2024 年 8 月 26 日一周》</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-pullquote"><blockquote><p>欢迎来到研究焦点，这是一系列博客文章，重点介绍 Microsoft 研究社区的著名出版物、活动、代码/数据集、新员工和其他里程碑。 </p></blockquote></figure><figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1.jpg" alt="背景为蓝色和紫色的波浪形状的装饰图形。中间左侧的文字叠加显示：“研究焦点：2024 年 8 月 26 日”" class="wp-image-1068687" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/08/RF48-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-b60fbfa117bcce78e07885aa24d19fc7" id="new-research">事件</h2><h2 class="wp-block-heading" id="heading">立即报名参加 9 月 3 日的研究论坛</h2><p>在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/?OCID=msr_researchforum_ep4_RF48_rfhome_2024" target="_blank" rel="noreferrer noopener">Microsoft 研究论坛<span class="sr-only">（在新选项卡中打开）</span></a>中了解 AI 世界的未来，这是一个探索最新研究进展、大胆新想法以及与全球研究界的重要讨论的活动系列。</p><p>在第 4 集中，了解 Microsoft 在多模式 AI 前沿的研究计划。发现用于自我改进、代理、天气预报等的新颖模型、基准和基础设施。</p><p>您的一次性注册包括在活动当天访问我们与研究人员的实时聊天。</p><p>第 4 集将于太平洋时间 9 月 3 日星期二上午 9:00 播出。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-1 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--fe8644c902b46673b1b38fafd7bf0d45"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://register.researchforum.microsoft.com/?OCID=msr_researchforum_ep4_RF48_register_2024" target="_blank" rel="noreferrer noopener">立即注册</a></div></div></div><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="970287"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-the-future-of-scale-with-ahmed-awadallah-and-ashley-llorens/" aria-label="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" data-bi-cN="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/Ahmed_AI_Frontiers_TW_LI_FB_1200x627_With_Name.png" alt="MSR 播客 |人工智能前沿 |艾哈迈德·阿瓦达拉" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4"> AI 前沿：Ahmed Awadallah 和 Ashley Llorens 的规模化未来</h2><p class="large">本集的主角是高级首席研究经理<a href="https://www.microsoft.com/en-us/research/people/hassanam/" target="_blank" rel="noreferrer noopener">Ahmed H. Awadallah</a> ，他致力于提高大规模人工智能模型的效率，并努力帮助推动该领域从研究到实践的进步<strong> </strong>使他处于人工智能新时代的前沿。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-the-future-of-scale-with-ahmed-awadallah-and-ashley-llorens/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">新研究</h2><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading" id="can-llms-learn-by-teaching-a-preliminary-study"> LLM 可以通过教学来学习吗？初步研究</h2><p>改进学生模型的教学（例如知识蒸馏）是大语言模型（LLM）中广泛研究的方法。然而，对于人类来说，教学不仅提高了学生，也提高了教师。在最近的一篇论文中： <a href="https://www.microsoft.com/en-us/research/publication/can-llms-learn-by-teaching-a-preliminary-study/" target="_blank" rel="noreferrer noopener">法学硕士可以通过教学来学习吗？</a>来自 Microsoft 的研究人员和外部同事<a href="https://www.microsoft.com/en-us/research/publication/can-llms-learn-by-teaching-a-preliminary-study/" target="_blank" rel="noreferrer noopener">进行了一项初步研究</a>，探讨该规则是否也适用于法学硕士。如果是这样，这可能使模型能够不断进步和改进，而无需仅仅依赖人类生成的数据或更强大的模型。</p><p>在本文中，研究人员表明，边教边学（LbT）实践可以纳入现有的法学硕士培训/提示流程中，并提供显着的改进。他们设计了三种方法，每种方法都模仿人类 LbT 的三个级别之一：观察学生的反馈；从反馈中学习；迭代学习，目标是在不训练的情况下提高答案准确性，并通过微调提高模型的固有能力。结果表明，LbT 是一种很有前途的范式，可以提高法学硕士在多个复杂任务（例如数学推理、竞赛级代码合成）上的推理能力和结果。主要发现是：（1）LbT 可以诱导弱到强的泛化——强模型可以通过教导其他弱模型来改进自身； (2) 学生模型的多样性可能会有所帮助——教授多个学生模型可能比教授一个学生模型或教师本身更好。这项研究还为未来将更多的教育策略融入法学硕士的学习过程提供了路线图。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-2 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-fill-github"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/imagination-research/lbt" target="_blank" rel="noreferrer noopener">下载代码</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">新研究</h2><h2 class="wp-block-heading" id="heading">竞技场学习：通过模拟聊天机器人竞技场为法学硕士培训后构建数据飞轮</h2><p>在聊天机器人之间进行人工注释的竞赛是评估大型语言模型 (LLM) 有效性的一种非常有效的方法。然而，这个过程需要很高的成本和时间，使得通过后期培训提高法学硕士的难度变得复杂。在最近的预印本： <a href="https://www.microsoft.com/en-us/research/publication/arena-learning-build-data-flywheel-for-llms-post-training-via-simulated-chatbot-arena/" target="_blank" rel="noreferrer noopener">Arena Learning：通过模拟聊天机器人竞技场为 LLM 构建数据飞轮后训练中</a>，来自 Microsoft 的研究人员和外部同事介绍了一种创新的离线策略，旨在模拟这些竞技场战斗。这包括一套全面的模拟战斗指令，使用人工智能驱动的注释来评估战斗结果，通过监督微调和强化学习促进目标模型的持续改进。这种方法的一个关键方面是确保精确评估并实现离线模拟和在线比赛之间的一致性。</p><p>为此，研究人员推出了<strong>WizardArena</strong> ，这是一个精心设计的离线测试集，旨在准确预测各种模型的 Elo 排名。他们的发现表明，WizardArena 的预测与在线竞技场的预测非常一致。他们应用这个新颖的框架来训练模型<strong>WizardLM-β</strong> ，该模型展示了各种指标的显着性能增强。这种完全自动化的培训和评估流程为各种法学硕士通过培训后持续不断的进步铺平了道路。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-3 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--969afd66044d52b0f67a1f309bc22baf"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/arena-learning-build-data-flywheel-for-llms-post-training-via-simulated-chatbot-arena/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/></div><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">新研究</h2><h2 class="wp-block-heading" id="heading">MInference 1.0：通过动态稀疏注意力加速长上下文法学硕士的预填充</h2><p>大语言模型 (LLM) 推理的计算挑战限制了其广泛部署，特别是随着提示长度不断增加。由于注意力计算的二次复杂度，80亿参数的LLM在单个NVIDIA A100图形处理单元（GPU）上处理100万个token的提示（即预填充阶段）需要30分钟。现有的加速预填充的方法通常无法保持可接受的准确性或效率。</p><p>在最近的预印本《 <a href="https://www.microsoft.com/en-us/research/publication/minference-1-0-accelerating-pre-filling-for-long-context-llms-via-dynamic-sparse-attention/">MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充》</a>中，微软的研究人员介绍了一种稀疏计算方法，旨在加速长序列处理的预填充。他们确定了长上下文注意力矩阵中的三种独特模式——<strong> </strong>A 形、垂直斜线和块稀疏——可用于在 GPU 上进行高效稀疏计算。他们离线确定每个注意力头的最佳模式，并在推理过程中根据分配的模式动态构建稀疏索引。然后，他们通过优化的 GPU 内核执行高效的稀疏注意力计算，以减少长上下文 LLM 预填充阶段的延迟。研究表明，MInference（百万代币推理）可将 A100 上预填充的推理延迟降低多达 10 倍，同时保持准确性。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-4 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--a63d49054c863e43b5be47dbeb92d714"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/minference-1-0-accelerating-pre-filling-for-long-context-llms-via-dynamic-sparse-attention/">阅读论文</a></div><div class="wp-block-button is-style-outline is-style-outline--823d5e5340866359644c7cda91be221a"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://aka.ms/MInference" target="_blank" rel="noreferrer noopener">查看 GitHub</a> </div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/></div><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">新研究</h2><h2 class="wp-block-heading" id="heading">Reef：快速简洁非交互式零知识正则表达式证明</h2><p>正则表达式 (regex) 用于在各种应用程序中表示和匹配文本文档中的模式：内容审核、输入验证、防火墙、临床试验等。现有用例假设正则表达式和文档都可供查询者使用，因此它们可以使用标准算法自行匹配正则表达式。但是，如果文档实际上由其他人持有，而除了文档与特定正则表达式匹配或不匹配的事实之外，他们不希望向查询者透露有关该文档的任何信息，那么情况又如何呢？证明这些事实的能力可以实现有趣的新应用。</p><p>在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/reef-fast-succinct-non-interactive-zero-knowledge-regex-proofs/" target="_blank" rel="noreferrer noopener">Reef：快速简洁非交互式零知识正则表达式证明中</a>，来自微软和宾夕法尼亚大学的研究人员提出了一个系统，用于生成可公开验证的、简洁的、非交互式的零知识证明，以证明提交的文档匹配或与正则表达式不匹配。他们描述了一些应用，例如证明密码的强度、尽管经过编辑的电子邮件的来源、不经意的 DNS 查询的有效性以及 DNA 突变的存在。实验评估证实Reef可以为3200万字符的文档生成证明；这些证据很小，验证起来也很便宜，只需不到一秒钟。</p><p> Reef 是基于 Microsoft Research <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/Nova" target="_blank" rel="noreferrer noopener">Nova: High-speed recursive Arguments from Folding Schemes <span class="sr-only">（在新选项卡中打开）</span>的开源项目构建的，</a>该项目实现了题为<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://eprint.iacr.org/2021/370.pdf" target="_blank" rel="noreferrer noopener">Nova：来自折叠方案的递归零知识参数的</a>论文中描述的早期研究工作<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://eprint.iacr.org/2021/370.pdf" target="_blank" rel="noreferrer noopener"><span class="sr-only">（在新选项卡中打开）</span></a>由微软、卡内基梅隆大学和纽约大学的研究人员完成。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-5 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--9251d0c5e4d0b699f3658e77cbbad7b3"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/reef-fast-succinct-non-interactive-zero-knowledge-regex-proofs/">阅读论文</a></div><div class="wp-block-button is-style-outline is-style-outline--73b3ab29aeab5586463c8686d4ef49ec"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/Nova" target="_blank" rel="noreferrer noopener">查看 GitHub</a> </div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/></div><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">新研究</h2><h2 class="wp-block-heading" id="heading">HyperNova：可定制约束系统的递归参数</h2><p>增量可验证计算（IVC）是一种强大的加密工具，允许用户生成<strong> </strong>以增量方式正确执行“长时间运行”计算的证明。 IVC 在去中心化设置中支持各种应用，包括可验证的延迟函数、简洁的区块链、汇总、可验证的状态机和机器执行的证明。</p><p>在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/hypernova-recursive-arguments-for-customizable-constraint-systems/" target="_blank" rel="noreferrer noopener">HyperNova：可定制约束系统的递归论证</a>中，微软和卡内基梅隆大学的研究人员介绍了一种新的递归论证，用于证明增量计算，其步骤用 CCS 表示，CCS 是一种可定制约束系统，可同时推广 Plonkish、R1CS 和 AIR没有管理费用。 HyperNova 解决了递归参数领域的四个主要问题。</p><p>首先，它为 CCS 提供了一种折叠方案，其中证明者的密码成本是大小等于约束系统中变量数量的单个多标量乘法 (MSM)，这在使用基于 MSM 的承诺方案时是最佳的。这使得构建 IVC 的泛化变得更加容易，例如携带数据的证明 (PCD)。其次，在有状态机器（例如，EVM、RISC-V）上证明程序执行的成本仅与表示程序步骤调用的指令的电路的大小成正比。第三，研究人员使用折叠方案“随机化”IVC 证明，“免费”实现零知识，并且无需使用零知识 SNARK。第四，研究人员展示了如何在椭圆曲线循环上有效实例化 HyperNova。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-6 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--b6559e3cb0fcf94b3fa3d871f20e901a"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/hypernova-recursive-arguments-for-customizable-constraint-systems/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p>《<a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-august-26-2024/">研究焦点：2024 年 8 月 26 日一周》</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>