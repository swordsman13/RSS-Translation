<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2023 年 12 月 7 日星期四 21:47:00 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.4.1</generator><item><title> MatterGen：属性引导的材料设计</title><link/>https://www.microsoft.com/en-us/research/blog/mattergen-property-guided-materials-design/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Thu, 07 Dec 2023 20:32:32 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=990009 </guid><description><![CDATA[<p>材料科学的中心问题是发现具有所需特性的材料。 MatterGen 支持广泛的属性引导材料设计。</p><p><a href="https://www.microsoft.com/en-us/research/blog/mattergen-property-guided-materials-design/">《MatterGen：属性引导材料设计》</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1.jpg" alt="物质生成器" class="wp-image-990387" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/MatterGen-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p><a href="https://www.microsoft.com/en-us/research/wp-admin/edit.php?post_type=post"></a>生成式人工智能彻底改变了我们创建文本和图像的方式。设计新颖的材料怎么样？<a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-ai4science/">微软研究院 AI4Science</a>很高兴宣布推出<a href="https://www.microsoft.com/en-us/research/publication/mattergen-a-generative-model-for-inorganic-materials-design/">MatterGen</a> ，这是我们的生成模型，可实现广泛的属性引导材料设计。</p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/mattergen-a-generative-model-for-inorganic-materials-design/">阅读论文</a></div></div><div class="wp-block-columns aligncenter"><div class="wp-block-column is-vertically-aligned-center aligncenter" style="flex-basis:45%"><figure class="wp-block-video aligncenter"><video autoplay controls loop src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/paper_final_video_firstframe.mp4" playsinline></video></figure></div></div><p>材料科学的核心挑战是发现具有所需特性的材料，例如电池材料的高锂离子电导率。传统上，这是通过首先寻找新颖材料，然后根据应用进行过滤来完成的。这就像尝试通过首先生成一百万个不同的图像，然后搜索有猫的图像来创建猫的图像。在 MatterGen 中，我们直接生成具有所需特性的新型材料，类似于 DALL·E 3 处理图像生成的方式。</p><p> MatterGen 是一种扩散模型，专门用于生成新颖、稳定的材料。 MatterGen 还拥有适配器模块，可以在给定广泛的限制（包括化学、对称性和属性）的情况下进行微调以生成材料。 MatterGen 生成的结构比 SOTA 模型 (CDVAE) 稳定 2.9 倍（训练 + 测试数据凸包的≤ 0.1 eV/原子）、新颖、独特的结构。它还生成接近能量局部最小值 17.5 倍的结构。 MatterGen 可以通过无分类器的指导直接生成满足所需磁性、电子、机械性能的材料。我们使用基于 DFT 的工作流程验证生成的材料。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" width="1600" height="2133" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/blog_figure_1.jpg" alt="图 1（替代文本）该图显示了六对晶体结构，每个属性约束有两对。属性约束为，从上到下、从左到右，高空间群对称性、高体积模量、目标化学体系、目标带隙、高磁密度、组合高磁密度和低 HHI 指数。" class="wp-image-990054" style="width:698px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/blog_figure_1.jpg 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/blog_figure_1-225x300.jpg 225w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/blog_figure_1-768x1024.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/blog_figure_1-1152x1536.jpg 1152w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/blog_figure_1-1536x2048.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/blog_figure_1-135x180.jpg 135w" sizes="(max-width: 1600px) 100vw, 1600px" /><figcaption class="wp-element-caption"><em>图 1：MatterGen 生成的稳定的新材料，但属性受到限制。</em></figcaption></figure><p>此外，MatterGen 可以不断生成满足高体积模量等目标特性的新型材料，而筛选方法却因数据库中的材料耗尽而饱和。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" width="1637" height="955" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/bulk_modulus_line-1.jpg" alt="这是一个线图。 x轴表示DFT属性计算调用次数； y 轴报告找到的结构的数量。剧情标题说"Bulk modulus greater than 400 gigapascal". 
The plot has three lines, one for MatterGen, one for Screening, and one for Labeled Data. The line for MatterGen increases linearly and reaches a y value of 260 at the x value of 500. The line for screening increases until an x value of 300 and then remains almost flat, plateauing at a level of y equaling 140 for x equaling 500. 
The line for Labeled Data remains flat at the y value of 2 for every value of x." class="wp-image-990024" style="width:672px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/bulk_modulus_line-1.jpg 1637w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/bulk_modulus_line-1-300x175.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/bulk_modulus_line-1-1024x597.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/bulk_modulus_line-1-768x448.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/bulk_modulus_line-1-1536x896.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/bulk_modulus_line-1-480x280.jpg 480w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/bulk_modulus_line-1-240x140.jpg 240w" sizes="(max-width: 1637px) 100vw, 1637px" /><figcaption class="wp-element-caption"><em>图 2：MatterGen 发现了比筛选基线更新颖的稳定高体积模量材料，并且不会因计算资源的增加而停滞不前。 MatterGen 可以找到超过 250 种体积模量 >; 400 GPa 的材料，而在参考数据集中只找到 2 种这样的材料。</em></figcaption></figure><p> MatterGen 还可以在给定目标化学系统的情况下生成材料。它的性能优于配备 MLFF 过滤的替换和随机结构搜索基线，特别是在具有挑战性的 5 元素系统中。 MatterGen 还可以生成给定目标空间群的结构。最后，我们解决了寻找低供应链风险磁铁的多属性材料设计问题。 MatterGen 提出了具有高磁场密度和低供应链风险化学成分的结构。</p><p>我们相信 MatterGen 是材料设计人工智能领域向前迈出的重要一步。我们的结果目前通过 DFT 进行验证，但它有许多已知的局限性。实验验证仍然是实际影响的最终检验，我们希望后续能有更多结果。</p><p>如果没有<a href="https://www.microsoft.com/en-us/research/people/fowlerandrew/">Andrew Fowler</a> 、 <a href="https://www.microsoft.com/en-us/research/people/claudiozeni/">Claudio Zeni</a> 、 <a href="https://www.microsoft.com/en-us/research/people/dzuegner/">Daniel Zügner</a> 、 <a href="https://www.microsoft.com/en-us/research/people/mahorton/">Matthew Horton</a> 、 <a href="https://www.microsoft.com/en-us/research/people/rpinsler/">Robert Pinsler</a> 、 <a href="https://www.microsoft.com/en-us/research/people/ryoto/">Ryota Tomioka</a> 、 <a href="https://www.microsoft.com/en-us/research/people/tianxie/">Tian Xie</a>以及我们出色的实习生 Fu 翔、Sasha Shysheya、Jonathan Crabbé 以及<a href="https://www.microsoft.com/en-us/research/people/jakesmith/">Jake</a>之间的高度协作，这一切都是不可能实现的<a href="https://www.microsoft.com/en-us/research/people/jakesmith/">Smith</a> 、<a href="https://www.microsoft.com/en-us/research/people/lixinsun/">孙立新</a>以及整个 AI4Science 材料设计团队。</p><p>我们还感谢 Microsoft Research、 <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-ai4science/">AI4Science</a>和 Azure Quantum 的所有帮助。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/mattergen-property-guided-materials-design/">《MatterGen：属性引导材料设计》</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </content:encoded><enclosure length="15811037" type="video/mp4" url="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/paper_final_video_firstframe.mp4"></enclosure></item><item><title> LLMLingua：​​通过即时压缩创新法学硕士效率</title><link/>https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Thu, 07 Dec 2023 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=987321 </guid><description><![CDATA[<p>法学硕士的高级提示技术可能会导致提示过长，从而引发问题。了解 LLMLingua 如何将提示压缩高达 20 倍、保持质量、减少延迟并支持改进的用户体验。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/">LLMLingua 文章：通过即时压缩创新 LLM 效率</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center"><strong><em>这篇研究论文发表在</em></strong><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://2023.emnlp.org/" target="_blank" rel="noreferrer noopener"><strong>2023 年自然语言处理经验方法会议</strong><span class="sr-only">（在新选项卡中打开）</span></a> <strong><em>(EMNLP 2023) 上，这是自然语言处理和人工智能的顶级会议。</em></strong> </p><figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1.png" alt="EMNLP 2023 徽标位于已接受论文的左侧"LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models" on a blue/green gradient background" class="wp-image-987333" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>随着大型语言模型 (LLM) 模型的进步及其潜力变得越来越明显，人们逐渐认识到其输出的质量与给出的提示的性质直接相关。这导致了提示技术的兴起，例如思维链（CoT）和情境学习（ICL），它们有助于增加提示长度。在某些情况下，提示现在扩展到数万个标记或文本单元，甚至更多。虽然较长的提示具有相当大的潜力，但它们也带来了许多问题，例如需要超过聊天窗口的最大限制、保留上下文信息的能力降低以及 API 成本增加（无论是在货币方面还是在计算资源方面）。</p><p>为了应对这些挑战，我们在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://2023.emnlp.org/" target="_blank" rel="noreferrer noopener">EMNLP <span class="sr-only">2023</span></a>上发表的论文“ <a href="https://www.microsoft.com/en-us/research/publication/llmlingua-compressing-prompts-for-accelerated-inference-of-large-language-models/">LLMLingua：​​压缩大型语言模型加速推理的提示<span class="sr-only">（在新选项卡中打开）</span></a> ”中引入了一种提示压缩方法（在新选项卡中打开）。使用训练有素的小语言模型（例如 GPT2-small 或 LLaMA-7B），LLMLingua 可以识别并删除提示中不重要的标记。这种压缩技术使封闭的法学硕士能够从压缩的提示中进行推理。尽管令牌级压缩提示对于人类来说可能难以理解，但事实证明它们对于法学硕士来说非常有效。图 1 对此进行了说明。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="3340" height="1938" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1.png" alt="这是 LLMLingua 框架的示例，该框架根据小型语言模型估计提示的重要标记。它由三个模块组成：预算控制器、迭代代币级提示压缩和分配对齐。该框架可以将 2,366 个令牌的复杂提示压缩到 117 个令牌，实现 20 倍的压缩，同时保持几乎不变的性能。" class="wp-image-988035" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1.png 3340w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-300x174.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-1024x594.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-768x446.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-1536x891.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-2048x1188.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-480x280.png 480w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-240x139.png 240w" sizes="(max-width: 3340px) 100vw, 3340px" /><figcaption class="wp-element-caption">图 1. LLMLingua 的框架</figcaption></figure><h2 class="wp-block-heading" id="llmlingua-s-method-and-evaluation">LLMLingua的方法和评估</h2><p>为了开发 LLMLingua 的框架，我们使用了预算控制器来平衡提示中不同模块的敏感性，从而保持语言的完整性。我们的两阶段过程涉及粗粒度的即时压缩。我们首先通过消除某些句子来简化提示，然后单独压缩剩余的标记。为了保持一致性，我们采用了迭代令牌级压缩方法，细化了令牌之间的个体关系。此外，我们对较小的模型进行了微调，通过将其与法学硕士生成数据中的模式对齐来捕获来自不同封闭法学硕士的分布信息。我们通过指令调整来做到这一点。</p><p>为了评估 LLMLingua 的性能，我们在 GSM8K、BBH、ShareGPT 和 Arxiv-March23 四个不同数据集上测试了压缩提示，包括 ICL、推理、总结和对话。我们的方法取得了令人印象深刻的结果，实现了高达 20 倍的压缩，同时保留了原始提示的功能，特别是在 ICL 和推理方面。 LLMLingua 还显着降低了系统延迟。</p><p>在我们的测试过程中，我们使用 LLaMA-7B 作为小语言模型，使用 OpenAI 的 LLM 之一 GPT-3.5-Turbo-0301 作为封闭的 LLM。结果表明，即使在最大压缩比为 20 倍的情况下，LLMLingua 仍保持了提示的原始推理、总结和对话能力，如表 1 和表 2 中的评估指标（EM）列所示。压缩方法未能保留提示中的关键语义信息，尤其是逻辑推理细节。有关这些结果的更深入讨论，请参阅<a href="https://www.microsoft.com/en-us/research/publication/llmlingua-compressing-prompts-for-accelerated-inference-of-large-language-models/" target="_blank" rel="noreferrer noopener">本文</a>第 5.2 节。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1396" height="1480" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1.png" alt="这些是使用GPT-3.5-turbo在GSM8K和BBH上的实验结果，展示了基于不同方法和压缩约束的上下文学习和推理能力。结果表明，LLMLingua 可以实现高达 20 倍的压缩率，而性能仅损失 1.5 点。" class="wp-image-988044" style="width:554px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1.png 1396w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1-283x300.png 283w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1-966x1024.png 966w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1-768x814.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1-170x180.png 170w" sizes="(max-width: 1396px) 100vw, 1396px" /><figcaption class="wp-element-caption">表 1. 不同方法在 GSM8K 和 BBH 数据集上不同目标压缩比的性能。 </figcaption></figure><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1706" height="468" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2.png" alt="这些是基于不同方法和压缩约束，使用 GPT-3.5-turbo 进行 ShareGPT（对话）和 Arxiv-March23（摘要）的实验结果。结果表明，LLMLingua 可以有效保留原始提示的语义信息，同时实现 3x-9x 的压缩率。" class="wp-image-988053" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2.png 1706w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-300x82.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-1024x281.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-768x211.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-1536x421.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-240x66.png 240w" sizes="(max-width: 1706px) 100vw, 1706px" /><figcaption class="wp-element-caption">表 2. 对话和摘要任务中不同方法在不同目标压缩比下的性能。 </figcaption></figure><h2 class="wp-block-heading" id="llmlingua-is-robust-cost-effective-efficient-and-recoverable"> LLMLingua 稳健、经济高效、高效且可恢复</h2><p>LLMLingua 还在各种小型语言模型和不同的封闭式法学硕士中显示了令人印象深刻的结果。当使用 GPT-2-small 时，LLMLingua 在 1/4 镜头约束下获得了 76.27 的强劲性能分数，接近 LLaMA-7B 的 77.33 的结果，并超过了标准提示结果 74.9。同样，即使没有调整 Claude-v1.3（后强大的 LLM 之一），LLMLingua 在 1/2 镜头约束下的得分为 82.61，优于标准提示结果 81.8。</p><p>事实证明，LLMLingua 在缩短响应长度方面也很有效，从而显着减少了 LLM 生成过程中的延迟，减少幅度在 20% 到 30% 之间，如图 2 所示。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="12000" height="8000" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2.png" alt="该图演示了压缩比与响应令牌数量之间的关系。在不同的任务中，随着压缩比的增加，响应长度都有不同程度的减少，最大减少20%-30%。" class="wp-image-988059" style="width:616px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2.png 12000w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-300x200.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-1024x683.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-768x512.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-1536x1024.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-2048x1365.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-240x160.png 240w" sizes="(max-width: 12000px) 100vw, 12000px" /><figcaption class="wp-element-caption">图 2. 在不同压缩比下生成的令牌长度分布。</figcaption></figure><p> LLMLingua 更令人印象深刻的是它的可恢复功能。当我们使用 GPT-4 恢复压缩提示时，它成功地从完整的九步思想链 (CoT) 提示中恢复了所有关键推理信息，这使得法学硕士能够通过连续的中间步骤来解决问题。恢复的提示符与原来的几乎相同，并且其含义被保留。这如表 3 和表 4 所示。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1780" height="2538" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3.png" alt="该图展示了原始提示、压缩提示以及使用 GPT-4 恢复压缩提示的结果。原始提示由9步思想链组成，压缩后的提示对于人类来说很难理解。然而，恢复的文本包含了思想链的所有 9 个步骤。" class="wp-image-988062" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3.png 1780w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-210x300.png 210w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-718x1024.png 718w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-768x1095.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-1077x1536.png 1077w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-1436x2048.png 1436w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-126x180.png 126w" sizes="(max-width: 1780px) 100vw, 1780px" /><figcaption class="wp-element-caption">表 3. GSM8K 上的延迟比较。 LLMLingua 可以将法学硕士的端到端推理加速 1.7-5.7 倍。 </figcaption></figure><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1391" height="378" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4.png" alt="该图显示了使用 LLMLingua 时的端到端延迟、不使用 LLMLingua 以及压缩提示时的延迟。随着压缩率的增加，LLMLingua 和端到端延迟都会减少，以 10 倍的令牌压缩率实现高达 5.7 倍的加速。" class="wp-image-988068" style="width:584px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4.png 1391w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4-300x82.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4-1024x278.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4-768x209.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4-240x65.png 240w" sizes="(max-width: 1391px) 100vw, 1391px" /><figcaption class="wp-element-caption">表 4. 使用 GPT-4 从 GSM8K 恢复压缩提示。 </figcaption></figure><h2 class="wp-block-heading" id="enhancing-the-user-experience-and-looking-ahead">增强用户体验并展望未来</h2><p>LLMLingua 已经通过实际应用证明了其价值。它已集成到<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/run-llama/llama_index/blob/main/llama_index/indices/postprocessor/longllmlingua.py" target="_blank" rel="noreferrer noopener">LlamaIndex <span class="sr-only">（在新选项卡中打开）</span></a>中，这是一种广泛采用的检索增强生成 (RAG) 框架。目前，我们正在与产品团队合作，减少 LLM 调用中所需的令牌数量，特别是对于多文档问答等任务。在这里，我们的目标是显着改善法学硕士的用户体验。</p><p>从长远来看，我们提出了<a href="https://www.microsoft.com/en-us/research/publication/longllmlingua-accelerating-and-enhancing-llms-in-long-context-scenarios-via-prompt-compression/">LongLLMLingua</a> ，这是一种专为长上下文场景而设计的提示压缩技术，例如聊天机器人等应用程序中的检索增强问答任务，当信息随时间动态变化时非常有用。它还适用于总结在线会议等任务。 LongLLMLingua 的主要目标是增强法学硕士感知关键信息的能力，使其适用于众多现实世界的应用程序，特别是基于信息的聊天机器人。我们希望这一创新能够为与法学硕士进行更复杂和用户友好的互动铺平道路。</p><p>在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://llmlingua.com/" target="_blank" rel="noreferrer noopener">LLMLingua <span class="sr-only">（在新选项卡中打开）</span></a>页面上了解有关我们工作的更多信息。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/">LLMLingua 文章：通过即时压缩创新 LLM 效率</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>