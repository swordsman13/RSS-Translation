<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2023 年 11 月 17 日星期五 19:07:17 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.3.2</generator><item><title>思想框架：并行解码加速并改进 LLM 输出</title><link/>https://www.microsoft.com/en-us/research/blog/sculpture-of-thought-parallel-decoding-speeds-up-and-improves-llm-output/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Fri, 17 Nov 2023 19:07:15 +0000</pubDate> <category><![CDATA[Research Blog]]></category><guid ispermalink="false"></guid><description><![CDATA[<p> LLaMA 和 OpenAI 的 GPT-4 等大型语言模型 (LLM) 正在彻底改变技术。然而，对法学硕士的常见抱怨之一是其速度或缺乏速度。很多情况下，需要很长时间才能得到他们的答复。这限制了法学硕士的应用及其在延迟关键功能中的有用性，例如聊天机器人、副驾驶，[…]</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/skeleton-of-thought-parallel-decoding-speeds-up-and-improves-llm-output/">《思想框架：并行解码加速并改进 LLM 输出》一文</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-full"><img decoding="async" fetchpriority="high" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1.png" alt="该图显示了正常顺序解码方法和思维骨架方法之间的差异。给定一个问题，图的左侧部分显示正常的顺序解码方法从头到尾顺序生成答案。图中右半部分显示，Skeleton-of-Thought方法首先提示LLM给出答案的骨架，然后并行扩展骨架中的多个点以获得最终答案。" class="wp-image-984846" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p> LLaMA 和 OpenAI 的 GPT-4 等大型语言模型 (LLM) 正在彻底改变技术。然而，对法学硕士的常见抱怨之一是其<em>速度</em>或缺乏速度。很多情况下，需要很长时间才能得到他们的答复。这限制了法学硕士的应用及其在延迟关键功能（例如聊天机器人、副驾驶和工业控制器）中的有用性。 </p><div class="annotations " data-bi-aN="margin-callout"><ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right"><li class="annotations__list-item"> <span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">出版物</span><a href="https://www.microsoft.com/en-us/research/publication/skeleton-of-thought-large-language-models-can-do-parallel-decoding/" target="_blank" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding" data-bi-aN="margin-callout" data-bi-cN="Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding">的思想框架：大型语言模型可以进行并行解码<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span></a></li></ul></div><p>为了解决这个问题，微软研究院和清华大学的研究人员提出了一种加速LLM生成的新方法<a href="https://www.microsoft.com/en-us/research/publication/skeleton-of-thought-large-language-models-can-do-parallel-decoding/" target="_blank" rel="noreferrer noopener">——Skeleton-of-Thought（SoT）</a> 。与大多数需要对 LLM 模型、系统或硬件进行修改的现有方法不同，SoT 将 LLM 视为黑匣子，因此可以应用于任何现成的开源（例如 LLaMA）甚至基于 API（例如，OpenAI 的 GPT-4）模型。我们的评估表明， <em>SoT 不仅大大加快了所审查的 12 个法学硕士的内容生成速度，而且在某些情况下还可能提高答案质量。</em>例如，在 OpenAI 的 GPT-3.5 和 GPT-4 上，SoT 提供了 2 倍的加速，同时提高了基准数据集的答案质量。</p><p>我们的代码和演示在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/imagination-research/sot/" target="_blank" rel="noreferrer noopener">https://github.com/imagination-research/sot/ 上开源<span class="sr-only">（在新选项卡中打开）</span></a> 。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="934" height="468" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT_Figure1_compressed.gif" alt="SoT - 图 1：正常顺序解码方法和思维框架方法演示的屏幕录制，用于回答“如何改进我的时间管理技术？”这一问题。左图显示了正常顺序解码的生成过程，答案是逐字生成的。右图展示了思维骨架的生成过程，答案中的多个点是并行生成的，因此速度更快。两种方法生成完成后，都会显示两种方法的生成时间，这表明 Skeleton-of-Thought 提供了 3.72 倍的加速比。" class="wp-image-984804"/><figcaption class="wp-element-caption">图 1：与普通方法（左）相比，SoT（右）在回答以下问题时速度提高了 3.72 倍：“我如何改进我的时间管理技术？”在一个 NVIDIA A100 GPU 上使用 LLaMA-2-7b 模型。</figcaption></figure><h2 class="wp-block-heading" id="sot-encouraging-structured-thinking-in-llms">SoT：鼓励法学硕士的结构化思维</h2><p>SoT 的想法源于法学硕士和人类处理信息方式的差异。法学硕士<em>按顺序</em>生成答案。例如，回答<em>“我怎样才能提高我的时间管理技巧？”</em>在图 1（左）中，法学硕士在进入下一分之前完成了一个分。相比之下，人类可能并不总是按顺序思考问题并写出答案。在许多情况下，人类首先得出答案的骨架，然后添加细节来解释每个点。例如，要回答图 1 中的同一问题，人们可能首先考虑一系列相关的时间管理技术，然后再深入研究每种技术的细节。对于提供咨询、参加测试、撰写论文等练习尤其如此。</p><p>我们能让法学硕士更加动态地、更少地线性地处理信息吗？如图 2 所示，SoT 发挥了作用。 SoT 不是按顺序生成答案，而是将生成过程分解为两个阶段：(1) SoT 首先要求 LLM 导出答案的骨架，然后 (2) 要求 LLM 提供骨架中每个点的答案。该方法提供了新的加速机会，因为<em>可以并行生成第 2 阶段中各个点的答案。这可以针对用户可以访问其权重的本地模型（例如，LLaMA）和只能通过 API 访问的基于 API 的模型（例如，OpenAI 的 GPT-4）来完成。</em></p><ul><li>对于基于 API 的模型，我们可以发出<em>并行</em>API 请求，每个点一个。</li><li>对于本地运行的模型，我们可以批量<em>同时</em>回答所有点。请注意，在许多场景（例如，本地服务、不饱和查询周期内的集中式服务）中，LLM 的解码阶段通常受到权重加载而不是激活加载或计算的瓶颈，因此未充分利用可用的硬件。在这些情况下，以增加的批量大小运行 LLM 推理可以提高硬件计算利用率，并且不会显着增加延迟。</li></ul><p>因此，如果答案中有<em>B 个</em>点，与当前 LLM 中的顺序生成相比，SoT 中并行生成这些点理论上可以放弃<em>B</em> x 加速。然而，在实践中，由于额外的骨架阶段、不平衡的点长度和其他开销，实际的加速可能会更小。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1400" height="882" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2.png" alt="该图显示了正常顺序解码方法和思维骨架方法之间的差异。给定一个问题，图的左侧部分显示正常的顺序解码方法从头到尾顺序生成答案。图中右半部分显示，Skeleton-of-Thought方法首先提示LLM给出答案的骨架，然后并行扩展骨架中的多个点以获得最终答案。" class="wp-image-983661" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2-300x189.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2-1024x645.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2-768x484.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2-240x151.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">图 2：思想骨架 (SoT) 的图示。 SoT（右）不是按<em>顺序</em>生成答案（左），<em>而是并行</em>生成答案的不同部分。给定问题，SoT首先提示LLM给出答案的骨架，然后进行批量解码或并行API调用以同时扩展多个点以获得最终答案。</figcaption></figure><p>我们在 12 个最近发布的模型上测试了 SoT，其中包括 9 个开源模型和 3 个基于 API 的模型，如表 1 所示。我们使用 Vicuna-80 数据集，其中包含 80 个问题，涵盖九个类别，例如编码、数学、写作、角色扮演等等。有关更多数据集和指标的结果，请参阅论文： <a href="https://www.microsoft.com/en-us/research/publication/skeleton-of-thought-large-language-models-can-do-parallel-decoding/">Skeleton-of-Thought：Large Language Models Can Do Parallel Decoding</a> 。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1400" height="533" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1.png" alt="SoT - 表 1：列出了本文中评估的模型的表格。有四列：（1）访问权限，即模型是开源的还是基于API的，（2）模型名称，（3）开发模型的机构，（4）模型的发布日期。因此表中有12行，对应9个开源模型和3个基于API的模型。" class="wp-image-983676" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1-300x114.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1-1024x390.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1-768x292.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1-240x91.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">表 1：评估模型列表。</figcaption></figure><p>图 3 显示 SoT 在所有模型中都提供了相当大的加速。特别是，SoT 在 12 个模型中的 8 个模型上获得了 >;2 倍的加速（高达 2.39 倍）。此外，SoT 实现了这种水平的加速，而没有显着降低答案质量。图 4 显示了 SoT 的获胜/平局/失败率（定义为 SoT 提供比正常顺序生成更好的答案的问题的比例；“更好”是由<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/lm-sys/FastChat" target="_blank" rel="noreferrer noopener">FastChat <span class="sr-only">（在新选项卡中打开）</span></a>和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/FreedomIntelligence/LLMZoo" target="_blank" rel="noreferrer noopener"><span class="sr-only">LLMZoo</span></a> （在新选项卡中打开）中提出的指标定义的。 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/FreedomIntelligence/LLMZoo" target="_blank" rel="noreferrer noopener"><span class="sr-only">在新选项卡中打开）</span></a> ，并由 GPT-4 法官进行评估）。我们可以看到 SoT 的答案质量与连续生成的结果相当。在论文中，我们进一步表明，由于骨架阶段明确要求法学硕士提前规划答案结构，SoT 在与问题的相关性和跨多个方面的全面性方面提高了答案质量。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1400" height="513" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3.png" alt="SoT - 图 3：条形图显示不同模型上思维框架的平均加速。每个条对应一个型号。该图显示，Skeleton-of-Thought 为所有模型提供了加速。加速范围为 1.13 倍至 2.39 倍。" class="wp-image-983664" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3-300x110.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3-1024x375.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3-768x281.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3-240x88.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">图 3：不同模型上 SoT 的平均加速比。 SoT 为我们评估的所有模型提供了加速。 </figcaption></figure><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1131" height="293" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned.png" alt="SoT - 图 4：该图显示了使用来自 FastChat 和 LLMZoo 的指标与正常顺序生成相比，Skeleton-of-Thought 的获胜/平局/失败率。对于 FastChat 指标，获胜/平局/失败率分别为 29.5%、29.3% 和 41.2%。对于 LLMZoo 指标，获胜/平局/失败率分别为 45.8%、19.6% 和 34.5%。总之，在大约 60% 的情况下，思想框架的表现优于或等于正常的顺序生成。" class="wp-image-983667" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned.png 1131w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned-300x78.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned-1024x265.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned-768x199.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned-240x62.png 240w" sizes="(max-width: 1131px) 100vw, 1131px" /><figcaption class="wp-element-caption">图 4：使用来自 FastChat 和 LLMZoo 的指标，SoT 与正常生成的获胜/平局/失败率。在大约 60% 的情况下，SoT 的性能优于或等于正常发电。 </figcaption></figure><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="956154"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究院播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/collaborators-gov4git-with-kasia-sitkiewicz-and-petar-maymounkov/" aria-label="Collaborators: Gov4git with Petar Maymounkov and Kasia Sitkiewicz" data-bi-cN="Collaborators: Gov4git with Petar Maymounkov and Kasia Sitkiewicz" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/04/collaborators_hero_1400x788.jpg" alt="GitHub 产品经理 Kasia Sitkiewicz 和协议实验室研究科学家 Petar Maymounkov 在 Microsoft Research 播客上讨论了他们在 Gov4git 上的合作" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">合作者：Gov4git、Petar Maymounkov 和 Kasia Sitkiewicz</h2><p class="large"> Gov4git 是一种去中心化、开源合作的治理工具，有助于为未来奠定基础，让每个人都可以更高效、透明、轻松地进行协作，并以满足各自社区独特愿望和需求的方式进行协作。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/collaborators-gov4git-with-kasia-sitkiewicz-and-petar-maymounkov/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Collaborators: Gov4git with Petar Maymounkov and Kasia Sitkiewicz" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="sot-r-adaptively-triggering-sot">SoT-R：自适应触发 SoT</h2><p> SoT对点进行独立并行的扩展。因此，它不适合需要逐步推理的问题，例如数学和编码。为此，我们提出了一种名为<em>SoT with Router (SoT-R)</em>的 SoT 扩展，仅在合适时自适应地触发 SoT。更具体地说，我们提出了一个路由器模块，该模块决定是否应将 SoT 应用于用户请求，然后我们相应地调用 SoT 或正常顺序解码。路由器模块可以通过在没有任何模型训练的情况下提示OpenAI的GPT-4（称为“<em>提示路​​由器”</em> ）或训练指定的RoBERTa模型（称为“<em>训练路由器”</em> ）来实现。我们表明，SoT-R 提高了 SoT 跨问题类别的通用性（图 5），同时保持相当大的加速（图 6）。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1400" height="538" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5.png" alt="SoT - 图 5：该图显示了 SoT 和 SoT-R 在 Vicuna-80 数据集上不同问题类别上的净胜率（定义为胜率减去输率）。对于不适合 SoT 的问题类别（例如编码、数学），SoT-R 的净胜率约为 0，因为 SoT-R 学会回退到正常生成模式。对于适合 SoT 的问题类别（例如，通用问题、反事实问题），SoT-R 具有与 SoT 类似的净胜率，并且 SoT-R 按预期触发 SoT。总体而言，SoT-R 改进了 SoT，并为所有问题类别保持了良好的答案质量。" class="wp-image-983670" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5-300x115.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5-1024x394.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5-768x295.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5-240x92.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">图 5：SoT 和 SoT-R 在 Vicuna-80 上不同问题类别上的净胜率（定义为胜率减去输率 - 越高越好）。 “人类路由器”是指使用人类偏好来决定每个问题是否应该应用 SoT 的预言机。对于不适合 SoT 的问题类别（例如编码、数学），SoT-R 学习回退到正常生成模式。因此，SoT-R 可以对所有问题类别保持良好的答案质量。 SoT-R 甚至可以超越人类路由器（例如，在角色扮演问题上）。 </figcaption></figure><figure class="wp-block-image aligncenter size-full"><img decoding="async" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure6.png" alt="SoT - 图 6：该图显示了 SoT 和 SoT-R 在不同型号的 Vicuna-80 上的加速情况。虽然 SoT-R 的加速比小于 SoT，但对于大多数模型来说，SoT-R 仍然可以保持 >;1 的加速比。" class="wp-image-983673"/><figcaption class="wp-element-caption">图 6：SoT 和 SoT-R 在不同型号的 Vicuna-80 上的加速效果。对于大多数模型，SoT-R 可以保持 >;1 的加速比。</figcaption></figure><h2 class="wp-block-heading" id="limitations-and-next-steps">限制和后续步骤</h2><h3 class="wp-block-heading" id="reducing-the-cost-of-sot">降低 SoT 成本</h3><p>与正常的顺序解码相比，SoT 使用更长的提示，这可能会导致 LLM API 的成本更高（按提示的长度收费），并可能降低服务系统的吞吐量。需要进一步研究来探索降低 SoT 的成本，包括压缩 SoT 提示或调整 LLM 以在必要时自动触发 SoT。</p><h3 class="wp-block-heading" id="improving-llm-capability">提高LLM能力</h3><p>SoT 的灵感来自于人类的结构化思维。因此，它的成功值得进一步研究人类思维过程，从而促进更有效和高效的人工智能。</p><p>鉴于人类思维的复杂性（我们可能会将其想象为更多的图表），SoT 可以被视为迈向“思维图表”（GoT）的过渡步骤。 GoT 是一个新框架，旨在代表更复杂的思维，更接近人们解决问题的方式。 GoT 呈现了以图结构连接的多个概念，其中图的边代表依赖关系，每个点根据其祖先点的内容进行解码。此外，我们期望需要一个动态的思维图，而不是遵守静态图，其中高层思维结构由法学硕士自己动态调整，试图模仿人类的思维方式。这可能会结合 SoT 的效率和全局思维优势，同时捕获思维过程中的更多复杂性。</p><p>另一个需要研究的问题是，如何使用 SoT 提供的更结构化的答案来微调法学硕士，以增强他们生成组织良好且全面的答案的能力。</p><h3 class="wp-block-heading" id="data-centric-efficiency-optimization">以数据为中心的效率优化</h3><p>与现有的模型和系统级提高 LLM 效率的努力相比，SoT 通过让 LLM 组织其输出内容，采取了一种新颖的“数据级”途径。由于最先进的法学硕士能力的不断发展，这种观点变得可行且越来越相关。我们希望这项工作能够激发以数据为中心的效率优化领域的更多研究。</p><p>如果您想讨论这项研究以及该主题的潜在合作，请随时与我们联系。</p><div class="wp-block-buttons"><div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button">联系我们</a></div></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/skeleton-of-thought-parallel-decoding-speeds-up-and-improves-llm-output/">《思想框架：并行解码加速并改进 LLM 输出》一文</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title>研究重点：2023 年 11 月 8 日当周</title><link/>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-november-8-2023/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Wed, 08 Nov 2023 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=981456 </guid><description><![CDATA[<p>欢迎来到研究焦点，这是一系列博客文章，重点介绍 Microsoft 研究社区的著名出版物、活动、代码/数据集、新员工和其他里程碑。生成合理且准确的全身化身运动对于在混合现实场景中创建高质量的沉浸式体验至关重要。头戴式设备 (HMD) 通常仅提供 [...]</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-november-8-2023/">《研究焦点：2023 年 11 月 8 日一周》一文</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-pullquote"><blockquote><p><em class="">欢迎来到研究焦点，这是一系列博客文章，重点介绍 Microsoft 研究社区的著名出版物、活动、代码/数据集、新员工和其他里程碑。</em> </p></blockquote></figure><figure class="wp-block-image size-full"><img decoding="async" loading="lazy" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1.png" alt="研究重点：2023 年 11 月 8 日，渐变图案背景" class="wp-image-981462" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/RF28-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><h3 class="wp-block-heading h6 has-blue-color has-text-color" id="new-research">新研究</h3><h2 class="wp-block-heading" id="hmd-nemo-online-3d-avatar-motion-generation-from-sparse-observations"> HMD-NeMo：根据稀疏观察生成在线 3D 头像运动</h2><p>生成合理且准确的全身化身运动对于在混合现实场景中创建高质量的沉浸式体验至关重要。头戴式设备 (HMD) 通常仅提供少量输入信号，例如头部和手部 6-DoF，或者刚体在三维空间中的六个运动自由度。最近的方法在仅给出头部和手部信号的情况下产生全身运动方面取得了令人印象深刻的性能。然而，所有已知的现有方法都依赖于全手可见性。例如，虽然使用运动控制器时就是这种情况，但相当一部分混合现实体验不涉及运动控制器，而是依赖于以自我为中心的手部跟踪。由于头戴式显示器的视野有限，这带来了部分手部可见性的挑战。</p><p>在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/hmd-nemo-online-3d-avatar-motion-generation-from-sparse-observations/">HMD-NeMo：根据稀疏观察生成在线 3D 阿凡达运动中</a>，微软的研究人员提出了 HMD-NeMo，这是第一个统一的方法，即使在手可能仅部分可见的情况下，也可以解决合理且准确的全身运动生成问题。 HMD-NeMo 是一种轻量级神经网络，可以在线实时预测全身运动。 HMD-NeMo 的核心是一个时空编码器，具有新颖的时间适应性掩模标记，可在没有手部观察的情况下鼓励合理的运动。研究人员对 HMD-NeMo 中不同组件的影响进行了广泛的分析，并通过评估引入了 AMASS 上最先进的技术，AMASS 是一个大型人体运动数据库，统一了不同的基于光学标记的运动捕捉数据集。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/hmd-nemo-online-3d-avatar-motion-generation-from-sparse-observations/">阅读论文</a></div></div><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="980709"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/intern-insights-dr-madeleine-daepp-with-jennifer-scurrell-and-alejandro-cuevas/" aria-label="Intern Insights: Dr. Madeleine Daepp with Jennifer Scurrell and Alejandro Cuevas" data-bi-cN="Intern Insights: Dr. Madeleine Daepp with Jennifer Scurrell and Alejandro Cuevas" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Madeline_Insights_Hero_Feature_No_Text_1400x788-651ecfa4ebcf8.png" alt="MSR：播客实习生见解" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">实习生见解：Madeleine Daepp 博士、Jennifer Scurrell 和 Alejandro Cuevas</h2><p class="large">在本集中，博士生<a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcss.ethz.ch%2Fen%2Fcenter%2Fpeople%2Fjennifer-victoria-scurrell.html&data=05%7C01%7Cv-amelfi%40microsoft.com%7Cdeb2b53d3b8d4c3a3ccf08dbbdec0d9e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638312593774254107%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=s8N98lzOV4kJIp2nzGFTx74SU%2BZQCxgXvyXGoIxk6S0%3D&reserved=0" target="_blank" rel="noreferrer noopener">Jennifer Scurrell</a>和<a href="https://www.alejandrocuevas.me/" target="_blank" rel="noreferrer noopener">Alejandro Cuevas</a>与高级研究员<a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Fpeople%2Fmdaepp%2F&data=05%7C01%7Cv-amelfi%40microsoft.com%7Cdeb2b53d3b8d4c3a3ccf08dbbdec0d9e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638312593774410340%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=PRbEab7C9R6Lv%2BwVQqz2Md1rE4WVmcPirGHNq9aqzYQ%3D&reserved=0" target="_blank" rel="noreferrer noopener">Madeleine Daepp 博士</a>进行了交谈。他们讨论了微软研究院的实习文化，从与研究人员联系的机会到他们所说的帮助他们取得成功的团队合作，以及他们希望对工作产生的影响。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/intern-insights-dr-madeleine-daepp-with-jennifer-scurrell-and-alejandro-cuevas/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Intern Insights: Dr. Madeleine Daepp with Jennifer Scurrell and Alejandro Cuevas" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h3 class="wp-block-heading h6 has-blue-color has-text-color" id="new-article">新文章</h3><h2 class="wp-block-heading" id="will-code-remain-a-relevant-user-interface-for-end-user-programming-with-generative-ai-models">代码是否仍然是使用生成式人工智能模型进行最终用户编程的相关用户界面？</h2><p>最终用户编程的研究领域主要关注帮助非专家学习足够好的编码以完成自己的任务。生成式人工智能允许用户根据自然语言提示生成代码，从而完全避免这种情况。</p><p>在最近的一篇文章中： <a href="https://www.microsoft.com/en-us/research/publication/will-code-remain-a-relevant-user-interface-for-end-user-programming-with-generative-ai-models/">代码仍然是使用生成式 AI 模型进行最终用户编程的相关用户界面吗？</a>微软的研究人员探索了在生成式人工智能世界中“传统”编程语言对于非专家最终用户程序员的相关性。他们提出了“生成转变假说”：生成人工智能将在传统的最终用户编程范围内实现定性和定量的扩展。他们概述了传统编程语言对于最终用户程序员仍然相关和有用的一些原因，并推测这些原因是否会随着生成人工智能的进一步改进和创新而持续存在或消失。最后，他们阐明了对最终用户编程研究的一系列影响，包括需要重新审视许多成熟的核心概念的可能性，例如 Ko 的学习障碍和 Blackwell 的注意力投资模型。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/will-code-remain-a-relevant-user-interface-for-end-user-programming-with-generative-ai-models/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/><h3 class="wp-block-heading h6 has-blue-color has-text-color" id="new-research-1">新研究</h3><h2 class="wp-block-heading" id="lut-nn-empower-efficient-neural-network-inference-with-centroid-learning-and-table-lookup"> LUT-NN：通过质心学习和表查找实现高效的神经网络推理</h2><p>设备端深度神经网络（DNN）推理广泛应用于智能手机和智能手表等移动设备，提供了无与伦比的智能服务，但也强调了这些设备上有限的硬件资源。</p><p>在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/lut-nn-empower-efficient-neural-network-inference-with-centroid-learning-and-table-lookup/">LUT-NN：通过质心学习和表查找增强高效神经网络推理中</a>，微软的研究人员提出了一种消耗更少延迟、内存、磁盘和电力的系统，以实现更高效的 DNN 推理。 LUT-NN 学习每个算子的典型特征（称为质心），并预先计算这些质心的结果以保存在查找表中。在推理过程中，可以直接从表中读取与输入最接近的质心的结果，作为无需计算的近似输出。</p><p> LUT-NN 集成了两项主要的新技术：（1）通过反向传播进行可微质心学习，它采用三个级别的近似来最小化质心对精度的影响； (2)查表推理执行，综合考虑不同级别的并行性、内存访问减少以及专用硬件单元以获得最佳性能。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/lut-nn-empower-efficient-neural-network-inference-with-centroid-learning-and-table-lookup/">阅读论文</a></div></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-november-8-2023/">《研究焦点：2023 年 11 月 8 日一周》一文</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>