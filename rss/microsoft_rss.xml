<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2024 年 2 月 29 日星期四 15:51:00 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.4.3</generator><item><title> ViSNet：用于预测分子特性和模拟分子动力学的通用分子几何建模框架</title><link/>https://www.microsoft.com/en-us/research/blog/visnet-a-general-分子-几何-建模-框架-for-预测-分子-属性-和-模拟-分子-动力学/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Thu, 29 Feb 2024 20:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1002609 </guid><description><![CDATA[<p>分子几何建模是理解分子结构与生物活性之间复杂关系的强大工具，这一领域被称为结构-活性关系 (SAR)。 SAR 的主要前提是分子的生物活性由其特定的化学结构决定，不仅由原子核之间的连接决定，还由[…]</p><p> <a href="https://www.microsoft.com/en-us/research/blog/visnet-a-general-molecular-geometry-modeling-framework-for-predicting-molecular-properties-and-simulating-molecular-dynamics/">ViSNet：用于预测分子特性和模拟分子动力学的通用分子几何建模框架</a>后的文章首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1.jpg" alt="图 1. ViSNet 的总体模型架构。 (a) ViSNet 模型草图。 ViSNet 嵌入分子的 3D 结构，并通过一系列 ViSNet 模块提取几何信息，并通过输出模块输出能量、力和 HOMO-LUMO 间隙等分子属性。 (b) 一个 ViSNet 块的流程图。一个 ViSNet 块由两个模块组成：i) Scalar2Vec，负责将标量嵌入附加到向量。 ii) Vec2Scalar。 Scalar2Vec 的输入是节点嵌入、边嵌入、方向单位和两个原子之间的相对位置。" class="wp-image-1003455" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/ViSNet-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>分子几何建模是理解分子结构与生物活性之间复杂关系的强大工具，这一领域被称为结构-活性关系 (SAR)。 SAR 的主要前提是分子的生物活性由其特定的化学结构决定，不仅由原子核之间的连接决定，还由分子如何扭曲和排列成三维结构决定。 SAR 的目标是能够预测分子构型如何影响药物相互作用、化学反应性和蛋白质功能等重要过程。如果这是可能的，科学家们可以在对人体进行测试之前很久就预测药物的功效及其副作用和毒性。</p><p>由微软开发的矢量标量交互式图神经网络（ViSNet）框架是一种分子几何建模的新颖方法。 ViSNet 旨在帮助研究人员预测分子特性、模拟分子动力学并更准确地了解构效关系。因此，ViSNet 有潜力帮助改变药物发现、材料科学和其他关键领域。</p><p>我们的研究旨在提高分子数据的可解释性，降低计算成本并评估现实世界的应用效用。 “通过等变矢量标量交互式消息传递增强分子的几何表示”于 2​​024 年 1 月<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41467-023-43720-2" target="_blank" rel="noreferrer noopener">发表在 Nature Communications <span class="sr-only">(opens in new tab)</span>上</a>，并入选“ <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/collections/ceiajcdbeb" target="_blank" rel="noreferrer noopener">AI 和机器学习<span class="sr-only">(opens in new tab)</span></a> ”中的“编辑亮点” <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/collections/ceiajcdbeb" target="_blank" rel="noreferrer noopener"><span class="sr-only">）</span></a> ”和“ <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/collections/idhhgedgig" target="_blank" rel="noreferrer noopener">生物技术和方法<span class="sr-only">（在新选项卡中打开）</span></a> ”类别。 </p><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">焦点：微软研究通讯</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">微软研究院通讯</h2><p class="large">与 Microsoft 研究社区保持联系。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button is-style-fill-chevron"> <a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">立即订阅</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="geometry-deep-learning-and-sar">几何深度学习和SAR</h2><p>几何深度学习是 SAR 研究最前沿的一种方法：一种强大的计算方法，利用深度学习技术的力量来分析和理解分子的三维结构。传统的深度学习方法主要侧重于处理以网格状结构组织的数据，例如图像或文本序列。然而，分子本质上是具有复杂几何形状的三维实体，这使得使用传统的深度学习方法对其进行分析具有挑战性。几何深度学习通过构建能够处理三维数据的专用架构和算法来解决这一挑战。这些方法使计算机能够从分子内原子的空间排列中学习和提取有意义的特征，捕获有关其结构和行为的关键信息。</p><p>然而，尽管几何深度学习最近取得了重大进展，但挑战仍然存在。这些包括：</p><ul><li><strong>分子可解释性不足</strong>——当应用于分子几何建模时，我们理解和解释深度神经网络内部运作的能力有限。虽然这些网络擅长根据大型数据集和复杂模式进行预测，但它们通常作为“黑匣子”运行，这意味着它们预测背后的基本原理并不总是可以理解或透明的。在分子几何学的背景下，这种可解释性的缺乏给理解为什么某些分子结构会导致特定的结果（例如生物活性或化学反应性）带来了挑战。</li><li><strong>随着分子尺寸的增加，计算成本迅速增加</strong>——随着分子尺寸和复杂性的增加，分析它们所需的计算资源急剧增加。当采用先进的计算技术（例如使用高阶 Clebsch-Gordan 系数的技术）时，这一挑战变得尤为明显。克莱布希-戈登系数是量子力学中用于描述粒子角动量特性耦合的数学量。在分子建模的背景下，这些系数用于复杂的量子力学计算，以帮助解释分子内电子和原子核之间的相互作用。对于大分子，所涉及的原子和电子的数量呈指数增长，导致必须考虑的可能相互作用的数量达到天文数字。因此，涉及高阶 Clebsch-Gordan 系数的计算变得极其复杂且计算量大。</li><li><strong>实际应用中需要进行盲测和评估</strong>——通过盲测评估实际应用中的预测模型对于评估其超出受控基准的可靠性和适用性至关重要。然而，由于缺乏多样化且具有代表性的数据集以及复杂的系统动力学，因此出现了挑战。动物和人体试验还存在伦理方面的考虑，这自然限制了此类数据的可用性。克服这些挑战需要跨学科合作、创新方法和透明的验证框架，以确保预测模型在应对现实世界挑战时的稳健性和可信性。</li></ul><h2 class="wp-block-heading" id="enhancing-molecular-geometry-representations-by-visnet"> 通过 ViSNet 增强分子几何表示</h2><p>最初，我们的目标是开发一种能够有效利用分子复杂结构的模型。传统的分子动力学 (MD) 模拟通过考虑键长、键角和二面角等因素来跟踪分子运动。受到这些方法的启发，我们引入了一种称为矢量标量交互式图神经网络（ViSNet）的新颖方法。</p><p>我们没有以直接的方式将键角和二面角信息直接集成到我们的模型中，而是引入了一个称为“方向单位”的概念。这些单元将分子结构内的节点表示为向量，通过对从中心节点指向其相邻节点的归一化向量求和来计算。我们将键长、键角和二面角的传统计算扩展到涉及原子对（二体）、原子三联体（三体）和原子四联体（四体）的相互作用。为了有效地管理这些相互作用，我们设计了一个运行时几何计算（RGC）模块，它可以准确地捕获分子中原子之间的复杂关系。值得注意的是，RGC模块对三体和四体相互作用的计算表现出线性时间复杂度，保证了计算效率。</p><p>此外，我们引入了一种称为矢量标量交互式消息传递（ViS-MP）的机制，促进分子图中节点和边之间的信息交换。该机制通过 RGC 模块根据节点和边的标量表示迭代更新节点的方向单元，反之亦然。 RGC 和 ViS-MP 的这些独特特征显着增强了我们的模型编码分子几何形状并简化分子图神经网络内信息交换过程的能力。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1430" height="826" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig1.png" alt="图 1. ViSNet 的总体模型架构。 (a) ViSNet 模型草图。 ViSNet 嵌入分子的 3D 结构，并通过一系列 ViSNet 模块提取几何信息，并通过输出模块输出能量、力和 HOMO-LUMO 间隙等分子属性。 (b) 一个 ViSNet 块的流程图。一个 ViSNet 块由两个模块组成：i) Scalar2Vec，负责将标量嵌入附加到向量。 ii) Vec2Scalar。 Scalar2Vec 的输入是节点嵌入、边嵌入、方向单位和两个原子之间的相对位置。" class="wp-image-1002630" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig1.png 1430w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig1-300x173.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig1-1024x591.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig1-768x444.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig1-240x139.png 240w" sizes="(max-width: 1430px) 100vw, 1430px" /><figcaption class="wp-element-caption">图 1. ViSNet 的总体模型架构。 </figcaption></figure><h2 class="wp-block-heading" id="visnet-in-real-world-applications-for-molecular-modeling-and-property-predictions"> ViSNet 在分子建模和属性预测的实际应用中</h2><p>为了衡量 ViSNet 的实际效用，我们使用预测分子特性的既定基准严格评估了其性能。在一系列数据集（包括 MD17、修订版 MD17、MD22、QM9 和 Molecule3D）中，ViSNet 始终优于现有算法，展示了其在表示分子几何形状方面的卓越准确性。</p><p>然后，我们通过分子动力学 (MD) 模拟来模拟 Chignolin 蛋白的行为，对 ViSNet 进行测试。 ViSNet 在 AIMD-Chig 数据集上进行训练，以使用先进密度泛函理论 (DFT) 方法计算的蛋白质数据为特色，其性能优于传统的经验力场，并且与当代机器学习力场相比，显示出前景。值得注意的是，ViSNet 的模拟密切反映了严格的 DFT 计算的结果，凸显了其精确、高效的数据模拟的潜力。</p><p>我们使用 ViSNet 参加了<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aistudio.baidu.com/competition/detail/1012/0/leaderboard" target="_blank" rel="noreferrer noopener">首届全球人工智能药物开发竞赛<span class="sr-only">（在新选项卡中打开）</span></a> ，这是一项国际竞赛，根据小分子的序列信息（即 SMILES）预测 SARS-CoV-2 主要蛋白酶的抑制剂。全球共有来自 878 个团队的 1,105 名参赛者参加了此次比赛。 ViSNet 帮助我们赢得了比赛，展示了其令人鼓舞的预测准确性。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1430" height="728" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig2.png" alt="图 2.PyTorch 几何库中的 ViSNet。一个 PyTorch 模块，用于实现“使用等变向量标量交互式消息传递增强分子的几何表示”论文中的等变向量标量交互式图神经网络 (ViSNet)。" class="wp-image-1002633" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig2.png 1430w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig2-300x153.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig2-1024x521.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig2-768x391.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig2-240x122.png 240w" sizes="(max-width: 1430px) 100vw, 1430px" /><figcaption class="wp-element-caption">图 2.PyTorch 几何库中的 ViSNet。</figcaption></figure><p>为了使 ViSNet 更易于访问和用户友好，微软已将其集成到<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.ViSNet.html" target="_blank" rel="noreferrer noopener">PyTorch 几何库<span class="sr-only">（在新选项卡中打开）</span></a>中，作为分子建模和属性预测的核心模型。此次集成旨在扩大应用范围并简化研究人员和从业人员对 ViSNet 的使用。此外，为了确保持续的支持和改进， <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/AI2BMD/tree/ViSNet" target="_blank" rel="noreferrer noopener">ViSNet 的定期更新版本现已在 GitHub <span class="sr-only">（在新选项卡中打开）</span>上提供</a>，为用户提供最新的增强功能。</p><p>认识到图神经网络的潜在局限性，例如随着模型变得更大、更复杂而出现“过度平滑”（即，使节点彼此无法区分）的风险，我们开发了一种基于 Transformer 的 ViSNet 版本，称为 Geoformer（几何变换器的缩写）。我们在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/AI2BMD/blob/Geoformer/Geoformer.pdf/" target="_blank" rel="noreferrer noopener">NeurIPS 2023 <span class="sr-only">（在新选项卡中打开）</span></a>上发表的出版物中介绍了这种新颖的变体，它通过将 ViSNet 的关键组件转移到 Transformer 架构中来解决可扩展性挑战。这包括将 RGC 模块合并到 Transformer 注意力机制中，并引入一种称为原子间位置编码（IPE）的新方法来捕获原子之间的空间关系。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1416" height="731" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig3.png" alt="图 3.AI2BMD 的整体流程（请参阅 https://microsoft.github.io/AI2BMD/index.html 上的演示）。蛋白质通过断裂过程分成蛋白质单元。 AI2BMD潜力是基于ViSNet设计的，数据集是在DFT级别生成的。它计算整个蛋白质的能量和原子力。 AI2BMD 模拟系统建立在所有这些组件的基础上，并提供了一个通用的解决方案来对各种蛋白质进行模拟。它使能量和力计算从头算准确。通过动力学和热力学的综合分析，AI2BMD与湿实验室实验数据表现出良好的一致性，并检测到与分子力学不同的现象。" class="wp-image-1002636" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig3.png 1416w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig3-300x155.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig3-1024x529.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig3-768x396.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/visnet_Fig3-240x124.png 240w" sizes="(max-width: 1416px) 100vw, 1416px" /><figcaption class="wp-element-caption">图 3.AI <sup>2</sup> BMD 的整体流程（请参阅<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/AI2BMD/index.html" target="_blank" rel="noreferrer noopener">https://microsoft.github.io/AI2BMD/index.html 上的演示<span class="sr-only">（在新选项卡中打开）</span></a> ）。 </figcaption></figure><h2 class="wp-block-heading" id="looking-forward-toward-ai-powered-md-simulations-with-ab-initio-accuracy">展望：从头开始实现人工智能驱动的分子动力学模拟</h2><p>作为<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/AI2BMD/index.html">人工智能驱动的从头开始分子动力学 (AI <sup>2</sup> BMD) 项目<span class="sr-only">（在新选项卡中打开）</span></a>的重要组成部分，ViSNet 在加速分子动力学模拟方面发挥着关键作用。该项目的主要目标是提高这些模拟的准确性和效率，旨在获得与通过严格的从头计算方法获得的结果相当的结果，即使对于大型分子系统也是如此。</p><p>通过将 ViSNet 集成到 AI <sup>2</sup> BMD，我们在实现这一目标方面取得了重大进展。 ViSNet 使 AI <sup>2</sup> BMD 能够达到非常接近从头计算方法的能量和力计算精度，甚至对于包含超过 10,000 个原子的复杂蛋白质也是如此。通过在蛋白质动力学模拟中利用 ViSNet，AI <sup>2</sup> BMD 旨在提高自由能估计的精度，并为蛋白质折叠热力学提供有价值的见解。</p><p> ViSNet 的贡献不仅限于能量计算，还包括各种蛋白质特性的表征。这些见解有可能通过提供预测能力并指导对蛋白质结构和功能的进一步研究来补充实验研究工作。创新的 ViSNet 框架证明了分子几何建模的进步，预示着计算化学和生物物理学进入了精度和效率的新时代。</p><p>通过精心设计和严格验证，ViSNet 已成为一种多功能工具，能够深入了解分子结构与生物活性之间的复杂关系，让我们离结构-活性关系的圣杯又近了一步。 ViSNet 与已建立的库和框架的集成，再加上为提高可扩展性和准确性而进行的持续研究工作，突显了其彻底改变药物发现、材料科学等领域的潜力。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/visnet-a-general-molecular-geometry-modeling-framework-for-predicting-molecular-properties-and-simulating-molecular-dynamics/">ViSNet：用于预测分子特性和模拟分子动力学的通用分子几何建模框架</a>后的文章首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>;</content:encoded></item><item><title>法学硕士的结构化知识提高了视觉语言模型的即时学习</title><link/>https://www.microsoft.com/en-us/research/blog/structed-knowledge-from-llms-improves-prompt-learning-for-visual-language-models/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Tue, 27 Feb 2024 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1009260 </guid><description><![CDATA[<p>使用法学硕士创建图像描述符的结构化图可以增强视觉语言模型生成的图像。了解结构化知识如何提高视觉和语言理解的及时调整。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/structured-knowledge-from-llms-improves-prompt-learning-for-visual-language-models/">法学硕士的结构化知识改善了视觉语言模型的即时学习一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center">这篇研究论文在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aaai.org/aaai-conference/" target="_blank" rel="noreferrer noopener"><strong><em>第 38 届 AAAI 人工智能年会</em></strong><span class="sr-only">（在新选项卡中打开）</span></a> (AAAI-24)<strong><em>上发表</em></strong><strong><em>，这是促进对智能及其在机器中的实现的理解的首要论坛。</em></strong> </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1.jpg" alt="第一页"Learning Hierarchical Prompt with Structured Linguistic Knowledge for Language Models" publication to the right of the AAAI conference on a blue and purple gradient background" class="wp-image-1009434" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>我们看到视觉语言模型在将文本描述转换为图像方面具有非凡的能力。然而，创建高质量的视觉效果需要制作精确的提示来捕获不同图像元素之间的关系，这是标准提示所缺乏的功能。在 AAAI-24 上发表的论文“使用<a href="https://www.microsoft.com/en-us/research/publication/learning-hierarchical-prompt-with-structured-linguistic-knowledge-for-vision-language-models/">语言模型的结构化语言知识学习分层提示</a>”中，我们介绍了一种使用大型语言模型 (LLM) 来增强视觉语言模型创建的图像的新颖方法。通过创建图像描述的详细图表，我们利用法学硕士的语言知识来生成更丰富的图像，从而扩展其在实际应用中的效用。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="3353" height="1315" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24.png" alt="VLM 中用于识别鸟类的三种类型提示的示例，即模板化提示（鸟的照片）、描述鸟类类别的基于自然语言的提示以及突出显示鸟类和鸟类的关键实体的树结构提示。相应的属性，例如喙、翅膀等。" class="wp-image-1009371" style="width:700px" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24.png 3353w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-300x118.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-1024x402.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-768x301.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-1536x602.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-2048x803.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-240x94.png 240w" sizes="(max-width: 3353px) 100vw, 3353px" /><figcaption class="wp-element-caption">图 1. 结构化图提供了每个类名称的描述。</figcaption></figure><p>图 1 说明了我们构建包含每个类别或类的关键详细信息的结构化图的方法。这些图包含结构化信息，包括实体（对象、人员和概念）、属性（特征）以及它们之间的关系。例如，在定义“睡莲”时，我们包括“叶子”或“花朵”等实体及其属性“圆形”和“白色”，然后应用法学硕士的推理能力来识别这些术语之间的相互关系。如图 2 所示。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="2210" height="2316" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24.png" alt="使用 LLM 自动生成类别描述和知识图的管道和指令。我们首先指示LLM给出类别描述，然后要求它从非结构化描述中解析关键实体、属性及其关系。" class="wp-image-1009374" style="width:700px" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24.png 2210w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-286x300.png 286w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-977x1024.png 977w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-768x805.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-1466x1536.png 1466w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-1954x2048.png 1954w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-172x180.png 172w" sizes="(max-width: 2210px) 100vw, 2210px" /><figcaption class="wp-element-caption">图 2. 通过将指令输入 LLM，我们可以接收与类别相关的描述以及相应的结构化图表。 </figcaption></figure><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="970287"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-the-future-of-scale-with-ahmed-awadallah-and-ashley-llorens/" aria-label="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" data-bi-cN="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/Ahmed_AI_Frontiers_TW_LI_FB_1200x627_With_Name.png" alt="MSR 播客 |人工智能前沿 |艾哈迈德·阿瓦达拉" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">人工智能前沿：艾哈迈德·阿瓦达拉 (Ahmed Awadallah) 和阿什利·洛伦斯 (Ashley Llorens) 的规模化未来</h2><p class="large">本集的主角是高级首席研究经理<a href="https://www.microsoft.com/en-us/research/people/hassanam/" target="_blank" rel="noreferrer noopener">Ahmed H. Awadallah</a> ，他致力于提高大规模人工智能模型的效率，并努力帮助推动该领域从研究到实践的进步<strong> </strong>使他处于人工智能新时代的前沿。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-the-future-of-scale-with-ahmed-awadallah-and-ashley-llorens/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="how-to-model-structural-knowledge"> 如何对结构知识进行建模</h2><p>在识别和构建生成的提示描述中的关系后，我们实现了分层提示调整（HTP），这是一种按层次结构组织内容的新提示调整框架。这种方法允许视觉语言模型辨别提示中不同级别的信息，从具体细节到更广泛的类别以及跨多个知识领域的总体主题，如图 3 所示。这有助于模型理解这些元素之间的联系，提高处理各种主题的复杂查询的能力。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1802" height="1655" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24.png" alt="建议分层提示调优的总体框架。带有类名的描述和关系引导图分别用作冻结文本编码器和分层提示文本编码器的输入。" class="wp-image-1009377" style="width:700px" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24.png 1802w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-300x276.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-1024x940.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-768x705.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-1536x1411.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-196x180.png 196w" sizes="(max-width: 1802px) 100vw, 1802px" /><figcaption class="wp-element-caption">图 3.HPT 基于双路径非对称网络，接收图像和各种类型的文本输入。</figcaption></figure><p>该方法的核心是最先进的关系引导注意力模块，旨在帮助模型识别和分析图中元素之间的复杂互连。该模块还通过跨级自注意力机制来理解不同实体和属性之间的交互。自注意力使模型能够根据输入数据的各个部分（这里是图表）的相关性来评估它们并确定其优先级。 “跨级”自注意力将这种能力扩展到图中的各个语义层，允许模型检查多个抽象级别的关系。此功能有助于模型辨别各个级别的提示（或输入命令/问题）之间的相互关系，从而帮助其更深入地了解类别或概念。</p><p>我们的研究结果为更有效地导航和理解复杂语言数据、改进模型的知识发现和决策过程提供了宝贵的见解。基于这些进步，我们通过引入分层提示文本编码器改进了传统的文本编码方法，如图 4 所示。我们的目标是改进文本信息与视觉数据的对齐或关联方式，这是视觉语言模型的必要条件必须解释文本和视觉输入。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="2178" height="1921" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24.png" alt="分层提示文本编码器的框架，我们应用三种类型的提示，低级提示、高级提示和全局级提示进行分层调整，并设计一个关系引导的注意模块以更好地建模结构知识。" class="wp-image-1009380" style="width:700px" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24.png 2178w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-300x265.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-1024x903.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-768x677.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-1536x1355.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-2048x1806.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-204x180.png 204w" sizes="(max-width: 2178px) 100vw, 2178px" /><figcaption class="wp-element-caption">图 4. 分层提示的文本编码器从多级提示中学习，并使用关系引导的注意力模块来建模结构知识。</figcaption></figure><h2 class="wp-block-heading" id="looking-ahead">展望未来</h2><p>通过将结构化知识纳入我们的模型训练框架，我们的研究为更复杂的应用奠定了基础。一个例子是增强的图像字幕，其中视觉语言模型能够以更高的准确性和深度描述照片、插图或任何视觉媒体的内容。这一改进可以显着有益于各种应用，例如帮助视力受损的用户。此外，我们设想文本到图像生成方面的进步，使视觉语言模型能够根据文本描述生成更精确、更详细且与上下文相关的视觉表示。</p><p>展望未来，我们希望我们的研究能够激发人们更广泛的兴趣，探索结构化知识在改善视觉和语言理解的即时调整方面的作用。这项探索预计会将这些模型的使用扩展到基本分类任务（模型对数据进行分类或标记数据）之外，从而实现人与人工智能系统之间更细致、更准确的交互。通过这样做，我们为人工智能系统更有效地解释人类语言的复杂性铺平了道路。</p><h2 class="wp-block-heading" id="acknowledgements">致谢</h2><p>感谢 Yubin Wang 在实现算法和执行实验方面做出的贡献。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/structured-knowledge-from-llms-improves-prompt-learning-for-visual-language-models/">法学硕士的结构化知识改善了视觉语言模型的即时学习一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>