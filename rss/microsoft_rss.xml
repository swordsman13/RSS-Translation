<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究博客 - 微软研究</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate>星期三，2025年2月5日17:32:36 +0000</lastbuilddate><language> En-us</language><sy:updateperiod>小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.7.1</generator><item><title>低位量化的进步使LLM在边缘设备上启用</title><link/>https://www.microsoft.com/en-us/research/blog/advances-to-low-bit-bit-quantization-enable-enable-llms-llms-one-degne-devices/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Wed, 05 Feb 2025 17:32:32 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1123686 </guid><description><![CDATA[<p>低位量化技术的进步可以在资源约束的边缘设备上有效地运行LLM。发现T-MAC，Ladder和Lut Tensor Core等创新如何提高计算效率并增强硬件兼容性。</p><p>该帖子<a href="https://www.microsoft.com/en-us/research/blog/advances-to-low-bit-quantization-enable-llms-on-edge-devices/">推进了低位量化的速度使Edge设备上的LLM</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1.jpg" alt="代表人工智能，系统和网络的三个白色图标。这些图标位于紫色至粉红色的梯度背景上。" class="wp-image-1124472" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-Bit-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>大型语言模型（LLMS）越来越多地部署在边缘设备上 - 磁性软件在数据源附近处理数据，例如智能手机，笔记本电脑和机器人。在这些设备上运行LLMS支持高级的AI和实时服务，但是它们的巨大尺寸（具有数亿个参数）需要大量的内存和计算能力，从而限制了广泛的采用。低位量化是一种压缩模型并减少内存需求的技术，它通过实现更有效的操作提供了解决方案。</p><p>低位量化的最新进展使LLMS的混合精液矩阵乘法（MPGEMM）。这种深度学习技术允许将相同或不同格式的数据倍增，例如INT8*INT1，INT8*INT2或FP16*INT4。通过结合各种精确水平，MPGEMM在速度，记忆效率和计算准确性之间达到平衡。</p><p>但是，大多数硬件仅支持对称计算（类似格式的数据的操作）在一般矩阵乘法过程中为混合精液计算（GEMM）引起的挑战，这是LLMS的关键操作。克服这些硬件限制对于完全受益于mpgemm和支持不对称计算至关重要。</p><p>为了在资源受限的边缘设备上解锁低位量化的潜力，硬件必须在本机上支持mpgemm。为了解决这个问题，我们开发了以下三种计算内核和硬件体系结构的方法：</p><ul class="wp-block-list"><li><strong>梯子</strong><strong>数据类型编译器：</strong>通过将不支持类型的类型转换为兼容硬件兼容的数据类型的各种低精度数据类型，而无需数据丢失，同时还会生成高性能转换代码。</li><li> <strong>T-MAC MPGEMM库：</strong>使用查找表（LUT）方法实现GEMM，消除乘法以显着减少计算开销。 T-MAC针对各种CPU进行了优化，可提供其他库的速度几倍。</li><li> <strong>LUT Tensor Core硬件体系结构：</strong>为下一代AI硬件引入了尖端设计，该硬件是针对低位量化和混合精液计算而定制的。</li></ul><p>以下各节详细描述了这些技术。 </p><h2 class="wp-block-heading" id="ladder-bridging-the-gap-between-custom-data-and-hardware-limits">梯子：桥接自定义数据和硬件限制之间的差距</h2><p>GPU，TPU和专业芯片等尖端硬件加速器旨在通过有效处理大型操作来加快计算密集型任务，例如深度学习。这些加速器现在将较低位计算单元（例如FP32，FP16甚至FP8）集成到其体系结构中。</p><p>但是，芯片区域和硬件成本的限制限制了这些单元对标准数据类型的可用性。例如，NVIDIA V100 Tensor Core GPU仅支持FP16，而A100支持INT2，INT4和INT8，但不支持FP8或OCP-MXFP等更新的格式。此外，LLM的快速开发通常超过硬件升级，留下许多新的数据格式，没有支持和复杂的部署。</p><p>此外，尽管硬件加速器可能缺乏对自定义数据类型的直接支持，但其内存系统可以将这些类型转换为存储任何数据格式的固定宽度数据块。例如，可以将NF4张量转换为FP16或FP32进行浮点操作。</p><p>在这些见解的基础上，我们开发了<a href="https://www.microsoft.com/en-us/research/publication/ladder-enabling-efficient-low-precision-deep-learning-computing-through-hardware-aware-tensor-transformation/" target="_blank" rel="noreferrer noopener">梯子</a>数据类型编译器，这是一种将数据存储与计算分开的方法，从而为自定义数据类型提供了更广泛的支持。它将新兴自定义数据格式之间的差距与当前硬件支持的精度类型桥接。</p><p> Ladder提供了一个灵活的系统，用于在算法特异性和硬件支持的数据类型之间转换而没有数据丢失。对于低位应用程序，它通过将低位数据转换为使用的硬件最有效的格式来优化性能。如图1所示，这包括将低位计算映射到支持的指令，并有效地管理整个内存层次结构的数据存储。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1024" height="465" src="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure1.png" alt="图1：说明梯子体系结构的图。在顶部，ttile-graph显示了一个计算流，其中NF4和FP16格式中的输入将输入矩阵乘法（MATMUL）操作，该操作输出在FP16中。该输出以及另一个FP16输入也将在FP16中进行加法（添加）操作。下面，TTILE设备示意图描绘了具有L2/Global Memory，L1/共享内存和L0/寄存器的分层内存结构，在“核心”下组织了L0/寄存器。转换发生在计算周围的加载和存储阶段，箭头指示数据流。调度机制将操作分配给内存层次结构的不同层以优化性能。" class="wp-image-1123677" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure1.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure1-300x136.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure1-768x349.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure1-240x109.png 240w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图1：梯子建筑</figcaption></figure><h3 class="wp-block-heading" id="evaluating-ladder">评估梯子</h3><p>对NVIDIA和AMD GPU的梯子的评估表明，它的表现优于现有的深神经网络（DNN）编译器，用于本质支持的数据类型。它还处理GPU不支持的自定义数据类型，达到高达14.6次的加速。</p><p>作为第一个支持用于在现代硬件加速器上运行DNN的自定义低精度数据类型的系统，梯子为研究人员提供了优化数据类型的灵活性。它还使硬件开发人员能够支持更广泛的数据类型，而无需进行硬件修改。</p><h2 class="wp-block-heading" id="t-mac-table-lookup-for-mpgemm-without-multiplication"> T-MAC：MPGEMM的桌面外观无乘法</h2><p>在边缘设备上部署低位量化的LLM通常需要取消模型以确保硬件兼容性。但是，这种方法有两个主要缺点：</p><ol class="wp-block-list"><li><strong>绩效：</strong>开销开销可能会导致性能差，否定了低位量化的好处。</li><li><strong>开发：</strong>开发人员必须重新设计数据布局和内核，以获得不同的混合精确度。</li></ol><p>为了应对这些挑战，我们引入了<a href="https://www.microsoft.com/en-us/research/publication/t-mac-cpu-renaissance-via-table-lookup-for-low-bit-llm-deployment-on-edge/" target="_blank" rel="noreferrer noopener">T-MAC</a> ，这是一种基于LUT的新方法，可以实现MPGEMM而不会取消或乘法。</p><p> T-MAC用位的表格替代了传统的乘法操作，为mpgemm提供了统一且可扩展​​的解决方案。它结合了减小表尺寸并将它们直接存储在芯片上的技术，从而最大程度地减少了从内存访问数据的开销。通过消除取消化和降低计算成本，T-MAC可以在资源受限的边缘设备上有效推断低位LLMS。图2说明了T-MAC的架构。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1268" height="719" src="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure2.png" alt="图2：显示位序列计算的脱机和在线过程的图。离线：整数重量分解为1位指数，并将其定为瓷砖。在线：通过查找表（LUT）处理的1位模式将激活预报，并使用比特式聚合中的加权总和进行聚合。" class="wp-image-1123683" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure2.png 1268w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure2-300x170.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure2-1024x581.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure2-768x435.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure2-240x136.png 240w" sizes="(max-width: 1268px) 100vw, 1268px" /><figcaption class="wp-element-caption">图2。T-MAC系统的概述</figcaption></figure><h3 class="wp-block-heading" id="evaluating-t-mac">评估T-MAC</h3><p> T-MAC在低位模型上的绩效评估显示出效率和速度的实质性好处。在带有Qualcomm Snapdragon X Elite芯片组的表面笔记本电脑7上，T-MAC实现了：</p><ul class="wp-block-list"><li> 3B BITNET-B1.58型号的每秒48个令牌</li><li>2位7B骆驼型的每秒30个令牌</li><li>4位7b美洲驼型号的每秒20个令牌</li></ul><p>这些速度远远超过了人类的平均阅读率，超过了Llama.cpp的速度4-5倍，并使专用的NPU加速器的速度翻了一番。即使在Raspberry Pi 5之类的低端设备上，T-MAC也使3B BITNET-B1.58模型每秒产生11个令牌。<sub>它也证明了高效，匹配的Llama.cpp的发电率，同时仅使用1/4</sub><sub>至1/6</sub><sup>的</sup><sup>CPU</sup>内核。</p><p>这些结果建立了T-MAC作为使用标准CPU在边缘设备上部署LLM的实用解决方案，而无需依赖GPU或NPU。 T-MAC允许LLM在资源约束设备上有效运行，从而在更广泛的方案中扩展其适用性。</p><h2 class="wp-block-heading" id="lut-tensor-core-driving-hardware-for-mpgemm"> LUT Tensor Core：MPGEMM的驾驶硬件</h2><p>尽管T-MAC和梯子在现有的CPU和GPU架构上优化了MPGEMM，从而提高了计算效率，但它们无法与内置的LUT支持相匹配专用硬件加速器的性能。实现绩效，权力和地区（PPA）的显着改善需要克服四个关键挑战：</p><ol class="wp-block-list"><li><strong>表格和存储：</strong>预先计算和存储LUTS增加了开销，增加了区域使用，延迟和存储要求，这可以降低总体效率的提高。</li><li><strong>位宽度灵活性：</strong>硬件必须支持各种精度级别，例如重量的INT4/2/1，FP16/8或INT8用于激活以及它们的组合。这种灵活性对于适应各种模型架构和用例至关重要。</li><li> <strong>LUT瓷砖形状：</strong>效率低下的瓷砖形状可以提高存储成本并限制重用机会，从而对性能和效率产生不利影响。</li><li><strong>指令和汇编：</strong>基于LUT的MPGEMM需要一个新的指令集。为标准Gemm硬件设计的现有汇编堆栈可能不会最佳地映射并安排这些说明，从而使LLM推理软件的集成变得复杂。</li></ol><p>作为响应，我们开发了<a href="https://www.microsoft.com/en-us/research/publication/lut-tensor-core-lookup-table-enables-efficient-low-bit-llm-inference-acceleration/">LUT Tensor Core</a> ，这是一种用于低位LLM推理的软件硬件代码。为了解决常规基于LUT的方法中的预先启动开销，我们介绍了基于软件的DFG转换，操作员融合和表对称性等技术，以优化表的预录和存储。此外，我们提出了一个具有细长瓷砖形状的硬件设计，以支持桌子的重复使用和一个派系设计，以处理mpgemm中的各种精确组合。</p><p>为了与现有的GPU微体系结构和软件堆栈集成，我们扩展了MMA指令集，添加了新的LMMA指令，并开发了类似Cublas的软件堆栈，以便于集成到现有的DNN Frameworks中。我们还创建了一个编译器，用于使用LUT Tensor Core在GPU上进行端到端的执行计划。在图3中说明的该设计和工作流程使LUT张量核心的快速采用。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1266" height="533" src="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure3.png" alt="图3：LUT张量核心工作流程的图。左侧显示了操作员融合，其中“标准”会产生前计算的激活，而“重量重新解释”过程低位重量。两者都使用激活表和重新解释的权重。右侧说明了LUT张量芯，其中包括用于预先计算值，低位权重和多路复用器（MUX）的LUT表。" class="wp-image-1123680" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure3.png 1266w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure3-300x126.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure3-1024x431.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure3-768x323.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/Low-bit_figure3-240x101.png 240w" sizes="auto, (max-width: 1266px) 100vw, 1266px" /><figcaption class="wp-element-caption">图3。LUT张量核心工作流程</figcaption></figure><h3 class="wp-block-heading" id="evaluating-lut-tensor-core">评估LUT张量核心</h3><p>在低位LLM上测试LUT张量核心（例如Bitnet和Llama）显示出显着的性能增长，达到了推理速度的6.93倍，同时仅使用了传统张量的核心面积的38.3％。凭借几乎相同的模型精度，这会导致计算密度增加20.9倍，并提高11.2倍的能源效率。随着AI模型的规模和复杂性的增长，LUT Tensor Core可以使低位LLMS应用于新的和不同的情况。</p><p>我们认为，LUT技术可以推动AI模型推断的范式转变。传统方法依赖于乘法和积累操作，而LUT实施可提供较高的晶体管密度，每个芯片区域的吞吐量更大，能源成本较低以及更好的可伸缩性。随着大型模型采用低位量化，LUT方法可以成为系统和硬件设计的标准，从而推进了下一代AI硬件创新。</p><h2 class="wp-block-heading" id="unlocking-new-possibilities-for-embodied-ai">解锁体现AI的新可能性</h2><p>低位量化提高了在边缘设备上运行大型模型的效率，同时还可以通过减少代表每个参数的位来实现模型缩放。如<a href="https://www.microsoft.com/en-us/research/publication/the-era-of-1-bit-llms-all-large-language-models-are-in-1-58-bits/">Bitnet模型</a>所示，这种缩放增强了模型功能，一般性和表达性，该模型从低位配置开始并扩展。</p><p> T-MAC，Ladder和Lut Tensor Core等技术为运行低位量化的LLM提供了解决方案，支持跨边缘设备的有效操作，并鼓励研究人员使用低位量化设计和优化LLMS。通过减少记忆和计算需求，低位LLM可以为机器人等动力体现的AI系统，从而实现动态感知和实时环境互动。</p><p> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/T-MAC" target="_blank" rel="noreferrer noopener">T-MAC <span class="sr-only">（在新标签中打开）</span></a>和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/BitBLAS" target="_blank" rel="noreferrer noopener">梯子<span class="sr-only">（在新标签中打开）</span></a>是开源的，在GitHub上可用。我们邀请您通过Microsoft Research在AI技术中测试和探索这些创新。 </p><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1085496"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">聚光灯：博客文章</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/" aria-label="Eureka: Evaluating and understanding progress in AI" data-bi-cN="Eureka: Evaluating and understanding progress in AI" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEWEureka-2024-BlogHeroFeature-1400x788-1-66e90c195464d.jpg" alt="代表Eureka实验管道的白色图标 - 在蓝色至绿色梯度背景上，及时处理，推理，及时处理，推理，数据处理，评估报告。" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">尤里卡：评估和理解AI的进步</h2><p class="large">我们如何严格评估和理解AI的最新进展？尤里卡（Eureka）是一个开源框架，用于标准化大型基础模型的评估，除了单分数报告和排名。了解有关扩展发现的更多信息。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="Eureka: Evaluating and understanding progress in AI" target="_blank">阅读更多</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新标签中打开</span></div><!--/.msr-promo--><span id="label-external-link" class="sr-only" aria-hidden="true">在新标签中打开</span><p>该帖子<a href="https://www.microsoft.com/en-us/research/blog/advances-to-low-bit-quantization-enable-llms-on-edge-devices/">推进了低位量化的速度使Edge设备上的LLM</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title>研究重点：2025年1月27日周</title><link/>https://www.microsoft.com/en-us/research/blog/research-focus-week-january-january-27-2025/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Fri, 31 Jan 2025 17:17:08 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1124682 </guid><description><![CDATA[<p>在本期：一种新的多模式预处理方法的方法； AI时代的托管保留记忆；改善了2型黄斑毛细血管扩张的检测；概括符号自动机。</p><p><a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-january-27-2025/">研究重点：2025年1月27日的一周</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center"><strong>在此版本中：</strong></p><ul class="wp-block-list"><li>我们介绍了Flavars，这是一种用于遥感的多模式基础语言和视觉对齐模型；托管保留内存，一种新的内存类别，更优化，以存储AI推理工作负载的关键数据结构；并使用自我监督的学习和合奏模型增强了黄斑毛毛虫2型（MACTEL 2）的检测。</li><li>我们提出了一种概括符号自动机的新方法，该方法将各种经典的自动机和逻辑汇集在统一框架中，并提供所有必要的成分，以支持符号模型检查Modulo <em>a</em> 。</li><li>我们邀请您加入即将举行的研讨会：LLM4EVAL@WSDM 2025：大型语言模型，用于评估信息检索。 LLM4EVAL是一种有前途的技术，在自动判断，自然语言生成和检索增强发电（RAG）系统领域。来自微软的研究人员以及来自行业和学术界的专家将在3月14日星期五在德国汉诺威的一次互动研讨会上探索这一技术。 </li></ul><figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1.jpg" alt="研究重点：2025年1月31日周" class="wp-image-1125636" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2025/01/NEWRF57-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">新研究</h2><h3 class="wp-block-heading h2" id="heading">类型：用于遥感的多模式基础语言和视觉对齐模型</h3><p>在遥感的领域，图像通常具有密集的物体和视觉内容，这些物体和视觉内容在全球范围内可能在区域变化。这会在描述图像时高度详细，并预处理以更好地平衡视觉任务性能，同时保留执行零照片分类和图像文本检索的能力。</p><p>一种策略是将配对的卫星图像和文本标题结合起来，以预处理性能编码器以进行下游任务。然而，尽管诸如剪辑之类的对比图形方法可以使视觉语言对准和零射击分类能力，但与仅图像预处理相比，夹具仅视力的下游性能倾向于降级，例如掩盖的自动装编码器（MAE）。</p><p>为了更好地接近遥感的多模式预处理，Microsoft的研究人员提出了一种训练方法，该方法结合了对比度学习和掩盖建模的最佳方法，以及通过对比度位置编码的地理空间对准，在最近的论文中， <a href="https://www.microsoft.com/en-us/research/publication/flavars-a-multimodal-foundational-language-and-vision-alignment-model-for-remote-sensing/">Flavars：一种多态的基础语言和视觉语言和视觉语言和视觉性语言和视觉性语言和视野</a>， <a href="https://www.microsoft.com/en-us/research/publication/flavars-a-multimodal-foundational-language-and-vision-alignment-model-for-remote-sensing/">遥感的对齐模型</a>。研究表明，与MAE预处理的方法不同，Flavars明显优于SkyClip的基线，用于仅视觉分类和语义分割等视觉任务，例如Spacenet1上的 +6％MIOU，而与MAE预处理的方法不同。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-1 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--1"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/flavars-a-multimodal-foundational-language-and-vision-alignment-model-for-remote-sensing/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/></div><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">新研究</h2><h3 class="wp-block-heading h2" id="heading">托管保留记忆：AI时代的新记忆类</h3><p>当今的AI簇是高绩效的计算机内存类型的高带宽内存（HBM）的主要用途之一。但是，由于多种原因，HBM的AI推理工作负载是次优的。分析表明，HBM在写作性能，密度不足并读取带宽的情况下过度规定，并且每位上方的能量明显。它也很昂贵，由于制造复杂性，其产量低于DRAM。</p><p>在最近的一篇论文中： <a href="https://www.microsoft.com/en-us/research/publication/managed-retention-memory-a-new-class-of-memory-for-the-ai-era/">托管 - 保留记忆：AI时代的新内存类别</a>，Microsoft的研究人员提出了一个内存类，该类别更优化，可以为AI推理工作负载存储关键数据结构。该论文表明，MRM最终可能为最初提议支持存储类记忆（SCM）的技术提供了生存能力的途径。这些技术传统上提供了长期的持久性（10多年），但提供了差的IO性能和/或耐力。 MRM进行了不同的权衡，MRM放弃了长期数据保留和写入绩效，以提高对AI推断很重要的指标的潜在绩效。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-2 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--2"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/managed-retention-memory-a-new-class-of-memory-for-the-ai-era/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/></div><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">新研究</h2><h3 class="wp-block-heading h2" id="heading">增强的黄斑telangiectisia类型2检测：利用自我监督的学习和合奏模型</h3><p>黄斑毛细血管扩张2型（MACTEL）是一种视网膜疾病，具有挑战性的诊断。尽管提高的意识导致诊断结果的改善，但MACTEL诊断非常依赖于多模式图像集和熟悉该疾病的临床医生的专业知识。光学相干断层扫描（OCT）成像已成为诊断和监测各种视网膜疾病的宝贵工具。随着OCT越来越多地整合到临床实践中，深度学习模型也可能能够实现与视网膜专家相当的准确的MACTEL预测，即使使用有限的数据。</p><p>微软和外部同事的研究人员在最近的一篇论文中应对这一挑战： <a href="https://www.microsoft.com/en-us/research/publication/enhanced-macular-telangiectasia-type-2-detection-leveraging-self-supervised-learning-and-ensemble-models/">增强的黄斑telangiectasia类型2检测：利用自我监督的学习和整体模型</a>。本文发表在《眼科科学杂志》上，重点介绍了使用OCT图像对2型黄斑毛细血管扩张的准确分类，其总体目的是促进对这种神经退行性疾病的早期和精确检测。</p><p>研究人员提出的结果利用了自我监督的学习和整体模型，表明他们的方法提高了Mactel的分类准确性和与单个模型的使用相比。合奏模型与对最有经验的个人专家以及人类专家合奏的评估表现出了较高的一致性。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-3 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--3"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/enhanced-macular-telangiectasia-type-2-detection-leveraging-self-supervised-learning-and-ensemble-models/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/></div><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="999693"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">聚光灯：事件系列</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://researchforum.microsoft.com/?OCID=msr_researchforum_MCR_Blog_Promo" aria-label="Microsoft Research Forum" data-bi-cN="Microsoft Research Forum" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MRF-24_WebImage_1400x788.png" alt="浅蓝色背景上的各种抽象3D形状" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">微软研究论坛</h2><p class="large">加入我们，在AI将军时代进行有关研究的不断交流。按需观看前四集。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://researchforum.microsoft.com/?OCID=msr_researchforum_MCR_Blog_Promo" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Watch on-demand" data-bi-cN="Microsoft Research Forum" target="_blank">观看按需</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新标签中打开</span></div><!--/.msr-promo--><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">新研究</h2><h3 class="wp-block-heading h2" id="heading">符号自动机：Omega-Regullity Modulo理论</h3><p>符号自动机是有限状态自动机，支持潜在的无限字母，例如一组有理数，通常应用于有限词的正则表达式和语言。在符号自动机（或自动机模型<em>A</em> ）中，字母由有效的布尔代数<em>A</em>表示，并由决策程序支持以满足。无限单词的普通语言（所谓的𝜔-regular语言）具有丰富的历史记录，与有限单词的普通语言平行，并具有众所周知的应用程序，可通过Büchi自动机和时间逻辑进行模型检查。</p><p>在最近的一篇论文中： <a href="https://www.microsoft.com/en-us/research/publication/symbolic-automata-omega-regularity-modulo-theories/">符号自动机：欧米茄（Omegar-Regultity Modulo）理论</a>，Microsoft的研究人员通过<em>过渡术语</em>和<em>符号导数</em>概括了象征性自动机，以支持𝜔型规范语言。这将各种经典的自动机和逻辑汇集在一个统一的框架中，该框架提供了支持符号模型检查Modulo <em>a的</em>所有必要成分。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-4 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--4"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/symbolic-automata-omega-regularity-modulo-theories/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/></div><div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"><h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-b60fbfa117bcce78e07885aa24d19fc7" id="new-research">事件</h2><h3 class="wp-block-heading h2" id="heading">LLM4EVAL@WSDM 2025：信息检索中的大型语言模型 -  2025年3月14日</h3><p>LLM已显示出较小模型中不存在的任务解决能力的提高。在自动判断，自然语言生成和检索增强发电（RAG）系统的领域，使用LLMS进行自动评估（LLM4EVAL）是一种有前途的技术。</p><p>加入微软的研究人员以及行业和学术界的专家，讨论使用LLMS在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://llm4eval.github.io/WSDM2025/" target="_blank" rel="noreferrer noopener">LLM4EVAL研讨会上进行信息检索评估 -  2025年WSDM 2025 <span class="sr-only">（新标签开放）</span></a> ，2025年3月14日，在德国汉诺威。</p><p>这个互动研讨会将涵盖自动判断，破布管道评估，改变人类评估，稳健性和LLMS的可信度，以进行评估，以及它们对现实世界应用的影响。组织者认为，信息检索社区可以通过设计，实施，分析和评估LLM的各个方面，并应用于LLM4Eval任务，从而为这一不断发展的研究领域做出重大贡献。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-5 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline is-style-outline--5"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://llm4eval.github.io/WSDM2025/" target="_blank" rel="noreferrer noopener">了解有关研讨会的更多信息</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/></div><div style="padding-bottom:64px; padding-top:64px" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section"><div class="container"><div class="wp-block-msr-immersive-section__inner"><div class="wp-block-msr-cards msr-cards msr-cards--default mt-4 has-text-align-left" data-bi-aN="microsoft-research-in-case-you-missed-it"><div class="msr-cards__inner"><div class="heading-wrapper"><h2 class="mb-5 ">微软研究|如果你错过了</h2></div><div class="row row-cols-1 row-cols-sm-2 row-cols-lg-3"><div class="msr-cards__card msr-cards__card--default col"><div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group"><div class="card-body bg-white p-4 pt-3"><h3 class="h5"> <a
						class="text-decoration-none text-black"
						data-bi-cN="Microsoft Team Uses Diffusion Model For Materials Science"
						href="https://www.forbes.com/sites/johnwerner/2025/01/21/microsoft-team-uses-diffusion-model-for-materials-science/"
					><span>微软团队使用扩散模型进行材料科学</span><span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span></a></h3><div class="card__description card__citation small"><p> 2025年1月21日</p><p>“寻找目标应用程序的新材料就像在干草堆中找到针头一样，”在微软的博客文章的作者写道，他们一直在从事这样的程序，恰当地使用了这种程序。 </p></div></div></div></div><div class="msr-cards__card msr-cards__card--default col"><div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group"><div class="card-body bg-white p-4 pt-3"><h3 class="h5"> <a
						class="text-decoration-none text-black"
						data-bi-cN="Microsoft AutoGen v0.4: A turning point toward more intelligent AI agents for enterprise developers"
						href="https://venturebeat.com/ai/microsoft-autogen-v0-4-a-turning-point-toward-more-intelligent-ai-agents-for-enterprise-developers/"
					><span>Microsoft Autogen V0.4：企业开发人员更聪明的AI代理的转折点</span><span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span></a></h3><div class="card__description card__citation small"><p> 2025年1月18日</p><p>AI代理商的世界正在进行一场革命，微软本周发布的Autogen V0.4在这一旅程中取得了重大飞跃。 Autogen被定位为可靠，可扩展和可扩展的框架，代表了微软的最新尝试，以解决为企业应用程序构建多代理系统的挑战。 </p></div></div></div></div><div class="msr-cards__card msr-cards__card--default col"><div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group"><div class="card-body bg-white p-4 pt-3"><h3 class="h5"> <a
						class="text-decoration-none text-black"
						data-bi-cN="2 AI breakthroughs unlock new potential for health and science"
						href="https://news.microsoft.com/source/features/ai/2-ai-breakthroughs-unlock-new-potential-for-health-and-science/"
					><span>2 AI突破解锁了健康和科学的新潜力</span><span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span></a></h3><div class="card__description card__citation small"><p> 2025年1月17日</p><p>本周在科学期刊上发表的两篇新的研究论文，一篇是自然界的一篇，其中一篇是机器智能，展示了生成性AI基础模型如何呈指数速度加快科学发现的新材料，并帮助医生更快地访问和分析放射学结果。 </p></div></div></div></div><div class="msr-cards__card msr-cards__card--default col"><div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group"><div class="card-body bg-white p-4 pt-3"><h3 class="h5"> <a
						class="text-decoration-none text-black"
						data-bi-cN="ChatGPT gets proactive with 'Tasks'"
						href="https://www.therundown.ai/p/chatgpt-gets-proactive-with-tasks"
					><span>Chatgpt主动使用“任务”</span> <span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span></a></h3><div class="card__description card__citation small"><p> 2025年1月15日</p><p>早上好，AI爱好者。 Openai的AI代理时代刚刚开始了非官方的开始 -  Chatppt具有安排和管理日常任务的能力。随着“任务”推出和神秘的“操作员”在空中窃窃私语，Openai终于准备从聊天机器人转移到全面的自主助手吗？ </p></div></div></div></div><div class="msr-cards__card msr-cards__card--default col"><div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group"><div class="card-body bg-white p-4 pt-3"><h3 class="h5"> <a
						class="text-decoration-none text-black"
						data-bi-cN="Mayo Clinic and Microsoft partner to advance generative AI in radiology"
						href="https://healthimaging.com/topics/artificial-intelligence/mayo-clinic-and-microsoft-partner-advance-generative-ai-radiology"
					><span>Mayo诊所和Microsoft合作伙伴可以推进放射学生成AI</span> <span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span></a></h3><div class="card__description card__citation small"><p> 2025年1月15日</p><p>Mayo诊所正在寻求通过与Microsoft Research进行的新合作来推进生成人工智能在成像中的使用。两人在第43届年度JP Morgan Healthcare会议上宣布了这一消息，该会议现在在旧金山举行。</p></div></div></div></div></div><div class="justify-content-center text-center mb-4"> <a
					href="https://www.microsoft.com/en-us/research/news-and-awards/"
					class="btn btn-outline-primary glyph-append glyph-append-small glyph-append-chevron-right msr-cards__cta"
					data-bi-cN="View more news and awards"
					data-bi-type="button"
				>查看更多新闻和奖项</a></div></div></div></div></div></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新标签中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-january-27-2025/">研究重点：2025年1月27日的一周</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>