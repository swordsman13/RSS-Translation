<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2023 年 10 月 6 日星期五 15:39:58 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.3.1</generator><item><title>使用 SpaceEvo 进行高效且硬件友好的神经架构搜索</title><link/>https://www.microsoft.com/en-us/research/blog/efficient-and-hardware-friend-neural-architecture-search-with-spaceevo/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Fri, 06 Oct 2023 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/blog/efficient-and-hardware-friend-neural-architecture-search-with-spaceevo/ </guid><description><![CDATA[<p>深度学习中持续存在的挑战是针对不同的硬件配置优化神经网络模型，平衡性能和低延迟。了解 SpaceEvo 如何自动执行硬件感知神经架构搜索，以微调 DNN 模型，以便在不同设备上快速执行。</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/efficient-and-hardware-friendly-neural-architecture-search-with-spaceevo/">使用 SpaceEvo 进行高效且硬件友好的神经架构搜索</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center">这篇研究论文在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://iccv2023.thecvf.com/" target="_blank" rel="noreferrer noopener"><strong><em>2023 年 IEEE/CVF 国际计算机视觉会议</em></strong><span class="sr-only">（在新选项卡中打开）</span></a> <strong><em>(ICCV)</em></strong><strong><em>上发表</em></strong>，这是计算机视觉领域的顶级学术会议。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" fetchpriority="high" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1.png" alt="ICCV 2023：SpaceEvo" class="wp-image-972249" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/ICCV-SpaceEVO-2023-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>在深度学习领域， <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://pypi.org/project/microsoftvision/" target="_blank" rel="noreferrer noopener">ResNet <span class="sr-only">（在新选项卡中打开）</span></a>和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/AzureML-BERT" target="_blank" rel="noreferrer noopener">BERT <span class="sr-only">（在新选项卡中打开）</span>等</a>模型取得了突破性的成功，但关键的挑战仍然存在：开发高效的深度神经网络 (DNN) 模型，这些模型都擅长性能并最大限度地减少不同设备之间的延迟。为了解决这个问题，研究人员引入了硬件感知神经架构搜索（NAS），以针对各种硬件配置自动进行高效的模型设计。这种方法涉及预定义的搜索空间、搜索算法、精度估计和特定于硬件的成本预测模型。</p><p>然而，优化搜索空间本身常常被忽视。当前的工作主要依赖于基于 MobileNets 的搜索空间，旨在最大限度地减少移动 CPU 上的延迟。但手动设计可能并不总是符合不同的硬件要求，从而限制了它们对各种设备的适用性。</p><p>在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://iccv2023.thecvf.com/" target="_blank" rel="noreferrer noopener">ICCV <span class="sr-only">2023</span></a>上发表的论文“ <a href="https://www.microsoft.com/en-us/research/publication/spaceevo-hardware-friendly-search-space-design-for-efficient-int8-inference/">SpaceEvo：用于高效 INT8 推理的硬件友好搜索空间设计<span class="sr-only">（在新选项卡中打开）</span></a> ”（在新选项卡中打开）中，我们介绍了 SpaceEvo，这是一种自动创建优化的专用搜索空间的新颖方法用于在特定硬件平台上进行高效的 INT8 推理。 SpaceEvo 的与众不同之处在于它能够自动执行此设计过程，从而创建专为硬件特定、量化友好的 NAS 量身定制的搜索空间。 </p><div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="970287"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-the-future-of-scale-with-ahmed-awadallah-and-ashley-llorens/" aria-label="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" data-bi-cN="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/Ahmed_AI_Frontiers_TW_LI_FB_1200x627_With_Name.png" alt="MSR 播客 |人工智能前沿 |艾哈迈德·阿瓦达拉" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4"> AI 前沿：Ahmed Awadallah 和 Ashley Llorens 的规模化未来</h2><p class="large">本集的主角是高级首席研究经理<a href="https://www.microsoft.com/en-us/research/people/hassanam/" target="_blank" rel="noreferrer noopener">Ahmed H. Awadallah</a> ，他致力于提高大规模人工智能模型的效率，并努力帮助推动该领域从研究到实践的进步<strong> </strong>使他处于人工智能新时代的前沿。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-the-future-of-scale-with-ahmed-awadallah-and-ashley-llorens/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><p>值得注意的是，SpaceEvo 的轻量级设计使其成为实际应用的理想选择，只需 25 个 GPU 小时即可创建特定于硬件的解决方案，使其成为硬件感知 NAS 的经济高效选择。这种专门的搜索空间具有硬件首选的运算符和配置，可以探索更大、更高效、低 INT8 延迟的模型。图 1 表明我们的搜索空间在 INT8 模型质量方面始终优于现有替代方案。在这个硬件友好的空间内进行神经架构搜索可以产生设定新的 INT8 精度基准的模型。</p><figure class="wp-block-image aligncenter size-full"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure1.png"><img decoding="async" width="4397" height="1033" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure1.png" alt="图 1：该图像显示 4 个子图，每个子图说明了在 VNNI CPU 上 10 毫秒、VNNI CPU 上 15 毫秒、Pixel 4 CPU 上 10 毫秒和 20 毫秒的 INT8 量化延迟内采样模型时的模型精度误差分布。用于各种搜索空间的像素 CPU。每个子图包含 4 – 5 条曲线，代表我们的搜索空间、ProxylessNAS 搜索空间、MobileNetv3 搜索空间、ResNet 搜索空间和 AttentiveNAS 搜索空间的模型精度误差分布。我们的搜索空间始终如一地提供卓越的 INT8 模型群，在不同的硬件和延迟限制下超越最先进的替代方案。" class="wp-image-971418" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure1.png 4397w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure1-300x70.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure1-1024x241.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure1-768x180.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure1-1536x361.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure1-2048x481.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure1-240x56.png 240w" sizes="(max-width: 4397px) 100vw, 4397px" /></a><figcaption class="wp-element-caption">图 1. 不同 NAS 搜索空间中 INT8 量化模型的误差分布。我们的搜索空间在 INT8 模型质量方面始终优于最先进的替代方案。</figcaption></figure><h2 class="wp-block-heading" id="on-device-quantization-latency-analysis">设备上量化延迟分析</h2><p>我们首先尝试了解 INT8 量化延迟因素及其对搜索空间设计的影响。我们在两种广泛使用的设备上进行了研究：具有 VNNI 指令和 onnxruntime 支持的 Intel CPU，以及具有 TFLite 2.7 的 Pixel 4 手机 CPU。</p><p>我们的研究揭示了两个重要发现：</p><ol type="1"><li>运算符类型和配置（如通道宽度）的选择都会显着影响 INT8 延迟，如图 2 所示。例如，Squeeze-and-Exitation 和 Hardswish 等运算符虽然以最小延迟提高准确性，但可能会导致 INT8 推理速度变慢英特尔 CPU。这种放缓主要是由于 INT32 和 INT8 之间数据转换的额外成本造成的，这超过了通过 INT8 计算实现的延迟减少。</li><li>不同设备的量化效率不同，并且首选的算子类型可能是矛盾的。</li></ol><figure class="wp-block-image aligncenter size-full is-resized"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure2.png"><img decoding="async" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure2.png" alt="图2：图像展示了表格（左）和图（右）。左边的表格，有标签"operator, intel CPU, pixel 4", highlights the INT8 latency speedup in comparison to Float32 latency of various operators on two hardware types. The rows are categorized as: Conv, DWConv, SE, hardswish, and swish. The figure on the right depicts the INT8 quantized speedup of Conv1x1 across different channel numbers. It includes two curves, each signifying speedups under diverse channel numbers on Intel CPU and Pixel 4 hardware.  " class="wp-image-971421" style="width:626px;height:260px" width="626" height="260" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure2.png 2295w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure2-300x125.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure2-1024x425.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure2-768x319.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure2-1536x638.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure2-2048x850.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure2-240x100.png 240w" sizes="(max-width: 626px) 100vw, 626px" /></a><figcaption class="wp-element-caption">图 2. 左：选择不同的运算符类型会带来显着不同的量化速度改进。右：不同通道数的 Conv1x1 速度增强。 </figcaption></figure><h2 class="wp-block-heading" id="finding-diverse-efficient-quantized-models-with-spaceevo">使用 SpaceEvo 寻找多样化、高效的量化模型</h2><p>与旨在找到最佳单一模型的传统架构搜索不同，我们的目标是在搜索空间中发现数十亿个准确且 INT8 延迟友好的架构的多样化群体。</p><p>受到神经架构搜索的启发，我们引入了一种进化搜索算法来探索 SpaceEvo 中这种量化友好的模型群体。我们的方法结合了三个关键技术：</p><ol type="1"><li>基于顶级子网的 INT8 准确度-延迟，引入 QT 分数作为衡量候选搜索空间量化友好性的指标。</li><li>重新设计的搜索算法专注于探索广阔的超空间内的模型群体集合（即搜索空间），如图 3 所示。这是通过“弹性阶段”实现的，该阶段将搜索空间划分为一系列弹性序列阶段，使得衰老进化等传统进化方法能够得到有效探索。</li><li>一种逐块搜索空间量化方案，用于减少与探索具有最大 QT 分数的搜索空间相关的训练成本。</li></ol><p>发现搜索空间后，我们采用两阶段 NAS 过程来训练搜索空间上的全量化超网。这确保了所有候选模型都可以达到相当的量化精度，而无需单独微调或量化。我们利用进化搜索和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/nn-Meter" target="_blank" rel="noreferrer noopener">nn-Meter <span class="sr-only">（在新选项卡中打开）</span></a>进行 INT8 延迟预测，以确定各种 INT8 延迟约束下的最佳量化模型。图3显示了总体设计流程。</p><figure class="wp-block-image aligncenter size-full is-resized"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure3.png"><img decoding="async" loading="lazy" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure3.png" alt="图 3：该图描绘了一个流程图，概述了完整的 SpaceEvo 流程及其在 NAS 中的应用。从一个大的超空间开始，进化搜索算法探索候选搜索空间。然后，质量估计器根据 INT8 延迟和准确性评估其质量分数。该分数用作算法的奖励，指导进一步的探索，直到找到合适的搜索空间。然后在这个空间上训练一个针对所有人的量化超网，使硬件感知 NAS 能够在各种 INT8 延迟限制内部署模型。" class="wp-image-971424" style="width:496px;height:256px" width="496" height="256" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure3.png 1315w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure3-300x155.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure3-1024x529.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure3-768x397.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/09/figure3-240x124.png 240w" sizes="(max-width: 496px) 100vw, 496px" /></a><figcaption class="wp-element-caption">图3：NAS的完整SpaceEvo流程和应用</figcaption></figure><p>在两个现实世界的边缘设备和 ImageNet 上进行的大量实验表明，我们自动设计的搜索空间显着超过了手动设计的搜索空间。表 1 展示了我们发现的模型 SEQnet，它为 INT8 量化精度-延迟权衡设定了新基准。 </p><figure class="wp-block-table"><table><tbody><tr><td class="has-text-align-center" data-align="center" colspan="6"> <strong>(a) 使用 onnxruntime 在 Intel VNNI CPU 上的结果</strong></td></tr><tr><td class="has-text-align-center" data-align="center" rowspan="2">模型</td><td class="has-text-align-center" data-align="center">前 1 名 Acc %</td><td class="has-text-align-center" data-align="center" colspan="2">潜伏</td><td class="has-text-align-center" data-align="center">前 1 名 Acc %</td><td class="has-text-align-center" data-align="center" rowspan="2">失败次数</td></tr><tr><td class="has-text-align-center" data-align="center">INT8</td><td class="has-text-align-center" data-align="center"> INT8</td><td class="has-text-align-center" data-align="center">加速</td><td class="has-text-align-center" data-align="center">FP32</td></tr><tr><td class="has-text-align-center" data-align="center"> MobileNetV3小型</td><td class="has-text-align-center" data-align="center">66.3</td><td class="has-text-align-center" data-align="center"> 4.4毫秒</td><td class="has-text-align-center" data-align="center">1.1倍</td><td class="has-text-align-center" data-align="center">67.4</td><td class="has-text-align-center" data-align="center"> 56M</td></tr><tr><td class="has-text-align-center" data-align="center"> <strong>SEQnet@cpu-A0</strong></td><td class="has-text-align-center" data-align="center"> <strong>74.7</strong></td><td class="has-text-align-center" data-align="center"> <strong>4.4毫秒</strong></td><td class="has-text-align-center" data-align="center"><strong>2.0倍</strong></td><td class="has-text-align-center" data-align="center"><strong>74.8</strong></td><td class="has-text-align-center" data-align="center"> 163M</td></tr><tr><td class="has-text-align-center" data-align="center">移动网络V3大型</td><td class="has-text-align-center" data-align="center">74.5</td><td class="has-text-align-center" data-align="center"> 10.3 毫秒</td><td class="has-text-align-center" data-align="center">1.5倍</td><td class="has-text-align-center" data-align="center">75.2</td><td class="has-text-align-center" data-align="center"> 219M</td></tr><tr><td class="has-text-align-center" data-align="center"> <strong>SEQnet@cpu-A1</strong></td><td class="has-text-align-center" data-align="center"> <strong>77.4</strong></td><td class="has-text-align-center" data-align="center"> <strong>8.8 毫秒</strong></td><td class="has-text-align-center" data-align="center"><strong>2.4倍</strong></td><td class="has-text-align-center" data-align="center"><strong>77.5</strong></td><td class="has-text-align-center" data-align="center"> 358M</td></tr><tr><td class="has-text-align-center" data-align="center"> FBNetV3-A</td><td class="has-text-align-center" data-align="center"> 78.2</td><td class="has-text-align-center" data-align="center"> 27.7 毫秒</td><td class="has-text-align-center" data-align="center">1.3倍</td><td class="has-text-align-center" data-align="center">79.1</td><td class="has-text-align-center" data-align="center"> 357M</td></tr><tr><td class="has-text-align-center" data-align="center"> <strong>SEQnet@cpu-A4</strong></td><td class="has-text-align-center" data-align="center"> <strong>80.0</strong></td><td class="has-text-align-center" data-align="center"> <strong>24.4 毫秒</strong></td><td class="has-text-align-center" data-align="center"><strong>2.4倍</strong></td><td class="has-text-align-center" data-align="center"><strong>80.1</strong></td><td class="has-text-align-center" data-align="center"> 1267M</td></tr><tr><td class="has-text-align-center" data-align="center" colspan="6"> <strong>(b) 使用 TFLite 的 Google Pixel 4 的结果</strong></td></tr><tr><td class="has-text-align-center" data-align="center">MobileNetV3小型</td><td class="has-text-align-center" data-align="center">66.3</td><td class="has-text-align-center" data-align="center"> 6.4 毫秒</td><td class="has-text-align-center" data-align="center">1.3倍</td><td class="has-text-align-center" data-align="center">67.4</td><td class="has-text-align-center" data-align="center"> 56M</td></tr><tr><td class="has-text-align-center" data-align="center"> <strong>SEQnet@pixel4-A0</strong></td><td class="has-text-align-center" data-align="center"> <strong>73.6</strong></td><td class="has-text-align-center" data-align="center"> <strong>5.9 毫秒</strong></td><td class="has-text-align-center" data-align="center"><strong>2.1倍</strong></td><td class="has-text-align-center" data-align="center"><strong>73.7</strong></td><td class="has-text-align-center" data-align="center"> 107M</td></tr><tr><td class="has-text-align-center" data-align="center">移动网络V3大型</td><td class="has-text-align-center" data-align="center">74.5</td><td class="has-text-align-center" data-align="center"> 15.7 毫秒</td><td class="has-text-align-center" data-align="center">1.5倍</td><td class="has-text-align-center" data-align="center">75.2</td><td class="has-text-align-center" data-align="center"> 219M</td></tr><tr><td class="has-text-align-center" data-align="center">高效网络-B0</td><td class="has-text-align-center" data-align="center"> 76.7</td><td class="has-text-align-center" data-align="center"> 36.4 毫秒</td><td class="has-text-align-center" data-align="center">1.7倍</td><td class="has-text-align-center" data-align="center">77.3</td><td class="has-text-align-center" data-align="center"> 390M</td></tr><tr><td class="has-text-align-center" data-align="center"> <strong>SEQnet@pixel4-A1</strong></td><td class="has-text-align-center" data-align="center"> <strong>77.6</strong></td><td class="has-text-align-center" data-align="center"> <strong>14.7 毫秒</strong></td><td class="has-text-align-center" data-align="center"><strong>2.2倍</strong></td><td class="has-text-align-center" data-align="center"><strong>77.7</strong></td><td class="has-text-align-center" data-align="center"> 274M</td></tr></tbody></table><figcaption class="wp-element-caption"><center>表 1. 在两台设备上的 ImageNet 结果中，我们的自动搜索空间优于手动搜索空间。加速：与 FP32 推理相比，INT8 延迟。 </center></figcaption></figure><h2 class="wp-block-heading" id="potential-for-sustainable-and-efficient-computing">可持续和高效计算的潜力</h2><p>SpaceEvo 是解决 NAS 中硬件友好的搜索空间优化挑战的首次尝试，为为各种现实世界边缘设备设计有效的低延迟 DNN 模型铺平了道路。展望未来，SpaceEvo 的影响远远超出其最初的成就。其潜力扩展到其他关键部署指标的应用程序，例如能源和内存消耗，从而增强边缘计算解决方案的可持续性。</p><p>我们正在探索调整这些方法来支持 Transformer 等不同的模型架构，进一步扩大其在发展深度学习模型设计和高效部署方面的作用。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/efficient-and-hardware-friendly-neural-architecture-search-with-spaceevo/">使用 SpaceEvo 进行高效且硬件友好的神经架构搜索</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title> HoloAssist：物理世界下一代人工智能副驾驶的多模式数据集</title><link/>https://www.microsoft.com/en-us/research/blog/holoassist-a-multimodal-dataset-for-next-gen-ai-copilots-for-the-physical-world/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Thu, 05 Oct 2023 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=972234 </guid><description><![CDATA[<p> HoloAssist 是一个新的多模式数据集，由 222 名参与者的 166 小时交互式任务执行组成。了解它如何提供宝贵的数据来提高下一代人工智能副驾驶执行实际任务的能力。</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/holoassist-a-multimodal-dataset-for-next-gen-ai-copilots-for-the-physical-world/">HoloAssist：物理世界下一代人工智能副驾驶的多模态数据集</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center">这篇研究论文在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://iccv2023.thecvf.com/" target="_blank" rel="noreferrer noopener"><strong><em>2023 年 IEEE/CVF 国际计算机视觉会议</em></strong><span class="sr-only">（在新选项卡中打开）</span></a> <strong><em>(ICCV)</em></strong><strong><em>上发表</em></strong>，这是计算机视觉领域的顶级学术会议。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1.png" alt=""ICCV23 PARIS" to the left of a picture of the first page of the HoloAssist publication on a blue and purple gradient background." class="wp-image-972549" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/BLG_-ICCV-2023-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>您最后一次面对不知道如何解决的任务是什么时候？也许是在修理坏掉的自行车、更换打印机碳粉，或者制作一杯浓缩咖啡？在这种情况下，您通常的选择可能包括向知识渊博的朋友或亲戚寻求帮助。或者，您可以求助于互联网、进行网络搜索、在在线论坛上提出问题或寻找相关的教学视频。但如果还有另一种选择呢？如果你可以向人工智能助手或<em>副驾驶</em>寻求帮助怎么办？</p><h2 class="wp-block-heading" id="ai-in-the-real-world">现实世界中的人工智能</h2><p>我们的日常生活充满了各种各样的任务，无论是工作还是休闲，跨越数字和物理领域。我们经常发现自己需要指导才能有效地学习和执行这些任务。人工智能的最新进展，特别是在大语言和多模式模型领域，催生了智能数字代理。然而，在我们执行大量任务的物理世界中，人工智能系统历来面临着更大的挑战。</p><p>人工智能社区长期以来的愿望是开发一种交互式人工智能助手，能够感知、推理并与现实世界中的人们协作。无论是自动驾驶、机器人导航和操纵、工业环境中的危险检测，还是混合现实任务的支持和指导等场景，与全数字化活动相比，物理活动的进展速度更慢、增量更大。 </p><div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="874872"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">焦点：点播活动</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/event/microsoft-research-summit-2022/?OCID=msr-researchsummit_Blog_PromoMod" aria-label="Microsoft Research Summit 2022" data-bi-cN="Microsoft Research Summit 2022" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2022/09/WebsiteHero_1400x788_B.jpg" alt="具有向上移动的蓝色、紫色和橙色瓷砖的抽象图像" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4"> 2022 年微软研究院峰会</h2><p class="large"><strong>一经请求</strong><br>立即观看，了解我们研究界面临的一些最紧迫的问题，并聆听与 120 多名研究人员围绕如何确保新技术为人类带来最广泛利益的对话。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/event/microsoft-research-summit-2022/?OCID=msr-researchsummit_Blog_PromoMod" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="Microsoft Research Summit 2022" target="_blank">探索会议</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="the-promise-and-challenge-of-interactive-ai-copilots">交互式人工智能副驾驶的承诺和挑战</h2><p>开发交互式人工智能副驾驶来协助人们完成现实世界的任务有巨大的潜力，但也存在障碍。关键的挑战是当前最先进的人工智能助手缺乏物理世界的第一手经验。因此，他们无法感知现实世界的状态并在必要时积极干预。这种限制源于缺乏对此类场景中感知、推理和建模所需的特定数据的培训。在人工智能发展方面，有句话叫“数据为王”。这次挑战也不例外。为了推进用于物理任务的交互式人工智能代理，我们必须彻底了解问题领域并为副驾驶的能力建立黄金标准。</p><h2 class="wp-block-heading" id="a-new-multimodal-interactive-dataset">一个新的多模态交互式数据集</h2><p>作为朝这个方向迈出的第一步，我们很高兴分享我们在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://iccv2023.thecvf.com/" target="_blank" rel="noreferrer noopener">ICCV <span class="sr-only">2023</span></a>上发表的论文“ <a href="https://www.microsoft.com/en-us/research/publication/holoassist-an-egocentric-human-interaction-dataset-for-interactive-ai-assistants-in-the-real-world/">HoloAssist：现实世界中交互式人工智能助手的以自我为中心的人类交互数据集<span class="sr-only">（在新选项卡中打开）</span></a> ”（在新选项卡中打开） 。 HoloAssist 是一个大规模的以自我为中心或第一人称的人类交互数据集，两个人协作执行物理操作任务。任务执行者戴着混合现实耳机执行任务，该耳机捕获七个同步数据流，如图 1 所示。同时，任务指导员实时观察执行者的第一人称视频并提供口头指导。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="480" height="498" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig1.png" alt="该图像展示了 HoloAssist 数据集的设置，该数据集具有两人交互式辅助任务完成设置。任务执行者戴着混合现实耳机，而教练则观看第一人称视频并提供说明。捕获八种模式：RGB、眼睛注视、手部姿势、头部姿势、深度、IMU、音频、文本转录。" class="wp-image-972576" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig1.png 480w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig1-289x300.png 289w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig1-173x180.png 173w" sizes="(max-width: 480px) 100vw, 480px" /><figcaption class="wp-element-caption">图 1：HoloAssist 具有两人交互式辅助任务完成设置。</figcaption></figure><p> HoloAssist 包含大量数据，包括 222 名不同参与者长达 166 小时的录音。这些参与者组成 350 个不同的指导者-表演者对，执行 20 项以对象为中心的操作任务。视频 1 显示了如何记录任务，而图 2 提供了任务分解。这些物品的范围从常见的电子设备到工厂和专业实验室中发现的稀有物品。这些任务通常要求很高，通常需要教练的帮助才能成功完成。为了提供全面的见解，我们捕获了七种不同的原始传感器模式：RGB、深度、头部姿势、3D 手势、眼睛注视、音频和 IMU。这些模式有助于理解人类意图、估计世界状态、预测未来行动等等。最后，第八种模式是第三人称手动注释的增强，包括文本摘要、干预类型、错误注释和操作片段，如图 3 所示。 </p><figure class="wp-block-video aligncenter"><video controls src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/holoassist4kzoom.mp4"></video><figcaption class="wp-element-caption">视频 1：展示颜色和深度（八种模式中的两种）的任务记录样本。 </figcaption></figure><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="624" height="263" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig2.png" alt="HoloAssist 中捕获的数据分布。左侧显示每项活动的会话数，右侧显示会话总时长（以分钟为单位）。有 20 个任务：GoPro、Nintendo Switch、DSLR、便携式打印机、电脑、Nespresso 机、独立打印机、大咖啡机、宜家家具（凳子、实用推车、托盘桌、床头柜）、NavVis 激光扫描仪、ATV 摩托车、轮带和断路器。每项活动有 25 到 180 节课，课时从 47 到 1390 分钟不等。" class="wp-image-972573" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig2.png 624w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig2-300x126.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig2-240x101.png 240w" sizes="(max-width: 624px) 100vw, 624px" /><figcaption class="wp-element-caption">图 2：HoloAssist 中捕获的数据分布。左侧是每个活动的会话数。右侧显示总会话时长（以分钟为单位）。 </figcaption></figure><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="480" height="414" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig3.png" alt="HoloAssist 包括动作和对话注释，并提供指示任务期间错误和干预的视频摘要。每个动作都标有“错误”或“正确”属性，而口头陈述则标有干预类型。该图显示了其中每一个的示例。" class="wp-image-972570" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig3.png 480w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig3-300x259.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Fig3-209x180.png 209w" sizes="(max-width: 480px) 100vw, 480px" /><figcaption class="wp-element-caption">图 3：HoloAssist 包括动作和对话注释，还提供指示任务期间错误和干预的视频摘要。每个动作都标有“错误”或“正确”属性，而口头陈述则标有干预类型。</figcaption></figure><h2 class="wp-block-heading" id="towards-proactive-ai-assistants">迈向主动的人工智能助手</h2><p>我们的工作建立在以自我为中心的愿景和具体人工智能方面的先前进步的基础上。与早期的数据集（例如表 1 中列出的数据集）不同，HoloAssist 因其多人、交互式任务执行设置而脱颖而出。任务执行过程中的人机交互为设计人工智能助手提供了宝贵的资源，这些助手具有前瞻性和主动性，可以提供基于环境的精确定时指令，这与当前等待您提问的“基于聊天”的人工智能助手形成鲜明对比。这种独特的场景非常适合开发辅助人工智能代理，并补充了现有的数据集，提供了丰富的知识和表示。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="508" height="325" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Table1.png" alt="该表显示了九个相关数据集和模拟平台的比较，以及每个数据集的设置，是否是协作和交互、指导和程序以及视频的小时数。 HoloAssist 具有多人辅助设置，这是对现有第一人称（以自我为中心）数据集的独特补充。" class="wp-image-972567" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Table1.png 508w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Table1-300x192.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/10/HoloAssist_Table1-240x154.png 240w" sizes="(max-width: 508px) 100vw, 508px" /><figcaption class="wp-element-caption">表1：相关数据集和模拟平台的比较。 HoloAssist 具有多人辅助设置，这是对现有自我中心（第一人称）数据集的独特补充。</figcaption></figure><p>最后，我们评估了数据集在动作分类和预期任务上的表现，提供了实证结果，阐明了不同模式在各种任务中的作用。借助该数据集，我们引入了新的任务和基准，重点关注错误检测、干预类型预测和 3D 手势预测，这些都是开发智能助手的关键要素。</p><h2 class="wp-block-heading" id="looking-forward">期待</h2><p>这项工作代表了更广泛研究的第一步，探索智能代理如何在现实世界的任务中与人类协作。我们很高兴与社区分享这项工作和我们的数据集，并预测许多未来的方向，例如注释对象姿势、研究人工智能辅助中以对象为中心的可供性和操作模型，以及人工智能辅助规划和状态跟踪等。我们相信 HoloAssist 及其相关基准和工具将有利于未来的研究工作，重点是为现实世界的日常任务构建强大的人工智能助手。您可以在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://holoassist.github.io/" target="_blank" rel="noreferrer noopener">GitHub 上访问 HoloAssist 数据集和代码<span class="sr-only">（在新选项卡中打开）</span></a> 。</p><h3 class="wp-block-heading" id="contributors">贡献者</h3><p>Taein Kwon、 <a href="https://www.microsoft.com/en-us/research/people/mahdirad/">Mahdi Rad</a> 、Bowen Pan、 <a href="https://www.microsoft.com/en-us/research/people/ischakra/">Ishani Chakraborty</a> 、 <a href="https://www.microsoft.com/en-us/research/people/sandrist/">Sean Andrist</a> 、 <a href="https://www.microsoft.com/en-us/research/people/dbohus/">Dan Bohus</a> 、 <a href="https://www.microsoft.com/en-us/research/people/ashleyf/">Ashley Feniello</a> 、Bugra Tekin、 <a href="https://www.microsoft.com/en-us/research/people/fevieira/">Felipe Vieira Frujeri</a> 、 <a href="https://www.microsoft.com/en-us/research/people/mapoll/">Marc Pollefeys</a></p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/holoassist-a-multimodal-dataset-for-next-gen-ai-copilots-for-the-physical-world/">HoloAssist：物理世界下一代人工智能副驾驶的多模态数据集</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </content:encoded><enclosure length="120799231" type="video/mp4" url="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/holoassist4kzoom.mp4"></enclosure></item></channel></rss>