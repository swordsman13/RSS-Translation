<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2023 年 5 月 18 日星期四 13:19:28 +0000</lastbuilddate><language> zh-CN</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1个</sy:updatefrequency><generator>https://wordpress.org/?v=6.2</generator><item><title> REACT——一种协同的云边融合架构</title><link/>https://www.microsoft.com/en-us/research/blog/react-a-synergistic-cloud-edge-fusion-architecture/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Thu, 18 May 2023 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=941124 </guid><description><![CDATA[<p>这篇研究论文被第八届 ACM/IEEE 物联网设计与实现会议 (IoTDI) 接受，这是物联网的主要场所。该论文描述了一个框架，该框架利用云资源以更高的精度执行大型深度神经网络 (DNN) 模型，以提高在边​​缘设备上运行的模型的准确性。这 […]</p><p>帖子<a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/react-a-synergistic-cloud-edge-fusion-architecture/">REACT — A synergistic cloud-edge fusion architecture</a>首次出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center h6"><em>这篇研究论文被第八届<a href="https://conferences.computer.org/iotDI/2023/" target="_blank" rel="noreferrer noopener">ACM/IEEE 物联网设计与实现会议</a>(IoTDI) 接受，这是物联网的主要场所。该论文描述了一个框架，该框架利用云资源以更高的精度执行大型深度神经网络 (DNN) 模型，以提高在边​​缘设备上运行的模型的准确性。</em> </p><figure class="wp-block-image size-large"><img decoding="async" loading="lazy" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-1024x576.jpg" alt="蓝色渐变背景上的 iotdi 标志" class="wp-image-941151" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IoTDI-23-1400x788-hero-ms-logo.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure><h2 id="heading-leveraging-the-cloud-and-edge-concurrently" class="wp-block-heading">同时利用云和边缘</h2><p>互联网正在向边缘计算架构发展，以支持新兴物联网和移动计算应用领域中对延迟敏感的 DNN 工作负载。然而，与云环境不同，边缘计算资源有限，<a href="https://arxiv.org/abs/2210.03204" target="_blank" rel="noreferrer noopener">无法运行大型、高精度的 DNN 模型</a>。因此，过去的工作重点是将一些计算<em>卸载</em>到云端以绕过这一限制。然而，这是以增加延迟为代价的。</p><p>例如，在道路交通监控、无人机监控和驾驶员辅助技术等边缘视频分析用例中，人们可以偶尔将帧传输到云端以执行对象检测——这项任务非常适合托管在强大 GPU 上的模型。另一方面，边缘通过对象跟踪处理插值中间帧——使用通用 CPU、低功率边缘 GPU 或其他边缘加速器（例如，Intel Movidius Neural Stick）执行的相对便宜的计算任务。然而，对于大多数实时应用程序来说，由于严格的延迟限制，在云端处理数据是不可行的。 </p><div style="height:15px" aria-hidden="true" class="wp-block-spacer"></div><div class="border-bottom border-top border-gray-300 mt-5 mt-md-4 mb-4 mb-md-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="931956"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">聚焦：点播视频</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" aria-label="AI Explainer: Foundation models ​and the next era of AI" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AIEx01_blog_hero_1400x788.png" alt="一个人的电脑屏幕截图" /></a></div><div class="msr-promo__content py-3 col-12 col-md"><h2 class="h4"> AI Explainer：基础模型和人工智能的下一个时代</h2><p class="large">探索 Transformer 架构、更大的模型和更多的数据以及情境学习如何帮助推动 AI 从感知到创造。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Watch video" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank">看视频</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--></div><!--/.msr-promo--><p>在我们的研究论文<a href="https://www.microsoft.com/en-us/research/publication/react-streaming-video-analytics-on-the-edge-with-asynchronous-cloud-support/" target="_blank" rel="noreferrer noopener">REACT：支持异步云的边缘流视频分析 中</a>，我们提出并展示了一种新颖的架构，该架构<em>同时利用边缘和云</em>在两端执行<em>冗余计算</em>。这有助于保持边缘的低延迟，同时利用云的力量提高准确性。我们的关键技术贡献是将异步接收的云输入融合到边缘的计算流中，从而在不牺牲延迟的情况下提高检测质量。</p><h2 id="heading-fusing-edge-and-cloud-detections" class="wp-block-heading">融合边缘和云检测</h2><div class="wp-block-columns"><div class="wp-block-column"><figure class="wp-block-image aligncenter size-full"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/FlowDiagram_1200.jpg"><img decoding="async" loading="lazy" width="1200" height="535" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/FlowDiagram_1200.jpg" alt="图 (a) 说明了 REACT 如何利用来自云和边缘的对象检测。中间帧使用对象跟踪，其性能会随着时间的推移而下降。边缘检测会立即收到，但来自云端的边缘检测会有一些延迟。" class="wp-image-941298" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/FlowDiagram_1200.jpg 1200w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/FlowDiagram_1200-300x134.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/FlowDiagram_1200-1024x457.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/FlowDiagram_1200-768x342.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/FlowDiagram_1200-240x107.jpg 240w" sizes="(max-width: 1200px) 100vw, 1200px" /></a><figcaption class="wp-element-caption">图 1(a)：橙色和绿色框表示来自边缘和云的检测。跟踪性能随着每一帧的下降而下降，由逐渐褪色的蓝色阴影表示。 </figcaption></figure></div><div class="wp-block-column"><figure class="wp-block-image aligncenter size-full"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IllustrationCloudEdge_1200.jpg"><img decoding="async" loading="lazy" width="1200" height="535" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IllustrationCloudEdge_1200.jpg" alt="图 (b) 显示了来自行车记录仪的几张图像以及 REACT 如何帮助提高物体检测性能。" class="wp-image-941301" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IllustrationCloudEdge_1200.jpg 1200w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IllustrationCloudEdge_1200-300x134.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IllustrationCloudEdge_1200-1024x457.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IllustrationCloudEdge_1200-768x342.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/IllustrationCloudEdge_1200-240x107.jpg 240w" sizes="(max-width: 1200px) 100vw, 1200px" /></a><figcaption class="wp-element-caption">图 1(b)：REACT 使用异步云检测来纠正框标签并检测更多对象。</figcaption></figure></div></div><p>我们在 REACT 中说明了我们用于视频中对象检测的融合方法。图 1 显示了使用轻量级边缘模型进行对象检测的结果。这受到遗漏对象（例如，未检测到第 1 帧中的汽车）和错误分类对象（例如，帧右侧的货车被错误分类为汽车）的影响。</p><p>为了解决边缘计算能力有限和使用边缘模型导致精度下降的挑战，我们采用了双管齐下的方法。首先，由于视频帧序列是时空相关的，因此每隔几帧调用一次边缘对象检测就足够了。如图 1(a) 所示，边缘检测每五帧运行一次。如图所示，为了插入中间帧，我们采用了相对轻量级的对象跟踪操作。其次，为了提高推理的准确性，将选择的帧异步传输到云端进行推理。根据网络延迟和云资源的可用性，云检测仅在几帧后到达边缘设备。接下来，较新的云检测（以前未检测到）与当前帧合并。为此，我们将在旧帧上进行的云检测提供给对象跟踪器的另一个实例，以“快进”到当前时间。只要场景不突然改变，新检测到的对象就可以合并到当前帧中。图 1(b) 显示了我们的方法在行车记录仪视频数据集上的视觉结果。</p><p>下面是关于 REACT 如何结合边缘和云检测的更详细描述。每个检测都包含由 ⟨class_label, bounding_box, confidence_score⟩ 元组表示的对象。每当我们收到新的检测（边缘或云）时，我们都会从当前列表中清除先前从同一检测源（云或边缘）获得的对象。然后我们形成一个大小为<em>(c, n) 的零矩阵。</em>这里， <em>c</em>和<em>n</em>分别是与当前列表和新源的检测相关的索引。我们用联合交集 (IoU) 值填充矩阵单元格（如果它大于 0.5）对应于特定的当前和新检测。然后我们执行线性求和分配，它匹配具有最大重叠的两个对象。对于重叠的对象，我们根据新检测的来源修改置信度值、边界框和类标签。具体来说，我们的分析表明边缘检测模型可以正确定位对象，但通常会出现误报，即它们错误地分配了类标签。相比之下，云检测具有较高的定位误差，但类标签的误差较低。最后，较新的对象（不匹配的对象）将被添加到当前对象的列表中，并返回置信度值、边界框和类标签。因此，REACT 的融合算法必须考虑多种情​​况——例如未对齐的边界框、类标签不匹配等——以将边缘和云检测合并到一个列表中。 </p><figure class="wp-block-table"><table><thead><tr><th class="has-text-align-center" data-align="center">探测器</th><th class="has-text-align-center" data-align="center">骨干</th><th class="has-text-align-center" data-align="center">在哪里</th><th class="has-text-align-center" data-align="center">#参数</th></tr></thead><tbody><tr><td class="has-text-align-center" data-align="center">更快的 R-CNN</td><td class="has-text-align-center" data-align="center"> ResNet50-FPN</td><td class="has-text-align-center" data-align="center">云</td><td class="has-text-align-center" data-align="center">41.5M</td></tr><tr><td class="has-text-align-center" data-align="center">视网膜网络</td><td class="has-text-align-center" data-align="center">ResNet50-FPN</td><td class="has-text-align-center" data-align="center">云</td><td class="has-text-align-center" data-align="center">36.1M</td></tr><tr><td class="has-text-align-center" data-align="center">中心网</td><td class="has-text-align-center" data-align="center">DLA34</td><td class="has-text-align-center" data-align="center">云</td><td class="has-text-align-center" data-align="center">2010万</td></tr><tr><td class="has-text-align-center" data-align="center">TinyYOLOv3</td><td class="has-text-align-center" data-align="center"> DN19</td><td class="has-text-align-center" data-align="center">边缘</td><td class="has-text-align-center" data-align="center">8.7M</td></tr><tr><td class="has-text-align-center" data-align="center">固态硬盘</td><td class="has-text-align-center" data-align="center">MobileNetV2</td><td class="has-text-align-center" data-align="center">边缘</td><td class="has-text-align-center" data-align="center">3.4M</td></tr></tbody></table><figcaption class="wp-element-caption"><center>表 1：我们评估中使用的模型</center></figcaption></figure><p>在我们的实验中，我们利用最先进的计算机视觉算法在边缘和云端进行对象检测（见表 1）。此外，我们使用 mAP@0.5 <em>（在 0.5 IoU 时的平均精度）</em> ，这是计算机视觉社区中流行的一种衡量对象检测性能的指标。此外，为了评估 REACT 的功效，我们查看了两个数据集：</p><ol type="i"><li> <a href="https://github.com/VisDrone/VisDrone-Dataset" target="_blank" rel="noreferrer noopener"><strong>VisDrone</strong></a> <strong>：</strong>基于无人机的监控</li><li><a href="https://paperswithcode.com/dataset/d2city" target="_blank" rel="noreferrer noopener"><strong>D2City</strong></a> <strong>：</strong>基于行车记录仪的驾驶员辅助</li></ol><p>根据我们的评估，我们观察到 REACT 的性能优于基准算法多达 50%。此外，我们注意到边缘和云模型可以相互补充，并且由于我们的边缘云融合算法，整体性能得到提高。</p><p>如前所述，对象检测器仅每隔几帧运行一次，并且在中间帧上执行轻量级对象跟踪。在边缘和云端冗余运行检测允许应用程序开发人员灵活地权衡边缘与云执行的频率，同时实现相同的准确性，如图 2 所示。例如，如果边缘设备遇到热节流，我们可以选择一个较低的边缘检测频率（例如，每 20 帧一次）并用每 30 帧一次的云检测对其进行补充，以获得大约 22.8 的 mAP@0.5。然而，如果边​​缘的约束较少，我们可以将边缘检测频率增加到每五帧一次，并将云检测减少到每 120 帧一次以获得类似的性能（mAP@0.5 为 22.7）。这为细粒度的编程控制提供了一个游乐场。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Heatmap.png" alt="该图显示了称为 mAP@0.5 的对象检测精度指标的热图，其中边缘和云检测频率发生了变化。为了获得更高的准确性，我们需要以更高的速率运行检测。该图突出了权衡，即为了保持准确性，可以增加云检测频率但降低边缘频率，反之亦然。" class="wp-image-941280" width="650" height="490"/><figcaption class="wp-element-caption">图 2：D2-City 数据集上不同云和边缘检测频率的 mAP@0.5 值。相似的阴影对应于相似的 mAP@0.5。</figcaption></figure><p>此外，可以通过让这些设备共享相同的云托管模型来分摊在多个边缘设备上使用云资源的成本。具体来说，如果应用程序可以容忍高达 500 毫秒的中值延迟，我们可以使用 V100 GPU 同时支持 60 多个并发设备（图 3）。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/Median_Response_time_scaling.jpg" alt="散点图显示随着共享同一 GPU 进行模型服务的并发边缘设备数量的增加，响应时间的中值。在这里，我们展示了 4 种类型的 GPU。最初，所有 GPU 的中值响应时间缓慢增加，直到达到拐点，之后增加速度更快。" class="wp-image-941283" width="675" height="476"/><figcaption class="wp-element-caption">图 3：第 50<sup>个</sup>百分位数响应时间与同时共享云 GPU 的边缘设备数量</figcaption></figure><h2 id="heading-conclusion" class="wp-block-heading">结论</h2><p>REACT 代表了边缘 + 云计算的新范例，它利用每个资源来提高准确性而不牺牲延迟。正如我们上面所展示的，卸载和设备上推理之间的选择不是二元的，云和边缘位置的冗余执行在谨慎使用时可以相互补充。虽然我们专注于对象检测，但我们相信这种方法可以用于其他环境，例如人体姿势估计、实例和语义分割应用程序，以实现“两全其美”。</p><p>帖子<a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/react-a-synergistic-cloud-edge-fusion-architecture/">REACT — A synergistic cloud-edge fusion architecture</a>首次出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title>使用 Microsoft Editor Neural Grammar Checker 实现零 COGS</title><link/> https://www.microsoft.com/en-us/research/blog/achieving-zero-cogs-with-microsoft-editor-neural-grammar-checker/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Thu, 18 May 2023 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=939033 </guid><description><![CDATA[<p> Microsoft Editor 为全球数百万用户提供 AI 驱动的写作帮助。所有级别和领域的作者都依赖的功能之一是语法检查器，它可以检测用户写作中的语法错误，并提供建议的更正和对检测到的错误的解释。语法检查器背后的技术已经发展 […]</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/achieving-zero-cogs-with-microsoft-editor-neural-grammar-checker/">使用 Microsoft Editor Neural Grammar Checker 实现零 COGS 的</a>帖子首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img decoding="async" loading="lazy" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788.jpg" alt="坐在桌前用笔记本电脑工作的黑人男性" class="wp-image-939045" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/DNNGrammerChecker_Stagnant_Hero_1400x788-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p> Microsoft Editor 为全球数百万用户提供 AI 驱动的写作帮助。所有级别和领域的作者都依赖的功能之一是语法检查器，它可以检测用户写作中的语法错误，并提供建议的更正和对检测到的错误的解释。</p><p>自 1970 年代以来，语法检查器背后的技术有了显着发展，当时第一代工具基于简单的模式匹配。重大突破发生在 1997 年，当时 Microsoft Word 97 引入了依赖成熟的自然语言处理系统的语法检查器（Heidorn，2000），从而实现了更复杂和准确的错误检测和纠正。另一个重大突破发生在 2020 年，当时微软推出了一种神经语法检查器，该检查器利用具有新颖流畅性提升学习和推理机制的深度神经网络，在 CoNLL-2014 和 JFLEG 基准数据集<sup>[1, 2]</sup> 。 2022 年，微软在 Word Win32、Word Online、Outlook Online 和 Editor Browser Extension 的扩展端点上发布了高度优化版本的 Microsoft Editor 神经语法检查器。</p><p>在这篇博文中，我们将描述我们如何使用 Microsoft Research (MSR) 开创的积极解码算法优化 Editor 神经语法检查器模型，并使用高性能 ONNX 运行时 (ORT) 进行加速。通过 Aggressive Decoding 算法和 ORT 优化，服务器模型的推理速度提高了约 200%，同时节省了三分之二的成本，与之前的生产模型相比，模型预测质量没有损失。 </p><div style="height:15px" aria-hidden="true" class="wp-block-spacer"></div><div class="border-bottom border-top border-gray-300 mt-5 mt-md-4 mb-4 mb-md-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="932112"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">聚焦：微软研究院播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-ai-for-health-and-the-future-of-research-with-peter-lee/" aria-label="AI Frontiers: AI for health and the future of research with Peter Lee" data-bi-cN="AI Frontiers: AI for health and the future of research with Peter Lee" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/03/PeterLee_podcast-2023Mar_hero_1400x788.png" alt="Peter Lee 戴着眼镜，对着镜头微笑，左边是 Microsoft Research Podcast 徽标" /></a></div><div class="msr-promo__content py-3 col-12 col-md"><h2 class="h4"> AI Frontiers：AI 与 Peter Lee 的健康和研究的未来</h2><p class="large">微软研究院负责人 Peter Lee 和 AI 科学家兼工程师 Ashley Llorens 讨论了 AI 研究的未来以及 GPT-4 作为医疗副驾驶的潜力。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-ai-for-health-and-the-future-of-research-with-peter-lee/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="AI Frontiers: AI for health and the future of research with Peter Lee" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--></div><!--/.msr-promo--><p>但我们并没有就此止步。我们还实现了MSR最前沿的端侧seq2seq建模技术EdgeFormer，获得了可以在用户设备上运行的具有竞争力性能的轻量级生成语言模型，让我们实现了最终的零销售成本。 (COGS) 目标。</p><p>除了实现零销货成本外，交付客户端模型还提供其他三个主要优势：</p><ol><li><strong>增加隐私。</strong>在用户设备上本地运行的客户端模型不需要将任何个人数据发送到远程服务器。</li><li><strong>提高可用性</strong>。客户端模型离线运行，不依赖于网络连接、带宽或服务器容量。</li><li><strong>降低成本并提高可扩展性</strong>。将客户端模型传送到用户的设备会消除服务器需要执行的所有计算，这使我们能够传送给更多的客户。</li></ol><p>此外，我们利用 GPT-3.5（当时最先进的 AI 模型）生成高质量的训练数据并识别和删除低质量的训练示例，从而提高模型性能。</p><h2 id="heading-innovation-aggressive-decoding" class="wp-block-heading">创新：积极解码</h2><p>Microsoft Editor 中基于 AI 的语法检查器的背后是 transformer 模型，该模型通过 MSR 用于语法校正的尖端研究创新<sup>[1,2,3]</sup>得到增强。与大多数 seq2seq 任务一样，我们使用自回归解码来进行高质量的语法校正。然而，传统的自回归解码效率非常低，因为它的计算并行性低，无法充分利用现代计算设备（CPU、GPU），这导致模型服务成本高，并阻止我们快速扩展到更多（网络/桌面）端点。</p><p>为了应对降低服务成本的挑战，我们采用了最新的解码创新，即<a href="https://www.microsoft.com/en-us/research/publication/instantaneous-grammatical-error-correction-with-shallow-aggressive-decoding/">Aggressive Decoding</a> ， <sup>[3]</sup>由 MSR 研究人员 Tao Ge 和 Furu Wei 在 ACL 2021 上发表。不同于以往以预测质量下降为代价加快推理的方法, Aggressive Decoding是第一个无损加速seq2seq任务的高效解码算法，例如语法检查和句子重写。激进解码适用于输入和目标输出高度相似的任务。它使用输入作为目标输出并并行验证它们，而不是像传统的自回归解码那样逐一解码。因此，它可以显着加快解码过程，每年处理数万亿个请求，而不会通过更好地利用现代计算设备（例如带有图形处理单元 (GPU) 的 PC）的强大并行计算能力而牺牲质量。 </p><figure class="wp-block-image size-full"><img decoding="async" loading="lazy" width="1920" height="1080" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/aggdec.gif" alt="Aggressive Decoding无损加速机制的gif演示。 Aggressive Decoding 推测性地使用输入文本作为草稿输出，以并行高效地验证草稿结果，从而可以以更少的时间成本获得相同的结果。" class="wp-image-940020"/></figure><p>上图显示了激进解码的工作原理。如果我们在积极解码期间发现分叉，我们将丢弃分叉后的所有预测并使用传统的一对一自回归解码重新解码它们。如果我们在一对一重新解码期间在输出和输入之间找到后缀匹配（即，一些用蓝色虚线突出显示的建议），我们通过复制标记（用橙色虚线突出显示）切换回积极解码行）并在解码器输入的输入中跟随匹配的标记，假设它们是相同的。这样，Aggressive Decoding 可以保证生成的 token 与 autoregressive greedy decoding 相同，但解码步骤少得多，显着提高了解码效率。</p><h2 id="heading-offline-evaluations" class="wp-block-heading">线下评价</h2><p>我们在语法校正和其他文本重写任务（例如文本简化）中测试了 Aggressive Decoding，使用 6+6 标准转换器以及具有深度编码器和浅层解码器的转换器。所有结果都证实，积极解码可以在不损失质量的情况下引入显着的加速。 </p><figure class="wp-block-table"><table><tbody><tr><td class="has-text-align-center" data-align="center" rowspan="2"></td><td class="has-text-align-center" data-align="center" colspan="2"> CoNLL14</td><td class="has-text-align-center" data-align="center" colspan="2"> NLCC-18</td><td class="has-text-align-center" data-align="center" colspan="3">维基百科</td></tr><tr><td class="has-text-align-center" data-align="center">F0.5</td><td class="has-text-align-center" data-align="center">加速</td><td class="has-text-align-center" data-align="center">F0.5</td><td class="has-text-align-center" data-align="center">加速</td><td class="has-text-align-center" data-align="center">纱丽</td><td class="has-text-align-center" data-align="center">蓝队</td><td class="has-text-align-center" data-align="center">加速</td></tr><tr><td class="has-text-align-center" data-align="center">6+6变压器（光束=1）</td><td class="has-text-align-center" data-align="center"> 61.3</td><td class="has-text-align-center" data-align="center"> 1个</td><td class="has-text-align-center" data-align="center">29.4</td><td class="has-text-align-center" data-align="center"> 1个</td><td class="has-text-align-center" data-align="center">36.1</td><td class="has-text-align-center" data-align="center"> 90.7</td><td class="has-text-align-center" data-align="center"> 1个</td></tr><tr><td class="has-text-align-center" data-align="center">6+6变压器（AD）</td><td class="has-text-align-center" data-align="center"> 61.3</td><td class="has-text-align-center" data-align="center"> 6.8</td><td class="has-text-align-center" data-align="center"> 29.4</td><td class="has-text-align-center" data-align="center"> 7.7</td><td class="has-text-align-center" data-align="center"> 36.1</td><td class="has-text-align-center" data-align="center"> 90.7</td><td class="has-text-align-center" data-align="center"> 8个</td></tr></tbody></table></figure><figure class="wp-block-table"><table><tbody><tr><td class="has-text-align-center" data-align="center" rowspan="2"></td><td class="has-text-align-center" data-align="center" colspan="2"> CoNLL14</td></tr><tr><td class="has-text-align-center" data-align="center"> F0.5</td><td class="has-text-align-center" data-align="center">加速</td></tr><tr><td class="has-text-align-center" data-align="center">12+2变压器（光束=1）</td><td class="has-text-align-center" data-align="center"> 66.4</td><td class="has-text-align-center" data-align="center"> 1个</td></tr><tr><td class="has-text-align-center" data-align="center">12+2变压器（AD）</td><td class="has-text-align-center" data-align="center"> 66.4</td><td class="has-text-align-center" data-align="center"> 4.2</td></tr></tbody></table></figure><p>它甚至可以在更强大的并行计算能力更强的计算设备上运行得更好（例如 A100）： </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="453" height="450" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/device_speedup.png" alt="四个图表显示了不同计算设备中积极解码带来的加速。 Aggressive Decoding 可以在更先进的计算设备（即 V100 和 A100 with fp16）中产生更好的加速结果，展示了它在未来更强大的计算设备（例如 H100 with fp8）中的巨大潜力。" class="wp-image-940023" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/device_speedup.png 453w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/device_speedup-300x298.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/device_speedup-150x150.png 150w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/device_speedup-180x180.png 180w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/device_speedup-181x180.png 181w" sizes="(max-width: 453px) 100vw, 453px" /></figure><h3 id="heading-online-evaluation" class="wp-block-heading">在线测评</h3><p>我们使用 ONNX 运行时在 Marian 服务器模型和具有激进解码的同等大小服务器模型之间运行了 A/B 实验。后者在 Marian 运行时显示了 2x+ 改进@p50 和 3x+ 改进@p95 和@p99，在 CPU 中使用传统的自回归解码，如下图所示。此外，它提供了比以前的自回归解码更好的效率稳定性，后者在延迟方面变化很大（大约与句子长度成正比），因为无论句子长度如何，激进解码都只需几步并行计算就可以大大降低解码成本。这种显着的推理时间加速导致生产端点的 COGS 减少了三分之二。 </p><figure class="wp-block-image size-full"><img decoding="async" loading="lazy" width="2064" height="657" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/MarianONNXLatencyComparison.jpg" alt="三个条形图显示了 Marian 服务器模型和 ONNX 服务器模型之间的模型延迟比较，在 50 个百分位、第 95 个百分位和第 99 个百分位处跨 15 个区域进行积极解码。第一个条形图显示 ONNX 模型在第 50 个百分位处的延迟提高了 2 倍。第二个和第三个条形图显示 ONNX 模型在第 95 个百分位和第 99 个百分位处的延迟提高了 3 倍。" class="wp-image-940029" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/MarianONNXLatencyComparison.jpg 2064w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/MarianONNXLatencyComparison-300x95.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/MarianONNXLatencyComparison-1024x326.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/MarianONNXLatencyComparison-768x244.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/MarianONNXLatencyComparison-1536x489.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/MarianONNXLatencyComparison-2048x652.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/MarianONNXLatencyComparison-240x76.jpg 240w" sizes="(max-width: 2064px) 100vw, 2064px" /></figure><p>离线/在线评估均证实，积极解码使我们能够在不损失模型预测质量的情况下显着降低 COGS。基于这种直觉，我们将<sup>[4]</sup> <a href="https://www.microsoft.com/en-us/research/publication/lossless-acceleration-for-seq2seq-generation-with-aggressive-decoding/">Aggressive Decoding 推广到更一般的 seq2seq 任务</a>。其高效率和无损质量使得 Aggressive Decoding 有可能成为 seq2seq 任务<em>事实上的</em>解码标准，并在 seq2seq 模型部署的成本降低方面发挥至关重要的作用。</p><h2 id="heading-accelerate-grammar-checker-with-onnx-runtime" class="wp-block-heading">使用 ONNX 运行时加速语法检查器</h2><p><a href="https://onnxruntime.ai/" target="_blank" rel="noreferrer noopener">ONNX Runtime</a>是由 Microsoft 开发的高性能引擎，可在各种硬件目标上运行 AI 模型。范围广泛的 ML 支持的 Microsoft 产品利用 ONNX 运行时来推理性能加速。为了进一步减少推理延迟，使用 PyTorch-ONNX 导出器将具有积极解码的 PyTorch 语法检查器导出为 ONNX 格式，然后使用 ONNX 运行时进行推理，从而实现转换器优化和量化，以加速 CPU 性能并减小模型大小。该端到端解决方案启用了多种技术来高效运行高级语法检查器模型。</p><p> PyTorch 提供了一个<a href="https://pytorch.org/docs/stable/onnx.html" target="_blank" rel="noreferrer noopener">内置功能，可以轻松地将 PyTorch 模型导出为 ONNX</a>格式。为了支持语法检查器模型的独特架构，我们启用了将复杂的嵌套控制流导出到导出器中的 ONNX。在此过程中，我们还扩展了关于序列类型和运算符的官方 ONNX 规范，以表示更复杂的场景（即自回归搜索算法）。这消除了单独导出模型编码器和解码器组件并在以后使用额外的序列生成实现将它们拼接在一起以进行生产的需要。借助 PyTorch-ONNX 导出器和 ONNX 运行时中的序列类型和运算符支持，我们能够导出单个 ONNX 图，包括编码器和解码器以及序列生成，这带来了高效的计算和更简单的推理逻辑。此外，PyTorch ONNX 导出器的形状类型推理组件得到增强，可以在更严格的 ONNX 形状类型约束下生成有效的 ONNX 模型。</p><p>语法检查器模型中引入的创新的 Aggressive Decoding 算法最初是在 Fairseq 中实现的。为了使其与 ONNX 兼容，我们在 HuggingFace 中重新实现了这种激进解码算法，以便于导出。在深入实施时，我们确定了 ONNX 标准运算符集中不直接支持的某些组件（例如，分叉检测器）。有两种方法可以将不受支持的运算符导出到 ONNX 并使用 ONNX Runtime 运行。我们可以创建一个由几个具有等效语义的标准 ONNX 运算符组成的图形，或者在 ONNX 运行时中以更有效的方式实现自定义运算符。 <a href="https://onnxruntime.ai/docs/reference/operators/add-custom-op.html" target="_blank" rel="noreferrer noopener">ONNX Runtime 自定义运算符功能</a>允许用户实现自己的运算符以更灵活地在 ONNX Runtime 中运行。这是实现成本和性能之间的权衡。考虑到这些组件的复杂性，标准 ONNX 运算符的组合可能成为性能瓶颈。因此，我们在 ONNX Runtime 中引入了自定义运算符来表示这些组件。</p><p> ONNX 运行时支持<a href="https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333" target="_blank" rel="noreferrer noopener">转换器优化</a>和<a href="https://medium.com/microsoftazure/faster-and-smaller-quantized-nlp-with-hugging-face-and-onnx-runtime-ec5525473bb7" target="_blank" rel="noreferrer noopener">量化，</a>在 CPU 和 GPU 上显示出非常有前途的性能提升。我们进一步增强了语法检查器模型的编码器注意力融合和解码器重塑融合。支持该模型的另一大挑战是多个模型子图。我们在 ONNX Runtime 转换器优化器和量化工具中实现了子图融合。 ONNX Runtime Quantization 应用于整个模型，进一步提高了吞吐量和延迟。</p><h2 id="heading-quality-enhancement-by-gpt-3-5-llms" class="wp-block-heading">通过 GPT-3.5 LLM 提高质量</h2><p>为了进一步提高生产模型的精度和召回率，我们采用强大的 GPT-3.5 作为教师模型。具体来说，GPT-3.5 模型通过以下两种方式来帮助改进结果：</p><ul><li><strong>训练数据增强</strong>：我们微调 GPT-3.5 模型并使用它为大量未注释的文本生成标签。获得的注释被证明是高质量的，可以用作增强训练数据来增强我们模型的性能。</li></ul><ul><li><strong>训练数据清理</strong>：我们利用 GPT-3.5 强大的零/少镜头功能来区分高质量和低质量的训练示例。然后，GPT-3.5 模型重新生成已识别的低质量示例的注释，从而产生更清洁、更高质量的训练集，这直接提高了我们模型的性能。 </li></ul><h2 id="heading-edgeformer-cost-effective-parameterization-for-on-device-seq2seq-modeling" class="wp-block-heading"> EdgeFormer：设备上 seq2seq 建模的经济高效参数化</h2><p>近年来，客户端设备的计算能力大大提高，允许使用深度神经网络来实现最终的零 COGS 目标。然而，在这些设备上运行生成语言模型仍然是一个重大挑战，因为必须严格控制这些模型的内存效率。当涉及到生成语言模型时，在自然语言理解中用于神经网络的传统压缩方法通常不适用。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="978" height="448" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/server_vs_client.png" alt="两个插图显示了服务器模型和客户端模型之间的差异。" class="wp-image-940032" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/server_vs_client.png 978w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/server_vs_client-300x137.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/server_vs_client-768x352.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/server_vs_client-240x110.png 240w" sizes="(max-width: 978px) 100vw, 978px" /></figure><p>要发布客户端语法模型，该模型应该是高效的（例如，在 100 毫秒延迟内），这已经通过前面提到的积极解码解决了。此外，客户端模型必须具有内存效率（例如，在 50MB RAM 占用空间内），这是在客户端设备上运行强大的（生成的）转换器模型（通常超过 5000 万个参数）的主要瓶颈。</p><p>为了应对这一挑战，我们引入了<a href="https://www.microsoft.com/en-us/research/publication/edgeformer-a-parameter-efficient-transformer-for-on-device-seq2seq-generation/">EdgeFormer</a> <sup>[6]</sup> ，这是一种尖端的设备上 seq2seq 建模技术，用于获得具有竞争力的性能的轻量级生成语言模型，可以在用户的​​计算机上轻松运行。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="373" height="272" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/client_shipping_bar.png" alt="图中显示了客户端 DNN 语法检查器的延迟和内存传输条。积极解码可以有效解决延迟挑战，而内存挑战则由另一项称为 EdgeFormer 的创新解决。" class="wp-image-940035" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/client_shipping_bar.png 373w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/client_shipping_bar-300x219.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/client_shipping_bar-240x175.png 240w" sizes="(max-width: 373px) 100vw, 373px" /></figure><p> EdgeFormer 的主要思想是两个原则，我们提出了这些原则以实现具有成本效益的参数化：</p><ul><li>编码器偏好的参数化</li><li>负载均衡参数化</li></ul><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="628" height="267" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/enc_favored_parameterization.png" alt="显示编码器偏好的参数化的插图和表格具有成本效益。" class="wp-image-940038" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/enc_favored_parameterization.png 628w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/enc_favored_parameterization-300x128.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/enc_favored_parameterization-240x102.png 240w" sizes="(max-width: 628px) 100vw, 628px" /></figure><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="630" height="285" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/load_balanced_parameterization.png" alt="（左）图显示了不同网络架构中的参数负载。 （右）图表显示参数使用不足或过度使用都是不可取的，这表明我们需要平衡参数负载。" class="wp-image-940041" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/load_balanced_parameterization.png 630w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/load_balanced_parameterization-300x136.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/load_balanced_parameterization-240x109.png 240w" sizes="(max-width: 630px) 100vw, 630px" /></figure><p>我们根据上述具有成本效益的参数化原则设计了 EdgeFormer，允许每个参数发挥其最大潜力，尽管客户端设备具有严格的计算和内存限制，但仍取得了有竞争力的结果。</p><p>在 EdgeFormer 的基础上，我们进一步提出了<a href="https://github.com/microsoft/unilm/tree/master/edgelm" target="_blank" rel="noreferrer noopener">EdgeLM</a> ——EdgeFormer 的预训练版本，这是第一个公开可用的预训练设备端 seq2seq 模型，可以轻松地针对 seq2seq 任务进行微调，并取得很好的结果。 EdgeLM 作为语法客户端模型的基础模型来实现零 COGS 目标，与服务器模型相比，它实现了超过<strong>5 倍的模型大小压缩</strong>和最小的质量损失。 </p><h3 id="heading-inference-cost-reduction-to-empower-client-device-deployment" class="wp-block-heading"><em>降低</em>推理成本以支持客户端设备部署</h3><p>客户端设备上的模型部署对硬件使用有严格的要求，例如内存和磁盘大小，以避免干扰其他用户应用程序。 ONNX Runtime 显示了设备上部署的优势以及其轻量级引擎和全面的以客户端推理为中心的解决方案，例如 ONNX Runtime 量化和 ONNX Runtime 扩展。此外，为了在满足出货要求的同时保持服务质量，MSR引入了一系列优化技术，包括系统感知模型优化、模型元数据简化、延迟参数加载以及自定义量化策略。基于 EdgeFormer 建模，这些系统优化可以在不牺牲模型性能的情况下将内存成本进一步降低<strong>2.7 倍</strong>。</p><p>我们将在以下部分详细说明每一个：</p><p><strong>系统感知模型优化。</strong>由于该模型表示为数据流图，因此该模型的主要内存成本来自生成的许多子图。如下图所示，PyTorch 代码中的一个分支被映射为一个子图。因此，我们优化模型实现以减少分支指令的使用。特别是，我们利用贪婪搜索作为解码器搜索算法，因为波束搜索包含更多分支指令。使用这种方法可以减少<strong>38% 的</strong>内存开销。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="957" height="460" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/graphMapping.gif" alt="两个图表显示了 PyTorch 模型和 ONNX 模型图的映射。左图显示了一个以 if_else 语句作为循环体的 while 循环。它是 PyTorch DNN 模型中的控制流示例。控制流的每个分支都映射到右图中的子图。右图说明了由连接节点组成的 ONNX 数据流图。每个节点都包含元数据。主图中的每个子图都映射到 PyTorch 分支。" class="wp-image-940044"/><figcaption class="wp-element-caption"> PyTorch模型与ONNX模型图的映射</figcaption></figure><p><strong>模型元数据简化。</strong>同样如上图所示，该模型包含大量消耗内存的元数据，例如节点名称和类型、输入和输出以及参数。为了降低成本，我们简化了元数据以仅保留推理所需的基本信息。例如，节点名称从一个长字符串简化为一个索引。除此之外，我们优化了 ONNX Runtime 中的模型图实现，只保留一份元数据副本，而不是每次生成子图时都复制所有可用的元数据。</p><p> <strong>ONNX Runtime 中的延迟权重加载。</strong>当前模型文件包括模型图和权重，然后在模型初始化期间将它们一起加载到内存中。但是，这会增加内存占用，如下图所示，因为在模型图解析和转换过程中会重复复制权重。为了避免这种情况，我们将模型图和权重分开保存。在 ONNX 运行时初始化期间，只有图被加载到内存中以进行实际的解析和转换。另一方面，权重仍然驻留在磁盘上，只有指针通过文件映射保存在内存中。实际加载到内存的权重将推迟到模型推理时。这种技术可以将峰值内存成本降低<strong>50%</strong> 。 </p><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="428" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/deferredWeights-1024x428.gif" alt="两个图表显示了延迟权重加载和默认 ONNX 运行时实现之间的差异。上图显示，在模型初始化阶段，模型图解析和转换的每一步都需要一个权重副本。这三个步骤从左到右分别是 FlatBuffer、TensorProto 和 OrtValue。在推理阶段，峰值内存成本增加了模式权重大小的三倍。下面的图表也显示了三个步骤，但每个步骤都有映射的权重。加载权重直到推理开始。因此，峰值内存仅添加了权重大小。" class="wp-image-940047" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/deferredWeights-1024x428.gif 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/deferredWeights-300x125.gif 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/deferredWeights-768x321.gif 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/05/deferredWeights-240x100.gif 240w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">在模型初始化期间通过文件映射延迟加载权重</figcaption></figure><p></p><p><strong>ONNX 运行时量化和 ONNX 运行时扩展。</strong>量化是一种众所周知的模型压缩技术，它可以在牺牲模型精度的同时带来性能加速和模型尺寸减小。 ONNX Runtime Quantization 提供了多种调整旋钮，使我们能够应用定制的量化策略。具体来说，我们将策略自定义为训练后、动态、UINT8、每通道和所有运算符量化，以将此模型的准确性影响降至最低。 <a href="https://github.com/microsoft/onnxruntime-extensions" target="_blank" rel="noreferrer noopener">Onnxruntime-extensions</a>提供了一组 ONNX Runtime 自定义算子来支持视觉、文本和自然语言处理模型的通用预处理和后处理算子。有了它，这个模型的预处理和后处理，包括标记化、字符串操作等，可以集成到一个独立的 ONNX 模型文件中，从而提高性能、简化部署、减少内存使用，并更好地可移植性。</p><h2 id="heading-conclusion" class="wp-block-heading">结论</h2><p>在这篇博文中，我们介绍了如何利用 MSR 和 ONNX Runtime 的前沿研究创新来优化服务器语法检查器模型，并通过客户端语法检查器模型实现最终的零 COGS 目标。服务器模型的推理速度提高了约 200%，同时节省了三分之二的成本，并且模型预测质量没有损失。与服务器模型相比，客户端模型实现了超过 5 倍的模型大小压缩，质量损失最小。这些优化使我们能够快速扩展到更多的 Web 和桌面端点，并为全球数百万用户提供 AI 驱动的写作帮助。</p><p>这篇博文中分享的创新只是我们长期持续努力降低生成 AI 模型的 COGS 的第一个里程碑。我们提出的方法不仅限于加速神经语法检查器；它可以很容易地推广并更广泛地应用于抽象摘要、翻译或搜索引擎等场景，以加速大型语言模型以减少 COGS <sup>[5,8]</sup> ，这不仅对微软而且对整个行业都至关重要通用人工智能（AGI）时代。</p><h2 id="heading-reference" class="wp-block-heading">参考</h2><p>[1] 涛哥，魏福如，周明：Fluency Boost Learning and Inference for Neural Grammatical Error Correction。在 ACL 2018 中。</p><p> [2] 涛哥，魏福如，周明：在自动语法纠错中达到人类水平的表现：一项实证研究。 <a href="https://arxiv.org/abs/1807.01270" target="_blank" rel="noreferrer noopener">https://arxiv.org/abs/1807.01270</a></p><p> [3] Xin Sun, Tao Ge, Shuming Ma, Jingjing Li, Furu Wei, Houfeng Wang：A Unified Strategy for Multilingual Grammatic Error Correction with Pre-trained Cross-lingual Language Model.在 IJCAI 2022 中。</p><p> [4] 孙鑫, 涛哥, 魏福如, 王厚峰: 浅层攻击解码的即时语法纠错。在 ACL 2021 中。</p><p> [5] Tao Ge, Heming Xia, Xin Sun, Si-Qing Chen, Furu Wei：无损加速Seq2seq生成与激进解码。 <a href="https://arxiv.org/pdf/2205.10350.pdf" target="_blank" rel="noreferrer noopener">https://arxiv.org/pdf/2205.10350.pdf</a></p><p> [6] Tao Ge、Si-Qing Chen、Furu Wei：EdgeFormer：用于设备上 Seq2seq 生成的参数高效转换器。在 EMNLP 2022 中。</p><p> [7] 海顿，乔治。 “智能写作辅助。”自然语言处理手册。 Robert Dale、Hermann L. Moisl 和 HL Somers，编辑。纽约：Marcel Dekker，2000：181-207。</p><p> [8] 杨楠，戈涛，王亮，焦斌兴，蒋大新，杨林军，Rangan Majumder，魏福如：Inference with Reference: Lossless Acceleration of Large Language Models. <a href="https://arxiv.org/abs/2304.04487" target="_blank" rel="noreferrer noopener">https://arxiv.org/abs/2304.04487</a></p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/achieving-zero-cogs-with-microsoft-editor-neural-grammar-checker/">使用 Microsoft Editor Neural Grammar Checker 实现零 COGS 的</a>帖子首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>