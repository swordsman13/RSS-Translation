<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2023 年 6 月 26 日星期一 13:59:56 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.2.2</generator><item><title>研究重点：2023 年 6 月 19 日当周</title><link/>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-june-19-2023/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Fri, 23 Jun 2023 21:57:51 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=949350 </guid><description><![CDATA[<p>本期：我们新的负责任的人工智能成熟度模型； FoundWright 帮助“重新查找”网页；递归函数程序的跟踪引导归纳综合；用于弱引用计数的无等待算法；以及并发测试的新研究。</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-june-19-2023/">研究焦点：2023 年 6 月 19 日一周</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img decoding="async" loading="lazy" width="1400" height="264" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/RF18-blog-banner-1400x264-1.jpg" alt="微软研究焦点 18 | 2023 年 6 月 19 日当周" class="wp-image-950397" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/RF18-blog-banner-1400x264-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/RF18-blog-banner-1400x264-1-300x57.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/RF18-blog-banner-1400x264-1-1024x193.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/RF18-blog-banner-1400x264-1-768x145.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/RF18-blog-banner-1400x264-1-240x45.jpg 240w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><figure class="wp-block-pullquote"><blockquote><p><em class="">欢迎来到研究焦点，这是一系列博客文章，重点介绍 Microsoft 研究社区的著名出版物、活动、代码/数据集、新员工和其他里程碑。</em> </p></blockquote></figure><aside id=accordion-86daf9bc-c02a-47f0-9424-0504dd335f88 class="msr-table-of-contents-block accordion mb-5 pb-0" data-bi-aN="table-of-contents"> <button class="btn btn-collapse bg-gray-100 mb-0 display-flex justify-content-between" type="button" data-mount="collapse" data-target="#accordion-collapse-86daf9bc-c02a-47f0-9424-0504dd335f88" aria-expanded="true" aria-controls="accordion-collapse-86daf9bc-c02a-47f0-9424-0504dd335f88"><span class="msr-table-of-contents-block__label subtitle">在本文中</span><span class="msr-table-of-contents-block__current mr-4 text-gray-600 font-weight-normal" aria-hidden="true"></span></button> <div id="accordion-collapse-86daf9bc-c02a-47f0-9424-0504dd335f88" class="msr-table-of-contents-block__collapse-wrapper collapse show" data-parent="#accordion-86daf9bc-c02a-47f0-9424-0504dd335f88"><div class="accordion-body bg-gray-100 border-top pt-4"><ol class="msr-table-of-contents-block__list"><li class="msr-table-of-contents-block__list-item"><a href="#responsible-ai-maturity-model" class="msr-table-of-contents-block__list-item-link">负责任的人工智能成熟度模型</a></li><li class="msr-table-of-contents-block__list-item"><a href="#foundwright-helps-people-re-find-web-content-they-previously-discovered" class="msr-table-of-contents-block__list-item-link">FoundWright 帮助人们重新查找他们以前发现的网络内容</a></li><li class="msr-table-of-contents-block__list-item"><a href="#trace-guided-inductive-synthesis-of-recursive-functional-programs" class="msr-table-of-contents-block__list-item-link">递归函数程序的跟踪引导归纳综合</a></li><li class="msr-table-of-contents-block__list-item"><a href="#wait-free-weak-reference-counting" class="msr-table-of-contents-block__list-item-link">无等待弱引用计数</a></li><li class="msr-table-of-contents-block__list-item"><a href="#disaggregating-stateful-network-functions" class="msr-table-of-contents-block__list-item-link">分解有状态的网络功能</a></li><li class="msr-table-of-contents-block__list-item"><a href="#industrial-strength-controlled-concurrency-testing-for-c-programs-with-coyote" class="msr-table-of-contents-block__list-item-link">使用 Coyote 对 C# 程序进行工业强度的受控并发测试</a></li></ul></div></div><span class="msr-table-of-contents-block__progress-bar"></span></aside><h6 class="wp-block-heading has-blue-color has-text-color" id="new-resource">新资源</h6><h2 class="wp-block-heading" id="responsible-ai-maturity-model">负责任的人工智能成熟度模型</h2><p>随着人工智能的使用持续激增，预计政府将出台新的法规。但构建和使用人工智能技术的组织无需等待就可以负责任地制定最佳实践来开发和部署人工智能系统。许多公司已采用负责任的人工智能（RAI）原则作为自我监管的一种形式。然而，有效地将这些原则转化为实践具有挑战性。</p><p>为了帮助组织确定当前和期望的 RAI 成熟度水平，Microsoft 的研究人员开发了<a href="https://www.microsoft.com/en-us/research/publication/responsible-ai-maturity-model/">Responsible AI 成熟度模型</a>(RAI MM)。 RAI MM 是一个框架，包含 24 个根据经验得出的维度，这些维度对于组织的 RAI 成熟度至关重要，并且还包含成熟度进展的路线图，以便组织和团队可以确定他们所处的位置以及下一步的发展方向。</p><p> RAI MM 源自对 90 多名 RAI 专家和 AI 从业者的采访和焦点小组，即使 RAI 不断发展，也可以帮助组织和团队驾驭他们的 RAI 之旅。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/uploads/prod/2023/05/RAI_Maturity_Model_Aether_Microsoft_whitepaper.pdf">了解更多</a></div></div><div style="height:15px" aria-hidden="true" class="wp-block-spacer"></div><div class="border-bottom border-top border-gray-300 mt-5 mt-md-4 mb-4 mb-md-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="935415"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">聚焦：人工智能聚焦领域</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/focus-area/ai-and-microsoft-research/" aria-label="AI and Microsoft Research" data-bi-cN="AI and Microsoft Research" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2020/07/newsletter-option-8-neural-network-3-1.png" alt="深蓝色背景上的抽象神经网络模式" /></a></div><div class="msr-promo__content py-3 col-12 col-md"><h2 class="h4">人工智能和微软研究院</h2><p class="large">详细了解 Microsoft 人工智能研究的广度</p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/focus-area/ai-and-microsoft-research/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Learn more" data-bi-cN="AI and Microsoft Research" target="_blank">了解更多</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--></div><!--/.msr-promo--><h6 class="wp-block-heading has-blue-color has-text-color" id="new-research">新研究</h6><h2 class="wp-block-heading" id="foundwright-helps-people-re-find-web-content-they-previously-discovered"> FoundWright 帮助人们重新查找他们以前发现的网络内容</h2><p>重新查找信息是一项常见任务 - 大多数在线搜索请求都涉及重新查找信息。然而，当人们难以表达他们所寻求的东西时，这可能会很困难。人们可能会忘记他们想要重新查找的信息的确切细节，从而很难编写查询来找到它。人们还可能难以恢复网络存储库中的信息，例如书签或历史记录，因为这些信息无法捕获足够的信息，或者无法提供允许模糊查询的体验。因此，当人们面临重新寻找任务时，可能会感到不知所措和认知疲惫。</p><p> Microsoft 研究人员的一篇新论文： <a href="https://www.microsoft.com/en-us/research/publication/foundwright-a-system-to-help-people-re-find-pages-from-their-web-history/" target="_blank" rel="noreferrer noopener">FoundWright：帮助人们从网络历史记录中重新查找页面的系统</a>介绍了一种解决这些问题的新系统。 FoundWright 利用语言转换器模型的最新进展，通过定义可吸引具有语义相似内容的文档的概念来扩展人们表达其所寻求内容的能力。研究人员使用 FoundWright 作为设计探针来了解人们如何创建和使用概念；这种扩展的能力如何帮助重新发现；以及人们如何利用 FoundWright 的机器学习支持进行参与和协作。研究表明，这种表达重新寻找目标的扩展方式补充了传统的搜索和浏览。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/foundwright-a-system-to-help-people-re-find-pages-from-their-web-history/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/><h6 class="wp-block-heading has-blue-color has-text-color" id="new-research-1">新研究</h6><h2 class="wp-block-heading" id="trace-guided-inductive-synthesis-of-recursive-functional-programs">递归函数程序的跟踪引导归纳综合</h2><p>近年来，研究人员在递归函数程序的综合方面取得了重大进展，包括从输入输出示例归纳综合递归程序的进展。然而，后一个问题仍然带来一些挑战。</p><p>微软和普渡大学的研究人员在一篇新论文《 <a href="https://www.microsoft.com/en-us/research/publication/trace-guided-inductive-synthesis-of-recursive-functional-programs/" target="_blank" rel="noreferrer noopener">递归函数式程序的跟踪引导归纳</a><a href="https://pldi23.sigplan.org/" target="_blank" rel="noreferrer noopener">综合》中提出了一种新颖的跟踪引导方法，该论文获得了 ACM SIGPLAN 编程语言设计与实现会议</a>(PLDI 2023) 杰出论文奖解决从示例合成递归函数程序时的歧义和泛化的挑战。这种方法通过由程序的递归子调用序列组成的递归跟踪来扩大程序的搜索空间。它基于新版本的空间代数 (VSA)，用于简洁表示和有效操作彼此一致的递归轨迹对和程序。研究人员在名为 SyRup 的工具中实施了这种方法。根据先前工作的基准评估 SyRup 表明，与现有合成器相比，它不仅需要更少的示例来实现一定的成功率，而且对示例的质量也不太敏感。</p><p>这些结果表明，利用递归跟踪来区分具有相似大小的令人满意的程序适用于广泛的任务。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/trace-guided-inductive-synthesis-of-recursive-functional-programs/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/><h6 class="wp-block-heading has-blue-color has-text-color" id="new-research-2">新研究</h6><h2 class="wp-block-heading" id="wait-free-weak-reference-counting">无等待弱引用计数</h2><p>引用计数是内存管理的常用方法。引用计数的一大挑战是循环阻止对象被释放。 C++ 和 Rust 标准库等系统引入了两种类型的引用：强引用和弱引用。强引用允许访问对象并防止对象被释放，而弱引用仅阻止释放。如果存在对该对象的其他强引用，则可以将弱引用升级为提供强引用。因此，升级操作是部分的，并且可能会动态失败。此升级操作的经典实现不是免等待的，如果引用计数存在争用，则可能需要任意长的时间才能完成。</p><p>在一篇新论文《 <a href="https://www.microsoft.com/en-us/research/publication/wait-free-weak-reference-counting/" target="_blank" rel="noreferrer noopener">Wait-Free Weak Reference Counting》</a>中，微软的研究人员提出了一种弱引用计数的无等待算法，该算法需要“比较和交换”、“获取和添加”等原始的无等待原子操作。该论文包括使用 Starling 验证工具的算法正确性证明、C++ 的完整实现，以及使用微基准测试的最佳和最差情况性能的演示。</p><p>新算法在最好的情况下比经典算法更快，但在最坏的情况下会产生开销。研究人员提出了一种更复杂的算法，有效地结合了经典算法和无等待算法，在最坏的情况下提供了更好的性能，同时保持了无等待算法的优点。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/wait-free-weak-reference-counting/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/><h6 class="wp-block-heading has-blue-color has-text-color" id="new-research-3">新研究</h6><h2 class="wp-block-heading" id="disaggregating-stateful-network-functions">分解有状态的网络功能</h2><p>出于安全、隔离、计量和其他目的，当今的公共云在每台服务器上实现了复杂的网络功能。当今的实施方式，无论是在软件中还是在连接到每个主机的 FPGA 和 ASIC 中，都变得越来越复杂和昂贵，从而造成了可扩展性的瓶颈。</p><p>在一篇新论文《 <a href="https://www.microsoft.com/en-us/research/publication/disaggregating-stateful-network-functions/">分解状态网络功能》</a>中，微软的研究人员提出了一种不同的设计，通过新颖地使用将通用 ARM 内核与高速状态匹配紧密集成的设备，将网络功能处理从主机分解并分解到共享资源池中处理 ASIC。当工作分布在虚拟机之间时，这种分解可以以更低的每台服务器成本提供比现有技术更好的可靠性和性能。该论文发表在<a href="https://www.usenix.org/conference/nsdi23" target="_blank" rel="noreferrer noopener">2023 年 USENIX 网络系统设计和实现 (NSDI) 研讨会</a>上，其中包括针对随之而来的挑战的解决方案，并介绍了大型公共云上的生产部署结果。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/disaggregating-stateful-network-functions/">阅读论文</a></div><div class="wp-block-button is-style-fill-download"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/uploads/prod/2023/04/sirius_nsdi_2023_v3_split.pptx">推介会</a></div><div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://learn.microsoft.com/en-us/azure/networking/nva-accelerated-connections" target="_blank" rel="noreferrer noopener">Azure 公共预览版</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/><h6 class="wp-block-heading has-blue-color has-text-color" id="new-research-4">新研究</h6><h2 class="wp-block-heading" id="industrial-strength-controlled-concurrency-testing-for-c-programs-with-coyote">使用 Coyote 对 C# 程序进行工业强度的受控并发测试</h2><p>并发测试程序具有挑战性，因为它们的执行是不确定的，使得错误很难发现、重现和调试。非确定性可能会导致<em>不稳定的</em>测试（在不更改任何代码的情况下可能会通过或失败），从而给开发团队带来巨大的工程负担。由于并发对于构建现代多线程或分布式系统至关重要，因此需要解决方案来帮助开发人员测试其并发代码的正确性。</p><p>测试并发程序有两个主要挑战。首先是再现性或控制问题，而第二个挑战是状态空间爆炸问题。并发程序即使具有固定的测试输入，也可能具有大量可能的行为。</p><p>在一篇新的研究论文： <a href="https://www.microsoft.com/en-us/research/publication/industrial-strength-controlled-concurrency-testing-for-c-programs-with-coyote/">使用 Coyote 对 C# 程序进行工业强度受控并发测试中，</a>微软的研究人员描述了用于测试用 C# 语言编写的并发程序的开源工具 Coyote 的设计和实现。该研究荣获欧洲软件科学与技术协会 (EASST) 颁发的<a href="https://pdeligia.github.io/lib/awards/easst_etaps_award_2023.pdf" target="_blank" rel="noreferrer noopener">2023 年最佳软件科学论文奖</a>。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/industrial-strength-controlled-concurrency-testing-for-c-programs-with-coyote/">阅读论文</a></div></div><p><a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-june-19-2023/">研究焦点：2023 年 6 月 19 日一周</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title> DeepSpeed ZeRO++：LLM 和聊天模型训练的速度飞跃，通信量减少了 4 倍</title><link/>https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Thu, 22 Jun 2023 16:18:42 +0000</pubDate> <category><![CDATA[Research Blog]]></category><guid ispermalink="false"></guid><description><![CDATA[<p>大型人工智能模型正在改变数字世界。 Turing-NLG、ChatGPT 和 GPT-4 等生成语言模型由大型语言模型 (LLM) 提供支持，用途极其广泛，能够执行摘要、编码和翻译等任务。同样，像 DALL·E、Microsoft Designer 和 Bing Image Creator 这样的大型多模式生成模型可以生成艺术、建筑、视频和其他数字 [...]</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/">DeepSpeed ZeRO++ 帖子：LLM 和聊天模型训练的速度飞跃，通信量减少了 4 倍，</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-1024x576.png" alt="DeepSpeed ZeRO++ 项目亮点图" class="wp-image-951687" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 1：ZeRO++ 项目亮点图片。左上子图显示，与 ZeRO 第 3 阶段相比，ZeRO++ 减少了 4 倍的通信量。右上子图显示了 ZeRO++ 在 RLHF 模型训练上的性能，其中 ZeRO++ 在 RLHF 训练方面实现了 1.3 倍的加速，在令牌生成方面实现了 2.x 的加速。</figcaption></figure><p>大型人工智能模型正在改变数字世界。 Turing-NLG、ChatGPT 和 GPT-4 等生成语言模型由大型语言模型 (LLM) 提供支持，用途极其广泛，能够执行摘要、编码和翻译等任务。同样，DALL·E、Microsoft Designer 和 Bing Image Creator 等大型多模式生成模型可以生成艺术、建筑、视频和其他数字资产，使内容创作者、建筑师和工程师能够探索创意生产力的新领域。</p><p>然而，训练这些大型模型需要数百甚至数千个 GPU 设备上的大量内存和计算资源。例如，训练<a href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/">Megatron-Turing NLG 530B</a>模型使用了 4,000 多个 NVidia A100 GPU。有效地利用这些资源需要一个复杂的优化系统，将模型划分为适合各个设备内存的部分，并有效地并行化这些设备之间的计算。同时，为了让深度学习社区能够轻松地进行大型模型训练，这些优化必须易于使用。 </p><div class="annotations " data-bi-aN="margin-callout"><ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right"><li class="annotations__list-item"> <span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">教程</span><a href="https://www.deepspeed.ai/tutorials/zeropp/" target="_blank" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="DeepSpeed ZeRO tutorials" data-bi-aN="margin-callout" data-bi-cN="DeepSpeed ZeRO tutorials">DeepSpeed ZeRO 教程<span class="glyph-append glyph-append-share glyph-append-xsmall"></span></a></li></ul></div><p>DeepSpeed 的 ZeRO<a href="https://www.deepspeed.ai/tutorials/zero/" target="_blank" rel="noreferrer noopener">系列优化</a>为这些挑战提供了强大的解决方案，并已广泛用于训练大型且强大的深度学习模型 TNLG-17B、Bloom-176B、MPT-7B、Jurrasic-1 等。尽管其具有变革性的能力在某些关键场景中，ZeRO 会在 GPU 之间产生较高的数据传输开销，从而难以实现高训练效率。这种情况尤其发生在以下情况：a) 在相对于全局批量大小的大量 GPU 上进行训练，这会导致每个 GPU 的批量大小较小，需要频繁的通信；或者 b) 在低端集群上进行训练，其中跨节点网络带宽受到限制，导致通信延迟较高。在这些情况下，ZeRO 提供便捷且高效的培训的能力是有限的。</p><p>为了解决这些限制，我们发布了<a href="https://www.microsoft.com/en-us/research/publication/zero-extremely-efficient-collective-communication-for-giant-model-training/" target="_blank" rel="noreferrer noopener">ZeRO++</a> ，这是一个构建在 ZeRO 之上的通信优化策略系统，可为大型模型训练提供无与伦比的效率，而不受批量大小限制或跨设备带宽限制的影响。与 ZeRO 相比，ZeRO++ 利用量化与数据和通信重新映射相结合，<strong><em>将总通信量减少了 4 倍</em></strong>，且不影响模型质量。这有两个关键含义：</p><ul><li> <em>ZeRO++加速大型模型预训练和微调</em><ul><li><strong>每个 GPU 的小批量：</strong>无论是在数千个 GPU 上预训练大型模型，还是在数百甚至数十个 GPU 上对其进行微调，当<em>每个 GPU 的批量</em>较小时，ZeRO++ 提供的吞吐量比 ZeRO 高出 2.2 倍，直接减少培训时间和成本。</li><li><strong>低带宽集群：</strong> ZeRO++ 使<em>低带宽</em>集群能够实现与带宽高 4 倍的集群相似的吞吐量。因此，ZeRO++ 可以跨更广泛的集群进行高效的大型模型训练。</li></ul></li><li> <em>ZeRO++ 使用 RLHF 加速类似 ChatGPT 的模型训练</em><br><br>虽然 ZeRO++ 主要是为训练而设计的，但其优化也自动适用于<a href="https://www.deepspeed.ai/2022/09/09/zero-inference.html#:~:text=ZeRO-Inference%20adapts%20and%20optimizes%20ZeRO-Infinity%20techniques%20for%20model,memory%2C%20thus%20hosting%20no%20%28zero%29%20weights%20in%20GPU." target="_blank" rel="noreferrer noopener">ZeRO-Inference</a> ，因为通信开销对于使用 ZeRO 进行训练和推理很常见。因此，ZeRO++ 提高了工作负载的效率，例如用于训练对话模型的人类反馈强化学习 (RLHF)，它结合了训练和推理。<br><br>通过与<a href="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat" target="_blank" rel="noreferrer noopener">DeepSpeed-Chat</a>集成，ZeRO++ 与原始 ZeRO 相比，可以将 RLHF 训练的生成阶段提高高达 2 倍，将强化学习训练阶段提高高达 1.3 倍。</li></ul><p>接下来，我们将更深入地研究 ZeRO 及其通信开销，并讨论 ZeRO++ 中用于解决这些问题的关键优化。然后我们将演示 ZeRO++ 对不同模型大小、批量大小和带宽限制的训练吞吐量的影响。我们还将讨论 ZeRO++ 如何应用于 DeepSpeed-Chat，以加速使用 RLHF 的对话模型的训练。</p><h2 class="wp-block-heading" id="deep-dive-into-zero">深入了解 ZeRO++ </h2><figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper"><iframe loading="lazy" title="DeepSpeed ZeRO 优化器工作流程" width="500" height="281" src="https://www.youtube-nocookie.com/embed/lQCG4zUCYao?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></div><figcaption class="wp-element-caption">图 2：ZeRO 优化器工作流程</figcaption></figure><p>ZeRO 是数据并行性的内存高效变体，其中模型状态在所有 GPU 上进行分区，而不是在训练期间使用基于收集/广播的通信集体进行复制和重建。这使得 ZeRO 能够有效地利用所有设备上的聚合 GPU 内存和计算，同时提供简单易用的数据并行训练。</p><p>假设模型大小为 M。在前向传递过程中，ZeRO 进行全收集/广播操作，以在需要之前收集每个模型层的参数（总共大小为 M）。在后向传递中，ZeRO 对每一层的参数采用类似的通信模式来计算其局部梯度（总共大小为 M）。此外，ZeRO 在使用reduce 或reduce-scatter 通信集合（总共大小为M）计算每个局部梯度后立即对每个局部梯度进行平均和分区。总的来说，ZeRO 的通信量为 3M，均匀分布在两次全收集/广播和一次减少-分散/减少操作中。</p><p>为了减少这些通信开销，ZeRO++ 有三组通信优化，分别针对上述三个通信集合： </p><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-1024x576.png" alt="DeepSpeed ZeRO++ 量化图形" class="wp-image-950718" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 3：qwZ 中基于块的量化。从图中可以看出，与基本量化相比，块量化具有更好的数据精度。</figcaption></figure><h3 class="wp-block-heading" id="quantized-weight-communication-for-zero-qwz">ZeRO 的量化重量通信 (qwZ)</h3><p>首先，为了减少全收集过程中的参数通信量，我们采用权重量化的方式，在通信前将每个模型参数从 FP16（两个字节）动态缩小为 INT8（一字节）数据类型，并在通信后对权重进行反量化。然而，天真地对权重进行量化可能会降低模型训练的准确性。为了保持良好的模型训练精度，我们采用基于块的量化，对模型参数的每个子集进行独立量化。目前还没有高性能、基于块的量化的实现。因此，我们从头开始实现高度优化的量化 CUDA 内核，与基本量化相比，其准确度提高了 3 倍，速度提高了 5 倍。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="720" height="360" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure4.png" alt="DeepSpeed ZeRO++ 权重分区图" class="wp-image-950721" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure4.png 720w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure4-300x150.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure4-240x120.png 240w" sizes="(max-width: 720px) 100vw, 720px" /><figcaption class="wp-element-caption">图 4：hpZ 中的层次权重划分。该图显示 hpZ 在每个 GPU 上保存辅助模型分区，而 0-3 仅保存主模型分区。</figcaption></figure><h3 class="wp-block-heading" id="hierarchical-weight-partition-for-zero-hpz"> ZeRO (hpZ) 的分层权重划分</h3><p>其次，为了减少向后传递过程中权重全收集的通信开销，我们用 GPU 内存来交换通信。更具体地说，我们没有像 ZeRO 那样将整个模型权重分散到所有机器上，而是在每台机器内维护完整的模型副本。以更高的内存开销为代价，这使我们能够用机器内全收集/广播取代昂贵的跨机器全收集/权重广播，由于更高的机器内通信带宽，速度要快得多。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1920" height="1080" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure5.gif" alt="DeepSpeed ZeRO++ 动画图形" class="wp-image-950724"/><figcaption class="wp-element-caption">图 5：qgZ 的端到端工作流程。该动画图展示了qgZ组件的整个工作流程，包括张量切片重排序、节点内量化、节点内All-to-All通信、节点内反量化、节点内缩减、节点间量化、节点间All -to-all通信、节点间反量化、节点间缩减。 </figcaption></figure><h3 class="wp-block-heading" id="quantized-gradient-communication-for-zero-qgz"> ZeRO (qgZ) 的量化梯度通信</h3><p>第三，使用减少分散来降低梯度的通信成本更具挑战性。直接应用量化来减少通信量是不可行的。即使我们将基于块的量化纳入低精度，梯度减少也会累积并放大量化误差。为了解决这个问题，我们只在通信之前量化梯度，但在任何归约操作之前将它们反量化到完全精度。为了有效地做到这一点，我们发明了一种基于全对全的新型量化梯度通信范式，称为 qgZ，它在功能上相当于压缩的减少分散集体操作。</p><p> qgZ 旨在解决两个挑战：i) 如果我们简单地在 INT4/INT8 中实现减少分散，克服低精度归约会导致的显着精度损失，以及 ii) 避免由于以下原因而导致的精度下降和显着延迟开销即使我们以全精度进行减少，传统方法也需要一长串量化和反量化步骤来减少基于环或树的散射。 qgZ 没有使用基于环或树的缩减分散算法，而是基于一种新颖的分层全对全方法。</p><p> qgZ 中有三个主要步骤：i）梯度切片重新排序，ii）节点内通信和缩减，以及 iii）节点间通信和缩减。首先，在发生任何通信之前，我们对梯度进行切片并进行张量切片重新排序，以确保通信结束时每个 GPU 上的最终梯度放置（即图 5 中的绿色块）是正确的。其次，我们对重新排序的梯度切片进行量化，在每个节点内进行全对全通信，对从全对全接收到的梯度切片进行反量化，并进行局部缩减。第三，我们再次量化局部约简梯度，进行节点间全对所有通信，再次对接收到的梯度进行反量化，并计算最终的高精度梯度约简，得到如图 5 中绿色块所示的结果。</p><p>采用这种分层方法的原因是为了减少跨节点通信量。更准确地说，给定每个节点 N 个 GPU、M 的模型大小和 Z 的量化比，单跳 all-to-all 将生成 M*N/Z 跨节点流量。相比之下，通过这种分层方法，我们将每个 GPU 的跨节点流量从 M/Z 减少到 M/(Z*N)。这样，总通信量从M*N/Z减少到M*N/(Z*N)=M/Z。我们通过重叠节点内和节点间通信以及融合 CUDA 内核（张量切片重新排序 + 节点内量化）和（节点内反量化 + 节点内缩减 +节点间量化）。</p><figure class="wp-block-table"><table><tbody><tr><td><strong>通讯量</strong></td><td><strong>前向全聚集举重</strong></td><td><strong>向后全聚集重量</strong></td><td><strong>梯度上的后向减少散射</strong></td><td><strong>全部的</strong></td></tr><tr><td>零</td><td>中号</td><td>中号</td><td>中号</td><td>3M</td></tr><tr><td>零度++</td><td> 0.5M</td><td> 0</td><td> 0.25M</td><td> 0.75M</td></tr></tbody></table></figure><h3 class="wp-block-heading" id="communication-volume-reduction">通讯量减少</h3><p>通过合并上述所有三个组件，我们将跨节点通信量从 3M 减少到 0.75M。更具体地说，我们使用 qwZ 将模型权重的前向全收集/广播从 M 减少到 0.5M。我们使用 hpZ 消除了反向传播期间的跨节点全收集，将通信从 M 减少到 0。最后，我们使用 qgZ 将反向传播期间的跨节点归约分散通信从 M 减少到 0.25M。</p><h2 class="wp-block-heading" id="zero-accelerates-llm-training"> ZeRO++加速LLM培训</h2><p>在这里，我们展示了 ZeRO++ 在 384 个 Nvidia V100 GPU 上的真实 LLM 训练场景的评估结果。 </p><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="459" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-1024x459.png" alt="DeepSpeed ZeRO++ 条形图" class="wp-image-950727" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-1024x459.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-300x134.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-768x344.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-240x108.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6.png 1165w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 6：zero++ 与 400 Gbps 互连的零的吞吐量比较。如图所示，zero++ 在每个 GPU 1k 令牌的情况下可以实现高达 1.56 倍的加速，而在每个 GPU 2k 令牌的情况下，zero++ 可以实现 1.41 倍的加速。</figcaption></figure><h3 class="wp-block-heading" id="high-efficiency-with-small-batch-per-gpu">每个 GPU 的小批量高效率</h3><p><strong>高带宽集群：</strong><em> </em>如图 6 所示，我们首先展示了使用 4x Infiniband (IB) 进行 400Gbps 跨节点互连（每个都以 100Gbps 运行）的不同模型大小和微批量大小的 ZeRO++ 吞吐量相对于 ZeRO 的改进。每个 GPU 具有 1k 令牌，ZeRO++ 的吞吐量比 ZeRO-3 提高了 28% 到 36%。对于 2k 微批量大小，ZeRO++ 的吞吐量比 ZeRO-3 提高了 24% 至 29%。 </p><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="433" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-1024x433.png" alt="DeepSpeed ZeRO++ 条形图" class="wp-image-950730" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-1024x433.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-300x127.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-768x325.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-665x280.png 665w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-240x102.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 7：ZeRO++ 与具有 100Gbps 互连的 ZeRO 的吞吐量比较。图中显示，与 ZeRO 相比，ZeRO++ 在每个 GPU 1k 令牌的情况下实现了 2.21 倍的加速，而在每个 GPU 2k 令牌的情况下实现了 1.77 倍的加速。</figcaption></figure><p><strong>低带宽集群：</strong><em> </em>在 100Gbps 网络等低网络环境中，ZeRO++ 的性能明显优于 ZeRO-3。如图 7 所示，与 ZeRO-3 相比，ZeRO++ 在端到端吞吐量方面实现了高达 2.2 倍的加速。平均而言，ZeRO++ 比 ZeRO-3 基准实现约 2 倍的加速。 </p><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="396" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-1024x396.png" alt="DeepSpeed ZeRO++ 条形图" class="wp-image-950733" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-1024x396.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-300x116.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-768x297.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-240x93.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8.png 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 8：采用低带宽互连的 ZeRO++ 实现的吞吐量与采用高带宽互连的 ZeRO 类似。该图显示，在 18B 和 138B 模型尺寸中，具有低带宽网络的 ZeRO++ 与具有高带宽互连的 ZeRO 实现了相似的吞吐量。 </figcaption></figure><h3 class="wp-block-heading" id="enabling-efficiency-equivalence-between-high-and-low-bandwidth-clusters">实现高带宽集群和低带宽集群之间的效率等效</h3><p>此外，与在更高带宽设置下的 ZeRO 相比，ZeRO ++ 在低带宽集群中可以实现相当的系统吞吐量。如图 8 所示，对于 18B 和 138B 型号，采用 200Gbps 跨节点链路的 ZeRO++ 与采用 800 Gbps 跨节点链路设置的 ZeRO-3 可以达到相似的 TFLOP。</p><p>鉴于 ZeRO++ 出色的可扩展性，我们将 ZeRO++ 视为下一代用于训练大型 AI 模型的 ZeRO。</p><h2 class="wp-block-heading" id="zero-for-rlhf-training-with-deepspeed-chat"> ZeRO++ 通过 DeepSpeed-Chat 进行 RLHF 训练</h2><h3 class="wp-block-heading" id="rlhf-training-background">RLHF培训背景</h3><p>类似 ChatGPT 的模型由 LLM 提供支持，并<a href="https://openai.com/blog/chatgpt" target="_blank" rel="noreferrer noopener">使用 RLHF 进行微调</a>。 RLHF 由生成（推理）阶段和训练阶段组成。在生成阶段，参与者模型将部分对话作为输入，并使用一系列前向传递生成响应。然后在训练阶段，批评者模型按质量对生成的响应进行排名，为演员模型提供强化信号。使用这些排名对参与者模型进行微调，使其能够在后续迭代中生成更准确、更合适的响应。</p><p> RLHF 训练会带来不小的内存压力，因为它使用四种模型（参与者、参考、评论家、奖励）。采用低秩自适应（LoRA）来解决 RLHF 的内存压力。 LoRA 冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到 Transformer 架构的每一层中，从而显着减少了可训练参数的数量。 LoRA 通过减少内存使用来加速 RLHF，允许更大的批量大小，从而大大提高吞吐量。</p><h3 class="wp-block-heading" id="deepspeed-chat-with-zero-for-rlhf-training"> DeepSpeed-Chat 与 ZeRO++ 进行 RLHF 训练</h3><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="435" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-1024x435.png" alt="DeepSpeed ZeRO++ 条形图" class="wp-image-950709" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-1024x435.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-300x128.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-768x326.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-240x102.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 9：RLHF 训练中的 ZeRO++ 加速。左图显示 ZeRO++ 在 RLHF 步骤 1 训练中实现了 1.26 倍的加速。右图显示 ZeRO++ 在 RLHF step3 代币生成中实现了高达 2.25 倍的加速。</figcaption></figure><p>带有 LoRA 的 RLHF 是 ZeRO++ 的独特应用，因为大多数模型权重都是冻结的。这意味着 ZeRO++ 可以将这些冻结权重保存在 INT4/8 中进行量化，而不是将它们存储在 FP16 中并在每次通信操作之前对其进行量化。通信后仍然进行反量化，以使权重为计算做好准备，但计算后简单地丢弃反量化的权重。</p><p>以这种方式使用 ZeRO++ 进行 RLHF 训练可以减少内存使用量和通信量。这通过减少通信以及由于减少内存使用而实现更大的批量大小来提高训练吞吐量。在生成阶段，ZeRO++使用hpZ将所有权重通信保留在每个节点内，以利用更高的节点内通信带宽并减少通信量，进一步提高生成吞吐量。</p><p> ZeRO++ 集成到 DeepSpeed-Chat 中，为类似 ChatGPT 模型的 RLHF 训练提供支持。在图 9 中，我们比较了不同大小的 Actor 模型的 RLHF 生成吞吐量，将 ZeRO 与 ZeRO++ 进行了比较，针对 32 V100 GPU 上的 30B 和 66B Actor 模型。结果表明，ZeRO++ 的 RLHF 生成吞吐量比 ZeRO 高出 2.25 倍。我们还展示了 16 个 V100 GPU 上训练阶段的加速情况，其中 ZeRO++ 的吞吐量比 ZeRO 高出 1.26 倍，这是由于 ZeRO++ 实现了更少的通信和更大的批量大小。</p><h2 class="wp-block-heading" id="release-try-deepspeed-zero-today">发布：立即尝试 DeepSpeed ZeRO++</h2><p>我们非常高兴能够发布 DeepSpeed ZeRO++ 并将其提供给 AI 社区中的任何人。首先，请访问我们的 GitHub 页面进行<a href="https://github.com/microsoft/DeepSpeed/blob/master/docs/_tutorials/zeropp.md" target="_blank" rel="noreferrer noopener">LLM 培训</a>。 DeepSpeed-Chat 的 ZeRO++ 将在未来几周内发布。</p><div class="wp-block-buttons"><div class="wp-block-button is-style-fill-github"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.deepspeed.ai/tutorials/zeropp/" target="_blank" rel="noreferrer noopener">获得法学硕士培训</a></div><div class="wp-block-button is-style-fill"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/zero-extremely-efficient-collective-communication-for-giant-model-training/">阅读技术论文</a></div></div><p>DeepSpeed-ZeRO++ 是 DeepSpeed 生态系统的一部分。要了解更多信息，请访问<a href="https://www.deepspeed.ai/" target="_blank" rel="noreferrer noopener">我们的网站</a>，您可以在其中找到详细的博客文章、教程和有用的文档。</p><p>有关 DeepSpeed 的最新新闻，请在社交媒体上关注我们：</p><div class="wp-block-buttons"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://twitter.com/MSFTDeepSpeed" target="_blank" rel="noreferrer noopener">在推特上（英文）</a></div><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://twitter.com/MSFTDeepSpeedJP" target="_blank" rel="noreferrer noopener">在推特上（日语）</a></div><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.zhihu.com/people/deepspeed" target="_blank" rel="noreferrer noopener">在知乎上（中文）</a></div></div><p> DeepSpeed 欢迎您做出贡献。我们鼓励您在<a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noreferrer noopener">DeepSpeed GitHub 页面上报告问题、贡献 PR 并加入讨论。</a>请参阅我们的<a href="https://github.com/microsoft/DeepSpeed/blob/master/CONTRIBUTING.md" target="_blank" rel="noreferrer noopener">贡献指南</a>了解更多详细信息。我们对与大学、研究实验室和公司的合作持开放态度。对于此类请求（以及其他不适合 GitHub 的请求），请直接发送电子邮件至<a href="mailto:deepspeed-info@microsoft.com" target="_blank" rel="noreferrer noopener">deepspeed-info@microsoft.com</a> 。</p><h2 class="wp-block-heading" id="contributors">贡献者</h2><p>该项目的实现得益于 DeepSpeed 团队以下人员的贡献：</p><p><a href="https://www.microsoft.com/en-us/research/people/guanhuawang/">王冠华</a>、秦何阳、Sam Ade Jacobs、Connor Holmes、 <a href="https://www.microsoft.com/en-us/research/people/samyamr/">Samyam Rajbhandari</a> 、 <a href="https://www.microsoft.com/en-us/research/people/olruwase/">Olatunji Ruwase</a> 、 <a href="https://www.microsoft.com/en-us/research/people/amawa/">Ammar Ahmad Awan</a> 、 <a href="https://www.microsoft.com/en-us/research/people/jerasley/">Jeff Rasley</a> 、Michael Wyatt、<a href="https://www.microsoft.com/en-us/research/people/yuxhe/">何宇雄</a>（<em>团队负责人</em>）</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/">DeepSpeed ZeRO++ 帖子：LLM 和聊天模型训练的速度飞跃，通信量减少了 4 倍，</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>