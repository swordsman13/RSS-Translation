<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2024 年 3 月 6 日星期三 16:41:49 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.4.3</generator><item><title>研究重点：2024 年 3 月 4 日当周</title><link/>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-4-2024/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Wed, 06 Mar 2024 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1010790 </guid><description><![CDATA[<p>本期：生成万花筒网络；具有强化条件作用的文本扩散；奖品——学习时间动作抽象作为序列压缩问题。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-4-2024/">《研究焦点：2024 年 3 月 4 日一周》一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-pullquote"><blockquote><p><em class="">欢迎来到研究焦点，这是一系列博客文章，重点介绍 Microsoft 研究社区的著名出版物、活动、代码/数据集、新员工和其他里程碑。</em> </p></blockquote></figure><figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1.png" alt="2024 年 3 月 4 日研究焦点周" class="wp-image-1011438" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/RF36-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-a584a2137da4151ecbde93fba771f798" id="new-research">新研究</h3><h2 class="wp-block-heading" id="generative-kaleidoscopic-networks">生成万花筒网络</h2><p>神经网络是深度学习模型，可以经过训练来学习数据中的复杂模式和关系。在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/generative-kaleidoscopic-networks/">生成万花筒网络</a>中，微软的研究人员详细介绍了他们如何发现“过度泛化”现象，这表明神经网络倾向于学习多对一映射。然后，他们利用这种现象通过创建数据集万花筒（称为“生成万花筒网络”）来引入生成建模的新范例。研究人员正在探索理论解释、多模态数据实验以及使用生成万花筒网络的条件生成。 </p><figure class="wp-block-image aligncenter"><img decoding="async" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/mnist_kaleidoscope-65d8df708fecd.gif" alt="MNIST 万花筒：使用多层感知器模型对 MNIST 数据图像进行流形学习。我们从均匀分布采样的输入噪声向量开始，并运行万花筒采样算法。图像之间的转换呈现出万花筒般的效果，直到样本最终找到稳定的最小值并收敛于一个数字。"/><figcaption class="wp-element-caption"> <strong>MNIST 万花筒</strong>：使用多层感知器模型对 MNIST 数据图像进行流形学习。我们从均匀分布采样的输入噪声向量开始，并运行万花筒采样算法。图像之间的转换呈现出万花筒般的效果，直到样本最终找到稳定的最小值并收敛于一个数字。 </figcaption></figure><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-layout-1 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/generative-kaleidoscopic-networks/">阅读论文</a></div><div class="wp-block-button is-style-fill-github"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/Harshs27/generative-kaleidoscopic-networks" target="_blank" rel="noreferrer noopener">获取代码</a></div></div><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="956142"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究院播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/collaborators-holoportation-communication-technology-with-spencer-fowers-and-kwame-darko/" aria-label="Collaborators: Holoportation&#x2122; communication technology with Spencer Fowers and Kwame Darko" data-bi-cN="Collaborators: Holoportation&#x2122; communication technology with Spencer Fowers and Kwame Darko" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/collaborators_3DTelemed_1400x788.jpg" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">合作者：全息传输<img src="https://s.w.org/images/core/emoji/14.0.0/72x72/2122.png" alt="™" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Spencer Fowers 和 Kwame Darko 的通信技术</h2><p class="large">Spencer Fowers 和 Kwame Darko 详细介绍了 Holoportation 背后的技术以及围绕该技术构建的电信设备如何将患者和医生聚集在一起（当患者和医生待在同一个房间并不容易的情况下），并讨论了这项工作的潜在影响。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/collaborators-holoportation-communication-technology-with-spencer-fowers-and-kwame-darko/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Collaborators: Holoportation&#x2122; communication technology with Spencer Fowers and Kwame Darko" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-a0197f8a07f9231b8c13f42625bef624" id="new-research"><br>新研究</h3><h2 class="wp-block-heading" id="generative-kaleidoscopic-networks">强化条件作用下的文本扩散</h2><p>扩散模型是一种机器学习模型，显示出生成高质量图像、视频和音频的卓越能力。由于它们在迭代细化中的适应性，它们提供了实现更好的非自回归序列生成的潜力——同时预测序列的所有元素，而不是预测序列中的下一个元素。</p><p>然而，由于处理语言离散性方面的挑战，现有的文本传播模型尚未发挥这一潜力。在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/text-diffusion-with-reinforced-conditioning/">带有强化调节的文本扩散</a>中，来自微软的研究人员和外部同事发现了文本扩散模型的两个显着局限性：训练过程中自我调节的退化以及训练和采样之间的不一致。为此，研究人员提出了一种名为 TREC 的新颖模型，该模型为文本扩散模型提供了强化条件作用，通过奖励信号直接激励自我条件的质量改进，从而减轻退化。在 2024 年人工智能促进协会会议 (AAAI) 上发表的论文中，他们进一步提出了时间感知方差缩放来解决错位问题。</p><p>大量实验证明了 TREC 相对于自回归、非自回归和扩散基线的竞争力。此外，定性分析显示其在精炼样品中充分利用扩散过程的先进能力。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-layout-2 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/text-diffusion-with-reinforced-conditioning/">阅读论文</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity"/><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-a584a2137da4151ecbde93fba771f798" id="new-research">新研究</h3><h2 class="wp-block-heading" id="generative-kaleidoscopic-networks">PRISE：学习时间动作抽象作为序列压缩问题</h2><p>时间动作抽象以及信念状态表示是用于顺序决策的强大知识共享机制。在最近的一篇论文<a href="https://www.microsoft.com/en-us/research/publication/prise-learning-temporal-action-abstractions-as-a-sequence-compression-problem/">PRISE：将时间动作抽象学习为序列压缩问题</a>中，来自微软和马里兰大学的研究人员提出了训练大型语言模型 (LLM) 和为连续控制域引入时间动作抽象这两个看似遥远的领域之间的新颖联系。例如机器人技术。研究人员引入了一种称为原始序列编码 (PRISE) 的方法，该方法将连续动作量化与 LLM 训练流程中微妙但关键的组成部分（通过字节对编码 (BPE) 进行输入标记化）相结合，以学习强大的可变时间跨度动作抽象。他们凭经验表明，PRIZE 从一组多任务机器人操作演示中发现的高级技能显着提高了多任务模仿学习和对未见过的任务的小样本模仿学习的性能。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-layout-3 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/prise-learning-temporal-action-abstractions-as-a-sequence-compression-problem/">阅读论文</a></div></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-4-2024/">《研究焦点：2024 年 3 月 4 日一周》一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title> Orca-Math：通过模型专业化展示 SLM 的潜力</title><link/>https://www.microsoft.com/en-us/research/blog/orca-math-demonstrating-the-pottial-of-slms-with-model-specialization/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Tue, 05 Mar 2024 14:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1010406 </guid><description><![CDATA[<p> Microsoft 的 Orca-Math 是一种专门的小型语言模型，在解决需要多步骤推理的数学问题时，其性能优于较大的模型，并显示了使用反馈来改进语言模型的潜力。了解更多。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/orca-math-demonstrating-the-potential-of-slms-with-model-specialization/">Orca-Math 后的文章：通过模型专业化展示 SLM 的潜力</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1.png" alt="渐变背景上的抽象波浪线" class="wp-image-1010433" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/ORCA-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>我们在<a href="https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/" target="_blank" rel="noreferrer noopener">Orca</a>和<a href="https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/" target="_blank" rel="noreferrer noopener">Orca 2</a>上的工作证明了改进训练信号和方法的力量，可以增强较小语言模型的推理能力，使其更接近大型语言模型的水平。 Orca-Math 是朝这个方向迈出的又一步，我们探索小语言模型 (SLM) 在专门从事某个领域时的能力，在本例中解决小学数学问题，这长期以来被认为是 SLM 的一项复杂任务。</p><p> Orca-Math 是通过微调 Mistral 7B 模型创建的 70 亿个参数模型。 Orca-Math 在 GSM8k pass@1 上达到 86.81%，超过了更大模型的性能，包括通用模型（例如 LLAMA-2-70、Gemini Pro 和 GPT-3.5）和数学特定模型（例如 MetaMath-70B 和 WizardMa8th） 70B）。请注意，基本模型 (Mistral-7B) 在 GSM8K 上达到了 37.83%。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="2560" height="1277" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/03/orca-math-scaled.jpg" alt="Alt Text：比较不同型号 GSM8K 分数的条形图，质量呈上升趋势。这些型号包括 LLAMA-2-70、GPT-3.5、Gemini Pro、WizardMath-70B、MetaMath-70B 和 Orca-Math-7B。该图显示 Orca-Math-7B 模型在 GSM8K 上的性能优于其他更大的模型。" class="wp-image-1011258" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/03/orca-math-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/orca-math-300x150.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/orca-math-1024x511.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/orca-math-768x383.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/orca-math-1536x766.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/orca-math-2048x1022.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/03/orca-math-240x120.jpg 240w" sizes="(max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">不同型号 GSM8K 得分比较的条形图，质量呈上升趋势。这些型号包括 LLAMA-2-70、GPT-3.5、Gemini Pro、WizardMath-70B、MetaMath-70B 和 Orca-Math-7B。该图显示 Orca-Math-7B 模型在 GSM8K 上的性能优于其他更大的模型。</figcaption></figure><p> Orca-Math 模型的最先进 (SOTA) 性能可归因于两个关键见解：</p><ul><li>使用多智能体 ( <a href="https://www.microsoft.com/en-us/research/project/autogen/" target="_blank" rel="noreferrer noopener">AutoGen</a> ) 创建的包含 200,000 个数学问题的高质量合成数据进行训练。这比其他可能有数百万个问题的数学数据集要小。较小的模型和较小的数据集意味着更快、更便宜的训练。</li><li>除了传统的监督微调之外，该模型还使用迭代学习过程进行训练，模型可以练习解决问题，并根据教师的反馈继续改进。</li></ul><p>我们的研究结果表明，较小的模型在专门的环境中很有价值，它们可以与较大模型的性能相匹配，同时还强调持续学习和使用反馈来改进语言模型的潜力。我们正在公开<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k">数据集<span class="sr-only">（在新选项卡中打开）</span></a>以及描述训练过程的<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2402.14830">报告<span class="sr-only">（在新选项卡中打开）</span></a> ，以鼓励对较小语言模型的改进和专业化的研究。</p><h2 class="wp-block-heading" id="teaching-slms-math">教授 SLM 数学</h2><p>长期以来，解决数学应用问题一直被认为是 SLM 的一项复杂任务。在 GSM8K 基准（GSM8K 代表小学数学 8K，是一个包含 8,500 个需要多步推理的高质量小学数学应用题的数据集）上实现超过 80% 准确率的模型通常超过 300 亿个参数。</p><p>为了使用更小的模型达到更高的性能水平，研究人员经常训练 SLM 来生成代码，或者使用计算器来帮助避免计算错误。此外，他们还采用了一种称为集成的技术，其中模型被调用多达 100 次，每次调用都会重新尝试解决问题。集成大大提高了准确性，但由于对模型的多次调用，计算成本显着增加。</p><p>这项研究旨在探索当较小的语言模型专门用于解决数学问题时，在不使用外部工具、验证器或集成的情况下，我们可以将它们的本机能力提升到什么程度。更具体地说，我们关注两个方向：</p><h3 class="wp-block-heading" id="agentinstruct">代理指令</h3><p>先前关于合成数据创建的工作经常使用前沿模型来基于种子问题生成类似的问题。提供具有不同数量和属性的种子的释义对于为较小的模型创建训练数据非常有用。我们建议采用多智能体流，使用 AutoGen 来创建新的问题和解决方案，这不仅可以创建更多的问题演示，还可以增加问题的多样性和难度范围。</p><p>为了生成更具挑战性的问题，我们创建了一个设置，由一组代理协作创建一个面向预定义目标的数据集。例如，我们可以使用两个代理，即<em>Suggester</em>和<em>Editor</em> 。建议者检查问题并提出几种增加其复杂性的方法，而编辑者则采用原始单词问题和建议者的建议来生成更新的、更具挑战性的问题。这个迭代过程可以发生多轮，每轮都会进一步增加先前生成的问题的复杂性。然后，第三个代理可以<em>验证</em>问题是否可以解决并创建解决方案。</p><h3 class="wp-block-heading" id="iterative-learning">迭代学习</h3><p>事实证明，使用可以引发更丰富的学习信号（例如解释）的高质量训练数据可以显着提高 SLM 获取技能的能力，而这些技能以前只是在更大规模的情况下才出现。</p><p>这种范式适合师生方法，其中大模型（教师）是<br>为 SLM（学生）创建演示以供学习。在这项工作中，我们将师生范式扩展到迭代学习设置，如下所示：</p><ul><li><strong>演示教学</strong>：在这个阶段，我们通过使用 AgentInstruct 来演示问题及其解决方案来训练 SLM。</li></ul><ul><li><strong>练习和反馈：</strong>我们让 SLM 练习自行解决问题。对于每个问题，我们允许 SLM 创建多个解决方案。然后，我们使用教师模型提供有关 SLM 解决方案的反馈。如果SLM无法正确解决问题，即使经过多次尝试，我们也会使用老师提供的解决方案。</li></ul><ul><li><strong>迭代改进：</strong>我们使用教师反馈来创建偏好数据，显示 SLM 对同一问题的好坏解决方案，然后重新训练 SLM。</li></ul><p>练习、反馈和迭代改进步骤可以重复多次。</p><h2 class="wp-block-heading" id="conclusion">结论</h2><p>我们的研究结果表明，较小的模型在专门的环境中很有价值，它们可以与较大模型的性能相匹配，但范围有限。通过在包含 200,000 个数学问题的小数据集上训练 Orca-Math，我们已经达到了与更大模型相媲美或超越的性能水平。</p><p>数据集相对较小的规模也显示了使用多代理流来模拟数据和反馈生成过程的潜力。小数据集大小会对训练成本产生影响，并强调具有更丰富学习信号的训练数据可以提高学习过程的效率。我们的研究结果还强调了持续学习和改进语言模型的潜力，其中模型随着从人或另一个模型收到更多反馈而不断改进。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/orca-math-demonstrating-the-potential-of-slms-with-model-specialization/">Orca-Math 后的文章：通过模型专业化展示 SLM 的潜力</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>