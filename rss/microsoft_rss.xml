<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2024 年 9 月 30 日星期一 19:15:35 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.6.2</generator><item><title>使用 RadEdit 对生物医学视觉模型进行压力测试：用于稳健模型部署的合成数据方法</title><link/>https://www.microsoft.com/en-us/research/blog/stress-testing-biomedical-vision-models-with-radedit-a-synthetic-data-approach-for-robust-model-deployment/<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator><pubDate> Mon, 30 Sep 2024 19:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1088394 </guid><description><![CDATA[<p>该论文已被第 18 届欧洲计算机视觉会议 (ECCV 2024)（在新选项卡中打开）接收，该会议是计算机视觉和机器学习的首要聚会。生物医学视觉模型是分析医学图像（如 X 射线、MRI 和 CT 扫描）的计算工具，用于预测医疗状况和结果。这些模型[…]</p><p> <a href="https://www.microsoft.com/en-us/research/blog/stress-testing-biomedical-vision-models-with-radedit-a-synthetic-data-approach-for-robust-model-deployment/">使用 RadEdit 对生物医学视觉模型进行压力测试后：用于稳健模型部署的合成数据方法</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center"><strong><em>该论文已被<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://eccv2024.ecva.net/">第 18 届欧洲计算机视觉会议 (ECCV 2024) <span class="sr-only">（在新选项卡中打开）</span></a>接收，该会议是计算机视觉和机器学习的首要聚会。</em></strong> </p><figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1.jpg" alt="左边是肺部的简单图。该图显示了左右肺的边界以及气管和左右主干支气管。绘图下方的文字为：原始图像。绘图右侧是 RadEdit 的 3 个附加输入。它们是垂直排列的。顶部有一个示例编辑提示。上面写着"Consolidation". Below there is the same drawing of the lung again but this time the left lung is shaded blue. The text reads: Edit mask according to prompt. Lastly, on the bottom, there is the same drawing of the lung but this time the right lung is shaded red. The text reads: "Do not edit mask". On the right of the 3 additional inputs there is a box saying “RadEdit”. Finally, on the right of the figure, there is the drawing of the lung again. The upper part of the left lung is shaded grey. The text reads: Edited image. Between all the elements, the drawing of the lung, the 3 additional inputs, the box that says “RadEdit”, and the edited image, there are arrows pointing to the next element from left to right. " class="wp-image-1089042" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>生物医学视觉模型是分析医学图像（如 X 射线、MRI 和 CT 扫描）的计算工具，用于预测医疗状况和结果。这些模型帮助医生进行疾病诊断、治疗计划、疾病监测和风险评估。然而，用于训练这些模型的数据集可能很小，并且不能代表现实世界的条件，这通常会导致这些模型在实际医疗环境中表现较差。为了避免误诊和其他错误，这些模型必须经过严格的测试和调整，以便在不同的条件下可靠地运行。</p><p>为了缓解缺乏足够多样化数据的数据集挑战并改进生物医学视觉模型的测试，我们开发了“ <a href="https://www.microsoft.com/en-us/research/publication/radedit-stress-testing-biomedical-vision-models-via-diffusion-image-editing/">RadEdit：通过扩散图像编辑对生物医学视觉模型进行压力测试</a>”，并在 ECCV 2024 上展示。与<a href="https://www.microsoft.com/en-us/ai/principles-and-approach/?msockid=37f5423dfa656a721ad151fefb9e6b78">Microsoft Responsible AI 原则</a>保持一致出于可靠性和安全性的考虑，RadEdit 可以帮助研究人员在将模型部署到医疗环境之前确定模型何时以及如何可能会失败。 RadEdit 使用生成图像编辑来模拟不同的数据集变化（例如，患者人口统计数据的变化），帮助研究人员识别模型中的弱点。通过采用在各种胸部 X 射线数据集上训练的文本到图像扩散模型，RadEdit 可以生成合成且逼真的 X 射线。</p><p> RadEdit 的方法涉及使用多个图像蒙版（表示参考图像指定区域的二进制图像），如图 1 所示，以限制对图像特定区域的更改，从而保持其完整性。它生成没有虚假相关性和伪影的合成数据集，解决了现有编辑技术的缺点。传统的编辑技术通常会忽视生成模型中的偏差，从而导致合成数据使这些偏差永久化。或者，这些其他编辑技术将编辑限制在不切实际的输出点。 </p><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1044936"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究博客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/microsoft-research-forum-episode-3-globally-inclusive-and-equitable-ai-new-use-cases-for-ai-and-more/" aria-label="Microsoft Research Forum Episode 3: Globally inclusive and equitable AI, new use cases for AI, and more" data-bi-cN="Microsoft Research Forum Episode 3: Globally inclusive and equitable AI, new use cases for AI, and more" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/06/RF-Ep3-Recap-BlogHeroFeature-1400x788-1.jpg" alt="微软研究论坛|第 3 集 |小组讨论" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">微软研究院论坛第 3 集：全球包容且公平的人工智能、人工智能的新用例等</h2><p class="large">在最新一期的微软研究论坛中，研究人员探讨了全球包容性和公平人工智能的重要性，分享了 AutoGen 和 MatterGen 的最新动态，介绍了人工智能的新用例，包括工业应用和多模式模型改善辅助技术的潜力。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/blog/microsoft-research-forum-episode-3-globally-inclusive-and-equitable-ai-new-use-cases-for-ai-and-more/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="Microsoft Research Forum Episode 3: Globally inclusive and equitable AI, new use cases for AI, and more" target="_blank">阅读更多</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="how-radedit-works">RadEdit 的工作原理</h2><p>RadEdit 使用三个关键输入改进生物医学图像编辑，如图 1 所示：</p><ul class="wp-block-list"><li><strong>文本提示</strong>：定义所需的修改。例如，可以添加一种疾病并添加“合并”等描述</li><li><strong>编辑掩模</strong>：二值掩模，指示要修改的主要区域，例如“右肺”</li><li><strong>保留掩模</strong>：二值掩模概述了要保留的原始图像的部分，例如“左肺” </li></ul><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature.jpg" alt="左边是肺部的简单图。该图显示了左右肺的边界以及气管和左右主干支气管。绘图下方的文字为：原始图像。绘图右侧是 RadEdit 的 3 个附加输入。它们是垂直排列的。顶部有一个示例编辑提示。上面写着"Consolidation". Below there is the same drawing of the lung again but this time the left lung is shaded blue. The text reads: Edit mask according to prompt. Lastly, on the bottom, there is the same drawing of the lung but this time the right lung is shaded red. The text reads: "Do not edit mask". On the right of the 3 additional inputs there is a box saying “RadEdit”. Finally, on the right of the figure, there is the drawing of the lung again. The upper part of the left lung is shaded grey. The text reads: Edited image. Between all the elements, the drawing of the lung, the 3 additional inputs, the box that says “RadEdit”, and the edited image, there are arrows pointing to the next element from left to right. " class="wp-image-1088415" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">图 1：RadEdit 的输入和输出。通过使用单独的“编辑”和“保留”蒙版，RadEdit 可以通过精确的空间控制和真实的输出对图像进行所需的修改。</figcaption></figure><p> RadEdit 依赖于图像编辑的扩散模型，其中图像首先通过反转扩散生成过程转换为潜在噪声表示。然后在多个时间步长上迭代地对噪声表示进行去噪。在每个步骤中，RadEdit：</p><ol class="wp-block-list"><li>使用文本提示在无分类器的指导下有条件地在编辑蒙版内生成像素。</li><li>根据原始图像和编辑区域生成剩余像素。</li><li>复制“保留”蒙版内原始图像的内容，确保该区域保持不变。</li></ol><p>最后，质量检查可确保编辑后的图像忠实于编辑提示。 RadEdit 使用 Microsoft 的<a href="https://www.microsoft.com/en-us/research/publication/learning-to-exploit-temporal-structure-for-biomedical-vision-language-processing/?msockid=2d32e418b7106f871a12f664b6ba6e2e">BioViL-T</a>来计算图像文本对齐分数，然后我们可以使用该分数来过滤掉低质量和不忠实的编辑。</p><h2 class="wp-block-heading" id="simulating-dataset-shifts">模拟数据集变化</h2><p>RadEdit 的一个关键功能是能够通过精确的空间控制来模拟数据集变化，以进行全面的模型性能评估。这包括图像采集、潜在病理表现和群体特征的差异。</p><p>特别值得注意的是 RadEdit 能够模拟来自不同来源（例如不同医院）的图像变化，帮助研究人员识别仅根据一个来源的数据训练的模型中的潜在偏差。例如，在一项 COVID-19 研究中，如果数据集中的所有阳性病例都来自一家医院，而所有阴性病例都来自另一家医院，则用于检测 COVID-19 的模型可能会过度依赖来自医院的特定指标X 射线图像。其中，我们考虑了 X 射线角落中的偏侧性标记（例如，X 射线左侧高度可见的字母“L”）以及图像边缘上的黑色空间量医院特定指标。为了测试模型是否过于依赖图像采集的差异，我们使用 RadEdit 创建了合成数据，其中我们删除了 COVID-19 特征，同时保留了医院特定的指标。创建不再存在 COVID-19 特征的合成数据集后，我们可以测试 COVID-10 检测模型是否仍然可以预测 COVID-19。这表明该模型在医院特定指标方面存在偏差。</p><p> RadEdit 还可以从图像中删除特定疾病，例如气胸（肺部塌陷），同时保留胸腔引流等治疗功能。这有助于研究人员了解模型如何检测和理解“视觉捷径”。由于 RadEdit 保留了主要解剖结构（如肺、肋骨和心脏）的大小和位置，因此它还可用于对分割模型进行压力测试。例如，RadEdit 可以向肺部图像添加罕见的异常或医疗设备，以测试分割模型处理新变化的效果，确保它们在不同人群中准确推广。图 2 说明了压力测试场景的这三个示例。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1195" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24.jpg" alt="所有肺部图均与图1相同。该图显示了左、右肺的边界以及气管和左、右主干支气管。左边第一行有两张肺的图。第一张肺部图标记为"COVID-19" has an arrow pointing upwards in the upper right corner of the drawing, outside of the lung. The arrow is meant to indicate a marking on an X-ray. In addition, the lungs are shaded light grey. The light grey shading is meant to represent the effects of covid on the lung. The second drawing of a lung on the right labelled "No COVID-19" has no arrow marking and the lungs are shaded in a dark grey. On top of these two drawings the text says, "Biased training datasets". Right next to the two drawings of lungs is a big arrow pointing to the right. The arrow is labelled "acquisition shift". On the right end of the arrow is another drawing of a lung. The lungs in the drawing are shaded dark grey, in addition the drawing features an arrow marking in the upper right corner. The drawing is labelled "No COVID-19", next to the drawing is a smaller arrow pointing to the right. At the right end of the arrow there is a text reading "False positive". 
 
In the second row on the left there are again two drawings of a lung. The first drawing of a lung labelled "PTX" has a tube in the left lung indicated by a white line. In addition, the top of the left lung is black indicating a pneumothorax. The second drawing of a lung on the right labelled "No PTX" has no tube and pneumothorax. On top of these two drawings the text says, "Biased training datasets". Right next to the two drawings of lungs is a big arrow pointing to the right. The arrow is labelled "manifestation shift". On the right end of the arrow is another drawing of a lung. The left lung in the drawing features a tube but no pneumothorax. The drawing is labelled "No PTX", next to the drawing is a smaller arrow pointing to the right. At the right end of the arrow there is a text reading "False positive". 

In the third row on the left there are again two drawings of a lung. The first drawing of a lung labelled "Healthy" without any changes. The second drawing of a lung on the right labelled "Lung segmentation" shows a segmentation mask overlayed with the left and right lung in green. On top of these two drawings the text says, "Biased training datasets". Right next to the two drawings of lungs is a big arrow pointing to the right. The arrow is labelled "population shift". On the right end of the arrow is another drawing of a lung. The right lung in the drawing features a pacemaker in the upper right lung lobe. The drawing is labelled "Abnormality". On the right is another drawing of a lung. The lung shows the pacemaker in the same location. The lungs in the drawing are overlayed with a segmentation mask of the lungs. However, the pacemaker is not included in the segmentation mask. " class="wp-image-1088418" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24.jpg 1195w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24-300x198.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24-1024x675.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24-768x506.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24-240x158.jpg 240w" sizes="(max-width: 1195px) 100vw, 1195px" /><figcaption class="wp-element-caption">图 2：通过图像编辑模拟数据集变化来进行压力测试模型。</figcaption></figure><h2 class="wp-block-heading" id="stress-testing-multimodal-models">压力测试多模式模型</h2><p>我们使用 RadEdit 对图像分类和分割模型进行压力测试，并且我们看到了未来在生成放射学报告等复杂多模态任务中的应用潜力。 RadEdit 可以帮助识别多模态大语言模型 (MLLM)（例如 Microsoft 的<a href="https://www.microsoft.com/en-us/research/publication/maira-1-a-specialised-large-multimodal-model-for-radiology-report-generation/">MAIRA-1</a>和<a href="https://www.microsoft.com/en-us/research/publication/maira-2-grounded-radiology-report-generation/">MAIRA-2）</a>的局限性，特别是在处理训练数据中未充分体现的罕见条件或异常结果组合时。这些 MLLM 将一幅或多幅放射图像和相关临床信息作为输入，以生成详细的文本报告。</p><p> RadEdit 可以为具有挑战性的场景生成合成图像报告对。例如，手动编辑报告来描述罕见的发现组合，然后使用 RadEdit 编辑相应的图像，为 MLLM 创建有价值的测试用例。这种方法使我们能够使用不同的合成数据对 MLLM 进行压力测试，识别弱点或偏差，并确保模型在现实场景中更加稳健。这是在临床环境中安全有效地使用这些模型的关键一步。</p><h2 class="wp-block-heading" id="implications-and-looking-forward">影响和展望</h2><p>RadEdit 为生物医学研究界提供了显着的优势。它有助于在部署之前识别偏差和盲点，有助于确保生物医学视觉模型在临床环境中可靠地执行。通过模拟数据集变化，RadEdit 减少了收集额外评估数据的需要，从而节省了时间和资源。</p><p> RadEdit 适用于多种设置，可用于对最先进的基础模型进行压力测试，例如 Microsoft 的<a href="https://www.microsoft.com/en-us/research/publication/rad-dino-exploring-scalable-medical-image-encoders-beyond-text-supervision/">Rad-DINO <span class="sr-only">（在新选项卡中打开）</span></a>和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/BiomedParse/">BiomedParse <span class="sr-only">（在新选项卡中打开）</span></a> 。通过将 RadEdit 集成到他们的研究工作流程中，研究人员可以验证他们的生物医学视觉模型不仅是最先进的，而且能够更好地应对现实世界部署的复杂性。未来，我们设想 RadEdit 应用于更复杂的多模式任务，例如生成放射学报告。</p><p> RadEdit 的代码以及我们使用的扩散模型的权重可以在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/microsoft/radedit">https://huggingface.co/microsoft/radedit <span class="sr-only">（在新选项卡中打开）</span></a>下找到。</p><h2 class="wp-block-heading" id="acknowledgments">致谢</h2><p>我们要感谢我们论文的共同作者： <a href="https://www.microsoft.com/en-us/research/people/fperezgarcia/">Fernando Pérez-García</a> 、 <a href="https://www.microsoft.com/en-us/research/people/sbondtaylor/">Sam Bond-Taylor</a> 、Pedro P. Sanchez、Boris van Breugel、 <a href="https://www.microsoft.com/en-us/research/people/harssharma/">Harshita Sharma</a> 、 <a href="https://www.microsoft.com/en-us/research/people/vsalvatelli/">Valentina Salvatelli</a> 、Maria TA Wetscherek、 <a href="https://www.microsoft.com/en-us/research/people/hamurfet/">Hannah Richardson</a> 、 <a href="https://www.microsoft.com/en-us/research/people/mlungren/">Matthew P. Lungren</a> 、 <a href="https://www.microsoft.com/en-us/research/people/adityan/">Aditya Nori</a>和Ozan Oktay，以及我们在<a href="https://www.microsoft.com/en-us/industry/health/microsoft-cloud-for-healthcare?msockid=191ee027a78e6e430e59f339a60a6f7a">Microsoft 医疗保健云</a>和<a href="https://www.microsoft.com/en-us/research/lab/microsoft-health-futures/">Microsoft Health Futures</a>领域的所有合作者。</p><p> <em>RadEdit 仅用于研究目的，不得用于任何商业或临床用途。</em></p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/stress-testing-biomedical-vision-models-with-radedit-a-synthetic-data-approach-for-robust-model-deployment/">使用 RadEdit 对生物医学视觉模型进行压力测试后：用于稳健模型部署的合成数据方法</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title>微软研究院论坛第 4 集：多模式模型的未来、新的“小”语言模型以及其他 AI 更新</title><link/>https://www.microsoft.com/en-us/research/blog/microsoft-research-forum-episode-4-the-future-of-multimodal-models-a-new-small-language-model-and-其他-ai-更新/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Thu, 26 Sep 2024 12:15:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1081755 </guid><description><![CDATA[<p>探索多模式和小语言模型，以及人工智能评估的高级基准。微软研究人员正在致力于在天气预报、材料设计，甚至用于人工智能推理和硬优化问题的新型计算机方面取得突破。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/microsoft-research-forum-episode-4-the-future-of-multimodal-models-a-new-small-language-model-and-other-ai-updates/">微软研究院论坛第 4 集：多模式模型的未来、新的“小”语言模型和其他人工智能更新</a>首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </description><content:encoded><![CDATA[
<p>微软研究院论坛是关于通用人工智能时代科技研究的持续交流的思想。在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/" target="_blank" rel="noreferrer noopener">最新一集中<span class="sr-only">（在新选项卡中打开）</span></a> ，研究人员讨论了最新的多模态人工智能模型、人工智能评估和模型自我改进的高级基准，以及用于人工智能推理和硬优化的全新计算机。微软的研究人员正在努力探索突破性技术，以帮助推进从天气预报到材料设计的一切。</p><p>以下是该活动的简要回顾，包括演讲中的精选引述。 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://register.researchforum.microsoft.com/" target="_blank" rel="noreferrer noopener">注册</a>参加未来的研究论坛剧集并<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/" target="_blank" rel="noreferrer noopener">查看以前的会议</a>。文字记录和其他资源可以在研究论坛<a href="https://www.microsoft.com/en-us/research/story/sep-2024-brief/?OCID=msr_researchforum_MCRevent_ep4_2024">简报</a>中找到。 </p><h2 class="wp-block-heading" id="keynote-phi-3-vision-a-highly-capable-and-small-language-vision-model">基调</h2><h3 class="wp-block-heading" id="phi-3-vision-a-highly-capable-and-small-language-vision-model"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=7f500652-53b2-4783-9bfd-0db145f6a501&compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&showFromPaylod=true">Phi-3-Vision：一种高性能且“小”的语言视觉模型<span class="sr-only">（在新选项卡中打开）</span></a></h3><figure class="wp-block-image aligncenter size-full"> <a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/keynote-phi-3-vision-a-highly-capable-and-small-language-vision-model/"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788.jpg" alt="研究论坛|第 4 集主题演讲 |高剑峰" class="wp-image-1079760" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></a></figure><p>高剑峰介绍了先进、经济的开源多式联运模型Phi-3-Vision。作为 Phi-3 模型家族的一员，Phi-3-Vision 通过集成多感官技能、无缝结合语言和视觉功能来增强语言模型。 </p><blockquote class="wp-block-quote is-style-spectrum--blue-green is-layout-flow wp-block-quote-is-layout-flow"><p> “Phi-3-Vision 是 Phi 小型型号系列中的首款多模式型号。它可以与更大型号的某些功能相媲美，有时甚至超过它们……而成本却低得多。为了帮助每个人构建更实惠、更容易使用的人工智能系统，我们已将模型权重发布到开源社区。”</p> <cite><em>—</em><strong><em><a href="https://www.microsoft.com/en-us/research/people/jfgao/">高剑锋</a></em></strong><em>，微软雷蒙德研究院杰出科学家兼副总裁</em></cite></blockquote><hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/><h2 class="wp-block-heading" id="panel-discussion-beyond-language-the-future-of-multimodal-models-in-healthcare-gaming-and-ai">小组讨论</h2><h3 class="wp-block-heading" id="beyond-language-the-future-of-multimodal-models-in-healthcare-gaming-and-ai"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=fd3037cd-0ad8-48c0-80be-b3175c508935&compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&showFromPaylod=true">超越语言：医疗保健、游戏和人工智能领域多模式模型的未来<span class="sr-only">（在新选项卡中打开）</span></a></h3><figure class="wp-block-image size-large"> <a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/panel-discussion-beyond-language-the-future-of-multimodal-models-in-healthcare-gaming-and-ai/"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-1024x576.jpg" alt="研究论坛|第 4 集小组讨论 |约翰·兰福德、潘海丰、卡佳·霍夫曼、杨建伟" class="wp-image-1079766" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure><p>本次讨论探讨了跨多个领域的多模态模型的变革潜力和核心挑战，包括精准健康、游戏智能和基础模型。微软研究人员<a href="https://www.microsoft.com/en-us/research/people/jcl/" target="_blank" rel="noreferrer noopener">John Langford</a> 、 <a href="https://www.microsoft.com/en-us/research/people/hoifung/" target="_blank" rel="noreferrer noopener">Hoifung Poon</a> 、 <a href="https://www.microsoft.com/en-us/research/people/kahofman/" target="_blank" rel="noreferrer noopener">Katja Hofmann</a>和<a href="https://www.microsoft.com/en-us/research/people/jianwyan/" target="_blank" rel="noreferrer noopener">Jenwei Yang</a>分享了他们对该领域未来发展方向、弥合差距和促进协同效应的想法。</p><p> “当今真正最前沿的癌症治疗方法之一是免疫疗法。这是通过调动免疫系统来对抗癌症而起作用的。其中一种重磅药物是 KEYTRUDA，它确实可以为一些晚期癌症创造奇迹……不幸的是，只有 20% 到 30% 的患者真正有反应。所以这就是……精准健康领域增长机会的一个典型例子。”<br> <em><sub>—</sub></em> <strong><sub><em><a href="https://www.microsoft.com/en-us/research/people/hoifung/">Hoifung Poon</a></em></sub></strong> <em><sub>，微软健康未来研究院总经理</sub></em></p><p>“在我们开始理解周围使用的任何语言之前，我们通过视觉、触觉和所有其他感官来体验世界。因此，当我们开始更多地了解我们可以建模的不同模式以及将它们结合起来的不同方式时，思考其潜在的影响真的非常非常有趣。”<br> <em><sub>—</sub></em> <strong><sub><em><a href="https://www.microsoft.com/en-us/research/people/kahofman/">Katja Hofmann</a></em></sub></strong> <em><sub>，微软研究院高级首席研究员</sub></em></p><p>“为了真正拥有一个强大的多模态模型，我们需要对来自不同模态的不同信息进行编码，例如来自视觉、来自语言、甚至来自音频、语音等。我们需要为每个领域开发一个非常强大的编码器，并且然后……对这些原始数据进行标记。”<br> <em><sub>—</sub></em><strong><sub><em><a href="https://www.microsoft.com/en-us/research/people/jianwyan/">杨建伟</a></em></sub></strong><em><sub>，微软雷德蒙研究院首席研究员</sub></em></p><hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots"/><h2 class="wp-block-heading" id="lightning-talks">闪电演讲</h2><h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=d16c66c0-a4a7-418e-88b5-572a31009137&compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&showFromPaylod=true">用于可持续人工智能及其他领域的模拟光学计算<span class="sr-only">（在新选项卡中打开）</span></a></h3><figure class="wp-block-image size-large"> <a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/analog-optical-computing-for-sustainable-ai-and-beyond"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-1024x576.jpg" alt="研究论坛|第 4 集谈话 1 |弗朗西斯卡·帕玛强尼与楚嘉琪" class="wp-image-1081761" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure><p>本次演讲介绍了一种新型计算机——模拟光学计算机，它有可能将人工智能推理和硬优化工作负载加速 100 倍，利用软硬件协同设计来提高实际应用的效率和可持续性。</p><p> “最有可能的是，您或您的亲人曾接受过 MRI 扫描<em>，</em>这并不是一个真正适合的地方。想象一下，如果您可以将时间从 20 到 40 分钟减少到不到 5 分钟。”<br> <sub><em>—</em> <strong><em><a href="https://www.microsoft.com/en-us/research/people/frparmig/">Francesca Parmigiani</a></em></strong> <em>，微软剑桥研究院首席研究员</em></sub></p><p>“我非常高兴地告诉大家，我们刚刚完成了第二代[这台]计算机。它的物理尺寸要小得多，这是世界首创，因为同一台计算机可以同时解决困难的优化问题并加速机器学习推理。展望未来，我们估计，在规模上，这台计算机每瓦每秒可实现约 450 兆兆次运算，这比最先进的 GPU 提高了 100 倍。”<br> <em><sub>—</sub></em> <strong><sub><em><a href="https://www.microsoft.com/en-us/research/people/jiaqchu/">Jiaqi Chu</a></em></sub></strong> <em><sub>，微软剑桥研究院首席研究员</sub></em></p><hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots"/><h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=5a7e35e6-9238-4d29-8c42-4bb720975311&compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&showFromPaylod=true">直接纳什优化：教授语言模型根据一般偏好进行自我改进<span class="sr-only">（在新选项卡中打开）</span></a></h3><figure class="wp-block-image size-large"> <a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/direct-nash-optimization-teaching-language-models-to-self-improve-with-general-preferences"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-1024x576.jpg" alt="研究论坛|第 4 集谈话 2 |科比·罗塞特" class="wp-image-1079778" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure><p>本演讲探讨了如何使用 AI 偏好反馈来教学语言模型进行自我改进，挑战模型与自身和强大的老师进行对抗，直到达到纳什均衡，从而在对抗 GPT-4 Turbo 时获得最先进的胜率在 AlpacaEval 和 MT-Bench 等基准测试上。</p><p> “在培训后对法学硕士进行微调的传统方法……基本上是告诉模型模仿良好的行为，但它不会针对或纠正其明确做出的任何错误或不良行为。 ......自我改进的训练后明确识别并尝试纠正模型所犯的不良行为或错误。”<br> <sub>— <em><strong><a href="https://www.microsoft.com/en-us/research/people/corbyrosset/">Corby Rosset</a></strong> ，微软人工智能前沿研究院高级研究员</em></sub></p><hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots"/><h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=2ee92d58-1623-466c-9f54-93e9734b6f9e&compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&showFromPaylod=true">极光计划：第一个大型大气基础模型<span class="sr-only">（在新选项卡中打开）</span></a></h3><figure class="wp-block-image size-large"> <a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/project-aurora-the-first-large-scale-foundation-model-of-the-atmosphere"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-1024x576.jpg" alt="研究论坛|第 4 集谈话 3 |梅根·斯坦利" class="wp-image-1079784" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure><p>本次演讲介绍了 Aurora，这是一种尖端的基础模型，它提供了一种新的天气预报方法，可以改变我们预测和减轻极端事件、空气污染和气候变化影响的能力。</p><p> “如果我们观察 Aurora 预测与人类活动排放密切相关的二氧化氮等污染物的能力，我们可以看到该模型已经学会在没有提供排放数据的情况下做出这些预测。它了解了导致气体浓度的隐含模式，这非常令人印象深刻。”<br> <sub>— <em><strong><a href="https://www.microsoft.com/en-us/research/people/meganstanley/">Megan Stanley</a></strong> ，微软人工智能科学研究院高级研究员</em></sub></p><hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots"/><h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=b8e7da83-cddf-42e5-a2e0-76e4f3f2290b&compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&showFromPaylod=true">用于计算机实验和发现的生物学生成模型<span class="sr-only">（在新选项卡中打开）</span></a></h3><figure class="wp-block-image size-large"> <a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/a-generative-model-of-biology-for-in-silico-experimentation-and-discovery"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-1024x576.jpg" alt="研究论坛|第 4 集谈话 4 |凯文·杨" class="wp-image-1079790" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure><p>本演讲探讨了深度学习如何生成新颖且有用的生物分子，使研究人员和从业者能够更好地理解生物学。其中包括 EvoDiff，这是一种通用扩散框架，它将进化规模的数据与扩散模型的独特调节能力相结合，在给定蛋白质序列的情况下生成新的蛋白质。</p><p> “通常，蛋白质工程师希望蛋白质具有与天然蛋白质相似的功能，或者他们希望生产出具有相同功能但具有其他所需特性（例如稳定性）的蛋白质。通过用一系列相关序列调节 EvoDiff，我们可以生成新的蛋白质，这些蛋白质在序列空间上与天然蛋白质非常不同，但预计会折叠成类似的三维结构。这些可能是寻找新功能或发现具有理想特性的蛋白质版本的良好起点。”<br> <sub>— <em><strong><a href="https://www.microsoft.com/en-us/research/people/kevyan/">Kevin Yang</a></strong> ，微软新英格兰研究院高级研究员</em></sub></p><hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots"/><h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=7981331c-c99b-4d8f-9d68-5066ddae0f81&compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&showFromPaylod=true">培养对人工智能的适当依赖<span class="sr-only">（在新选项卡中打开）</span></a></h3><figure class="wp-block-image size-large"> <a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/fostering-appropriate-reliance-on-ai"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-1024x576.jpg" alt="研究论坛|第 4 集谈话 5 |米哈埃拉·沃尔沃雷亚努" class="wp-image-1079799" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure><p>由于人工智能系统具有概率性，因此它们可能会犯错误。人机交互的主要挑战之一是避免过度依赖人工智能，并让人们能够决定何时接受或不接受人工智能系统的建议。本演讲探讨了 Microsoft 在该领域的工作。</p><p> “我认为，作为用户体验学科的人们，作为研究用户体验和人机交互的人们，我们有责任真正真正地站到前面，看看现在是我们如何发光发热并解决这个问题的时候了。 ”。<br> <sub>— <em><strong><a href="https://www.microsoft.com/en-us/research/people/mivorvor/">Mihaela Vorvoreanu</a></strong> ，微软工程和研究中的人工智能道德与影响 (Aether) 用户体验研究和负责任的人工智能教育总监</em></sub></p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/microsoft-research-forum-episode-4-the-future-of-multimodal-models-a-new-small-language-model-and-other-ai-updates/">微软研究院论坛第 4 集：多模式模型的未来、新的“小”语言模型和其他人工智能更新</a>首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>;</content:encoded></item></channel></rss>