<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2024 年 11 月 18 日星期一 10:20:44 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.6.2</generator><item><title> BiomedParse：更智能、一体化生物医学图像分析的基础模型</title><link/>https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Mon, 18 Nov 2024 10:14:40 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1101105 </guid><description><![CDATA[<p> BiomedParse 重新构想了医学图像分析，集成先进的人工智能来捕获跨成像类型的复杂见解——这是诊断和精准医学的一步。</p><p>后<a href="https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/">BiomedParse：更智能、一体化生物医学图像分析的基础模型</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1.jpg" alt="一只绿色线条画的手握着透明棱镜的风格化插图，在黑色背景下，彩色光带通过棱镜折射。" class="wp-image-1102929" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>在癌症诊断或免疫疗法等高级治疗中，医学图像中的每个细节都很重要。放射科医生和病理学家依靠这些图像来跟踪肿瘤，了解它们的边界，并分析它们如何与周围细胞相互作用。这项工作需要多项任务的精确性——识别肿瘤是否存在、精确定位、在复杂的 CT 扫描或病理切片上绘制其轮廓。</p><p>然而，这些关键步骤——对象识别、检测和分割——通常是分开处理的，这会限制分析的深度。 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41467-024-44824-z" target="_blank" rel="noreferrer noopener">MedSAM <span class="sr-only">（在新选项卡中打开）</span></a>和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://segment-anything.com/" target="_blank" rel="noreferrer noopener">SAM <span class="sr-only">（在新选项卡中打开）</span></a>等当前工具仅专注于细分，因此错过了全面融合这些见解并将对象降级为事后想法的机会。</p><p>在本博客中，我们介绍<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41592-024-02499-w" target="_blank" rel="noreferrer noopener">BiomedParse <span class="sr-only">（在新选项卡中打开）</span></a> ，这是一种通过将对象视为一等公民来进行整体图像分析的新方法。通过将对象识别、检测和分割统一到一个框架中，BioMedParse 允许用户通过简单的自然语言提示指定他们正在寻找的内容。其结果是一种更有凝聚力、更智能的医学图像分析方法，支持更快、更综合的临床洞察。</p><p>虽然生物医学分割数据集比比皆是，但之前关于生物医学中的对象检测和识别的工作相对较少，更不用说涵盖所有三个任务的数据集了。为了预训练 BiomedParse，我们利用 OpenAI 的 GPT-4 从<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/microsoft/BiomedParseData" target="_blank" rel="noreferrer noopener">标准分割数据集<span class="sr-only">（在新选项卡中打开）</span></a>进行数据合成，创建了第一个此类数据集。</p><p> BiomedParse 是一个单一的基础模型，可以跨九种模式准确地分割生物医学对象，如图 1 所示，其性能优于先前的最佳方法，同时需要的用户操作数量减少了几个数量级，因为它不需要特定于对象的边界框。通过学习单个对象类型的语义表示，BiomedParse 的优越性在具有不规则形状对象的最具挑战性的情况下尤其明显。通过对象识别、检测和分割的联合预训练，BiomedParse 为生物医学中的整体图像分析和基于图像的发现开辟了新的可能性。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" width="2185" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-scaled.jpg" alt="a，GPT-4 构造的本体显示了用于统一跨数据集的语义概念的对象类型的层次结构。显示包含该对象类型的图像数量的条形图。 b，条形图显示 BiomedParseData 中每种模态的图像-掩模-描述三元组的数量。 CT 是计算机断层扫描的缩写。 MRI 是磁共振成像的缩写。 OCT 是光学相干断层扫描的缩写。 c，BiomedParse 流程图。 BiomedParse 将图像和文本提示作为输入，然后输出提示中指定的对象的分割掩码。我们的框架不需要特定于图像的手动交互，例如边界框或单击。为了促进图像编码器的语义学习，BiomedParse 还纳入了一个学习目标来对元对象类型进行分类。对于在线推理，GPT-4 用于使用对象本体将文本提示解析为对象类型，该本体还使用 BiomedParse 输出的元对象类型来缩小候选语义标签的范围。 d，统一流形逼近和投影 (UMAP) 图对比了来自 BiomedParse 文本编码器（左）和 PubMedBERT（右）的不同细胞类型的文本嵌入。 e，UMAP 绘图对比了来自 BiomedParse 图像编码器（左）和 Focal（右）的不同细胞类型的图像嵌入。" class="wp-image-1102950" style="width:825px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-scaled.jpg 2185w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-256x300.jpg 256w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-874x1024.jpg 874w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-768x900.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-1311x1536.jpg 1311w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-1748x2048.jpg 1748w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-154x180.jpg 154w" sizes="(max-width: 2185px) 100vw, 2185px" /><figcaption class="wp-element-caption">图 1. BiomedParse 和 BiomedParseData 概述<em>。</em> </figcaption></figure><h2 class="wp-block-heading" id="image-parsing-a-unifying-framework-for-holistic-image-analysis">图像解析：整体图像分析的统一框架</h2><p>早在 2005 年，研究人员就首次引入了“图像解析”的概念——一种联合进行对象识别、检测和分割的图像分析统一方法。这种早期模型建立在贝叶斯网络的基础上，尽管其范围和应用受到限制，但它让我们得以一睹图像分析中联合学习和推理的未来。快进到今天，生成式人工智能的前沿进展为这一愿景注入了新的活力。通过我们的模型 BiomedParse，我们为生物医学图像解析奠定了基础，该解析利用了三个子任务之间的相互依赖性，从而解决了传统方法中的关键局限性。 BiomedParse 使用户能够简单地输入对象的自然语言描述，模型使用该描述来预测对象标签及其分割掩模，从而消除了对边界框的需要（图 1c）。换句话说，这种联合学习方法可以让用户仅根据文本来分割对象。 </p><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="931956"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">焦点：点播视频</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" aria-label="AI Explainer: Foundation models ​and the next era of AI" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AIEx01_blog_hero_1400x788.png" alt="电脑屏幕截图 一名男子的屏幕截图" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4"> AI 解释者：基础模型和 AI 的下一个时代</h2><p class="large">探索 Transformer 架构、更大的模型和更多数据以及情境学习如何帮助推动人工智能从感知到创造。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Watch video" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank">观看视频</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="harnessing-gpt-4-for-large-scale-data-synthesis-from-existing-datasets">利用 GPT-4 从现有数据集中进行大规模数据合成</h2><p>我们利用 GPT-4 从 45 个现有生物医学分割数据集进行大规模数据合成，创建了第一个生物医学成像解析数据集（图 1a 和 1b）。关键的见解是利用这些数据集中已有的自然语言描述，并使用 GPT-4 通过已建立的生物医学对象分类法来组织这些经常混乱的非结构化文本。</p><p>具体来说，我们使用 GPT-4 帮助创建用于图像分析的统一生物医学对象分类法，并将现有数据集的自然语言描述与该分类法相协调。我们进一步利用 GPT-4 来合成对象描述的其他变体，以促进更强大的文本提示。</p><p>这使我们能够构建 BiomedParseData，这是一个生物医学图像分析数据集，包含超过 600 万组图像、分割掩模以及从超过 100 万张图像中提取的文本描述。该数据集包括 64 种主要生物医学对象类型、82 种细粒度子类型，涵盖九种成像模式。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="2242" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-scaled.jpg" alt="a，比较我们的方法和竞争方法在 9 种模式的 102,855 个测试实例（图像-掩模-标签三元组）上的 Dice 分数的箱线图。 MedSAM 和 SAM 需要边界框作为输入。我们考虑两种设置：预言机边界框（覆盖黄金掩模的最小边界框）； Grounding DINO（一种最先进的基于文本的接地模型）根据文本提示生成边界框。每个模态类别包含多种对象类型。每个对象类型都聚合为要在图中显示的实例中位数。图中的 n 表示相应模态中测试实例的数量。 b，仅使用顶部的文本提示，比较 BiomedParse 的分割结果和真实情况的九个示例。 c，比较我们的方法和竞争方法在 n=42 图像的细胞分割测试集上的 Dice 分数的箱线图。 BiomedParse 仅需要单个用户操作（文本提示“结肠病理学中的腺体结构”）。相比之下，为了获得有竞争力的结果，MedSAM 和 SAM 需要 430 次操作（每个细胞一个边界框）。 d，对比 BiomedParse 和 MedSAM 的分割结果的五个示例，以及 BiomedParse 使用的文本提示和 MedSAM 使用的边界框。 e，BiomedParse 和 MedSAM 在良性肿瘤图像（上）和恶性肿瘤图像（下）上的比较。 BiomedParse 相对于 MedSAM 的改进对于形状不规则的异常细胞更为明显。 f，比较有效文本提示和无效文本提示之间的两侧 K-S 检验 P 值的箱线图。 BiomedParse 学会拒绝描述图像中不存在的对象类型的无效文本提示（小 P 值）。我们总共评估了 4,887 个无效提示和 22,355 个有效提示。 g，该图显示了我们的方法在不同 K-S 测试 P 值截止值上检测无效文本提示的精确度和召回率。 h,i，散点图，比较 BiomedParse 和 Grounding DINO 检测无效描述时的受试者工作特征曲线 (AUROC) (h) 和 F1 (i) 下的面积。" class="wp-image-1102956" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-scaled.jpg 2242w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-263x300.jpg 263w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-897x1024.jpg 897w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-768x877.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-1345x1536.jpg 1345w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-1793x2048.jpg 1793w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-158x180.jpg 158w" sizes="(max-width: 2242px) 100vw, 2242px" /><figcaption class="wp-element-caption">图 2：大规模生物医学图像分割数据集的比较。 </figcaption></figure><h2 class="wp-block-heading" id="state-of-the-art-performance-across-64-major-object-types-in-9-modalities">以 9 种模式跨 64 种主要对象类型提供最先进的性能</h2><p>我们在一个大型保留测试集上评估了 BiomedParse，该测试集包含 9 种模式的 64 个主要对象类型的 102,855 个图像-掩模-标签集。即使提供了预言机每个对象的边界框，BiomedParse 的性能也优于 MedSAM 和 SAM 等之前的最佳方法。在更现实的环境中，当 MedSAM 和 SAM 使用最先进的对象检测器 (Grounding DINO) 来提出边界框时，BiomedParse 的表现大幅优于它们，骰子得分绝对值在 75 到 85 分之间（图 2a） 。 BiomedParse 的性能还优于其他各种著名方法，例如 SegVol、Swin UNETR、nnU-Net、DeepLab V3+ 和 UniverSeg。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2170" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-scaled.jpg" alt="a，文本提示的注意力图不规则形状的物体，表明 BiomedParse 学会了对其典型形状的相当忠实的表示。美国，超声波。 b-d，散点图比较了 BiomedParse 相对于 MedSAM 的 Dice 评分改进以及凸比 (b)、盒比 (c) 和反转动惯量 (d) 方面的形状规律性。 x 轴上的数字越小意味着平均不规则性越高。每个点代表一个对象类型。 e，对比 BiomedParse 和 MedSAM 在检测不规则形状物体方面的六个示例。绘图按照从最不规则的图（左）到最不规则的图（右）的顺序排列。 f,g BiomedParseData 与 MedSAM 使用的基准数据集在凸比（f）和盒比（g）方面的比较。 BiomedParseData 更忠实地代表了现实世界中不规则形状物体的挑战。 h，比较 BiomedParse 和 BiomedParseData 上的竞争方法以及 MedSAM 使用的基准数据集的箱线图。 BiomedParse 在 BiomedParseData 上有较大的改进，包含更多样的图像和更多不规则形状的物体。对象类型的数量如下：对于 MedSAM 基准，n=50；对于 BiomedParseData，n=112。" class="wp-image-1102965" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-scaled.jpg 2170w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-254x300.jpg 254w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-868x1024.jpg 868w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-768x906.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-1302x1536.jpg 1302w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-1736x2048.jpg 1736w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-153x180.jpg 153w" sizes="(max-width: 2170px) 100vw, 2170px" /><figcaption class="wp-element-caption">图 3. 检测不规则形状物体的评估。 </figcaption></figure><h2 class="wp-block-heading" id="recognizing-and-segmenting-irregular-and-complex-objects">识别和分割不规则和复杂的物体</h2><p>生物医学对象通常具有复杂且不规则的形状，这给分割带来了巨大的挑战，即使使用预言机边界框也是如此。通过与对象识别和检测的联合学习，BiomedParse 学习对特定对象的形状进行建模，对于最具挑战性的情况，其优越性尤其明显（图 3）。 BiomedParseData 包含九种模式的大量不同对象类型，还提供了生物医学中对象复杂性的更真实的表示。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2276" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-scaled.jpg" alt="a，六个例子显示了我们的方法的物体识别结果。对象识别可以识别并分割图像中的所有对象，而不需要任何用户提供的输入提示。 b-d，散点图，比较 BiomedParse 和 Grounding DINO 在识别图像中呈现的对象时的 F1 (b)、精度 (c) 和召回率 (d) 分数。 e，BiomedParse 和 Grounding DINO 在图像中不同数量对象的中值 F1 分数方面的对象识别比较。 f，比较 BiomedParse 和 MedSAM/SAM（使用 Grounding DINO 生成的边界框）在与各种模式相关的端到端对象识别（包括分割）方面的箱线图。 g，BiomedParse 和 MedSAM/SAM（使用由 Grounding DINO 生成的边界框）在端到端对象识别（包括分割）方面与图像中不同对象数量的比较。" class="wp-image-1102968" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-scaled.jpg 2276w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-267x300.jpg 267w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-910x1024.jpg 910w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-768x864.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-1366x1536.jpg 1366w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-1821x2048.jpg 1821w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-160x180.jpg 160w" sizes="(max-width: 2276px) 100vw, 2276px" /><figcaption class="wp-element-caption">图 4. 物体识别评估。 </figcaption></figure><h2 class="wp-block-heading" id="promising-step-toward-scaling-holistic-biomedical-image-analysis">朝着扩展整体生物医学图像分析迈出了有希望的一步</h2><p>通过通过简单的文本提示进行操作，BiomedParse 比以前通常需要特定于对象的边界框的最佳方法所需的用户工作量大大减少，特别是当图像包含大量对象时（图 2c）。通过对对象识别阈值进行建模，BiomedParse 可以检测无效提示，并在图像中不存在对象时拒绝分割请求。 BiomedParse 可用于一次性识别和分割图像中的所有已知对象（图 4）。通过扩展整体图像分析，BiomedParse 有可能应用于关键的精准健康应用，例如早期检测、预后、治疗决策支持和进展监测。</p><p>展望未来，还有大量的增长机会。 BiomedParse 可以扩展以处理更多模式和对象类型。它可以集成到<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/llava-med" target="_blank" rel="noreferrer noopener">LLaVA-Med <span class="sr-only">（在新选项卡中打开）</span></a>等先进的多模式框架中，通过“与数据对话”来促进对话式图像分析。为了促进生物医学图像分析研究，我们使用 Apache 2.0 许可证将 BiomedParse <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/biomedparse-release" target="_blank" rel="noreferrer noopener">开源<span class="sr-only">（在新选项卡中打开）</span></a> 。我们还在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.azure.com/explore/models/MedImageParse/version/3/registry/azureml/latest" target="_blank" rel="noreferrer noopener">Azure AI <span class="sr-only">（在新选项卡中打开）</span></a>上提供了它，用于直接部署和实时推理。欲了解更多信息，请查看我们的<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/BiomedParse/" target="_blank" rel="noreferrer noopener">演示。 <span class="sr-only">（在新选项卡中打开）</span></a></p><p> BiomedParse 是与普罗维登斯和华盛顿大学 Paul G. Allen 计算机科学与工程学院的联合项目，并带来了 Microsoft* 内多个团队的协作。它反映了微软对推进多模式生成人工智能以实现精准健康的更大承诺，以及其他令人兴奋的进展，例如<a href="https://www.microsoft.com/en-us/research/blog/gigapath-whole-slide-foundation-model-for-digital-pathology/" target="_blank" rel="noreferrer noopener">GigaPath <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a href="https://www.microsoft.com/en-us/research/publication/large-scale-domain-specific-pretraining-for-biomedical-vision-language-processing/" target="_blank" rel="noreferrer noopener">BiomedCLIP <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a href="https://www.microsoft.com/en-us/research/project/project-maira/" target="_blank" rel="noreferrer noopener">LLaVA-Rad（在新选项卡中打开）、BiomedJourney <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2310.10765" target="_blank" rel="noreferrer noopener">BiomedJourney <span class="sr-only">（在新选项卡中打开）新选项卡）</span></a> 、 <a href="https://www.microsoft.com/en-us/research/publication/rad-dino-exploring-scalable-medical-image-encoders-beyond-text-supervision/" target="_blank" rel="noreferrer noopener">MAIRA <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a href="https://www.microsoft.com/en-us/research/publication/rad-dino-exploring-scalable-medical-image-encoders-beyond-text-supervision/" target="_blank" rel="noreferrer noopener">Rad-DINO <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/2309.07778v5" target="_blank" rel="noreferrer noopener">Virchow <span class="sr-only">（在新选项卡中打开）</span></a> 。</p><p> <em>（致谢脚注）*：在 Microsoft 内部，这是 Health Futures、MSR 深度学习和 Nuance 之间的精彩合作。</em></p><p>论文合著者：Theodore Zhao、Yu Gu、 <a href="https://www.microsoft.com/en-us/research/people/jianwyan/">Jianwei Yang <span class="sr-only">(在新选项卡中打开)</span></a> 、 <a href="https://www.microsoft.com/en-us/research/people/naotous/">Naoto Usuyama <span class="sr-only">(在新选项卡中打开)</span></a> 、Ho Hin Lee、Sid Kiblawi、 <a href="https://www.microsoft.com/en-us/research/people/tristan/">Tristan Naumann <span class="sr-only">(在新选项卡中打开)</span></a> 、 <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Taka <span class="sr-only">(在新选项卡中打开</span></a>)<a href="https://www.microsoft.com/en-us/research/people/jfgao/"><span class="sr-only">新标签）</span></a> 、Angela Crabtree、Jacob Abel、Christine Moung-Wen、Brian Piening、Carlo Bifulco、Mu Wei、 <a href="https://www.microsoft.com/en-us/research/people/hoifung/">Hoifung Poon <span class="sr-only">（在新标签中打开）</span></a> 、 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://homes.cs.washington.edu/~swang/">Sheng Wang <span class="sr-only">（在新标签中打开）</span></a></p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p>后<a href="https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/">BiomedParse：更智能、一体化生物医学图像分析的基础模型</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title> GraphRAG：通过动态社区选择改进全局搜索</title><link/>https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Fri, 15 Nov 2024 16:52:47 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1101987 </guid><description><![CDATA[<p>检索增强生成 (RAG) 有助于人工智能系统在生成对用户查询的响应时向大型语言模型 (LLM) 提供更多信息。一种进行“全局”查询的新方法可以优化 GraphRAG 中全局搜索的性能。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/">GraphRAG：通过动态社区选择改进全局搜索</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1.jpg" alt="该图像在渐变背景上具有三个白色图标，从左侧的蓝色过渡到右侧的绿色。第一个图标位于左侧，描绘了类似于带有连接方块的工作流程的分层结构。中间的图标代表 GraphRAG（互连的节点和线）。右侧的第三个图标显示了一个地球仪，上面覆盖着一个放大镜，象征着全球搜索。" class="wp-image-1102437" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-Global-Search-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>检索增强生成 (RAG) 允许人工智能系统在生成对用户查询的响应时向大型语言模型 (LLM) 提供附加信息和上下文。然而，传统的基于 RAG 的方法可能很难检索需要整个数据集的高级知识的信息，尤其是对于抽象和全局问题，例如无关键字查询：“了解最近两周的更新情况”。这些类型的查询被称为“全局”查询，因为它们需要对数据集的整体理解才能回答问题。 <a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/" target="_blank" rel="noreferrer noopener">GraphRAG</a>旨在通过两个主要步骤解决这些问题：索引和查询。索引引擎首先将文本文档集合分解为片段，然后将这些片段聚集成分层社区，其中实体和关系通过更高级别的抽象将每个片段连接起来。然后，我们使用法学硕士生成每个社区的摘要，称为社区报告。因此，索引引擎创建数据集的分层知识图，分层结构中的每个级别代表原始材料的不同抽象和摘要级别。在查询步骤中，GraphRAG 使用这种结构化知识为 LLM 提供额外的上下文，以帮助回答问题。在这篇博文中，我们展示了一种进行“全局”查询的新方法，该方法有效地利用知识图表示并优化 GraphRAG 中全局搜索的性能。</p><h2 class="wp-block-heading" id="static-vs-dynamic-global-search">静态与动态全局搜索</h2><p>GraphRAG 中的<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/graphrag/query/global_search/" target="_blank" rel="noreferrer noopener">全局搜索<span class="sr-only">（在新选项卡中打开）</span></a>算法旨在回答需要了解整个数据集的抽象问题。它通过在知识图中预定级别搜索社区来生成答案。然后，法学硕士在这个抽象级别上结合并总结了所有社区报告。最后，摘要用作法学硕士的附加上下文，以生成对用户问题的响应。这个映射缩减过程允许法学硕士从所有社区报告中选择相关文本以生成最终答案。这种静态方法既昂贵又低效，因为它包含许多对用户查询没有信息的较低级别的报告。由于所有社区报告（尤其是高层报告）不太可能与回答查询相关，因此非常需要一种在资源密集型映射缩减操作之前首先考虑报告相关性的方法。</p><p>在这里，我们将动态社区选择引入全局搜索算法，该算法利用索引数据集的知识图结构。从知识图谱的根部开始，我们使用法学硕士来评估社区报告在回答用户问题时的相关性。如果报告被认为不相关，我们只需从搜索过程中删除它及其节点（或子社区）。另一方面，如果报告被认为是相关的，我们就会向下遍历其子节点并重复该操作。最后，仅将相关报告传递给 Map-Reduce 操作以生成对用户的响应。图 1 说明了动态社区选择过程的实际应用。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="517" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1.png" alt="显示全局搜索中动态社区选择工作流程的图像。每个节点图示一个社区报告，箭头表示速率操作。" class="wp-image-1101996" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1-300x111.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1-1024x378.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1-768x284.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/GraphRAG-global-dynamics_Figure-1-240x89.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">图 1：动态社区选择工作流程</figcaption></figure><p>动态全局搜索方法有两个主要好处。首先，它尽早修剪不相关的报告，从而减少映射缩减操作中要考虑的社区报告总数。其次，它使用户能够搜索整个知识图谱，而不是预先定义静态的社区级别，并且可以得到更详细的答案。这使得它能够收集不同抽象级别的信息。此外，评级操作是一个分类问题，比摘要和文本生成更容易执行，因此可以使用不太复杂的模型。在我们利用 OpenAI 模型的实验中，GPT-4o-mini 评估器实现了与 GPT-4o 评估器非常相似的检索率，而运行成本和时间仅为其一小部分。总体而言，我们在速率操作中使用更小且更具成本效益的模型 GPT-4o-mini 来修剪任何不相关的社区报告，然后我们使用 GPT-4o 执行 map-reduce 操作以生成最终响应。</p><h2 class="wp-block-heading" id="dynamic-community-selection-on-the-ap-news-dataset">美联社新闻数据集上的动态社区选择</h2><p>为了证明动态全局搜索在保持相似响应质量的同时带来的成本节省，我们在美联社新闻的数据集上并排评估了这两种方法。我们测试了 50 个全局问题的静态和动态搜索，并使用法学硕士评估器评估了最终的回答质量。此外，我们还比较了两种方法的总代币成本。为了直接比较这两种方法，我们限制了动态全局搜索的最大搜索深度，以便两种方法使用相同的基础信息。</p><p>我们使用 LLM 评估器来选择 3 个关键指标的最佳响应（即胜率）：</p><ul class="wp-block-list"><li>全面性：答案提供了多少细节来涵盖问题的所有方面和细节？</li><li>多样性：对问题提供不同观点和见解的答案有多多样化和丰富？</li><li>授权：答案如何帮助读者理解该主题并做出明智的判断？ </li></ul><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1045008"><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/gigapath-whole-slide-foundation-model-for-digital-pathology/" aria-label="GigaPath: Whole-Slide Foundation Model for Digital Pathology" data-bi-cN="GigaPath: Whole-Slide Foundation Model for Digital Pathology" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/05/GigaPath-TW_FB_LI.png" alt="数字病理学有助于解码肿瘤微环境以进行精准免疫治疗。 GigaPath 是一种新颖的视觉转换器，可以通过适应数字病理学的扩张注意力来扩展到十亿像素的全幻灯片图像。在与普罗维登斯大学和华盛顿大学的合作中，我们正在共享 Prov-GigaPath，这是第一个基于大规模真实数据进行预训练的全幻灯片病理学基础模型，用于推进临床研究和发现。" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4"> GigaPath：数字病理学的全幻灯片基础模型</h2><p class="large">数字病理学有助于解码肿瘤微环境以进行精准免疫治疗。我们与普罗维登斯大学和华盛顿大学合作，共享 Prov-GigaPath，这是第一个全玻片病理学基础模型，用于推进临床研究。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/blog/gigapath-whole-slide-foundation-model-for-digital-pathology/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="GigaPath: Whole-Slide Foundation Model for Digital Pathology" target="_blank">阅读更多</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="significant-cost-reduction-while-maintaining-output-quality">显着降低成本，同时保持输出质量</h2><p>动态社区选择生成的响应质量与静态社区选择相当，同时降低了总代币成本。我们的法学硕士评估表明，这两种方法在美联社新闻数据集的 50 个全局问题的三个关键指标中的输出质量相似，并且它们之间没有统计显着性。更重要的是，我们观察到使用新方法时总代币成本显着降低，与社区级别 1 现有的静态全局搜索相比<strong>，平均成本降低了 77%。</strong>这是由于大量社区报告被消除了评级过程，因此在映射减少操作中需要更少的提示和输出标记。例如，现有的静态全局搜索方法在map-reduce操作中处理大约1500个1级社区报告，而在动态搜索中平均只选择470个社区报告来生成最终答案。</p><p>此外，如果我们允许动态搜索继续将评级过程进一步深入到更深层次的社区报告，我们会观察到其最终响应的改进。在这里，我们进行了相同的实验，但允许动态搜索持续到社区级别 3。在 50 个全局问题中，29 个包含的社区报告比我们的静态搜索基线更多，这表明一些更深级别的社区报告与用户问题相关。事实上，我们观察到综合性和赋权方面都有适度且具有统计显着性的改善。  使用 LLM 评估器对响应对进行评分，我们观察到动态全局搜索相对于第 1 级静态搜索的胜率分别为 58% 和 60%。尽管如此，虽然评级操作是由较小的模型执行的，因此尽管引起的成本可以忽略不计，但由于地图归约操作处理的社区报告数量增加，它仍然可能导致更高的总体成本。在此实验中，第 3 级动态搜索的总成本平均高出 34%。表 1 总结了第 1 级静态搜索与第 1 级和第 3 级动态搜索的结果。 </p><table width="100%" align="center" border="1" font-size="80%" cellpadding="3px"><tr><td rowspan="2">动态搜索</td><td rowspan="2" style="text-align: center;">编号。查询</td><td colspan="3" style="text-align: center;">静态搜索胜率（级别 1）</td><td rowspan="2" style="text-align: center;">相对成本</td></tr><tr><td style="text-align: center;">综合性</td><td style="text-align: center;">多样性</td><td style="text-align: center;">赋权</td></tr><tr><td>1级</td><td style="text-align: center;">50</td><td style="text-align: center; background-color: #eeeeee;"> 49.5%</td><td style="text-align: center; background-color: #eeeeee;"> 46.0%</td><td style="text-align: center; background-color: #eeeeee;"> 48.0%</td><td style="text-align: center;"> -77%</td></tr><tr><td> 3级</td><td style="text-align: center;">29*</td><td style="text-align: center; background-color: #8de971;"> 58.8%</td><td style="text-align: center; background-color: #eeeeee;"> 48.0%</td><td style="text-align: center; background-color: #8de971;"> 60.0%</td><td style="text-align: center;"> +34% </td></tr></table><div style="height:10px" aria-hidden="true" class="wp-block-spacer"></div><p class="has-text-align-center">表 1. LLM 在不同社区级别对全局问题的动态搜索与静态搜索的评估。我们显示了第 1 级和第 3 级动态搜索相对于第 1 级静态搜索的获胜率，即 LLM 评估者更喜欢动态搜索生成的响应而不是静态搜索的次数。绿色背景表示胜率具有统计显着性，灰色单元表示不显着。最右边的列显示了针对第 1 级静态搜索的平均相对令牌成本。 * 请注意，我们仅评估社区第 3 级动态搜索的答案，其中包含比第 1 级静态搜索更多的社区报告。</p><p>总体而言，动态社区选择实现了 77% 的成本降低，同时实现了与基线静态方法类似的输出质量。这些数字看起来很有希望，但结果质量如何呢？让我们通过一个例子来进行一些直接的并排比较：</p><h2 class="wp-block-heading" id="comparable-response-quality-and-detail">类似的响应质量和细节</h2><table width="100%" align="center" border="1" font-size="80%" cellpadding="12px"><tr><td colspan="2">查询：在整个数据集中，描述主要疾病疫苗接种率的共同趋势。</td></tr><tr style="vertical-align: top;"><td> 1 级静态全局搜索</td><td>具有第 3 级动态社区选择的全局搜索</td></tr><tr style="vertical-align: top;"><td><p>### 主要疾病疫苗接种率的常见趋势</p><p>#### 疫苗接种率下降</p><p>各地区观察到的一个显着趋势是主要疾病疫苗接种率下降。例如，明尼苏达州的免疫接种率下降，导致最近爆发麻疹[数据：报告（335）]。同样，COVID-19 大流行加剧了这一问题，导致密歇根州和其他地区的儿童免疫接种率下降 [数据：报告 (1231, 915)]。这种下降通常归因于有关疫苗的错误信息，特别是将 MMR 疫苗与自闭症联系起来的错误说法[数据：报告 (912, 1335)]。</p><p> #### COVID-19 的影响</p>COVID-19 大流行对疫苗接种率产生了深远影响。努力提高 COVID-19 疫苗接种率一直是公共卫生政策的主要重点，但大流行也导致儿童常规疫苗接种率下降 [数据：报告 (920, 194)]。这一流行病影响了公众对卫生措施和疫苗接种的态度，导致更广泛的公共卫生挑战[数据：报告（920）]。</p><p> #### 地区差异</p><p>疫苗接种率因地区而异。例如，西弗吉尼亚州拥有美国最严格的学校疫苗接种政策之一，导致儿童免疫接种率很高[数据：报告（1199）]。相比之下，爱达荷州的儿童疫苗接种总体豁免率是美国最高的，构成了重大的公共卫生风险[数据：报告（998）]。伊利诺伊州面临学校疫苗接种率低的挑战，导致麻疹爆发[数据：报告 (108, 82)]。</p><p> #### 公共卫生举措</p><p>已经实施了各种公共卫生举措来解决疫苗接种率问题。 CDC 在监测和报告麻疹、腮腺炎和脊髓灰质炎等疾病的疫苗接种率方面发挥着至关重要的作用，并提供有关可预防疾病的重要信息 [数据：报告 (914)]。西弗吉尼亚州的“Do It For Babydog”抽奖活动等项目鼓励居民接种疫苗[数据：报告 (1199)]。此外，比尔及梅琳达·盖茨基金会积极参与资助和支持针对脊髓灰质炎、疟疾、艾滋病毒和霍乱等主要疾病的疫苗接种计划[数据：报告 (1091)]。</p><p> ＃＃＃ 结论</p><p>该数据集揭示了受区域政策、公共卫生举措和 COVID-19 大流行影响影响的疫苗接种率的复杂情况。虽然一些地区通过严格的政策和公共卫生运动设法保持了高疫苗接种率，但其他地区则因错误信息和豁免率而面临重大挑战。全球努力和合作对于应对这些挑战和提高全球疫苗接种覆盖率仍然至关重要。</p></td><td><p> ### 主要疾病疫苗接种率的常见趋势</p><p>#### 疫苗接种率下降</p><p>整个数据集中观察到的一个显着趋势是各种疾病的疫苗接种率下降，包括麻疹、腮腺炎、风疹 (MMR) 和脊髓灰质炎。<b>这种下降在美国幼儿园中尤为明显，疫苗接种覆盖率从2019-2020学年的95.2%下降到2022-2023学年的93.1%</b> 。这一减少低于实现群体免疫所需的 95% 阈值，每年使大约 250,000 名幼儿园儿童面临风险[数据：报告（5668、15469、15703、15481、15471，+更多）]。</p><p> #### COVID-19 大流行的影响</p><p>COVID-19 大流行对疫苗接种率产生了深远影响。推广 COVID-19 疫苗接种的努力一直是一个主要焦点，密歇根州和伊利诺伊州等地区的举措为学生提供免费餐点并推广更新的 COVID-19 疫苗 [数据：报告 (19125, 15466)]。然而，这一流行病也扰乱了常规疫苗接种计划，导致麻疹等疾病的覆盖率降低，并导致最近的疫情爆发[数据：报告（15464、15674、15482、15481，+更多）]。</p><p> #### 地区差异和立法措施</p><p>美国各地的疫苗接种率和豁免率存在显着的地区差异。例如，西弗吉尼亚州对入学儿童实施了严格的免疫要求，导致儿童免疫率较高[数据：报告（5674, 18874, 18899）]。相比之下，明尼苏达州和伊利诺伊州等州的非医疗豁免有所增加，导致疫苗接种率降低和最近的疫情爆发[数据：报告（15483、15481、108、2705，+更多）]。</p><p> #### 提高疫苗接种率的努力</p><p>已经采取了各种举措和立法措施来解决疫苗接种率下降的问题。例如，信德省政府提出了一项脊髓灰质炎疫苗接种法案，其中包括对不给孩子接种疫苗的父母进行监禁的条款[数据：报告（15398）]。在美国，CDC 建议 6 个月及以上的所有人接种新的 COVID-19 疫苗，并启动了确保公平获得疫苗的举措，特别是在发展中国家 [数据：报告 (15847, 15571, 15691, 15694, +更多)]。</p><p> ＃＃＃ 结论</p><p>该数据集揭示了受 COVID-19 大流行、疫苗犹豫、错误信息和地区差异影响的疫苗接种率的复杂情况。虽然提高疫苗接种率的努力仍在继续，但免疫覆盖率的下降带来了重大的公共卫生风险，凸显出需要持续保持警惕并采取积极措施，以确保高疫苗接种率并防止疫苗可预防疾病的爆发。 </p></td></tr></table><div style="height:10px" aria-hidden="true" class="wp-block-spacer"></div><p class="has-text-align-center">表 2. 通过静态搜索（级别 1）和动态搜索（级别 3）对美联社新闻数据集上的同一全局问题生成的响应。</p><p>表 2 显示了针对同一问题的第 1 级静态搜索和第 3 级动态搜索的示例输出。虽然这两个输出包含类似的高级主题，但动态搜索的响应提供了具体数据，例如某些人群的疫苗接种率下降。我们还注意到，动态搜索的响应明显更多地引用了源材料，如文本中的“[数据报告]”所示。通过选择性地提供与问题相关的信息，这减轻了映射缩减操作必须一次过滤和处理所有社区报告的负担，因此它可以生成更全面且针对用户问题的响应。</p><p>总体而言，动态社区选择提出了一种替代方法，通过利用索引知识图并在速率相关性操作中使用更便宜的 LLM 模型，在 GraphRAG 中执行全局搜索。这些变化降低了总代币成本，并可能改善响应细节和质量。</p><h2 class="wp-block-heading" id="availability">可用性</h2><div class="annotations " data-bi-aN="margin-callout"><ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right"><li class="annotations__list-item"> <span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">博客</span><a href="https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/" target="_self" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="Introducing DRIFT Search: Combining global and local search methods to improve quality and efficiency" data-bi-aN="margin-callout" data-bi-cN="Introducing DRIFT Search: Combining global and local search methods to improve quality and efficiency">DRIFT 搜索简介：结合全局和局部搜索方法提高质量和效率<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span></a></li></ul></div><p><strong>您可以在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/graphrag">GraphRAG GitHub 存储库<span class="sr-only">（在新选项卡中打开）</span></a>上尝试动态全局搜索。 <span data-contrast="auto" xml:lang="EN-US" lang="EN-US" class="TextRun EmptyTextRun SCXW249363172 BCX8" style="-webkit-user-drag: none; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; user-select: text; font-size: 12pt; line-height: 20.7px; font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, sans-serif; font-variant-ligatures: none !important;"></span></strong></p><p>动态全局搜索是正在探索的 GraphRAG 几项主要优化中的第二项。如果您对本地问题的优化感兴趣，请查看我们最近关于<a href="https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/" target="_blank" rel="noreferrer noopener">DRIFT 搜索</a>的博客文章。请继续关注我们即将开展的工作，我们将探索一种完全不同的图形支持 RAG 方法，该方法显着提高成本效益，同时提高本地和全局问题的答案质量。</p><div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/">GraphRAG：通过动态社区选择改进全局搜索</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>