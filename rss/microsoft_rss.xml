<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2023 年 11 月 17 日星期五 20:42:52 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.3.2</generator><item><title>大语言模型中的终身模型编辑：平衡低成本有针对性的编辑和灾难性遗忘</title><link/>https://www.microsoft.com/en-us/research/blog/lifelong-model-editing-in-large-language-models-balancing-low-cost-targeted-edits-and-catastropic-forgetting/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Mon, 20 Nov 2023 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=984255 </guid><description><![CDATA[<p>终身模型编辑修复模型部署后发现的错误。这项工作可以将顺序编辑扩展到公平性和隐私等模型属性，并实现一类新的解决方案，以适应 LLM 的长期部署生命周期。</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/lifelong-model-editing-in-large-language-models-balancing-low-cost-targeted-edits-and-catastrophic-forgetting/">大语言模型中的终身模型编辑：平衡低成本目标编辑和灾难性遗忘</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-full"><img decoding="async" fetchpriority="high" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1.png" alt="用 GRACE 进行终生模型编辑的插图。左边是一个问题以及模型的现有答案（这是不正确的）。编辑方法需要更新它的正确答案。中间显示了架构，其中语言模型被冻结，嵌入被提取以从码本中检索适当的值（新嵌入）。右侧显示了密码本，其中包括一组可训练的嵌入。" class="wp-image-984291" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-2023-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>大型语言模型 (LLM) 对于大量困难任务非常有用。但他们有时会犯不可预测的错误或使带有偏见的语言长期存在。由于底层数据或用户行为的变化，这些类型的错误往往会随着时间的推移而出现。这就需要对这些模型及其支持的实际应用程序进行有针对性的、具有成本效益的修复。</p><p>可以使用重复的预训练或微调来实现这些修复。然而，这些解决方案的计算成本往往太高。 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/2302.13971.pdf" target="_blank" rel="noreferrer noopener">例如<span class="sr-only">（在新选项卡中打开）</span></a> ，LLAMA 1 在 2,048 个 A100 GPU 上训练了 21 天，成本超过 240 万美元。微调法学硕士需要比许多研究实验室能够持续且经济地访问的更大的 GPU。另外，在很大程度上仍然不知道应该在数据语料库中添加或删除哪些数据以纠正特定行为而不影响不相关的输入。</p><p>为了让法学硕士在不需要昂贵培训的情况下保持最新状态，<em>模型编辑</em>最近被提议作为对大型模型进行有针对性的更新的范例。大多数模型编辑器更新模型<em>一次</em>，注入一批修正。但随着时间的推移，错误往往会逐渐被发现，并且必须迅速纠正。换句话说，在部署模型时，<em>终身</em>模型编辑至关重要，其中遇到一系列错误并且必须立即解决。这需要按顺序进行多次编辑，已知现有编辑器在这种情况下会失败。这里的成功意味着按顺序纠正所有编辑，不会忘记旧的修复，也不会降低不相关输入的性能。但<em>编辑</em>到底是什么？在<a href="https://www.microsoft.com/en-us/research/publication/aging-with-grace-lifelong-model-editing-with-discrete-key-value-adaptors/" target="_blank" rel="noreferrer noopener">优雅地老化：使用离散键值适配器进行终身模型编辑中，</a>考虑了三种类型的编辑：</p><ol type="1"><li><em>更新事实知识</em>。假设我们有一个预先训练的问答模型：我们传入问题，模型返回答案。但随着世界的变化，这些答案变得过时了。例如，回答“谁是美国总统？”选举后应该改变。因此，编辑是一个元组 - 或值的有序序列 - 包含问题（例如，“谁是美国总统？”）和该问题的正确答案（例如，“拜登”）。</li><li><em>跟上翻转标签的步伐</em>。分类任务中的基本事实可能会随着时间而改变。例如，当美国法院使用新语言描述现有主题时，文档的正确标签可能会发生变化。在这种情况下，必须纠正在旧标签上训练的模型。当仅重新标记特定类型的数据时（这种情况很常见），有针对性的编辑尤其重要。在这种情况下，编辑是配对输入（例如，法庭文件）和新标签（例如，主题）。</li><li><em>减少法学硕士中的捏造和不连贯性</em>。使用法学硕士的一个关键挑战是避免它们生成不符合现实的语言的情况。但这种情况在某些模型中可能比其他模型更常见。因此，当它确实发生时，随后的编辑应该尽可能小。为了探索这种方法的有效性，研究人员考虑在生成名人传记时缓解这个问题。在识别出手工注释的捏造行为后，他们编辑法学硕士，以从真实的维基百科文章中生成相应的句子。在这种情况下，编辑是一个提示和相应的响应，现有模型认为这是不可能的。</li></ol><figure class="wp-block-image aligncenter size-full"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig1.png"><img decoding="async" width="2155" height="520" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig1.png" alt="该图显示了所提议方法的概述。左侧显示一个问题（最近的流行病是什么？）以及模型的现有答案（猪流感），这是一个错误答案，编辑方法需要将其更新为正确答案（COVID）。中间显示了架构，其中语言模型被冻结，嵌入被提取以从码本中检索适当的值（新嵌入）。右侧显示了密码本，其中包括一组可训练的嵌入。" class="wp-image-984267" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig1.png 2155w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig1-300x72.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig1-1024x247.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig1-768x185.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig1-1536x371.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig1-2048x494.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig1-240x58.png 240w" sizes="(max-width: 2155px) 100vw, 2155px" /></a><figcaption class="wp-element-caption"><strong>图 1.</strong>使用 GRACE 进行终身模型编辑的概述。模型会犯一些必须纠正的重要错误。因此，GRACE 通过学习、缓存和有选择地检索层之间的新转换来进行编辑。经过长时间的编辑（偶尔出现并需要快速修复），GRACE 密码本会不断增长和适应。</figcaption></figure><p>为了对法学硕士进行经济有效的编辑，我们提出了一种称为连续编辑通用检索适配器（GRACE）的方法。 GRACE 是第一种仅使用流错误即可对任何预训练模型架构进行数千次顺序编辑的方法。这种方法简单而有效：当您想要编辑模型以确保其为输入输出选定的标签时，只需在模型中选择一个层并在该层选择一个嵌入作为输入的嵌入即可。作为示例，可以使用模型第四层计算的输入句子中最终标记的嵌入。然后，缓存此嵌入并学习新的嵌入，这样如果用新的嵌入替换旧的嵌入，模型就会产生所需的响应。原始嵌入称为<em>键</em>，学习的嵌入称为<em>值。</em>通过梯度下降学习该值很简单。然后，键和值存储在<em>代码本</em>中，该代码本充当字典。如果您随后将新输入传递给模型，在计算其嵌入（称为<em>查询）</em>后，可以将新查询与现有键进行比较。如果查询与某个键匹配，则可以查找该值并应用编辑。随着许多编辑的流入，它们可以简单地添加到密码本中，顺序应用许多编辑。</p><figure class="wp-block-image aligncenter size-full"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/main_table-1.png"><img decoding="async" width="2920" height="977" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/main_table-1.png" alt="带有四个主要列标记的表格"Method", "zsRE", "SCOTUS", and "Hallucination". The "Method" column contains the names of methods we compare against. The last three columns are the names of datasets and each has an associated model. The first is T5, the second is BERT, and the third is GPT2-XL. Each dataset also contains a set of metrics: Edit Retention Rate, Test Retention Rate, the average of Edit and Test Retention Rates, and the number of edits made. The Hallucination dataset contains two extra metrics, which are the Accurate Retention Rate and Inference Time. We compare against seven baselines, which are each shown in a row. The methods in order are Finetune, Finetune with Elastic Weight Consolidation, Finetune with Retraining, MEND, Defer, ROME, Memory, and our method GRACE. For each dataset, GRACE outperforms the comparisons significantly, especially when considering the average of the Edit and Test Retention Rates, which measures the balance between these conflicting goals. The other methods target one or the other, failing to balance. We also show that making edits with GRACE is fast compared to most other methods. " class="wp-image-985347" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/main_table-1.png 2920w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/main_table-1-300x100.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/main_table-1-1024x343.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/main_table-1-768x257.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/main_table-1-1536x514.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/main_table-1-2048x685.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/main_table-1-240x80.png 240w" sizes="(max-width: 2920px) 100vw, 2920px" /></a><figcaption class="wp-element-caption"><strong>表 1.</strong> GRACE 成功编辑模型，而不会忘记以前的编辑或不相关的训练数据，从而优于现有的模型编辑器。在 zsRE 和 SCOTUS 数据集上，GRACE 实现了大幅压缩。在 Hallucination 数据集上，GRACE 成功地将未来很长的标记序列嵌入到缓存值中。</figcaption></figure><p>但这不就是记忆吗？如何在不记住每个新输入的情况下实现通用编辑？每个新键都与一个<em>影响半径</em>配对，而不是总是添加新键，影响半径是一个围绕任何新键的半径为 ε 的球。然后，如果<em>任何</em>查询落在该 ε 球内，则会检索该键的相应值并应用编辑。因此，与任何缓存编辑<em>类似</em>的输入也将被更新。有时，在创建新钥匙时，其 ε 球可能会与另一个钥匙发生冲突。在这种情况下，当冲突的键具有<em>不同的</em>值时，它们的 ε 球被设置为几乎不接触。如果它们具有<em>相同的</em>值，则现有密钥的 ε 会增加以包含新输入。调整 ε 有助于实现可通用的小型密码本，并且可以成功地连续进行数千次编辑。</p><p>为了将 GRACE 的能力与现有方法进行可概括编辑的能力进行比较，使用了两种双向模型（T5 和 BERT）和一种自回归模型（GPT2-XL）。对于问答 (QA)，T5 与<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/1706.04115.pdf" target="_blank" rel="noreferrer noopener">QA 数据集<span class="sr-only">（在新选项卡中打开）</span></a>一起使用，其中包括针对关系提取的问题。每个问题都有 20 个重新表述的版本，其中 10 个在编辑过程中使用，另外 10 个作为看不见的保留版本。在连续纠正 1,000 个编辑时，所提出的方法显示出比现有方法更好的性能，如表 1 所示。它<em>仅使用 137 个键</em>进行编辑，这显示了所提出方法的效率。这种水平的概括比以前的工作更好，并且显示出纠正未来错误的巨大潜力。所提出的方法还可以成功编辑 BERT 模型，该模型在 1992 年之前<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="http://supremecourtdatabase.org/" target="_blank" rel="noreferrer noopener">的美国最高法院文件<span class="sr-only">（在新选项卡中打开）</span></a>上进行训练，并在 1992 年之后标签分布发生变化的文件上进行测试。还使用 GRACE 和自回归模型 GPT2-XL 进行了一项实验，以编辑与制造相关的错误，这有望鼓励长序列编辑。例如，当被要求生成 Brian Hughes 的传记时，GRACE 成功地鼓励 GPT2-XL 做出回应：“Brian Hughes（出生于 1955 年）是一位加拿大吉他手，其作品借鉴了流畅的爵士乐和世界音乐流派”，这与<em>仅使用一个缓存值来</em>请求传记。另一个有趣的观察是，GRACE 编辑对于编辑图层的选择是稳健的，尽管<em>后面的图层更难编辑</em>。此外，在选择 ε 时，在记忆和泛化之间观察到了明显的平衡，如图 2 所示。最后，GRACE 的一个关键特征是<em>码本与预训练模型分离，其权重保持不变</em>。这有助于随时撤消任何编辑，并且还可以检查编辑的行为，而无需高昂的计算成本。</p><figure class="wp-block-image aligncenter size-full"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig2.png"><img decoding="async" loading="lazy" width="2145" height="1175" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig2.png" alt="包含八个子图的图形，显示为两行四列。每行代表一个 epsilon 值，它是我们提出的方法中控制泛化的超参数。第一行显示 epsilon 为 0.1，第二行显示 epsilon 为 0.2。每列显示不同指标的折线图。每行显示了使用 zsRE 数据集对 T5 QA 模型进行 3,000 次连续编辑后指标如何变化。每个图包含四行；每行用于编辑不同的 T5 块。我们比较对块 0、2、4 和 6 所做的编辑。从左列开始，我们考虑 TRR 指标，它衡量编辑后原始测试数据的模型准确性。对于 0.1 的 epsilon，TRR 指标始终保持在 0.72，每个块没有差异。对于 3.0 的 epsilon，TRR 指标仅对于块 6 保持在 0.72，并且对于块 0 最低，在编辑结束时降至 0.7 以下。第二列显示 ERR 指标，即每个步骤先前编辑的准确性。在这里我们看到，对于 0.1 的 epsilon，块 2、4 和 6 仍然保持在接近 1.0 的高位。对于 3.0 的 epsilon，块 6 仍然很高，而其他块则下降到 0.9 左右。第三列显示了未见保留编辑的保留表现，这些编辑是已见编辑的改写。每次编辑后，我们都会对编辑后的模型运行所有保留编辑，并记录其在整个集合上的准确性。因此，在这两个图中，我们看到性能随着时间的推移而提高，因为编辑慢慢地覆盖了保留集的更多改写。通过这种方式，我们可以衡量 GRACE 的泛化能力。我们看到，对于 0.1 的 epsilon，块 6 的泛化能力略好于其他块。但对于 3.0 的 epsilon，Block 6 的表现明显低于其他方法。块 0 稍好一些，块 2 和 4 好得多。在最后一栏中，我们报告了 GRACE 进行所有 3,000 次编辑所使用的按键数量。在这里我们看到块 6 只是记住所有编辑，因为它的键数量线性增长。经过 3,000 次编辑后，共有 3,000 个键。但对于块 0、2 和 4，该值会饱和，并且使用更少的键进行编辑。当 epsilon 为 0.1 时，这些块使用大约 2,000 个键。当 epsilon 为 3.0 时，块 0 使用大约 1,000 个密钥，而块 2 和 4 使用大约 800 个密钥。这说明了选择块和 epsilon 如何影响记忆和泛化之间的权衡。总体而言，似乎可概括的编辑发生在内部模型层中，而不是第一层或最后一层以及稍大的 epsilon 选择。" class="wp-image-984282" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig2.png 2145w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig2-300x164.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig2-1024x561.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig2-768x421.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig2-1536x841.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig2-2048x1122.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/GRACE-Fig2-240x131.png 240w" sizes="(max-width: 2145px) 100vw, 2145px" /></a><figcaption class="wp-element-caption"><strong>图 2.</strong>针对不同 epsilon 选择编辑 T5 模型的不同模块时 GRACE 的性能。这种选择推动了无关训练数据 (TRR) 和先前编辑 (ERR) 的准确性之间的平衡，如小 epsilon (a) 和大 epsilon (b) 所示。</figcaption></figure><h2 class="wp-block-heading" id="summary-1">概括</h2><p>GRACE 为模型编辑提供了不同的视角，其中直接修改表示并按顺序缓存转换。编辑可以连续进行数千次，在整个编辑过程中维护一小组密码本。此步骤缩小了实际应用程序部署需求的差距，其中编辑会随着时间的推移而被发现，并应以经济高效的方式解决。通过有效地纠正行为并将顺序编辑扩展到其他模型属性（例如公平性和隐私性），这项工作有可能实现一类新型解决方案，用于调整 LLM 以满足长期部署生命周期内的用户需求。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/lifelong-model-editing-in-large-language-models-balancing-low-cost-targeted-edits-and-catastrophic-forgetting/">大语言模型中的终身模型编辑：平衡低成本目标编辑和灾难性遗忘</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>;</content:encoded></item><item><title>思想框架：并行解码加速并改进 LLM 输出</title><link/>https://www.microsoft.com/en-us/research/blog/sculpture-of-thought-parallel-decoding-speeds-up-and-improves-llm-output/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Fri, 17 Nov 2023 19:07:15 +0000</pubDate> <category><![CDATA[Research Blog]]></category><guid ispermalink="false"></guid><description><![CDATA[<p> LLaMA 和 OpenAI 的 GPT-4 等大型语言模型 (LLM) 正在彻底改变技术。然而，对法学硕士的常见抱怨之一是其速度或缺乏速度。很多情况下，需要很长时间才能得到他们的答复。这限制了法学硕士的应用及其在延迟关键功能中的有用性，例如聊天机器人、副驾驶，[…]</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/skeleton-of-thought-parallel-decoding-speeds-up-and-improves-llm-output/">《思想框架：并行解码加速并改进 LLM 输出》一文</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1.png" alt="该图显示了正常顺序解码方法和思维骨架方法之间的差异。给定一个问题，图的左侧部分显示正常的顺序解码方法从头到尾顺序生成答案。图中右半部分显示，Skeleton-of-Thought方法首先提示LLM给出答案的骨架，然后并行扩展骨架中的多个点以获得最终答案。" class="wp-image-984846" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p> LLaMA 和 OpenAI 的 GPT-4 等大型语言模型 (LLM) 正在彻底改变技术。然而，对法学硕士的常见抱怨之一是其<em>速度</em>或缺乏速度。很多情况下，需要很长时间才能得到他们的答复。这限制了法学硕士的应用及其在延迟关键功能（例如聊天机器人、副驾驶和工业控制器）中的有用性。 </p><div class="annotations " data-bi-aN="margin-callout"><ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right"><li class="annotations__list-item"> <span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">出版物</span><a href="https://www.microsoft.com/en-us/research/publication/skeleton-of-thought-large-language-models-can-do-parallel-decoding/" target="_blank" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding" data-bi-aN="margin-callout" data-bi-cN="Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding">的思想框架：大型语言模型可以进行并行解码<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span></a></li></ul></div><p>为了解决这个问题，微软研究院和清华大学的研究人员提出了一种加速LLM生成的新方法<a href="https://www.microsoft.com/en-us/research/publication/skeleton-of-thought-large-language-models-can-do-parallel-decoding/" target="_blank" rel="noreferrer noopener">——Skeleton-of-Thought（SoT）</a> 。与大多数需要对 LLM 模型、系统或硬件进行修改的现有方法不同，SoT 将 LLM 视为黑匣子，因此可以应用于任何现成的开源（例如 LLaMA）甚至基于 API（例如，OpenAI 的 GPT-4）模型。我们的评估表明， <em>SoT 不仅大大加快了所审查的 12 个法学硕士的内容生成速度，而且在某些情况下还可能提高答案质量。</em>例如，在 OpenAI 的 GPT-3.5 和 GPT-4 上，SoT 提供了 2 倍的加速，同时提高了基准数据集的答案质量。</p><p>我们的代码和演示在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/imagination-research/sot/" target="_blank" rel="noreferrer noopener">https://github.com/imagination-research/sot/ 上开源<span class="sr-only">（在新选项卡中打开）</span></a> 。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="934" height="468" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT_Figure1_compressed.gif" alt="SoT - 图 1：正常顺序解码方法和思维框架方法演示的屏幕录制，用于回答“如何改进我的时间管理技术？”这一问题。左图显示了正常顺序解码的生成过程，答案是逐字生成的。右图展示了思维骨架的生成过程，答案中的多个点是并行生成的，因此速度更快。两种方法生成完成后，都会显示两种方法的生成时间，这表明 Skeleton-of-Thought 提供了 3.72 倍的加速比。" class="wp-image-984804"/><figcaption class="wp-element-caption">图 1：与普通方法（左）相比，SoT（右）在回答以下问题时速度提高了 3.72 倍：“我如何改进我的时间管理技术？”在一个 NVIDIA A100 GPU 上使用 LLaMA-2-7b 模型。</figcaption></figure><h2 class="wp-block-heading" id="sot-encouraging-structured-thinking-in-llms">SoT：鼓励法学硕士的结构化思维</h2><p>SoT 的想法源于法学硕士和人类处理信息方式的差异。法学硕士<em>按顺序</em>生成答案。例如，回答<em>“我怎样才能提高我的时间管理技巧？”</em>在图 1（左）中，法学硕士在进入下一分之前完成了一个分。相比之下，人类可能并不总是按顺序思考问题并写出答案。在许多情况下，人类首先得出答案的骨架，然后添加细节来解释每个点。例如，要回答图 1 中的同一问题，人们可能首先考虑一系列相关的时间管理技术，然后再深入研究每种技术的细节。对于提供咨询、参加测试、撰写论文等练习尤其如此。</p><p>我们能让法学硕士更加动态地、更少地线性地处理信息吗？如图 2 所示，SoT 发挥了作用。 SoT 不是按顺序生成答案，而是将生成过程分解为两个阶段：(1) SoT 首先要求 LLM 导出答案的骨架，然后 (2) 要求 LLM 提供骨架中每个点的答案。该方法提供了新的加速机会，因为<em>可以并行生成第 2 阶段中各个点的答案。这可以针对用户可以访问其权重的本地模型（例如，LLaMA）和只能通过 API 访问的基于 API 的模型（例如，OpenAI 的 GPT-4）来完成。</em></p><ul><li>对于基于 API 的模型，我们可以发出<em>并行</em>API 请求，每个点一个。</li><li>对于本地运行的模型，我们可以批量<em>同时</em>回答所有点。请注意，在许多场景（例如，本地服务、不饱和查询周期内的集中式服务）中，LLM 的解码阶段通常受到权重加载而不是激活加载或计算的瓶颈，因此未充分利用可用的硬件。在这些情况下，以增加的批量大小运行 LLM 推理可以提高硬件计算利用率，并且不会显着增加延迟。</li></ul><p>因此，如果答案中有<em>B 个</em>点，与当前 LLM 中的顺序生成相比，SoT 中并行生成这些点理论上可以放弃<em>B</em> x 加速。然而，在实践中，由于额外的骨架阶段、不平衡的点长度和其他开销，实际的加速可能会更小。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1400" height="882" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2.png" alt="该图显示了正常顺序解码方法和思维骨架方法之间的差异。给定一个问题，图的左侧部分显示正常的顺序解码方法从头到尾顺序生成答案。图中右半部分显示，Skeleton-of-Thought方法首先提示LLM给出答案的骨架，然后并行扩展骨架中的多个点以获得最终答案。" class="wp-image-983661" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2-300x189.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2-1024x645.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2-768x484.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure2-240x151.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">图 2：思想骨架 (SoT) 的图示。 SoT（右）不是按<em>顺序</em>生成答案（左），<em>而是并行</em>生成答案的不同部分。给定问题，SoT首先提示LLM给出答案的骨架，然后进行批量解码或并行API调用以同时扩展多个点以获得最终答案。</figcaption></figure><p>我们在 12 个最近发布的模型上测试了 SoT，其中包括 9 个开源模型和 3 个基于 API 的模型，如表 1 所示。我们使用 Vicuna-80 数据集，其中包含 80 个问题，涵盖九个类别，例如编码、数学、写作、角色扮演等等。有关更多数据集和指标的结果，请参阅论文： <a href="https://www.microsoft.com/en-us/research/publication/skeleton-of-thought-large-language-models-can-do-parallel-decoding/">Skeleton-of-Thought：Large Language Models Can Do Parallel Decoding</a> 。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1400" height="533" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1.png" alt="SoT - 表 1：列出了本文中评估的模型的表格。有四列：（1）访问权限，即模型是开源的还是基于API的，（2）模型名称，（3）开发模型的机构，（4）模型的发布日期。因此表中有12行，对应9个开源模型和3个基于API的模型。" class="wp-image-983676" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1-300x114.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1-1024x390.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1-768x292.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Table1-240x91.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">表 1：评估模型列表。</figcaption></figure><p>图 3 显示 SoT 在所有模型中都提供了相当大的加速。特别是，SoT 在 12 个模型中的 8 个模型上获得了 >;2 倍的加速（高达 2.39 倍）。此外，SoT 实现了这种水平的加速，而没有显着降低答案质量。图 4 显示了 SoT 的获胜/平局/失败率（定义为 SoT 提供比正常顺序生成更好的答案的问题的比例；“更好”是由<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/lm-sys/FastChat" target="_blank" rel="noreferrer noopener">FastChat <span class="sr-only">（在新选项卡中打开）</span></a>和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/FreedomIntelligence/LLMZoo" target="_blank" rel="noreferrer noopener"><span class="sr-only">LLMZoo</span></a> （在新选项卡中打开）中提出的指标定义的。 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/FreedomIntelligence/LLMZoo" target="_blank" rel="noreferrer noopener"><span class="sr-only">在新选项卡中打开）</span></a> ，并由 GPT-4 法官进行评估）。我们可以看到 SoT 的答案质量与连续生成的结果相当。在论文中，我们进一步表明，由于骨架阶段明确要求法学硕士提前规划答案结构，SoT 在与问题的相关性和跨多个方面的全面性方面提高了答案质量。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1400" height="513" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3.png" alt="SoT - 图 3：条形图显示不同模型上思维框架的平均加速。每个条对应一个型号。该图显示，Skeleton-of-Thought 为所有模型提供了加速。加速范围为 1.13 倍至 2.39 倍。" class="wp-image-983664" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3-300x110.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3-1024x375.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3-768x281.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure3-240x88.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">图 3：不同模型上 SoT 的平均加速比。 SoT 为我们评估的所有模型提供了加速。 </figcaption></figure><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1131" height="293" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned.png" alt="SoT - 图 4：该图显示了使用来自 FastChat 和 LLMZoo 的指标与正常顺序生成相比，Skeleton-of-Thought 的获胜/平局/失败率。对于 FastChat 指标，获胜/平局/失败率分别为 29.5%、29.3% 和 41.2%。对于 LLMZoo 指标，获胜/平局/失败率分别为 45.8%、19.6% 和 34.5%。总之，在大约 60% 的情况下，思想框架的表现优于或等于正常的顺序生成。" class="wp-image-983667" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned.png 1131w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned-300x78.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned-1024x265.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned-768x199.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure4-patterned-240x62.png 240w" sizes="(max-width: 1131px) 100vw, 1131px" /><figcaption class="wp-element-caption">图 4：使用来自 FastChat 和 LLMZoo 的指标，SoT 与正常生成的获胜/平局/失败率。在大约 60% 的情况下，SoT 的性能优于或等于正常发电。 </figcaption></figure><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">焦点：微软研究通讯</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">微软研究院通讯</h2><p class="large">与 Microsoft 研究社区保持联系。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button is-style-fill-chevron"> <a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">立即订阅</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="sot-r-adaptively-triggering-sot">SoT-R：自适应触发 SoT</h2><p> SoT对点进行独立并行的扩展。因此，它不适合需要逐步推理的问题，例如数学和编码。为此，我们提出了一种名为<em>SoT with Router (SoT-R)</em>的 SoT 扩展，仅在合适时自适应地触发 SoT。更具体地说，我们提出了一个路由器模块，该模块决定是否应将 SoT 应用于用户请求，然后我们相应地调用 SoT 或正常顺序解码。路由器模块可以通过在没有任何模型训练的情况下提示OpenAI的GPT-4（称为“<em>提示路​​由器”</em> ）或训练指定的RoBERTa模型（称为“<em>训练路由器”</em> ）来实现。我们表明，SoT-R 提高了 SoT 跨问题类别的通用性（图 5），同时保持相当大的加速（图 6）。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1400" height="538" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5.png" alt="SoT - 图 5：该图显示了 SoT 和 SoT-R 在 Vicuna-80 数据集上不同问题类别上的净胜率（定义为胜率减去输率）。对于不适合 SoT 的问题类别（例如编码、数学），SoT-R 的净胜率约为 0，因为 SoT-R 学会回退到正常生成模式。对于适合 SoT 的问题类别（例如，通用问题、反事实问题），SoT-R 具有与 SoT 类似的净胜率，并且 SoT-R 按预期触发 SoT。总体而言，SoT-R 改进了 SoT，并为所有问题类别保持了良好的答案质量。" class="wp-image-983670" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5-300x115.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5-1024x394.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5-768x295.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure5-240x92.png 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">图 5：SoT 和 SoT-R 在 Vicuna-80 上不同问题类别上的净胜率（定义为胜率减去输率 - 越高越好）。 “人类路由器”是指使用人类偏好来决定每个问题是否应该应用 SoT 的预言机。对于不适合 SoT 的问题类别（例如编码、数学），SoT-R 学习回退到正常生成模式。因此，SoT-R 可以对所有问题类别保持良好的答案质量。 SoT-R 甚至可以超越人类路由器（例如，在角色扮演问题上）。 </figcaption></figure><figure class="wp-block-image aligncenter size-full"><img decoding="async" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/SoT-blog_Figure6.png" alt="SoT - 图 6：该图显示了 SoT 和 SoT-R 在不同型号的 Vicuna-80 上的加速情况。虽然 SoT-R 的加速比小于 SoT，但对于大多数模型来说，SoT-R 仍然可以保持 >;1 的加速比。" class="wp-image-983673"/><figcaption class="wp-element-caption">图 6：SoT 和 SoT-R 在不同型号的 Vicuna-80 上的加速效果。对于大多数模型，SoT-R 可以保持 >;1 的加速比。</figcaption></figure><h2 class="wp-block-heading" id="limitations-and-next-steps">限制和后续步骤</h2><h3 class="wp-block-heading" id="reducing-the-cost-of-sot">降低 SoT 成本</h3><p>与正常的顺序解码相比，SoT 使用更长的提示，这可能会导致 LLM API 的成本更高（按提示的长度收费），并可能降低服务系统的吞吐量。需要进一步研究来探索降低 SoT 的成本，包括压缩 SoT 提示或调整 LLM 以在必要时自动触发 SoT。</p><h3 class="wp-block-heading" id="improving-llm-capability">提高LLM能力</h3><p>SoT 的灵感来自于人类的结构化思维。因此，它的成功值得进一步研究人类思维过程，从而促进更有效和高效的人工智能。</p><p>鉴于人类思维的复杂性（我们可能会将其想象为更多的图表），SoT 可以被视为迈向“思维图表”（GoT）的过渡步骤。 GoT 是一个新框架，旨在代表更复杂的思维，更接近人们解决问题的方式。 GoT 呈现了以图结构连接的多个概念，其中图的边代表依赖关系，每个点根据其祖先点的内容进行解码。此外，我们期望需要一个动态的思维图，而不是遵守静态图，其中高层思维结构由法学硕士自己动态调整，试图模仿人类的思维方式。这可能会结合 SoT 的效率和全局思维优势，同时捕获思维过程中的更多复杂性。</p><p>另一个需要研究的问题是，如何使用 SoT 提供的更结构化的答案来微调法学硕士，以增强他们生成组织良好且全面的答案的能力。</p><h3 class="wp-block-heading" id="data-centric-efficiency-optimization">以数据为中心的效率优化</h3><p>与现有的模型和系统级提高 LLM 效率的努力相比，SoT 通过让 LLM 组织其输出内容，采取了一种新颖的“数据级”途径。由于最先进的法学硕士能力的不断发展，这种观点变得可行且越来越重要。我们希望这项工作能够激发以数据为中心的效率优化领域的更多研究。</p><p>如果您想讨论这项研究以及该主题的潜在合作，请随时与我们联系。</p><div class="wp-block-buttons"><div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button">联系我们</a></div></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/skeleton-of-thought-parallel-decoding-speeds-up-and-improves-llm-output/">《思想框架：并行解码加速并改进 LLM 输出》一文</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>