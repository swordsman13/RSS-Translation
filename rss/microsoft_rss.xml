<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2024 年 11 月 22 日星期五 21:20:32 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.6.2</generator><item><title>松下康幸简介：在微软亚洲研究院（东京）利用人工智能应对社会挑战</title><link/>https://www.microsoft.com/en-us/research/blog/introducing-yasuyuki-matsushita-tackling-societal-challenges-with-ai-at-microsoft-research-asia-tokyo/<dc:creator><![CDATA[Alyssa Hughes (2ADAPTIVE LLC dba 2A Consulting)]]></dc:creator><pubDate> Mon, 18 Nov 2024 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1104399 </guid><description><![CDATA[<p>松下康幸重新加入微软，领导新的微软亚洲研究院 - 东京实验室。详细了解他的旅程以及他对东京实验室在人工智能进化中的作用的看法。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/introducing-yasuyuki-matsushita-tackling-societal-challenges-with-ai-at-microsoft-research-asia-tokyo/">松下康之介绍：在微软亚洲研究院（东京）利用 AI 应对社会挑战</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p>今年早些时候，微软研究院<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/apac/2024/04/10/microsoft-to-invest-us2-9-billion-in-ai-and-cloud-infrastructure-in-japan-while-boosting-the-nations-skills-research-and-cybersecurity/?msockid=397efc5596ce69550b73ee32976668d9">宣布<span class="sr-only">（在新选项卡中打开）</span></a>其位于日本东京的最新实验室。今天，我们庆祝其盛大开业，加强了微软研究院对亚太地区人工智能研究的承诺。这个新实验室将专注于具体人工智能、福祉和神经科学、社会人工智能和行业创新——所有领域都符合日本的社会经济优先事项。该举措将加强与当地学术和工业合作伙伴的合作，为全球创新和人才发展做出贡献。</p><p>我们最近采访了新成立的东京实验室负责人 Yasuyuki Matsushita。松下幸之助于 2003 年至 2015 年在微软亚洲研究院工作，过去十年在大阪大学担任教授，于 10 月回国。他回顾了自己的旅程、技术的演变以及微软亚洲研究院（东京）面临的机遇。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img fetchpriority="high" decoding="async" width="678" height="1017" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/image.jpeg" alt="Yasuyuki Matsushita，微软亚洲研究院高级首席研究经理 - 东京" class="wp-image-1104402" style="width:459px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/image.jpeg 678w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/image-200x300.jpeg 200w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/image-120x180.jpeg 120w" sizes="(max-width: 678px) 100vw, 678px" /><figcaption class="wp-element-caption"> Yasuyuki Matsushita，微软亚洲研究院 - 东京</figcaption></figure><h2 class="wp-block-heading h3" id="why-return-to-microsoft-research-asia">为何重返微软亚洲研究院？</h2><p><strong>问：</strong>我们很高兴您领导东京的新实验室。您于 2003 年至 2015 年在北京的微软亚洲研究院工作，之后转入学术界。是什么促使您在近十年后回归？</p><p><strong>松下康之：</strong>微软亚洲研究院一直是开展前沿研究的特殊场所，尤其是在人工智能时代。今年早些时候，我了解到微软亚洲研究院的扩张，包括在东京建立一个新实验室。这提供了一个令人兴奋的机会，可以在当地和全球范围内产生有意义的影响，激发了我返回的动力。此外，微软处于人工智能进步的最前沿，这使得现在成为重新参与的理想时机。我相信我的工作能够为这个充满活力的领域做出有意义的贡献。当今人工智能的发展速度是无与伦比的，因此这是一个令人兴奋的参与时刻。</p><h2 class="wp-block-heading h3" id="what-has-changed-over-the-decade">十年来发生了什么变化？</h2><p><strong>问：</strong>您回来已经几周了，从您的角度来看，微软亚洲研究院发生了哪些变化，与您上次来时相比，哪些方面保持不变？</p><p> <strong>Yasuyuki Matsushita：</strong>我注意到的最直接的变化是一系列员工工具和资源，这些工具和资源在过去十年中发生了显着变化。我仍在熟悉这些旨在优化效率和协作的新系统。过去十年，微软在推动其他公司数字化转型方面发挥了关键作用，它内部也进行了转型。</p><p>除了这些变化之外，微软亚洲研究院的独特之处在很大程度上仍然没有改变。文化和人民继续营造创新和协作的环境。该组织仍然吸引着杰出的人才，研究精神一如既往地充满活力。其最大的优势之一是开放、协作的方式。它与大学和研究机构保持着长期的合作伙伴关系，鼓励跨地区、跨文化、跨学科的交流。这种协同效应刺激创新并支持行业发展。对卓越的承诺仍然是微软亚洲研究院身份的核心。</p><h2 class="wp-block-heading h3" id="plans-for-the-microsoft-research-asia-tokyo-lab">微软亚洲研究院东京实验室规划</h2><p><strong>问题</strong>：随着微软亚洲研究院向东京、温哥华、新加坡和香港等地区扩张，您作为东京实验室负责人有何计划？您如何看待它对该地区创新生态系统的贡献？</p><p> <strong>Yasuyuki Matsushita：</strong>我的首要目标是将东京实验室的发展与微软亚洲研究院推进科学技术造福人类的使命结合起来。我们在该实验室重点开展的研究工作旨在解决紧迫的社会问题，同时推进人工智能技术造福整个社会。</p><p>例如，日本的人口老龄化带来了独特的挑战，需要有效的社会解决方案——这是当今许多国家面临的问题。通过我们的研究，我们的目标是产生可在全球范围内应用的见解，以主动应对和缓解此类挑战。</p><p>日本在电子、材料科学和机器人等领域也拥有强大的科学研究遗产。其先进的产业基础，包括汽车、电子、机械领域的知名企业，为我们的研究成果提供了丰富的应用场景。此外，日本健全的教育体系为我们的深入研究提供了至关重要的智力基础。</p><p>我们致力于维持开放的研究实践。通过发布我们的研究结果并开源我们的工具，我们确保我们的工作惠及更广泛的行业并丰富全球知识库。我们的目标是分享推动全球社会进步和创新的见解。</p><h2 class="wp-block-heading h3" id="cultivating-the-next-generation">培养下一代</h2><p><strong>问：</strong>人才是微软研究院使命和文化的核心。微软东京亚洲研究院正在寻找什么样的人才？东京实验室可以通过哪些方式加强努力，为该地区培养下一代技术创新者？</p><p> <strong>Yasuyuki Matsushita：</strong>加入 Microsoft 的主要优势之一是我们与实际应用程序的紧密联系。研究与实践之间的这座桥梁使我们的工作能够产生直接的社会影响，确保创新技术产生有意义和有益的成果。</p><p>在招聘新人才时，我们寻找聪明、自我驱动、具有天生的好奇心和解决社会挑战的热情的人才。我们寻找的最重要的特质是渴望了解复杂问题背后的“原因”。虽然技术专业知识至关重要，但致力于解决社会问题可以激发创造力并推动有意义的进步。这种好奇心和目标的结合激发了创新，推动我们在微软亚洲研究院不断前进。</p><p>在东京实验室，我们愿景的核心部分是培养下一波技术创新者。我们计划继承微软亚洲研究院在整个地区倡导的成功人才计划，例如联合研究计划、访问学者计划和实习机会。这些为早期职业专业人士和学生提供了宝贵的实践经验，使他们具备必要的研究技能，并加深了他们对复杂技术挑战的理解。</p><p>我们致力于创造一个培育环境，让人才能够茁壮成长、协作并为全球科技格局做出贡献。通过将创新与现实世界的影响相结合，我们的目标是激励下一代突破界限并推动社会进步。</p><h2 class="wp-block-heading h3" id="rapid-evolution-in-computer-vision">计算机视觉的快速发展</h2><p><strong>问：</strong>当今世界，一切都在走向数字化、智能化。十年前，您的研究重点是光度测定和视频分析。您能否分享这一时期的一些关键成果，并解释您如何看待人工智能等新兴技术对计算机视觉领域的影响？</p><p> <strong>Yasuyuki Matsushita：</strong>当时，我的研究集中在计算机视觉，特别是用于 3D 重建的光度测量和旨在提高视频质量的视频分析。那个时期最突出的项目之一是开发能够捕获高分辨率 3D 信息的十亿像素相机。该相机在<a href="https://www.microsoft.com/en-us/research/video/eheritage-program-collaboration-with-dunhuang-academy/">敦煌莫高窟项目</a>中发挥了至关重要的作用，该项目旨在以前所未有的精度以数字方式保存敦煌壁画和佛龛的文化遗产。 <span data-contrast="auto" xml:lang="EN-US" lang="EN-US" class="TextRun EmptyTextRun SCXW203067509 BCX8" style="-webkit-user-drag: none; -webkit-tap-highlight-color: transparent; margin: 0px; padding: 0px; user-select: text; font-size: 12pt; line-height: 20.85px; font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, sans-serif; font-variant-ligatures: none !important;"></span></p><p>另一个值得注意的项目是视频稳定技术的开发，该技术作为 Media Foundation 的一部分集成到 Windows 7 中。该技术通过补偿不必要的摄像机移动来提高视频质量，提供更流畅、更专业的输出。能够处理和稳定视频的实时算法的创建在当时具有开创性。</p><p>从那时起，深度学习、大型数据集和复杂的神经网络架构的引入将计算机视觉推向了新的高度。曾经被认为困难的任务，例如目标检测、识别和分割，现在已成为现代人工智能技术的标准。当前的研究通过探索创新的网络架构、新的学习策略和增强的数据集继续突破界限。一个特别令人兴奋的趋势是人工智能在现实世界交互场景中的使用，导致了具体人工智能的出现，这是我当前工作的一个主要焦点。</p><h2 class="wp-block-heading h3" id="understanding-embodied-ai-beyond-robotics">了解机器人之外的具体人工智能</h2><p><strong>问：</strong>您目前的研究兴趣包括嵌入式人工智能，这也是微软东京亚洲研究院的重点领域之一。实体人工智能到底是什么？它与机器人技术有何不同？</p><p> <strong>Yasuyuki Matsushita：</strong>嵌入式人工智能超越了传统机器人技术。虽然机器人通常是配备执行特定任务的执行器的机器，但嵌入式人工智能专注于开发智能系统，该系统可以执行复杂的任务，同时在物理和虚拟环境中理解和交互。机器人技术和人工智能是独立发展的，但实体人工智能是这两个领域的融合，将人工智能与能够在动态现实环境中感知、行动和学习的物理代理相结合。</p><p>该领域本质上是跨学科的，涉及机器人控制、强化学习、空间意识、人机交互、推理等方面。例如，嵌入式人工智能包括推断因果关系的能力，例如了解不受支撑的笔记本电脑会因重力而掉落。这些类型的交互和解释源于对物理世界的参与和理解，使实体人工智能成为一个令人兴奋且多方面的研究领域。</p><p>鉴于嵌入式人工智能的复杂性，没有任何一个组织能够单独涵盖其开发的所有方面。我们期待与日本当地的行业和学术机构合作，利用他们的专业知识和我们在人工智能方面的优势来推动该领域的发展。</p><h2 class="wp-block-heading h3" id="advice-for-aspiring-researchers-in-computer-vision-and-ai"> 为计算机视觉和人工智能领域有抱负的研究人员提供建议</h2><p><strong>问：</strong>您在学术界和工业界拥有广泛的职业生涯。根据您作为教育者和研究人员的经验，您会给有兴趣从事计算机视觉和人工智能研究的年轻人什么建议？</p><p> <strong>Yasuyuki Matsushita：</strong>对于对计算机视觉和人工智能感兴趣的学生来说，即使特定的研究主题和技术不断发展，数学和计算机科学的坚实基础也是必不可少的。对梯度、雅可比行列式和向量空间等基本数学概念的深入理解是必不可少的。无论编程语言或开发平台如何变化，掌握这些原则都是有益的。</p><p>保持持续学习的心态同样重要，因为该领域在不断发展。例如，深度学习在十年前并不那么突出，但现在已成为该领域的核心。在微软，我们强调成长心态的重要性——适应能力强，对新技术持开放态度，并愿意随着行业进步而变化。早期职业生涯的专业人士应该在基础知识的基础上培养快速获取新技能的能力。这种适应性是研发长期成功的关键。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/introducing-yasuyuki-matsushita-tackling-societal-challenges-with-ai-at-microsoft-research-asia-tokyo/">松下康之介绍：在微软亚洲研究院（东京）利用 AI 应对社会挑战</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>上。</p> ]]>;</content:encoded></item><item><title> BiomedParse：更智能、一体化生物医学图像分析的基础模型</title><link/>https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Mon, 18 Nov 2024 10:14:40 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1101105 </guid><description><![CDATA[<p> BiomedParse 重新构想了医学图像分析，集成先进的人工智能来捕获跨成像类型的复杂见解——这是诊断和精准医学的一步。</p><p>后<a href="https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/">BiomedParse：更智能、一体化生物医学图像分析的基础模型</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1.jpg" alt="一只绿色线条画的手握着透明棱镜的风格化插图，在黑色背景下，彩色光带通过棱镜折射。" class="wp-image-1102929" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/BiomedParse-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>在癌症诊断或免疫疗法等高级治疗中，医学图像中的每个细节都很重要。放射科医生和病理学家依靠这些图像来跟踪肿瘤，了解它们的边界，并分析它们如何与周围细胞相互作用。这项工作需要多项任务的精确性——识别肿瘤是否存在、精确定位、在复杂的 CT 扫描或病理切片上绘制其轮廓。</p><p>然而，这些关键步骤——对象识别、检测和分割——通常是分开处理的，这会限制分析的深度。 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41467-024-44824-z" target="_blank" rel="noreferrer noopener">MedSAM <span class="sr-only">（在新选项卡中打开）</span></a>和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://segment-anything.com/" target="_blank" rel="noreferrer noopener">SAM <span class="sr-only">（在新选项卡中打开）</span></a>等当前工具仅专注于细分，因此错过了全面融合这些见解并将对象降级为事后想法的机会。</p><p>在本博客中，我们介绍<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41592-024-02499-w" target="_blank" rel="noreferrer noopener">BiomedParse <span class="sr-only">（在新选项卡中打开）</span></a> ，这是一种通过将对象视为一等公民来进行整体图像分析的新方法。通过将对象识别、检测和分割统一到一个框架中，BiomedParse 允许用户通过简单的自然语言提示指定他们正在寻找的内容。其结果是一种更有凝聚力、更智能的医学图像分析方法，支持更快、更综合的临床洞察。</p><p>虽然生物医学分割数据集比比皆是，但之前关于生物医学中的对象检测和识别的工作相对较少，更不用说涵盖所有三个任务的数据集了。为了预训练 BiomedParse，我们利用 OpenAI 的 GPT-4 从<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/microsoft/BiomedParseData" target="_blank" rel="noreferrer noopener">标准分割数据集<span class="sr-only">（在新选项卡中打开）</span></a>进行数据合成，创建了第一个此类数据集。</p><p> BiomedParse 是一个单一的基础模型，可以跨九种模式准确地分割生物医学对象，如图 1 所示，其性能优于先前的最佳方法，同时需要的用户操作数量减少了几个数量级，因为它不需要特定于对象的边界框。通过学习单个对象类型的语义表示，BiomedParse 的优越性在具有不规则形状对象的最具挑战性的情况下尤其明显。通过对象识别、检测和分割的联合预训练，BiomedParse 为生物医学中的整体图像分析和基于图像的发现开辟了新的可能性。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" width="2185" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-scaled.jpg" alt="a，GPT-4 构建的本体显示了用于统一跨数据集的语义概念的对象类型层次结构。显示包含该对象类型的图像数量的条形图。 b，条形图显示 BiomedParseData 中每种模态的图像-掩模-描述三元组的数量。 CT 是计算机断层扫描的缩写。 MRI 是磁共振成像的缩写。 OCT 是光学相干断层扫描的缩写。 c，BiomedParse 流程图。 BiomedParse 将图像和文本提示作为输入，然后输出提示中指定的对象的分割掩码。我们的框架不需要特定于图像的手动交互，例如边界框或单击。为了促进图像编码器的语义学习，BiomedParse 还纳入了一个学习目标来对元对象类型进行分类。对于在线推理，GPT-4 用于使用对象本体将文本提示解析为对象类型，该本体还使用 BiomedParse 输出的元对象类型来缩小候选语义标签的范围。 d，统一流形逼近和投影 (UMAP) 图对比了来自 BiomedParse 文本编码器（左）和 PubMedBERT（右）的不同细胞类型的文本嵌入。 e，UMAP 绘图对比了来自 BiomedParse 图像编码器（左）和 Focal（右）的不同细胞类型的图像嵌入。" class="wp-image-1102950" style="width:825px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-scaled.jpg 2185w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-256x300.jpg 256w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-874x1024.jpg 874w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-768x900.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-1311x1536.jpg 1311w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-1748x2048.jpg 1748w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure1_BiomedParse-154x180.jpg 154w" sizes="(max-width: 2185px) 100vw, 2185px" /><figcaption class="wp-element-caption">图 1. BiomedParse 和 BiomedParseData 概述<em>。</em> </figcaption></figure><h2 class="wp-block-heading" id="image-parsing-a-unifying-framework-for-holistic-image-analysis">图像解析：整体图像分析的统一框架</h2><p>早在 2005 年，研究人员就首次引入了“图像解析”的概念——一种联合进行对象识别、检测和分割的图像分析统一方法。这种早期模型建立在贝叶斯网络的基础上，尽管其范围和应用受到限制，但它让我们得以一睹图像分析中联合学习和推理的未来。快进到今天，生成式人工智能的前沿进展为这一愿景注入了新的活力。通过我们的模型 BiomedParse，我们为生物医学图像解析奠定了基础，该解析利用了三个子任务之间的相互依赖性，从而解决了传统方法中的关键局限性。 BiomedParse 使用户能够简单地输入对象的自然语言描述，模型使用该描述来预测对象标签及其分割掩模，从而消除了对边界框的需要（图 1c）。换句话说，这种联合学习方法可以让用户仅根据文本来分割对象。 </p><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1085514"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">焦点：博客文章</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/graphrag-auto-tuning-provides-rapid-adaptation-to-new-domains/" aria-label="GraphRAG auto-tuning provides rapid adaptation to new domains" data-bi-cN="GraphRAG auto-tuning provides rapid adaptation to new domains" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/GraphRag-3-BlogHeroFeature-1400x788-1.png" alt="蓝色到绿色渐变的 GraphRAG 图像" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4"> GraphRAG 自动调优可快速适应新领域</h2><p class="large">GraphRAG 使用 LLM 生成的知识图来大幅改进检索增强生成 (RAG) 的复杂问答。了解 GraphRAG 针对新数据集的自动调整，使其更加准确和相关。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/blog/graphrag-auto-tuning-provides-rapid-adaptation-to-new-domains/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="GraphRAG auto-tuning provides rapid adaptation to new domains" target="_blank">阅读更多</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="harnessing-gpt-4-for-large-scale-data-synthesis-from-existing-datasets">利用 GPT-4 从现有数据集中进行大规模数据合成</h2><p>我们利用 GPT-4 从 45 个现有生物医学分割数据集进行大规模数据合成，创建了第一个生物医学成像解析数据集（图 1a 和 1b）。关键的见解是利用这些数据集中已有的自然语言描述，并使用 GPT-4 通过已建立的生物医学对象分类法来组织这些经常混乱的非结构化文本。</p><p>具体来说，我们使用 GPT-4 帮助创建用于图像分析的统一生物医学对象分类法，并将现有数据集的自然语言描述与该分类法相协调。我们进一步利用 GPT-4 来合成对象描述的其他变体，以促进更强大的文本提示。</p><p>这使我们能够构建 BiomedParseData，这是一个生物医学图像分析数据集，包含超过 600 万组图像、分割掩模以及从超过 100 万张图像中提取的文本描述。该数据集包括 64 种主要生物医学对象类型、82 种细粒度子类型，涵盖九种成像模式。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2242" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-scaled.jpg" alt="a，比较我们的方法和竞争方法在 9 种模式的 102,855 个测试实例（图像-掩模-标签三元组）上的 Dice 分数的箱线图。 MedSAM 和 SAM 需要边界框作为输入。我们考虑两种设置：预言机边界框（覆盖黄金掩模的最小边界框）； Grounding DINO（一种最先进的基于文本的接地模型）根据文本提示生成边界框。每个模态类别包含多种对象类型。每个对象类型都聚合为要在图中显示的实例中位数。图中的 n 表示相应模态中测试实例的数量。 b，仅使用顶部的文本提示，比较 BiomedParse 的分割结果和真实情况的九个示例。 c，箱线图，比较我们的方法和竞争方法在 n=42 图像的细胞分割测试集上的 Dice 分数。 BiomedParse 仅需要单个用户操作（文本提示“结肠病理学中的腺体结构”）。相比之下，为了获得有竞争力的结果，MedSAM 和 SAM 需要 430 次操作（每个细胞一个边界框）。 d，对比 BiomedParse 和 MedSAM 的分割结果的五个示例，以及 BiomedParse 使用的文本提示和 MedSAM 使用的边界框。 e，BiomedParse 和 MedSAM 在良性肿瘤图像（上）和恶性肿瘤图像（下）上的比较。 BiomedParse 相对于 MedSAM 的改进对于形状不规则的异常细胞更为明显。 f，比较有效文本提示和无效文本提示之间的两侧 K-S 检验 P 值的箱线图。 BiomedParse 学会拒绝描述图像中不存在的对象类型的无效文本提示（小 P 值）。我们总共评估了 4,887 个无效提示和 22,355 个有效提示。 g，该图显示了我们的方法在不同 K-S 测试 P 值截止值上检测无效文本提示的精确度和召回率。 h,i，散点图，比较 BiomedParse 和 Grounding DINO 检测无效描述时的受试者工作特征曲线 (AUROC) (h) 和 F1 (i) 下的面积。" class="wp-image-1102956" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-scaled.jpg 2242w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-263x300.jpg 263w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-897x1024.jpg 897w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-768x877.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-1345x1536.jpg 1345w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-1793x2048.jpg 1793w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure2_BiomedParse-158x180.jpg 158w" sizes="(max-width: 2242px) 100vw, 2242px" /><figcaption class="wp-element-caption">图 2：大规模生物医学图像分割数据集的比较。 </figcaption></figure><h2 class="wp-block-heading" id="state-of-the-art-performance-across-64-major-object-types-in-9-modalities">以 9 种模式跨 64 种主要对象类型提供最先进的性能</h2><p>我们在一个大型保留测试集上评估了 BiomedParse，该测试集包含 9 种模式的 64 个主要对象类型的 102,855 个图像-掩模-标签集。即使提供了预言机每个对象的边界框，BiomedParse 的性能也优于 MedSAM 和 SAM 等之前的最佳方法。在更现实的环境中，当 MedSAM 和 SAM 使用最先进的对象检测器 (Grounding DINO) 来提出边界框时，BiomedParse 的表现大幅优于它们，骰子得分绝对值在 75 到 85 分之间（图 2a） 。 BiomedParse 的性能还优于其他各种著名方法，例如 SegVol、Swin UNETR、nnU-Net、DeepLab V3+ 和 UniverSeg。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2170" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-scaled.jpg" alt="a，文本提示的注意力图不规则形状的物体，表明 BiomedParse 学会了对其典型形状的相当忠实的表示。美国，超声波。 b-d，散点图比较了 BiomedParse 相对于 MedSAM 的 Dice 评分改进以及凸比 (b)、盒比 (c) 和反转动惯量 (d) 方面的形状规律性。 x 轴上的数字越小意味着平均不规则性越高。每个点代表一个对象类型。 e，对比 BiomedParse 和 MedSAM 在检测不规则形状物体方面的六个示例。绘图按照从最不规则的图（左）到最不规则的图（右）的顺序排列。 f,g BiomedParseData 与 MedSAM 使用的基准数据集在凸比（f）和盒比（g）方面的比较。 BiomedParseData 更忠实地代表了现实世界中不规则形状物体的挑战。 h，比较 BiomedParse 和 BiomedParseData 上的竞争方法以及 MedSAM 使用的基准数据集的箱线图。 BiomedParse 在 BiomedParseData 上有较大改进，包含更多样的图像和更多不规则形状的物体。对象类型的数量如下：对于 MedSAM 基准，n=50；对于 BiomedParseData，n=112。" class="wp-image-1102965" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-scaled.jpg 2170w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-254x300.jpg 254w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-868x1024.jpg 868w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-768x906.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-1302x1536.jpg 1302w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-1736x2048.jpg 1736w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure3_BiomedParse-153x180.jpg 153w" sizes="(max-width: 2170px) 100vw, 2170px" /><figcaption class="wp-element-caption">图 3. 检测不规则形状物体的评估。 </figcaption></figure><h2 class="wp-block-heading" id="recognizing-and-segmenting-irregular-and-complex-objects">识别和分割不规则和复杂的物体</h2><p>生物医学对象通常具有复杂且不规则的形状，这给分割带来了巨大的挑战，即使使用预言机边界框也是如此。通过与对象识别和检测的联合学习，BiomedParse 学习对特定对象的形状进行建模，对于最具挑战性的情况，其优越性尤其明显（图 3）。 BiomedParseData 包含九种模式的大量不同对象类型，还提供了生物医学中对象复杂性的更真实的表示。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2276" height="2560" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-scaled.jpg" alt="a，六个例子显示了我们的方法的物体识别结果。对象识别可以识别并分割图像中的所有对象，而不需要任何用户提供的输入提示。 b-d，散点图，比较 BiomedParse 和 Grounding DINO 在识别图像中呈现的对象时的 F1 (b)、精度 (c) 和召回率 (d) 分数。 e，BiomedParse 和 Grounding DINO 在图像中不同数量对象的中值 F1 分数方面的对象识别比较。 f，比较 BiomedParse 和 MedSAM/SAM（使用 Grounding DINO 生成的边界框）在与各种模式相关的端到端对象识别（包括分割）方面的箱线图。 g，BiomedParse 和 MedSAM/SAM（使用由 Grounding DINO 生成的边界框）在端到端对象识别（包括分割）方面与图像中不同对象数量的比较。" class="wp-image-1102968" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-scaled.jpg 2276w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-267x300.jpg 267w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-910x1024.jpg 910w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-768x864.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-1366x1536.jpg 1366w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-1821x2048.jpg 1821w, https://www.microsoft.com/en-us/research/uploads/prod/2024/11/Figure4_BiomedParse-160x180.jpg 160w" sizes="(max-width: 2276px) 100vw, 2276px" /><figcaption class="wp-element-caption">图 4. 物体识别评估。 </figcaption></figure><h2 class="wp-block-heading" id="promising-step-toward-scaling-holistic-biomedical-image-analysis">朝着扩展整体生物医学图像分析迈出了有希望的一步</h2><p>通过通过简单的文本提示进行操作，BiomedParse 比以前通常需要特定于对象的边界框的最佳方法所需的用户工作量大大减少，特别是当图像包含大量对象时（图 2c）。通过对对象识别阈值进行建模，BiomedParse 可以检测无效提示，并在图像中不存在对象时拒绝分割请求。 BiomedParse 可用于一次性识别和分割图像中的所有已知对象（图 4）。通过扩展整体图像分析，BiomedParse 有可能应用于关键的精准健康应用，例如早期检测、预后、治疗决策支持和进展监测。</p><p>展望未来，还有大量的增长机会。 BiomedParse 可以扩展以处理更多模式和对象类型。它可以集成到<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/llava-med" target="_blank" rel="noreferrer noopener">LLaVA-Med <span class="sr-only">（在新选项卡中打开）</span></a>等先进的多模式框架中，通过“与数据对话”来促进对话式图像分析。为了促进生物医学图像分析研究，我们使用 Apache 2.0 许可证将 BiomedParse <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/biomedparse-release" target="_blank" rel="noreferrer noopener">开源<span class="sr-only">（在新选项卡中打开）</span></a> 。我们还在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.azure.com/explore/models/MedImageParse/version/3/registry/azureml/latest" target="_blank" rel="noreferrer noopener">Azure AI <span class="sr-only">（在新选项卡中打开）</span></a>上提供了它，用于直接部署和实时推理。欲了解更多信息，请查看我们的<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/BiomedParse/" target="_blank" rel="noreferrer noopener">演示。 <span class="sr-only">（在新选项卡中打开）</span></a></p><p> BiomedParse 是与普罗维登斯和华盛顿大学 Paul G. Allen 计算机科学与工程学院的联合项目，并带来了 Microsoft* 内多个团队的协作。它反映了微软对推进多模式生成人工智能以实现精准健康的更大承诺，以及其他令人兴奋的进展，例如<a href="https://www.microsoft.com/en-us/research/blog/gigapath-whole-slide-foundation-model-for-digital-pathology/" target="_blank" rel="noreferrer noopener">GigaPath <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a href="https://www.microsoft.com/en-us/research/publication/large-scale-domain-specific-pretraining-for-biomedical-vision-language-processing/" target="_blank" rel="noreferrer noopener">BiomedCLIP <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a href="https://www.microsoft.com/en-us/research/project/project-maira/" target="_blank" rel="noreferrer noopener">LLaVA-Rad（在新选项卡中打开）、BiomedJourney <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2310.10765" target="_blank" rel="noreferrer noopener">BiomedJourney <span class="sr-only">（在新选项卡中打开）新选项卡）</span></a> 、 <a href="https://www.microsoft.com/en-us/research/publication/rad-dino-exploring-scalable-medical-image-encoders-beyond-text-supervision/" target="_blank" rel="noreferrer noopener">MAIRA <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a href="https://www.microsoft.com/en-us/research/publication/rad-dino-exploring-scalable-medical-image-encoders-beyond-text-supervision/" target="_blank" rel="noreferrer noopener">Rad-DINO <span class="sr-only">（在新选项卡中打开）</span></a> 、 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/2309.07778v5" target="_blank" rel="noreferrer noopener">Virchow <span class="sr-only">（在新选项卡中打开）</span></a> 。</p><p> <em>（致谢脚注）*：在 Microsoft 内部，这是 Health Futures、MSR 深度学习和 Nuance 之间的精彩合作。</em></p><p>论文合著者：Theodore Zhao、Yu Gu、 <a href="https://www.microsoft.com/en-us/research/people/jianwyan/">Jianwei Yang <span class="sr-only">(在新选项卡中打开)</span></a> 、 <a href="https://www.microsoft.com/en-us/research/people/naotous/">Naoto Usuyama <span class="sr-only">(在新选项卡中打开)</span></a> 、Ho Hin Lee、Sid Kiblawi、 <a href="https://www.microsoft.com/en-us/research/people/tristan/">Tristan Naumann <span class="sr-only">(在新选项卡中打开)</span></a> 、 <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Taka <span class="sr-only">(在新选项卡中打开</span></a>)<a href="https://www.microsoft.com/en-us/research/people/jfgao/"><span class="sr-only">新标签）</span></a> 、Angela Crabtree、Jacob Abel、Christine Moung-Wen、Brian Piening、Carlo Bifulco、Mu Wei、 <a href="https://www.microsoft.com/en-us/research/people/hoifung/">Hoifung Poon <span class="sr-only">（打开在新选项卡中）</span></a> ， <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://homes.cs.washington.edu/~swang/">王胜<span class="sr-only">（在新选项卡中打开）</span></a></p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p>后<a href="https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/">BiomedParse：更智能、一体化生物医学图像分析的基础模型</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>