<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2024 年 5 月 8 日星期三 17:52:37 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.5.2</generator><item><title> LLM 分析指导 KV 缓存优化</title><link/>https://www.microsoft.com/en-us/research/blog/llm-profiling-guides-kv-cache-optimization/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Wed, 08 May 2024 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1030206 </guid><description><![CDATA[<p> LLM 依赖键值 (KV) 缓存等内存密集型机制来存储和快速检索数据。 FastGen 优化 KV 缓存使用，在保持性能的同时将 LLM 内存需求减少高达 50%。</p><p><a href="https://www.microsoft.com/en-us/research/blog/llm-profiling-guides-kv-cache-optimization/">LLM 后分析指南 KV 缓存优化</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center">这篇研究论文在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://iclr.cc/" target="_blank" rel="noreferrer noopener"><strong><em><sup>第十二届</sup>学习表示国际会议</em></strong><span class="sr-only">（在新选项卡中打开）</span></a> （ICLR 2024）<strong><em>上发表</em></strong><strong><em>，这是致力于推进深度学习的重要会议。</em></strong> </p><figure class="wp-block-image aligncenter size-full"><img fetchpriority="high" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1.png" alt="白色 ICLR 徽标位于已接受论文“模型告诉您要丢弃什么：LLM 的自适应 KV 缓存压缩”第一页左侧，紫色背景。" class="wp-image-1030227" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1.png 1401w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ModelDiscard-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1401px) 100vw, 1401px" /></figure><p>大型语言模型 (LLM) 依赖于复杂的内部机制，这些机制需要比标准设备上通常可用的内存更多的内存。其中一种机制是键值 (KV) 缓存，它存储和检索先前计算的数据，帮助模型快速生成响应，而无需重新计算已处理的信息。此方法使用大量内存，因为它保留了大量易于访问的数据，以提高模型的速度和效率。因此，随着任务复杂性的增加，KV 缓存可能会变得过大，有时单个操作需要高达 320 GB 的空间。为了解决这个问题，我们开发了 FastGen，这是一种旨在减少法学硕士内存需求的新颖方法。 </p><div class="annotations " data-bi-aN="margin-callout"><ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right"><li class="annotations__list-item"> <span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">出版</span><a href="https://www.microsoft.com/en-us/research/publication/model-tells-you-what-to-discard-adaptive-kv-cache-compression-for-llms/" target="_self" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs" data-bi-aN="margin-callout" data-bi-cN="Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs">模型告诉您要放弃什么：LLM 的自适应 KV 缓存压缩<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span></a></li></ul></div><p>我们在 ICLR 2024 上发表的论文“ <a href="https://www.microsoft.com/en-us/research/publication/model-tells-you-what-to-discard-adaptive-kv-cache-compression-for-llms/" target="_blank" rel="noreferrer noopener">模型告诉您要丢弃什么：LLM 的自适应 KV 缓存压缩<span class="sr-only">（在新选项卡中打开）</span></a> ”，描述了 FastGen 如何优化 LLM 存储和访问数据的方式，从而可能将内存使用量减少一半，同时保持他们的效率。这种方法代表着朝着让复杂的人工智能工具更容易被更广泛的应用程序访问和负担得起的方向迈出了重要的一步。我们很高兴地告诉大家，这篇论文已获得<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://blog.iclr.cc/2024/05/06/iclr-2024-outstanding-paper-awards/" target="_blank" rel="noreferrer noopener">杰出论文奖荣誉奖<span class="sr-only">（在新选项卡中打开）</span></a> 。</p><h2 class="wp-block-heading" id="observations-of-the-kv-cache"> KV缓存的观察</h2><p>FastGen 的开发基于我们对 KV 缓存功能的观察。我们首先观察到，LLM 完成其所需任务并不需要 KV 缓存中的所有数据，如图 1 所示。通过为 KV 缓存提供丢弃不必要数据的机制，可以显着减少内存使用。例如，一些法学硕士模块不需要广泛的上下文来处理输入。为此，可以构建一个 KV 缓存，删除包含不太重要的远程上下文的数据，例如几个句子或段落。此外，一些 LLM 模块主要只关注特殊标记，例如标点符号，为此可以创建仅保留这些标记的 KV 缓存。最后，一些 LLM 模块广泛需要所有令牌，为此我们可以使用标准 KV 缓存并存储所有单词。</p><p>我们研究中的另一个关键观察结果是，LLM 中不同层和位置的注意力模块的行为不同，并且需要不同的 KV 缓存偏好，如图 1 右侧所示。 </p><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="932112"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究院播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-ai-for-health-and-the-future-of-research-with-peter-lee/" aria-label="AI Frontiers: AI for health and the future of research with Peter Lee" data-bi-cN="AI Frontiers: AI for health and the future of research with Peter Lee" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/03/PeterLee_podcast-2023Mar_hero_1400x788.png" alt="Peter Lee 戴着眼镜，对着镜头微笑，左侧有 Microsoft Research Podcast 徽标" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4"> AI 前沿：Peter Lee 的 AI 健康与研究的未来</h2><p class="large">微软研究院院长 Peter Lee 和 AI 科学家兼工程师 Ashley Llorens 讨论了 AI 研究的未来以及 GPT-4 作为医疗副驾驶的潜力。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-ai-for-health-and-the-future-of-research-with-peter-lee/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="AI Frontiers: AI for health and the future of research with Peter Lee" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1401" height="450" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ICLR-Figure1-1400x-450-1.png" alt="描述 KV 缓存不同结构的图表。左图包含常见结构。右侧的圆形图包含三个处于同一层的模块组成，但它们存储数据的方式不同。" class="wp-image-1030860" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ICLR-Figure1-1400x-450-1.png 1401w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ICLR-Figure1-1400x-450-1-300x96.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ICLR-Figure1-1400x-450-1-1024x329.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ICLR-Figure1-1400x-450-1-768x247.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/05/ICLR-Figure1-1400x-450-1-240x77.png 240w" sizes="(max-width: 1401px) 100vw, 1401px" /><figcaption class="wp-element-caption">图 1：这些图描绘了 KV 缓存的不同结构。左图包含常见结构。右侧的圆形图包含三个处于同一层的模块组成，但它们存储数据的方式不同。 </figcaption></figure><h2 class="wp-block-heading" id="fastgen-accounts-for-the-diversity-of-kv-cache-structures"> FastGen 考虑了 KV 缓存结构的多样性</h2><p>由于不同的KV缓存结构不同，因此需要进行不同的处理。我们根据我们的观察开发了 FastGen 算法，使其能够对存储在给定 KV 缓存中的数据进行分类和优化。 FastGen 首先分析不同模块的具体行为以了解其结构，这种方法称为<em>profiling</em> 。然后，它使用结果来实时调整数据存储方式，从而使过程更加高效。我们的测试表明 FastGen 可以在不牺牲质量的情况下减少 50% 的内存量。我们的<a href="https://www.microsoft.com/en-us/research/publication/model-tells-you-what-to-discard-adaptive-kv-cache-compression-for-llms/" target="_blank" rel="noreferrer noopener">论文</a>中详细讨论的其他实验证实了分析过程至关重要，并且显着提高了 KV 缓存的效率。</p><h2 class="wp-block-heading" id="the-broader-picture">更广阔的前景</h2><p>在数据处理和计算能力空前进步的推动下，法学硕士预训练已成为深度学习的基石，改变了自然语言处理任务，并不断挑战我们对学习和认知的理解。</p><p>然而，更大的能力也会带来挑战。随着模型规模越来越大，为特定任务定制它们可能会变得更加资源密集。在微软研究院，我们正在探索更有效的模型编辑的不同方法。关键策略涉及有针对性的模型分析，它可以识别与预定义目标一致的模型的基本组件。这种分析可以为精确的模型修改提供信息，从而优化资源的使用和有效性。</p><p>我们在 ICLR 2024 上展示的两个研究项目支持这些目标。两者都采用配置文件然后编辑范例来解决不同的问题。 FastGen 减少了内存消耗。我们的相关工作， <a href="https://www.microsoft.com/en-us/research/publication/tell-your-model-where-to-attend-post-hoc-attention-steering-for-llms/" target="_blank" rel="noreferrer noopener">法学硕士事后注意力引导（PASTA）</a> ，专注于更好的可控性。这些方法旨在提高资源效率，因为它们不需要调整或反向传播。展望未来，我们的目标是进一步开发这些技术，以提高法学硕士申请的资源效率，使更多的受众更容易获得它们。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/llm-profiling-guides-kv-cache-optimization/">LLM 后分析指南 KV 缓存优化</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title> LoftQ：通过更智能的初始化重新构想 LLM 微调</title><link/>https://www.microsoft.com/en-us/research/blog/loftq-reimagining-llm-fine-tuning-with-smarter-initialization/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Tue, 07 May 2024 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/blog/loftq-reimagining-llm-fine-tuning-with-smarter-initialization/ </guid><description><![CDATA[<p> LoftQ 通过简化微调过程来提高法学硕士效率，减少计算需求，同时保持高性能。此类创新有助于提高人工智能技术的能源效率。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/loftq-reimagining-llm-fine-tuning-with-smarter-initialization/">《LoftQ：通过更智能的初始化重新想象 LLM 微调》</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center">这篇研究论文在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://iclr.cc/Conferences/2024" target="_blank" rel="noreferrer noopener"><strong><em><sup>第十二届</sup>学习表示国际会议</em></strong><span class="sr-only">（在新选项卡中打开）</span></a> （ICLR 2024）<strong><em>上发表</em></strong><strong><em>，这是致力于推进深度学习的重要会议。</em></strong> </p><figure class="wp-block-image size-full"><img decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1.png" alt="青色背景，右侧（头部和面部）带有 ICLR 徽标，右侧为 LoftQ 纸。" class="wp-image-1027119" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1.png 1401w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1401px) 100vw, 1401px" /></figure><div class="annotations " data-bi-aN="margin-callout"><ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right"><li class="annotations__list-item"> <span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">出版物</span><a href="https://www.microsoft.com/en-us/research/publication/loftq-lora-fine-tuning-aware-quantization-for-large-language-models/" target="_self" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models" data-bi-aN="margin-callout" data-bi-cN="LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models">LoftQ：大型语言模型的 LoRA 微调感知量化<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span></a></li></ul></div><p>大型语言模型 (LLM) 使用广泛的数据集和高级算法来生成细致入微、上下文相关的内容。然而，它们的开发需要大量的计算资源。为了解决这个问题，我们开发了 LoftQ，这是一种简化微调过程的创新技术，用于调整预先训练的语言模型，以便在分析医疗文档等专业应用中表现良好。在微调过程中，模型会在较小的特定于任务的数据集上接受额外的训练。这会提高性能，例如更准确的预测、更好地理解特定领域的语言以及在专业领域的背景下做出更相关的响应。</p><p> LoftQ的优势在于它能够在微调过程中结合量化和自适应初始化。量化降低了模型参数的精度，从而降低了内存和计算需求。这不仅加速了处理速度，还降低了功耗。自适应初始化将模型的参数与其最佳预训练状态紧密结合，保留其功能，同时最大限度地减少资源使用。我们在 ICLR 2024 上发表的论文“ <a href="https://www.microsoft.com/en-us/research/publication/loftq-lora-fine-tuning-aware-quantization-for-large-language-models/" target="_blank" rel="noreferrer noopener">LoftQ：大型语言模型的 LoRA 微调感知量化</a>”详细介绍了该方法如何帮助提高 AI 技术的效率和可持续性。</p><h2 class="wp-block-heading" id="how-loftq-works"> LoftQ 的工作原理</h2><p>LoftQ 建立在<a href="https://www.microsoft.com/en-us/research/publication/lora-low-rank-adaptation-of-large-language-models/" target="_blank" rel="noreferrer noopener">LoRA <span class="sr-only">（在新选项卡中打开）</span></a>和<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/artidoro/qlora" target="_blank" rel="noreferrer noopener">QLoRA <span class="sr-only">（在新选项卡中打开）</span></a>的原则之上。 LoRA 是一种大大减少训练所需参数数量、降低微调所需内存的方法。 QLoRA 是一种微调方法，使用 4 位量化、冻结权重和低等级适配器，显着降低内存需求，同时保持高性能。表 1 对此进行了说明，其中显示了微调具有 70 亿个参数的 LLM 所需的内存量以及 LoRA 和 QLoRA 的内存要求。 LoRA 将内存使用量减少了四倍，而 QLoRA 则进一步减少了两倍。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="3053" height="1210" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/finetuning-1.png" alt="LoftQ - 表 1：该表显示了 70 亿参数 LLM 的 GPU 内存使用情况，配置如下：左侧完全微调，中间 LoRA，右侧 QLoRA。" class="wp-image-1029312" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/finetuning-1.png 3053w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/finetuning-1-300x119.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/finetuning-1-1024x406.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/finetuning-1-768x304.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/finetuning-1-1536x609.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/finetuning-1-2048x812.png 2048w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/finetuning-1-240x95.png 240w" sizes="(max-width: 3053px) 100vw, 3053px" /><figcaption class="wp-element-caption">表 1：此表显示了具有以下配置的 70 亿参数 LLM 的 GPU 内存使用情况：左侧为完全微调，中间为 LoRA，右侧为 QLoRA。</figcaption></figure><p>与 LoRA 不同，QLoRA 需要权衡，即由于权重量化而牺牲了预训练模型的某些质量。 LoftQ 认识到这一点并优化量化和低秩适应矩阵的初始化。也就是说，LoftQ 寻求识别量化矩阵和低秩矩阵的组合，使得它们的总和非常接近原始的预训练权重。对于将在模型中调整的每个矩阵都执行此操作。</p><p> LoftQ 算法在两个主要步骤之间交替。首先，它量化（简化）权重，然后找到近似预训练权重和低秩权重之间量化的最佳低秩因子。该过程重复几个步骤。这种方法使微调过程能够从更有效的初始状态开始，从而在使用更少的计算能力和更简化的权重的同时保持准确性。</p><p> LoftQ 需要一次性设置来简化和准备这些权重，从而允许调整模型参数的固定部分（例如 5%）。一旦建立，当模型在各种任务和设置之间转换时，可以重复应用此配置。</p><h2 class="wp-block-heading" id="evaluating-loftq">评估 LoftQ</h2><p>使用各种类型的 LLM（包括具有不同编码和解码功能组合的 Llama-2）进行的测试表明，使用 LoftQ 初始化的模型始终能够实现强大的性能，通常匹配或超过使用 QLoRA 配置的模型。</p><p>实际上，使用 Llama-2 模型系列比较 LoftQ 和 QLoRA 在不同任务上的性能会产生不同的结果，如表 2 所示。对于 WikiText-2 数据集，它衡量模型的困惑度（越低越好）， GSM8K 数据集测试模型解决基本数学问题的能力（越高越好），我们展示了不同程度的权重简化的有效性 - 每个权重平均 3、2.5 和 2.25 位。我们的<a href="https://www.microsoft.com/en-us/research/publication/loftq-lora-fine-tuning-aware-quantization-for-large-language-models/" target="_blank" rel="noreferrer noopener">论文</a>更详细地讨论了结果。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1605" height="1196" src="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ_llama2_table2.png" alt="LoftQ - 表 2。该表比较了在 Wikitext-2 和 GSM8K 数据集上微调两个 Llama-2 模型期间的 LoftQ 和 QLoRA。" class="wp-image-1027116" style="width:731px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ_llama2_table2.png 1605w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ_llama2_table2-300x224.png 300w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ_llama2_table2-1024x763.png 1024w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ_llama2_table2-768x572.png 768w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ_llama2_table2-1536x1145.png 1536w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ_llama2_table2-80x60.png 80w, https://www.microsoft.com/en-us/research/uploads/prodnew/2024/04/LoftQ_llama2_table2-240x180.png 240w" sizes="(max-width: 1605px) 100vw, 1605px" /><figcaption class="wp-element-caption">表 2。该表比较了在 Wikitext-2 和 GSM8K 数据集上微调两个 Llama-2 模型期间的 LoftQ 和 QLoRA。 </figcaption></figure><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="931956"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">焦点：点播视频</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" aria-label="AI Explainer: Foundation models ​and the next era of AI" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AIEx01_blog_hero_1400x788.png" alt="电脑屏幕截图 一名男子的屏幕截图" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4"> AI 解释者：基础模型和 AI 的下一个时代</h2><p class="large">探索 Transformer 架构、更大的模型和更多数据以及情境学习如何帮助推动人工智能从感知到创造。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Watch video" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank">看视频</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="implications-and-looking-forward"> 影响和展望</h2><p>LoftQ 承诺通过加速研究和促进尖端工具的创建，同时支持可持续发展，推动人工智能领域的发展。虽然最初专注于法学硕士，但 LoftQ 的灵活设计还支持其他类型模型的微调，例如视觉和语音技术模型。随着研究的进展，我们期望进一步增强功能，以提高下游任务的性能。我们希望这些改进将导致各种人工智能应用程序得到更广泛的采用。我们对这项技术的广泛适用性感到兴奋，并鼓励人工智能社区探索其优势。 LoftQ 可通过<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/huggingface/peft/blob/56773b9a92b141111d65fe3548d0c30233358868/examples/loftq_finetuning/README.md" target="_blank" rel="noreferrer noopener">Hugging Face PEFT 库<span class="sr-only">（在新选项卡中打开）</span></a>开源。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/loftq-reimagining-llm-fine-tuning-with-smarter-initialization/">《LoftQ：通过更智能的初始化重新想象 LLM 微调》</a>一文首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>