<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2023 年 6 月 22 日星期四 18:13:13 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.2.2</generator><item><title> DeepSpeed ZeRO++：LLM 和聊天模型训练的速度飞跃，通信量减少了 4 倍</title><link/>https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Thu, 22 Jun 2023 16:18:42 +0000</pubDate> <category><![CDATA[Research Blog]]></category><guid ispermalink="false"></guid><description><![CDATA[<p>大型人工智能模型正在改变数字世界。 Turing-NLG、ChatGPT 和 GPT-4 等生成语言模型由大型语言模型 (LLM) 提供支持，用途极其广泛，能够执行摘要、编码和翻译等任务。同样，像 DALL·E、Microsoft Designer 和 Bing Image Creator 这样的大型多模式生成模型可以生成艺术、建筑、视频和其他数字 [...]</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/">DeepSpeed ZeRO++ 帖子：LLM 和聊天模型训练的速度飞跃，通信量减少了 4 倍，</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-large"><img decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-1024x576.png" alt="DeepSpeed ZeRO++ 项目亮点图" class="wp-image-951687" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure1-edited.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 1：ZeRO++ 项目亮点图片。左上子图显示，与 ZeRO 第 3 阶段相比，ZeRO++ 减少了 4 倍的通信量。右上子图显示了 ZeRO++ 在 RLHF 模型训练上的性能，其中 ZeRO++ 在 RLHF 训练方面实现了 1.3 倍的加速，在令牌生成方面实现了 2.x 的加速。</figcaption></figure><p>大型人工智能模型正在改变数字世界。 Turing-NLG、ChatGPT 和 GPT-4 等生成语言模型由大型语言模型 (LLM) 提供支持，用途极其广泛，能够执行摘要、编码和翻译等任务。同样，DALL·E、Microsoft Designer 和 Bing Image Creator 等大型多模式生成模型可以生成艺术、建筑、视频和其他数字资产，使内容创作者、建筑师和工程师能够探索创意生产力的新领域。</p><p>然而，训练这些大型模型需要数百甚至数千个 GPU 设备上的大量内存和计算资源。例如，训练<a href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/">Megatron-Turing NLG 530B</a>模型使用了 4,000 多个 NVidia A100 GPU。有效地利用这些资源需要一个复杂的优化系统，将模型划分为适合各个设备内存的部分，并有效地并行化这些设备之间的计算。同时，为了让深度学习社区能够轻松地进行大型模型训练，这些优化必须易于使用。 </p><div class="annotations " data-bi-aN="margin-callout"><ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right"><li class="annotations__list-item"> <span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">教程</span><a href="https://www.deepspeed.ai/tutorials/zeropp/" target="_blank" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="DeepSpeed ZeRO tutorials" data-bi-aN="margin-callout" data-bi-cN="DeepSpeed ZeRO tutorials">DeepSpeed ZeRO 教程<span class="glyph-append glyph-append-share glyph-append-xsmall"></span></a></li></ul></div><p>DeepSpeed 的 ZeRO<a href="https://www.deepspeed.ai/tutorials/zero/" target="_blank" rel="noreferrer noopener">系列优化</a>为这些挑战提供了强大的解决方案，并已广泛用于训练大型且强大的深度学习模型 TNLG-17B、Bloom-176B、MPT-7B、Jurrasic-1 等。尽管其具有变革性的能力在某些关键场景中，ZeRO 会在 GPU 之间产生较高的数据传输开销，从而难以实现高训练效率。这种情况尤其发生在以下情况：a) 在相对于全局批量大小的大量 GPU 上进行训练，这会导致每个 GPU 的批量大小较小，需要频繁的通信；或者 b) 在低端集群上进行训练，其中跨节点网络带宽受到限制，导致通信延迟较高。在这些情况下，ZeRO 提供便捷且高效的培训的能力是有限的。</p><p>为了解决这些限制，我们发布了<a href="https://www.microsoft.com/en-us/research/publication/zero-extremely-efficient-collective-communication-for-giant-model-training/" target="_blank" rel="noreferrer noopener">ZeRO++</a> ，这是一个构建在 ZeRO 之上的通信优化策略系统，可为大型模型训练提供无与伦比的效率，而不受批量大小限制或跨设备带宽限制的影响。与 ZeRO 相比，ZeRO++ 利用量化与数据和通信重新映射相结合，<strong><em>将总通信量减少了 4 倍</em></strong>，且不影响模型质量。这有两个关键含义：</p><ul><li> <em>ZeRO++加速大型模型预训练和微调</em><ul><li><strong>每个 GPU 的小批量：</strong>无论是在数千个 GPU 上预训练大型模型，还是在数百甚至数十个 GPU 上对其进行微调，当<em>每个 GPU 的批量</em>较小时，ZeRO++ 提供的吞吐量比 ZeRO 高出 2.2 倍，直接减少培训时间和成本。</li><li><strong>低带宽集群：</strong> ZeRO++ 使<em>低带宽</em>集群能够实现与带宽高 4 倍的集群相似的吞吐量。因此，ZeRO++ 可以跨更广泛的集群进行高效的大型模型训练。</li></ul></li><li> <em>ZeRO++ 使用 RLHF 加速类似 ChatGPT 的模型训练</em><br><br>虽然 ZeRO++ 主要是为训练而设计的，但其优化也自动适用于<a href="https://www.deepspeed.ai/2022/09/09/zero-inference.html#:~:text=ZeRO-Inference%20adapts%20and%20optimizes%20ZeRO-Infinity%20techniques%20for%20model,memory%2C%20thus%20hosting%20no%20%28zero%29%20weights%20in%20GPU." target="_blank" rel="noreferrer noopener">ZeRO-Inference</a> ，因为通信开销对于使用 ZeRO 进行训练和推理很常见。因此，ZeRO++ 提高了工作负载的效率，例如用于训练对话模型的人类反馈强化学习 (RLHF)，它结合了训练和推理。<br><br>通过与<a href="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat" target="_blank" rel="noreferrer noopener">DeepSpeed-Chat</a>集成，ZeRO++ 与原始 ZeRO 相比，可以将 RLHF 训练的生成阶段提高高达 2 倍，将强化学习训练阶段提高高达 1.3 倍。</li></ul><p>接下来，我们将更深入地研究 ZeRO 及其通信开销，并讨论 ZeRO++ 中用于解决这些问题的关键优化。然后我们将演示 ZeRO++ 对不同模型大小、批量大小和带宽限制的训练吞吐量的影响。我们还将讨论 ZeRO++ 如何应用于 DeepSpeed-Chat，以加速使用 RLHF 的对话模型的训练。</p><h2 class="wp-block-heading" id="deep-dive-into-zero">深入了解 ZeRO++ </h2><figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper"><iframe loading="lazy" title="DeepSpeed ZeRO 优化器工作流程" width="500" height="281" src="https://www.youtube-nocookie.com/embed/lQCG4zUCYao?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></div><figcaption class="wp-element-caption">图 2：ZeRO 优化器工作流程</figcaption></figure><p>ZeRO 是数据并行性的内存高效变体，其中模型状态在所有 GPU 上进行分区，而不是在训练期间使用基于收集/广播的通信集体进行复制和重建。这使得 ZeRO 能够有效地利用所有设备上的聚合 GPU 内存和计算，同时提供简单易用的数据并行训练。</p><p>假设模型大小为 M。在前向传递过程中，ZeRO 进行全收集/广播操作，以在需要之前收集每个模型层的参数（总共大小为 M）。在后向传递中，ZeRO 对每一层的参数采用类似的通信模式来计算其局部梯度（总共大小为 M）。此外，ZeRO 在使用reduce 或reduce-scatter 通信集合（总共大小为M）计算每个局部梯度后立即对每个局部梯度进行平均和分区。总的来说，ZeRO 的通信量为 3M，均匀分布在两次全收集/广播和一次减少-分散/减少操作中。</p><p>为了减少这些通信开销，ZeRO++ 有三组通信优化，分别针对上述三个通信集合： </p><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-1024x576.png" alt="DeepSpeed ZeRO++ 量化图形" class="wp-image-950718" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure3.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 3：qwZ 中基于块的量化。从图中可以看出，与基本量化相比，块量化具有更好的数据精度。</figcaption></figure><h3 class="wp-block-heading" id="quantized-weight-communication-for-zero-qwz">ZeRO 的量化重量通信 (qwZ)</h3><p>首先，为了减少全收集过程中的参数通信量，我们采用权重量化的方式，在通信前将每个模型参数从 FP16（两个字节）动态缩小为 INT8（一字节）数据类型，并在通信后对权重进行反量化。然而，天真地对权重进行量化可能会降低模型训练的准确性。为了保持良好的模型训练精度，我们采用基于块的量化，对模型参数的每个子集进行独立量化。目前还没有高性能、基于块的量化的实现。因此，我们从头开始实现高度优化的量化 CUDA 内核，与基本量化相比，其准确度提高了 3 倍，速度提高了 5 倍。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="720" height="360" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure4.png" alt="DeepSpeed ZeRO++ 权重分区图" class="wp-image-950721" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure4.png 720w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure4-300x150.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure4-240x120.png 240w" sizes="(max-width: 720px) 100vw, 720px" /><figcaption class="wp-element-caption">图 4：hpZ 中的层次权重划分。该图显示 hpZ 在每个 GPU 上保存辅助模型分区，而 0-3 仅保存主模型分区。</figcaption></figure><h3 class="wp-block-heading" id="hierarchical-weight-partition-for-zero-hpz"> ZeRO (hpZ) 的分层权重划分</h3><p>其次，为了减少向后传递过程中权重全收集的通信开销，我们用 GPU 内存来交换通信。更具体地说，我们没有像 ZeRO 那样将整个模型权重分散到所有机器上，而是在每台机器内维护完整的模型副本。以更高的内存开销为代价，这使我们能够用机器内全收集/广播取代昂贵的跨机器全收集/权重广播，由于更高的机器内通信带宽，速度要快得多。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1920" height="1080" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure5.gif" alt="DeepSpeed ZeRO++ 动画图形" class="wp-image-950724"/><figcaption class="wp-element-caption">图 5：qgZ 的端到端工作流程。该动画图展示了qgZ组件的整个工作流程，包括张量切片重排序、节点内量化、节点内All-to-All通信、节点内反量化、节点内缩减、节点间量化、节点间All -to-all通信、节点间反量化、节点间缩减。 </figcaption></figure><h3 class="wp-block-heading" id="quantized-gradient-communication-for-zero-qgz"> ZeRO (qgZ) 的量化梯度通信</h3><p>第三，使用减少分散来降低梯度的通信成本更具挑战性。直接应用量化来减少通信量是不可行的。即使我们将基于块的量化纳入低精度，梯度减少也会累积并放大量化误差。为了解决这个问题，我们只在通信之前量化梯度，但在任何归约操作之前将它们反量化到完全精度。为了有效地做到这一点，我们发明了一种基于全对全的新型量化梯度通信范式，称为 qgZ，它在功能上相当于压缩的减少分散集体操作。</p><p> qgZ 旨在解决两个挑战：i) 如果我们简单地在 INT4/INT8 中实现减少分散，克服低精度归约会导致的显着精度损失，以及 ii) 避免由于以下原因而导致的精度下降和显着延迟开销即使我们以全精度进行减少，传统方法也需要一长串量化和反量化步骤来减少基于环或树的散射。 qgZ 没有使用基于环或树的缩减分散算法，而是基于一种新颖的分层全对全方法。</p><p> qgZ 中有三个主要步骤：i）梯度切片重新排序，ii）节点内通信和缩减，以及 iii）节点间通信和缩减。首先，在发生任何通信之前，我们对梯度进行切片并进行张量切片重新排序，以确保通信结束时每个 GPU 上的最终梯度放置（即图 5 中的绿色块）是正确的。其次，我们对重新排序的梯度切片进行量化，在每个节点内进行全对全通信，对从全对全接收到的梯度切片进行反量化，并进行局部缩减。第三，我们再次量化局部约简梯度，进行节点间全对所有通信，再次对接收到的梯度进行反量化，并计算最终的高精度梯度约简，得到如图 5 中绿色块所示的结果。</p><p>采用这种分层方法的原因是为了减少跨节点通信量。更准确地说，给定每个节点 N 个 GPU、M 的模型大小和 Z 的量化比，单跳 all-to-all 将生成 M*N/Z 跨节点流量。相比之下，通过这种分层方法，我们将每个 GPU 的跨节点流量从 M/Z 减少到 M/(Z*N)。这样，总通信量从M*N/Z减少到M*N/(Z*N)=M/Z。我们通过重叠节点内和节点间通信以及融合 CUDA 内核（张量切片重新排序 + 节点内量化）和（节点内反量化 + 节点内缩减 +节点间量化）。</p><figure class="wp-block-table"><table><tbody><tr><td><strong>通讯量</strong></td><td><strong>前向全聚集举重</strong></td><td><strong>向后全聚集重量</strong></td><td><strong>梯度上的后向减少散射</strong></td><td><strong>全部的</strong></td></tr><tr><td>零</td><td>中号</td><td>中号</td><td>中号</td><td>3M</td></tr><tr><td>零度++</td><td> 0.5M</td><td> 0</td><td> 0.25M</td><td> 0.75M</td></tr></tbody></table></figure><h3 class="wp-block-heading" id="communication-volume-reduction">通讯量减少</h3><p>通过合并上述所有三个组件，我们将跨节点通信量从 3M 减少到 0.75M。更具体地说，我们使用 qwZ 将模型权重的前向全收集/广播从 M 减少到 0.5M。我们使用 hpZ 消除了反向传播期间的跨节点全收集，将通信从 M 减少到 0。最后，我们使用 qgZ 将反向传播期间的跨节点归约分散通信从 M 减少到 0.25M。</p><h2 class="wp-block-heading" id="zero-accelerates-llm-training"> ZeRO++加速LLM培训</h2><p>在这里，我们展示了 ZeRO++ 在 384 个 Nvidia V100 GPU 上的真实 LLM 训练场景的评估结果。 </p><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="459" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-1024x459.png" alt="DeepSpeed ZeRO++ 条形图" class="wp-image-950727" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-1024x459.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-300x134.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-768x344.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6-240x108.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure6.png 1165w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 6：zero++ 与 400 Gbps 互连的零的吞吐量比较。如图所示，zero++ 在每个 GPU 1k 令牌的情况下可以实现高达 1.56 倍的加速，而在每个 GPU 2k 令牌的情况下，zero++ 可以实现 1.41 倍的加速。</figcaption></figure><h3 class="wp-block-heading" id="high-efficiency-with-small-batch-per-gpu">每个 GPU 的小批量高效率</h3><p><strong>高带宽集群：</strong><em> </em>如图 6 所示，我们首先展示了使用 4x Infiniband (IB) 进行 400Gbps 跨节点互连（每个都以 100Gbps 运行）的不同模型大小和微批量大小的 ZeRO++ 吞吐量相对于 ZeRO 的改进。每个 GPU 具有 1k 令牌，ZeRO++ 的吞吐量比 ZeRO-3 提高了 28% 到 36%。对于 2k 微批量大小，ZeRO++ 的吞吐量比 ZeRO-3 提高了 24% 至 29%。 </p><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="433" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-1024x433.png" alt="DeepSpeed ZeRO++ 条形图" class="wp-image-950730" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-1024x433.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-300x127.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-768x325.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-665x280.png 665w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7-240x102.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure7.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 7：ZeRO++ 与具有 100Gbps 互连的 ZeRO 的吞吐量比较。图中显示，与 ZeRO 相比，ZeRO++ 在每个 GPU 1k 令牌的情况下实现了 2.21 倍的加速，而在每个 GPU 2k 令牌的情况下实现了 1.77 倍的加速。</figcaption></figure><p><strong>低带宽集群：</strong><em> </em>在 100Gbps 网络等低网络环境中，ZeRO++ 的性能明显优于 ZeRO-3。如图 7 所示，与 ZeRO-3 相比，ZeRO++ 在端到端吞吐量方面实现了高达 2.2 倍的加速。平均而言，ZeRO++ 比 ZeRO-3 基准实现约 2 倍的加速。 </p><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="396" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-1024x396.png" alt="DeepSpeed ZeRO++ 条形图" class="wp-image-950733" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-1024x396.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-300x116.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-768x297.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8-240x93.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure8.png 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 8：采用低带宽互连的 ZeRO++ 实现的吞吐量与采用高带宽互连的 ZeRO 类似。该图显示，在 18B 和 138B 模型尺寸中，具有低带宽网络的 ZeRO++ 与具有高带宽互连的 ZeRO 实现了相似的吞吐量。 </figcaption></figure><h3 class="wp-block-heading" id="enabling-efficiency-equivalence-between-high-and-low-bandwidth-clusters">实现高带宽集群和低带宽集群之间的效率等效</h3><p>此外，与在更高带宽设置下的 ZeRO 相比，ZeRO ++ 在低带宽集群中可以实现相当的系统吞吐量。如图 8 所示，对于 18B 和 138B 型号，采用 200Gbps 跨节点链路的 ZeRO++ 与采用 800 Gbps 跨节点链路设置的 ZeRO-3 可以达到相似的 TFLOP。</p><p>鉴于 ZeRO++ 出色的可扩展性，我们将 ZeRO++ 视为下一代用于训练大型 AI 模型的 ZeRO。</p><h2 class="wp-block-heading" id="zero-for-rlhf-training-with-deepspeed-chat"> ZeRO++ 通过 DeepSpeed-Chat 进行 RLHF 训练</h2><h3 class="wp-block-heading" id="rlhf-training-background">RLHF培训背景</h3><p>类似 ChatGPT 的模型由 LLM 提供支持，并<a href="https://openai.com/blog/chatgpt" target="_blank" rel="noreferrer noopener">使用 RLHF 进行微调</a>。 RLHF 由生成（推理）阶段和训练阶段组成。在生成阶段，参与者模型将部分对话作为输入，并使用一系列前向传递生成响应。然后在训练阶段，批评者模型按质量对生成的响应进行排名，为演员模型提供强化信号。使用这些排名对参与者模型进行微调，使其能够在后续迭代中生成更准确、更合适的响应。</p><p> RLHF 训练会带来不小的内存压力，因为它使用四种模型（参与者、参考、评论家、奖励）。采用低秩自适应（LoRA）来解决 RLHF 的内存压力。 LoRA 冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到 Transformer 架构的每一层中，从而显着减少了可训练参数的数量。 LoRA 通过减少内存使用来加速 RLHF，允许更大的批量大小，从而大大提高吞吐量。</p><h3 class="wp-block-heading" id="deepspeed-chat-with-zero-for-rlhf-training"> DeepSpeed-Chat 与 ZeRO++ 进行 RLHF 训练</h3><figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="435" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-1024x435.png" alt="DeepSpeed ZeRO++ 条形图" class="wp-image-950709" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-1024x435.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-300x128.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-768x326.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9-240x102.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/DeepSpeedZero-opp_figure9.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">图 9：RLHF 训练中的 ZeRO++ 加速。左图显示 ZeRO++ 在 RLHF 步骤 1 训练中实现了 1.26 倍的加速。右图显示 ZeRO++ 在 RLHF step3 代币生成中实现了高达 2.25 倍的加速。</figcaption></figure><p>带有 LoRA 的 RLHF 是 ZeRO++ 的独特应用，因为大多数模型权重都是冻结的。这意味着 ZeRO++ 可以将这些冻结权重保存在 INT4/8 中进行量化，而不是将它们存储在 FP16 中并在每次通信操作之前对其进行量化。通信后仍然进行反量化，以使权重为计算做好准备，但计算后简单地丢弃反量化的权重。</p><p>以这种方式使用 ZeRO++ 进行 RLHF 训练可以减少内存使用量和通信量。这通过减少通信以及由于减少内存使用而实现更大的批量大小来提高训练吞吐量。在生成阶段，ZeRO++使用hpZ将所有权重通信保留在每个节点内，以利用更高的节点内通信带宽并减少通信量，进一步提高生成吞吐量。</p><p> ZeRO++ 集成到 DeepSpeed-Chat 中，为类似 ChatGPT 模型的 RLHF 训练提供支持。在图 9 中，我们比较了不同大小的 Actor 模型的 RLHF 生成吞吐量，将 ZeRO 与 ZeRO++ 进行了比较，针对 32 V100 GPU 上的 30B 和 66B Actor 模型。结果表明，ZeRO++ 的 RLHF 生成吞吐量比 ZeRO 高出 2.25 倍。我们还展示了 16 个 V100 GPU 上训练阶段的加速情况，其中 ZeRO++ 的吞吐量比 ZeRO 高出 1.26 倍，这是由于 ZeRO++ 实现了更少的通信和更大的批量大小。</p><h2 class="wp-block-heading" id="release-try-deepspeed-zero-today">发布：立即尝试 DeepSpeed ZeRO++</h2><p>我们非常高兴能够发布 DeepSpeed ZeRO++ 并将其提供给 AI 社区中的任何人。首先，请访问我们的 GitHub 页面进行<a href="https://github.com/microsoft/DeepSpeed/blob/master/docs/_tutorials/zeropp.md" target="_blank" rel="noreferrer noopener">LLM 培训</a>。 DeepSpeed-Chat 的 ZeRO++ 将在未来几周内发布。</p><div class="wp-block-buttons"><div class="wp-block-button is-style-fill-github"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.deepspeed.ai/tutorials/zeropp/" target="_blank" rel="noreferrer noopener">获得法学硕士培训</a></div><div class="wp-block-button is-style-fill"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/zero-extremely-efficient-collective-communication-for-giant-model-training/">阅读技术论文</a></div></div><p>DeepSpeed-ZeRO++ 是 DeepSpeed 生态系统的一部分。要了解更多信息，请访问<a href="https://www.deepspeed.ai/" target="_blank" rel="noreferrer noopener">我们的网站</a>，您可以在其中找到详细的博客文章、教程和有用的文档。</p><p>有关 DeepSpeed 的最新新闻，请在社交媒体上关注我们：</p><div class="wp-block-buttons"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://twitter.com/MSFTDeepSpeed" target="_blank" rel="noreferrer noopener">在推特上（英文）</a></div><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://twitter.com/MSFTDeepSpeedJP" target="_blank" rel="noreferrer noopener">在推特上（日语）</a></div><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.zhihu.com/people/deepspeed" target="_blank" rel="noreferrer noopener">在知乎上（中文）</a></div></div><p> DeepSpeed 欢迎您做出贡献。我们鼓励您在<a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noreferrer noopener">DeepSpeed GitHub 页面上报告问题、贡献 PR 并加入讨论。</a>请参阅我们的<a href="https://github.com/microsoft/DeepSpeed/blob/master/CONTRIBUTING.md" target="_blank" rel="noreferrer noopener">贡献指南</a>了解更多详细信息。我们对与大学、研究实验室和公司的合作持开放态度。对于此类请求（以及其他不适合 GitHub 的请求），请直接发送电子邮件至<a href="mailto:deepspeed-info@microsoft.com" target="_blank" rel="noreferrer noopener">deepspeed-info@microsoft.com</a> 。</p><h2 class="wp-block-heading" id="contributors">贡献者</h2><p>该项目的实现得益于 DeepSpeed 团队以下人员的贡献：</p><p><a href="https://www.microsoft.com/en-us/research/people/guanhuawang/">王冠华</a>、秦何阳、Sam Ade Jacobs、Connor Holmes、 <a href="https://www.microsoft.com/en-us/research/people/samyamr/">Samyam Rajbhandari</a> 、 <a href="https://www.microsoft.com/en-us/research/people/olruwase/">Olatunji Ruwase</a> 、 <a href="https://www.microsoft.com/en-us/research/people/amawa/">Ammar Ahmad Awan</a> 、 <a href="https://www.microsoft.com/en-us/research/people/jerasley/">Jeff Rasley</a> 、Michael Wyatt、<a href="https://www.microsoft.com/en-us/research/people/yuxhe/">何宇雄</a>（<em>团队负责人</em>）</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/">DeepSpeed ZeRO++ 帖子：LLM 和聊天模型训练的速度飞跃，通信量减少了 4 倍，</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title>微软在 CVPR 2023：突破计算机视觉的界限</title><link/>https://www.microsoft.com/en-us/research/blog/microsoft-at-cvpr-2023-pushing-the-boundaries-of-computer-vision/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Tue, 20 Jun 2023 16:19:50 +0000</pubDate> <category><![CDATA[Research Blog]]></category><guid ispermalink="false"></guid><description><![CDATA[<p>在广阔的人工智能领域中，很少有领域能够像计算机视觉那样激发我们的想象力并突破可能性的界限。这一研究和创新领域的核心是为现实世界中基于视觉的系统提供技术支持的雄心，使机器能够以无与伦比的方式吸收和响应视觉刺激[...]</p><p>这篇文章<a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/microsoft-at-cvpr-2023-pushing-the-boundaries-of-computer-vision/">《Microsoft at CVPR 2023：突破计算机视觉的界限》</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img decoding="async" loading="lazy" width="1400" height="264" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/BlogBanner-1400x264_AH.jpg" alt="CVPR 2023 会议徽标，显示不列颠哥伦比亚省温哥华市的天际线以及会议日期，2023 年 6 月 18 日至 23 日。背景中有一张温哥华市在阳光明媚的日子里的褪色照片。" class="wp-image-948327" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/06/BlogBanner-1400x264_AH.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/BlogBanner-1400x264_AH-300x57.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/BlogBanner-1400x264_AH-1024x193.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/BlogBanner-1400x264_AH-768x145.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/06/BlogBanner-1400x264_AH-240x45.jpg 240w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>在广阔的人工智能领域中，很少有领域能够像<a href="https://www.microsoft.com/en-us/research/research-area/computer-vision/?">计算机视觉</a>那样激发我们的想象力并突破可能性的界限。该研究和创新领域的核心是为现实世界中基于视觉的系统提供技术支持，使机器能够以无与伦比的精度和复杂性接收和响应视觉刺激。通过人工智能、深度学习和海量数据的结合，计算机视觉近年来取得了长足的进步，将我们带入了一个看似不可能变为现实的时代。</p><p> 2023 年<a href="https://cvpr2023.thecvf.com/" target="_blank" rel="noreferrer noopener">计算机视觉和模式识别</a>(CVPR) 于 6 月 10 日至 22 日举行，是一项广受认可的活动，汇聚了计算机视觉领域的领先专家。它作为展示该领域一些最引人注目和创新作品的平台。</p><p>微软研究人员及其合作者在今年的 CVPR 上做出的贡献涵盖了广泛的研究工作。从生成模型和网络预训练到手语理解和神经视频编解码器，这些尖端进步强调了系统从视觉数据中分析和提取有价值见解的不断发展的能力。</p><p>以下是一些亮点（请参阅下面的已发表论文及其作者列表）：</p><h2 class="wp-block-heading" id="uniting-vision-language-and-multi-modal-encoding">联合视觉、语言和多模式编码</h2><p>论文《 <a href="https://www.microsoft.com/en-us/research/publication/image-as-a-foreign-language-beit-pretraining-for-vision-and-vision-language-tasks/">图像作为外语：BEiT 视觉和视觉语言任务预训练</a>》处于视觉、语言和多模态预训练的交叉点。为了从这些不同形式的数据中学习，我们提出了一个通用的基础模型，将图像视为“外语”。来自不同模态的数据使用 Multiway Transformers 进行编码，这是一种模块化架构，可实现模态特定的编码和深度融合。该模型在图像、文本和图像-文本对上进行预训练，从而将掩码语言建模方法推广到不同的模态。通过大幅扩展模型和数据，我们发现基础架构和预训练方面的进步在各种视觉和视觉语言任务上带来了出色的传输性能，包括对象检测、语义分割、图像分类、视觉推理、视觉问答、图像字幕和跨模式图像检索。</p><h2 class="wp-block-heading" id="scaling-training-data-for-large-vision-models">扩展大型视觉模型的训练数据</h2><p>大型语言模型的优势源于它们大规模利用未标记训练数据的能力。通过使用这些数据，这些模型获得了对语言的广泛理解，增强了泛化能力，并提高了在广泛的语言相关任务中的性能。受这一成就的启发，我们的研究重点是扩展大型视觉模型训练数据的可能性。在论文“ <a href="https://www.microsoft.com/en-us/research/publication/on-data-scaling-in-masked-image-modeling/">关于掩模图像建模中的数据缩放</a>”中，我们探讨了数据缩放对通过掩模图像建模预训练的大型视觉模型的影响。通过广泛的调查，我们发现大型视觉模型中的蒙版图像建模需要大规模数据来进行有效的预训练。然而，与大型语言模型不同，大型视觉模型无法从非过度拟合场景中的更多数据中受益。这些发现加深了我们对蒙版图像建模的理解，并可能为大规模视觉模型的未来进步铺平道路。</p><h2 class="wp-block-heading" id="creating-3d-avatars-with-a-diffusion-network">使用扩散网络创建 3D 头像</h2><p>在图像生成领域，在将文本描述转化为令人惊叹的视觉效果方面取得了令人难以置信的进步。 DALL-E 和扩散模型的兴起将这些尖端工具带到了日常用户的手中。在论文“ <a href="https://www.microsoft.com/en-us/research/publication/rodin-a-generative-model-for-sculpting-3d-digital-avatars-using-diffusion/">RODIN：使用扩散雕刻 3D 数字化身的生成模型</a>”中，我们通过将扩散的力量引入 3D 化身生成来扩展这一创新。为此，有必要将扩散从 2D 转移到 3D。然而，将扩散从 2D 转移到 3D 是一项重大挑战，因为在 3D 中生成具有丰富细节的高质量结果需要高昂的内存和处理成本。我们通过提出推出扩散网络 (RODIN) 来克服这个问题，该网络将 3D 神经辐射场展开到单个 2D 特征平面中，并在其上执行 3D 感知扩散。在其他技术贡献的支持下，包括促进全局一致性的潜在调节和进一步增强细节的分层合成，RODIN 显着加速了原本繁琐的 3D 建模过程，并为 3D 艺术家带来了新的机会。 </p><div style="height:15px" aria-hidden="true" class="wp-block-spacer"></div><div class="border-bottom border-top border-gray-300 mt-5 mt-md-4 mb-4 mb-md-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="935415"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">聚焦：人工智能聚焦领域</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/focus-area/ai-and-microsoft-research/" aria-label="AI and Microsoft Research" data-bi-cN="AI and Microsoft Research" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2020/07/newsletter-option-8-neural-network-3-1.png" alt="深蓝色背景上的抽象神经网络模式" /></a></div><div class="msr-promo__content py-3 col-12 col-md"><h2 class="h4">人工智能和微软研究院</h2><p class="large">详细了解 Microsoft 人工智能研究的广度</p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/focus-area/ai-and-microsoft-research/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Learn more" data-bi-cN="AI and Microsoft Research" target="_blank">了解更多</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--></div><!--/.msr-promo--><p>微软在 CVPR 2023 上发表的论文及其作者：</p><ol type="1"><li> <a href="https://www.microsoft.com/en-us/research/publication/3d-human-mesh-estimation-from-virtual-markers/">根据虚拟标记进行 3D 人体网格估计</a><br>马晓轩，<em>北京大学</em>；苏家俊，<em>北京大学</em>；<a href="https://www.microsoft.com/en-us/research/people/chnuwa/">王春雨</a>，<em>微软研究院</em>；朱文涛，<em>北京大学</em>；王一洲，<em>北京大学</em><em>视觉技术国家工程研究中心</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/limap-global-mapper-3d-line-mapping-revisited/">重新审视 3D 线映射</a><br>刘少辉，<em>苏黎世联邦理工学院</em>；于一凡，<em>苏黎世联邦理工学院</em>； Rémi Pautrat，<em>苏黎世联邦理工学院</em>； <a href="https://www.microsoft.com/en-us/research/people/mapoll/">Marc Pollefeys</a> ，<em>苏黎世联邦理工学院和微软研究院</em>；维克托·拉尔森，<em>隆德大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/blendfields-few-shot-example-driven-facial-modeling/">BlendFields：少样本示例驱动的面部建模</a><br>Kacper Kania，<em>华沙理工大学</em>； Stephan J. Garbin，<em>微软研究院</em>；安德里亚·塔利亚萨基 (Andrea Tagliasacchi)、<em>西蒙·弗雷泽 (Simon Fraser) 以及大学和 Google Brain</em> ；弗吉尼亚埃斯特尔斯，<em>微软研究院</em>； Kwang Moo Yi，<em>不列颠哥伦比亚大学</em>； Julien Valentin，<em>微软研究院</em>； Tomasz Trzciński，<em>雅盖隆大学</em>； Marek Kowalski，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/cico-domain-aware-sign-language-retrieval-via-cross-lingual-contrastive-learning/">CiCo：通过跨语言对比学习进行领域感知手语检索<br></a>程怡婷，<em>复旦大学</em>；<a href="https://www.microsoft.com/en-us/research/people/fawe/">魏方云</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/jianbao/">包建民</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/doch/">陈东</a>，<em>微软研究院</em>；张文强，<em>复旦大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/deep-frequency-filtering-for-domain-generalization/">用于域泛化的深度频率过滤</a><br>林士奇，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/zhizzhang/">张志正</a>，<em>微软研究院</em>；黄志鹏，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/yanlu/">陆岩</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/culan/">兰翠玲</a>，<em>微软研究院</em>；彭楚，<em>微软</em>；尤全增，<em>微软</em>；王江，<em>微软</em>；<a href="https://www.microsoft.com/en-us/research/people/zliu/">刘子成</a>，<em>微软研究院</em>；艾米·帕鲁卡 (Amey Parulkar)，<em>微软</em>； Viraj Navkal，<em>微软</em>；陈志波，<em>中国科学技术大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/deeplsd-line-segment-detection-and-refinement-with-deep-image-gradients/">DeepLSD：利用深度图像梯度进行线段检测和细化</a><br>Rémi Pautrat，<em>苏黎世联邦理工学院</em>；丹尼尔·巴拉特 (Daniel Barath)，<em>苏黎世联邦理工学院</em>；维克托·拉尔森，<em>隆德大学；</em>马丁·奥斯瓦尔德，<em>阿姆斯特丹大学</em>； <a href="https://www.microsoft.com/en-us/research/people/mapoll/">Marc Pollefeys</a> ，<em>苏黎世联邦理工学院和微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/detrs-with-hybrid-matching/">具有混合匹配的 DETR<br></a>丁佳，<em>北京大学</em>；袁宇辉，<em>微软研究院</em>；何浩迪，<em>斯坦福大学</em>；吴晓培，<em>浙江大学</em>；于浩军，<em>北京大学</em>；林伟红，<em>微软研究院</em>；孙磊，<em>微软研究院</em>；张超，<em>北京大学</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡瀚</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/efficientvit-memory-efficient-vision-transformer-with-cascaded-group-attention/">EfficientViT：具有级联组注意力的内存高效视觉转换器<br></a>刘新宇，<em>香港中文大学</em>；<a href="https://www.microsoft.com/en-us/research/people/hopeng/">彭厚文</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/nizhen/">郑宁欣</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/yuqyang/">杨玉清</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡涵</a>，<em>微软研究院</em>；袁逸轩，<em>香港中文大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/four-view-geometry-with-unknown-radial-distortion/">具有未知径向畸变的四视图几何</a><br>Petr Hruby、Viktor Korotynskiy、Timothy Duff、Luke Oeding、 <a href="https://www.microsoft.com/en-us/research/people/mapoll/">Marc Pollefeys</a> 、<em>苏黎世联邦理工学院和微软研究院</em>；托马斯·帕杰拉 (Tomas Pajdla)、维克多·拉尔森 (Viktor Larsson)，<em>隆德大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/high-fidelity-and-freely-controllable-talking-head-video-generation/">高保真且可自由控制的头显视频生成</a><br><a href="https://www.microsoft.com/en-us/research/people/yuegao/">高跃</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/zhouyuan/">周远</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/jinglwa/">王静璐</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/xili11/">李晓</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/xiangming/">向明</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/yanlu/">陆岩</a><em>，微软研究院</em></li><li> <a href="https://www.microsoft.com/en-us/research/publication/human-pose-as-compositional-tokens/">作为构图标记的人体姿势</a><br>耿子刚，<em>中国科学技术大学</em>、<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/chnuwa/">王春雨</a>，<em>微软研究院</em>；魏一轩，<em>清华大学</em><em>和</em><em>微软研究院</em>；刘泽，<em>中国科学技术大学和</em><em>微软研究院</em>；李厚强，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡瀚</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/iclip-bridging-image-classification-and-contrastive-language-image-pre-training-for-visual-recognition/">iCLIP：桥接图像分类和对比语言图像预训练以实现视觉识别</a><br>魏一轩，<em>清华大学和微软研究院</em>；曹悦，<em>微软研究院</em>；张峥，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/hopeng/">彭厚文</a>，<em>微软研究院</em>；姚祝良，<em>清华大学和微软研究院</em>；谢振达，<em>清华大学和微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡涵</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/bainguo/">郭柏宁</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/image-as-a-foreign-language-beit-pretraining-for-all-vision-and-vision-language-tasks/">图像作为外语：所有视觉和视觉语言任务的 BEiT 预训练<br></a>王文辉，<em>微软；</em>鲍航波，<em>微软；</em><a href="https://www.microsoft.com/en-us/research/people/lidong1/">李东</a>，<em>微软研究院；</em>约翰·比约克，<em>微软；</em>彭志良，<em>微软；</em>刘强，<em>微软；</em> <a href="https://www.microsoft.com/en-us/research/people/kragga/">Kriti Aggarwal</a> ，<em>微软研究院；</em> Owais Khan Mohammed，<em>微软；</em> <a href="https://www.microsoft.com/en-us/research/people/saksingh/">Saksham Singhal</a> ，<em>微软研究院；</em> Subhojit Som，<em>微软；</em><a href="https://www.microsoft.com/en-us/research/people/fuwei/">魏福如</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/iterative-proposal-refinement-for-weakly-supervised-video-grounding/">弱监督视频接地的迭代提案细化</a><br>曹萌，<em>北京大学</em>；<a href="https://www.microsoft.com/en-us/research/people/fawe/">魏方云</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/caxu/">徐灿</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/xigeng/">耿秀波</a>，<em>微软研究院</em>；陈龙，<em>香港科技大学</em>；张灿，<em>北京大学；</em>邹月贤，<em>北京大学；</em>沉涛，<em>微软</em>；<a href="https://www.microsoft.com/en-us/research/people/djiang/">姜大新</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/layoutformer-conditional-graphic-layout-generation-via-constraint-serialization-and-decoding-space-restriction/">LayoutFormer++：通过约束序列化和解码空间限制生成条件图形布局</a><br>蒋兆云，<em>西安交通大学</em>；郭嘉琪，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/shizsu/">孙世钊</a>，<em>微软研究院</em>；邓华宇，<em>上海交通大学</em>；吴仲凯，<em>北京航空航天大学</em>； Vuksan Mijovic，<em>微软</em>；紫江 James Yang，<em>西安交通大学</em>；<a href="https://www.microsoft.com/en-us/research/people/jlou/">楼建光</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/dongmeiz/">张冬梅</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/learning-to-exploit-temporal-structure-for-biomedical-vision-language-processing/">学习利用时间结构进行生物医学视觉语言处理</a><br><a href="https://www.microsoft.com/en-us/research/people/shbannur/">Shruthi Bannur</a> ，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/sthyland/">斯蒂芬妮·海兰德</a>，<em>微软研究院</em>；刘谦初， <a href="https://www.microsoft.com/en-us/research/people/fperezgarcia/">Fernando Pérez García</a> ，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/maxilse/">Maximilian Ilse</a> ，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/dacoelh/">丹尼尔·C·卡斯特罗 (Daniel C. Castro)</a> ，<em>微软研究院</em>； Benedikt Boecking、 <a href="https://www.microsoft.com/en-us/research/people/harssharma/">Harshita Sharma</a> ，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/t-kbouzid/">Kenza Bouzid</a> ，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/anthie/">Anja Thieme</a> ，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/antonsc/">Anton Schwaihofer</a> ，<em>微软研究院</em>； Maria Wetscherek、Matthew P. Lungren、 <a href="https://www.microsoft.com/en-us/research/people/adityan/">Aditya Nori，</a><em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/jaalvare/">哈维尔·阿尔瓦雷斯-瓦莱</a>，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/ozoktay/">Ozan Oktay</a><em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/look-before-you-match-instance-understanding-matters-in-video-object-segmentation/">匹配前先看：实例理解在视频对象分割中很重要</a><br>王俊科，<em>上海智能视觉计算协同创新中心</em>；<a href="https://www.microsoft.com/en-us/research/people/dochen/">陈东东</a>，<em>微软研究院</em>；吴祖轩，<em>上海市智能视觉计算协同创新中心；</em><a href="https://www.microsoft.com/en-us/research/people/cluo/">罗冲</a>，<em>微软研究院；</em><a href="https://www.microsoft.com/en-us/research/people/chutan/">唐传鑫</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/xidai/">戴喜阳</a>，<em>微软研究院</em>；赵宇成，<em>微软研究院</em>；谢宇佳，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/luyuan/">陆远</a>，<em>微软研究院</em>；姜玉刚，<em>上海智能视觉计算协同创新中心</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/maskclip-masked-self-distillation-advances-contrastive-language-image-pretraining/">MaskCLIP：掩模自蒸馏推进对比语言图像预训练</a><br>董晓义，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/jianbao/">包建民</a>，<em>微软研究院</em>；郑英林，<em>厦门大学</em>；<a href="https://www.microsoft.com/en-us/research/people/tinzhan/">张婷</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/dochen/">陈东东</a>，<em>微软研究院</em>；杨浩，<em>微软研究院</em>；曾明，<em>厦门大学</em>；张伟明，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/luyuan/">陆远</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/doch/">陈东</a>，<em>微软研究院</em>；方文，<em>微软研究院</em>；于能海，<em>中国科学技术大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/metaportrait-identity-preserving-talking-head-generation-with-fast-personalized-adaptation/">MetaPortrait：具有快速个性化适应功能的保留身份的头像生成<br></a>张博文，<em>中国科学技术大学</em>；齐晨阳，<em>香港科技大学</em>；张攀，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/zhanbo/">张波</a>，<em>微软研究院</em>；吴祥涛，<em>微软</em>；陈东，<em>香港科技大学</em>；陈奇峰，<em>香港科技大学</em>；王勇，<em>中国科学技术大学</em>；方文，<em>微软</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/mm-diffusion-learning-multi-modal-diffusion-models-for-joint-audio-and-video-generation/">MM-Diffusion：学习用于联合音频和视频生成的多模态扩散模型</a><br>阮鲁丹，<em>中国人民大学</em>；马一阳，<em>北京大学</em>；<a href="https://www.microsoft.com/en-us/research/people/huayan/">杨欢</a>，<em>微软研究院</em>；何惠国，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/libei/">刘蓓</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/jianf/">付建龙</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/nicholasyuan/">袁靖</a>，<em>微软研究院</em>；秦进，<em>中国人民大学</em>；<a href="https://www.microsoft.com/en-us/research/people/bainguo/">郭柏宁</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/motion-information-propagation-for-neural-video-compression/">神经视频压缩的运动信息传播</a><br>齐林峰，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/jiahali/">李嘉豪</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/libin/">李斌</a>，<em>微软研究院</em>；李厚强，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/yanlu/">陆岩</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/natural-language-assisted-sign-language-recognition/">自然语言辅助手语识别<br></a>左荣来，<em>香港科技大学</em>；<a href="https://www.microsoft.com/en-us/research/people/fawe/">魏方云</a>，<em>微软研究院</em>； Brian Mak，<em>香港科技大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/neural-video-compression-with-diverse-contexts/">具有不同上下文的神经视频压缩<br></a><a href="https://www.microsoft.com/en-us/research/people/jiahali/">李嘉豪</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/libin/">李斌</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/yanlu/">陆岩</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/on-data-scaling-in-masked-image-modeling/">关于蒙版图像建模中的数据缩放</a><br>谢振达，<em>清华大学和微软研究院</em>；张峥，<em>微软研究院</em>；曹悦，<em>微软研究院</em>；林雨桐，<em>西安交通大学和微软研究院</em>；魏一轩，<em>清华大学和微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/qid/">戴奇</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡瀚</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/paint-by-example-exemplar-based-image-editing-with-diffusion-models/">通过示例进行绘制：使用扩散模型进行基于示例的图像编辑<br></a>杨斌新，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/shuyanggu/">顾书阳</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/zhanbo/">张波</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/tinzhan/">张婷</a>，<em>微软研究院</em>；陈学进，<em>中国科学技术大学</em>；孙晓燕，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/doch/">陈东</a>，<em>微软研究院</em>；方文，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/reco-region-controlled-text-to-image-generation/">ReCo：区域控制的文本到图像生成</a><br><a href="https://zyang-ur.github.io/">杨正元</a>，<em>微软研究院；</em>王剑锋，<em>微软；</em>甘哲，<em>微软；</em><a href="https://www.microsoft.com/en-us/research/people/linjli/">李林杰</a>，<em>微软研究院；</em> <a href="https://www.microsoft.com/en-us/research/people/keli/">Kevin Lin</a> ，<em>微软研究院；</em><a href="https://www.microsoft.com/en-us/research/people/chewu/">吴晨飞</a>，<em>微软研究院；</em><a href="https://www.microsoft.com/en-us/research/people/nanduan/">段南</a>，<em>微软；</em><a href="https://www.microsoft.com/en-us/research/people/zliu/">刘子成</a>，<em>微软研究院；</em>刘策，<em>微软；</em> <a href="https://www.microsoft.com/en-us/research/people/nzeng/">Michael Zeng</a> ，<em>微软研究院；</em><a href="https://www.microsoft.com/en-us/research/people/lijuanw/">王丽娟</a><em>，微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/resformer-scaling-vits-with-multi-resolution-training/">ResFormer：通过多分辨率训练扩展 ViT<br></a>田睿，<em>复旦大学·上海智能视觉计算协同创新中心</em>；吴祖轩，<em>复旦大学·上海智能视觉计算协同创新中心</em>；<a href="https://www.microsoft.com/en-us/research/people/qid/">戴奇</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡涵</a>，<em>微软研究院</em>；乔宇，<em>上海人工智能实验室</em>；蒋玉刚，<em>复旦大学·上海智能视觉计算协同创新中心</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/revealing-the-dark-secrets-of-masked-image-modeling/">揭示蒙版图像建模的黑暗秘密<br></a>谢振达，<em>清华大学和微软研究院</em>；耿子刚，<em>中国科学技术大学、微软研究院</em>；胡景成，<em>清华大学和微软研究院</em>；张峥，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡涵</a>，<em>微软研究院</em>；曹悦，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/rodin-a-generative-model-for-sculpting-3d-digital-avatars-using-diffusion/">RODIN：使用扩散雕刻 3D 数字化身的生成模型</a><br>王腾飞，<em>香港科技大学</em>；<a href="https://www.microsoft.com/en-us/research/people/zhanbo/">张波</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/tinzhan/">张婷</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/shuyanggu/">顾书阳</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/jianbao/">包建民</a>，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/tabaltru/">Tadas Baltrusaitis</a> ，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/jinshen/">沉晶晶</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/doch/">陈东</a>，<em>微软研究院</em>；方文，<em>微软研究院</em>；陈奇峰，<em>香港科技大学；</em><a href="https://www.microsoft.com/en-us/research/people/bainguo/">郭柏宁</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/seqtrack-sequence-to-sequence-learning-for-visual-object-tracking/">SeqTrack：用于视觉对象跟踪的序列到序列学习<br></a>陈鑫，<em>大连理工大学</em>；<a href="https://www.microsoft.com/en-us/research/people/hopeng/">彭厚文</a>，<em>微软研究院</em>；王栋，<em>大连理工大学</em>；陆虎川，<em>大连理工大学彭程实验室</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡瀚</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/side-adapter-network-for-open-vocabulary-semantic-segmentation/">用于开放词汇语义分割的侧适配器网络</a><br>徐孟德，<em>华中科技大学、微软研究院</em>；张峥，<em>华中科技大学和微软研究院；</em><a href="https://www.microsoft.com/en-us/research/people/fawe/">魏方云</a>，<em>微软研究院；</em><a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡涵</a>，<em>微软研究院；</em>白翔；<em>华中科技大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/streaming-video-model/">流媒体视频模型<br></a>赵玉成，<em>中国科学技术大学</em>；<a href="https://www.microsoft.com/en-us/research/people/cluo/">罗冲</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/chutan/">唐传鑫</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/dochen/">陈东东，</a><em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/ncodella/">诺埃尔·科德拉 (Noel Codella)</a> ，<em>微软研究院</em>；查正军，<em>中国科学技术大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/structural-multiplane-image-bridging-neural-view-synthesis-and-3d-reconstruction/">结构多平面图像：桥接神经视图合成和 3D 重建<br></a>张明芳，<em>东京大学和微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/jinglwa/">王静璐</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/xili11/">李晓</a>，<em>微软研究院</em>；黄一飞，<em>东京大学</em>；佐藤洋一，<em>东京大学</em>；<a href="https://www.microsoft.com/en-us/research/people/yanlu/">陆岩</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/svformer-semi-supervised-video-transformer-for-action-recognition/">SVFormer：用于动作识别的半监督视频转换器<br></a>邢振，<em>复旦大学·上海智能视觉计算协同创新中心</em>；<a href="https://www.microsoft.com/en-us/research/people/qid/">戴奇</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡涵</a>，<em>微软研究院</em>；陈晶晶，<em>复旦大学·上海智能视觉计算协同创新中心；</em>吴祖轩，<em>复旦大学·上海智能视觉计算协同创新中心；</em>蒋玉刚，<em>复旦大学·上海智能视觉计算协同创新中心</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/tinymim-an-empirical-study-of-distilling-mim-pre-trained-models/">TinyMIM：提炼 MIM 预训练模型的实证研究</a><br>任素成，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/fawe/">魏方云</a>，<em>微软研究院</em>；张峥，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/hanhu/">胡瀚</a>，<em>微软研究院</em></li><li> <a href="https://www.microsoft.com/en-us/research/publication/two-shot-video-object-segmentation/">双镜头视频对象分割</a><br>严坤，<em>北京大学</em>；<a href="https://www.microsoft.com/en-us/research/people/xili11/">李晓</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/fawe/">魏方云</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/jinglwa/">王静璐</a>，<em>微软研究院</em>；张晨斌，<em>北京大学</em>；王平，<em>北京大学</em>；<a href="https://www.microsoft.com/en-us/research/people/yanlu/">陆岩</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/unifying-layout-generation-with-a-decoupled-diffusion-model/">使用解耦扩散模型统一布局生成</a><br>穆德惠，<em>西安交通大学</em>；<a href="https://www.microsoft.com/en-us/research/people/zhizzhang/">张志正</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/xiaoyizhang/">张晓毅</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/wenxie/">谢文轩</a>，<em>微软研究院</em>；王宇旺，<em>清华大学</em>；<a href="https://www.microsoft.com/en-us/research/people/yanlu/">陆岩</a>，<em>微软研究院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/videotrack-learning-to-track-objects-via-video-transformer/">VideoTrack：学习通过视频转换器跟踪对象</a><br>谢飞，<em>上海交通大学</em>；<a href="https://www.microsoft.com/en-us/research/people/leichu/">储雷</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/jiahali/">李嘉豪</a>，<em>微软研究院</em>；<a href="https://www.microsoft.com/en-us/research/people/yanlu/">陆岩</a>，<em>微软研究院</em>；马超，<em>上海交通大学</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/volrecon-volume-rendering-of-signed-ray-distance-functions-for-generalizable-multi-view-reconstruction/">VolRecon：用于可泛化多视图重建的符号射线距离函数的体积渲染</a><br>任宇凡，<em>洛桑联邦理工学院</em>； Fangjinhua Wang<em>苏黎世联邦理工学院</em>；张桐，<em>洛桑联邦理工学院；</em> <a href="https://www.microsoft.com/en-us/research/people/mapoll/">Marc Pollefeys</a> ，<em>苏黎世联邦理工学院和微软研究院；</em>萨宾·苏斯斯特伦克 (Sabine Süsstrunk)，<em>洛桑联邦理工学院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/x-avatar-expressive-human-avatars/">X-Avatar：富有表现力的人类头像</a><br>Kaiyue Shen，<em>苏黎世联邦理工学院</em>；陈果，<em>苏黎世联邦理工学院</em>；曼努埃尔·考夫曼，<em>苏黎世联邦理工学院</em>；胡安·何塞·萨拉特，<em>苏黎世联邦理工学院</em>； Julien Valentin，<em>微软研究院</em>；宋杰，<em>苏黎世联邦理工学院</em>；奥特玛·希利格斯 (Otmar Hilliges)，<em>苏黎世联邦理工学院</em></li><li><a href="https://www.microsoft.com/en-us/research/publication/unifying-vision-text-and-layout-for-universal-document-processing/" target="_blank" rel="noreferrer noopener">统一视觉、文本和布局以实现通用文档处理</a><br>唐子能，<em>北卡罗来纳大学 (UNC) 教堂山分校</em>； <a href="https://www.microsoft.com/en-us/research/people/ziyiyang/" target="_blank" rel="noreferrer noopener">杨紫怡</a>，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/guow/" target="_blank" rel="noreferrer noopener">王国新</a>，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/yuwfan/" target="_blank" rel="noreferrer noopener">方宇伟</a>，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/yaliu10/" target="_blank" rel="noreferrer noopener">刘洋</a>，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/chezhu/" target="_blank" rel="noreferrer noopener">朱晨光</a>，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/nzeng/" target="_blank" rel="noreferrer noopener">Michael Zeng</a> ，<em>微软研究院</em>； <a href="https://www.microsoft.com/en-us/research/people/chazhang/" target="_blank" rel="noreferrer noopener">张茶</a>，<em>微软研究院</em>； Mohit Bansal，<em>北卡罗来纳大学 (UNC) 教堂山分校</em></li></ol><p>这篇文章<a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/microsoft-at-cvpr-2023-pushing-the-boundaries-of-computer-vision/">《Microsoft at CVPR 2023：突破计算机视觉的界限》</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>