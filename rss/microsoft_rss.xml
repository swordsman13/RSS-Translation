<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2023 年 9 月 5 日，星期二 17:44:08 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.2.2</generator><item><title>重新思考人工智能时代对直接消息的信任</title><link/>https://www.microsoft.com/en-us/research/blog/rethinking-trust-in-direct-messages-in-the-ai-era/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Tue, 05 Sep 2023 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=964494 </guid><description><![CDATA[<p>微软研究人员提出了一种新方法，以确保电子邮件、短信、社交平台上的直接消息甚至电话中更大的信任和责任，以帮助减轻来自人工智能相关诈骗和欺诈的复杂威胁。</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/rethinking-trust-in-direct-messages-in-the-ai-era/">重新思考人工智能时代对直接消息的信任一文</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-large"><img decoding="async" loading="lazy" width="1024" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-1024x576.png" alt="重新思考人工智能时代对直接消息的信任 - 博客英雄展示流程图" class="wp-image-965043" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-Online-Accountability-blog-hero-1400x788-1.png 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure><p>这篇博文是探索我们在隐私、安全和密码学方面的研究的系列文章的一部分。对于上一篇文章，请参阅<a href="https://www.microsoft.com/en-us/research/blog/research-trends-in-privacy-security-and-cryptography" target="_blank" rel="noreferrer noopener">https://www.microsoft.com/en-us/research/blog/research-trends-in-privacy-security-and-cryptography</a> 。虽然人工智能有潜力大幅提高生产力，但这种能力同样可以用于恶意目的，例如自动创建复杂的诈骗消息。在这篇文章中，我们探讨了人工智能可能对在线通信生态系统构成的威胁，并概述了缓解这些威胁的高级方法。</p><h2 class="wp-block-heading" id="communication-in-the-age-of-ai"> AI时代的沟通</h2><p>政策制定者、人工智能研究人员、商界领袖和其他个人越来越担心人工智能对在线通信完整性的影响。这些担忧是有根据的，因为良性的人工智能聊天机器人可以很容易地被用来冒充人类，帮助传播错误信息，并影响公众舆论和个人信仰。针对目标进行个性化定制的所谓“鱼叉式网络钓鱼”攻击已被证明非常有效。如果受害者没有使用多因素身份验证，则尤其如此，这意味着通过网络钓鱼邮件窃取其登录凭据的攻击者可以使用这些凭据访问真实的服务。有组织的网络犯罪并没有错过这个机会；面向诈骗者和欺诈者销售的人工智能工具已经出现。这令人不安，因为民主制度、商业诚信和人际关系都取决于可信和有效的沟通——这一过程已明显迁移到数字领域。</p><p>当我们进入一个人们越来越多地与人工智能互动的世界时，必须承认生成式人工智能带来的这些挑战不仅仅是假设的。在我们提供的 Microsoft 产品中，它们表现为我们正在积极应对的真正威胁。 <a href="https://www.microsoft.com/en-us/corporate-responsibility/earn-trust" target="_blank" rel="noreferrer noopener"></a>我们开始见证人工智能以个性化、自动化和可扩展的方式生成高度特定类型的文本（电子邮件、报告、脚本、代码）的影响。在工作场所，人工智能驱动的工具有望带来生产力的巨大提高，让人们能够专注于工作中更具创造性的部分，而不是乏味、重复的细节。此外，人工智能驱动的工具可以提高残疾人或不同语言群体之间的生产力和沟通能力。</p><p>在这篇博文中，我们重点关注在（两个人之间）直接沟通中建立信任和责任的挑战，例如电子邮件、社交媒体平台上的直接消息、短信，甚至电话。在所有这些场景中，消息传递通常发生在很少或没有先前上下文或联系的个人之间，但这些消息可能携带非常重要的信息。一些例子包括讨论工作前景的电子邮件、共同朋友的新联系以及不请自来但重要的电话。通信可能是代表组织或个人发起的，但在任何一种情况下，我们都会遇到同样的问题：如果消息被证明具有误导性、恶意或其他不适当的内容，那么追究任何人的责任是不切实际的，可能需要困难和法律程序缓慢，且无法跨越不同的沟通平台。</p><p>随着这些活动规模的增加，对灵活的跨平台<em>问责机制的</em>需求也越来越大，该机制允许消息发送者和接收者明确声明其通信的性质。具体来说，发送者应该能够声明对其消息的责任，如果消息不适当，接收者应该能够让发送者承担责任。</p><h2 class="wp-block-heading" id="elements-of-accountability">问责制要素</h2><p>上述问题并不完全是新问题，但人工智能的最新进展使这些问题变得更加紧迫。在过去的几年里，科技界与媒体组织和其他机构一起研究了区分文本或图像是否由人工智能创建的方法；例如，C2PA是一种水印技术，也是其中一种可能的解决方案。随着人工智能驱动的工具越来越多地在工作场所使用，微软相信它将采取多种方法相结合，为用户提供最高的价值和最大的透明度。</p><p>关注问责制就是其中一种方法。我们可以首先列出我们期望任何可行解决方案的一些属性：</p><ul><li>个人和组织需要能够对他们发送的信息承担责任。</li><li>如果消息不适当或恶意，接收者需要能够追究发送者的责任，以保护未来的潜在受害者。</li><li>必须存在激励发送者宣布责任的动机。</li><li>该机制应该只解决问责问题，而不是其他问题。它不能产生意想不到的副作用，例如诚实参与者的隐私丧失。</li><li>不应要求接收者注册任何服务。</li><li>问责机制必须与当今人们用来沟通的多元化方法兼容。</li></ul><p>建立问责机制的一种方法是使用<em>声誉系统</em>来验证现实世界的身份，将我们的数字交互与有形且最终负责的组织或人类身份连接起来。在线声誉现已成为组织和个人拥有既得利益的资产。它激励诚实和值得信赖的行为，最终有助于为每个人创造一个更安全、更可靠的数字环境。</p><h2 class="wp-block-heading" id="reputation-system-for-online-accountability">在线问责声誉系统</h2><p>考虑一下集成声誉系统的在线通信用户体验会是什么样子。在此解决方案中，消息发送者可以通过将消息以加密<em>信誉标签</em>的形式绑定到信誉系统中的帐户来声明其责任。相反，接收者使用该标签来验证发送者的声誉，如果消息不适当，可以使用它来报告发送者，从而降低发送者的声誉。发送者有责任判断接收者是否会认为该消息不适当。</p><p>带有附加信誉标签的消息称为<em>信誉消息</em>，而那些没有关联信誉的消息称为<em>通用消息</em>。信誉良好的消息通常在发送者针对特定接收者的一对一通信或针对少数接收者的一对多通信中最有意义。例如，讨论商业交易的提案、婚礼邀请电子邮件、来自公司计费部门的付款提醒短信或讨论联合项目的工作电子邮件可能会作为可信消息发送。通用消息通常不针对特定接收者。例如，发送到邮件列表（许多收件人）的电子邮件或非个性化广告（大规模）应作为通用发送。</p><p>图 1 概括地描述了我们问责机制的不同组件和工作流程。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="1186" height="843" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-blog_2023Aug_system-diagram.png" alt="系统图" class="wp-image-964695" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-blog_2023Aug_system-diagram.png 1186w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-blog_2023Aug_system-diagram-300x213.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-blog_2023Aug_system-diagram-1024x728.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-blog_2023Aug_system-diagram-768x546.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/Rethinking-blog_2023Aug_system-diagram-240x171.png 240w" sizes="(max-width: 1186px) 100vw, 1186px" /><figcaption class="wp-element-caption">图 1：问责机制设计，显示帐户创建和消息发送/报告工作流程。</figcaption></figure><p>举一个具体的例子，想象一下这样一种情况：您收到银行发来的一封电子邮件，要求您验证帐户的安全设置。您知道网络钓鱼电子邮件通常针对此类情况，因此您的第一反应是忽略该消息。但是，在这种情况下，您的电子邮件客户端已记录有效的信誉标签，并自动将电子邮件移至<em>信誉良好的邮件</em>文件夹。它在邮件旁边显示发件人的信誉，<em>高</em>。您决定检查电子邮件中的链接是否真正将您引导至银行网站，而不是删除未经请求的且略有可疑的电子邮件。您现在确信这是一条合法消息，并继续按照建议检查您的安全设置。</p><p>再举一个例子，假设您在公司的计费部门工作。您发现客户的账单信息有问题，并决定向他们发送电子邮件以获取更多信息。由于这是一件重要的事情，您希望通过在邮件上附加计费部门的声誉标签来最大限度地提高他们看到您的邮件的机会。客户看到电子邮件进入<em>信誉消息</em>文件夹，注意到发件人的<em>高</em>声誉，并以适当的紧急程度进行回复。</p><p>第三个例子，假设您接到一个自称是您远亲的人主动打来的电话，想要讨论他们正在组织的家庭聚会。他们询问你有关家庭的问题，让你有点不安。在给您打电话之前，他们通过短信向您发送了一个声誉标签，编码了他们的声誉和通话背景。您验证标签是否有效，但其声誉为<em>中等</em>。您决定结束通话并使用他们共享的标签举报他们，因为您认为他们的通话要求此类敏感信息是不合适的。</p><p>这些例子强调，这个单一系统可以用于多种不同的通信模式，从电子邮件到社交媒体消息再到电话，从而在当今使用的整个直接通信方法中培养信任和安全。 </p><div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div><div class="border-bottom border-top border-gray-300 mt-5 mt-md-4 mb-4 mb-md-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="956154"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究院播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/collaborators-gov4git-with-kasia-sitkiewicz-and-petar-maymounkov/" aria-label="Collaborators: Gov4git with Petar Maymounkov and Kasia Sitkiewicz" data-bi-cN="Collaborators: Gov4git with Petar Maymounkov and Kasia Sitkiewicz" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/04/collaborators_hero_1400x788.jpg" alt="GitHub 产品经理 Kasia Sitkiewicz 和协议实验室研究科学家 Petar Maymounkov 在 Microsoft Research 播客上讨论了他们在 Gov4git 上的合作" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">合作者：Gov4git、Petar Maymounkov 和 Kasia Sitkiewicz</h2><p class="large"> Gov4git 是一种去中心化、开源合作的治理工具，有助于为未来奠定基础，让每个人都可以更高效、透明、轻松地进行协作，并以满足各自社区独特愿望和需求的方式进行协作。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/collaborators-gov4git-with-kasia-sitkiewicz-and-petar-maymounkov/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Collaborators: Gov4git with Petar Maymounkov and Kasia Sitkiewicz" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="call-to-action">呼吁采取行动</h2><p>在这篇博文中，我们试图概述一个解决方案，解决因现代人工智能而加剧的现有问题。抓住这个问题的核心并不容易，而且之前提出的许多解决方案都会产生意想不到的后果，导致它们无法实施。例如，我们解释了为什么试图限制人工智能使用的方法不太可能成功。</p><p>解决方案也并不容易。消息传递生态系统非常复杂，任何需要对其进行根本性改变的解决方案都不太可能被接受。可用性也是一个关键问题：如果系统的设计只是为了传达风险，我们可能希望避免无意中传达安全性，就像作为 HTTPS 标志的挂锁符号的存在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://blog.chromium.org/2023/05/an-update-on-lock-icon.html" target="_blank" rel="noreferrer noopener">给网络浏览器用户造成了混乱和低估风险</a>一样<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://blog.chromium.org/2023/05/an-update-on-lock-icon.html" target="_blank" rel="noreferrer noopener"><span class="sr-only">（在新选项卡中打开）</span></a> 。</p><p>是否有一个全面的身份框架可以将现实世界的身份与数字身份连接起来？这种与<em>独特的</em>现实世界身份的联系至关重要，否则任何人都可以简单地创建任意数量的不同声誉帐户以达到任何邪恶目的。</p><p>对于组织来说，情况要容易一些，因为国家和国家往往持有证明其存在和“身份”的公共记录。对于个人而言，Reddit、TripAdvisor 和 Stack Overflow 等平台已经建立了供其内部使用的声誉系统，但如果没有确认独特人类身份的基础层，这些系统就无法用来解决我们的问题，就像 Facebook 的“实名”政策和 X 一样Premium（以前称为 Twitter Blue）不足以防止虚假帐户的创建和使用。尽管如此，这并不是一个不可能解决的问题： <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/help/linkedin/answer/a1458457/" target="_blank" rel="noreferrer noopener">LinkedIn 已经与 CLEAR <span class="sr-only">（在新选项卡中打开）</span>合作，</a>将政府 ID 验证绑定到用户个人资料中的验证标记，并<a href="https://www.microsoft.com/en-us/security/blog/2023/04/12/linkedin-and-microsoft-entra-introduce-a-new-way-to-verify-your-workplace/" target="_blank" rel="noreferrer noopener">与 Microsoft Entra Verified ID <span class="sr-only">（在新选项卡中打开）合作</span></a>进行验证就业状况。 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://worldcoin.org/" target="_blank" rel="noreferrer noopener">Worldcoin <span class="sr-only">（在新选项卡中打开）</span></a>正在构建一种加密货币，每个钱包都通过生物识别技术与现实世界中唯一的人相关联，而苹果公司最近宣布了<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.apple.com/ca/newsroom/2023/06/introducing-apple-vision-pro/" target="_blank" rel="noreferrer noopener">Optic ID <span class="sr-only">（在新选项卡中打开）</span></a>通过其 Vision Pro 耳机进行生物识别身份验证。</p><p>每当我们谈论身份（尤其是现实世界的身份）时，我们都需要谈论隐私。人们在不同的社区使用不同的数字身份和通信方式，这些身份需要保持分离。信任具有此类敏感信息的声誉系统需要仔细考虑。我们的初步研究表明，现代密码学技术可用于提供强大的安全和隐私保证，以便声誉系统学习或揭示任何不必要的内容，并且不能以意想不到的方式使用。</p><p>声誉系统的治理怎么样？在极端情况下，单个中心化方托管系统，同时提供正确操作的加密透明度保证。另一个极端，我们应该探讨纯粹的去中心化实施是否可行。在这两个极端之间也有一些选择；例如，由不同公司和组织托管的多个较小的声誉系统。</p><p>这些开放性问题为研究界提供了机会和责任。在微软研究院，我们正在与隐私保护的可验证信息和身份、安全硬件、透明系统和媒体来源方面的研究合作，努力解决这个问题的各个方面。我们邀请研究界的其他成员加入进来，要么遵循我们在此概述的路径，要么提出更好的替代方案。这是广泛探索的开始，需要我们所有人的坚定承诺和贡献。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/rethinking-trust-in-direct-messages-in-the-ai-era/">重新思考人工智能时代对直接消息的信任一文</a>首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>;</content:encoded></item><item><title>打造人工智能编译器“重金属四重奏”</title><link/> https://www.microsoft.com/en-us/research/blog/building-a-heavy-metal-quartet-of-ai-compilers/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Wed, 30 Aug 2023 16:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=963594 </guid><description><![CDATA[<p>全新的 AI 编译器四重奏：Rammer、Roller、Welder 和 Grinder，基于相同的图块抽象解决了一系列编译器优化挑战，提供了将 AI 模型与硬件加速器连接起来的全面解决方案。</p><p> <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/building-a-heavy-metal-quartet-of-ai-compilers/">构建人工智能编译器的“重金属四重奏”</a>一文首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </description><content:encoded><![CDATA[
<p>作者：MSR 编辑</p><p>编译是程序开发中的一个重要过程，其中称为编译器的程序将用编程语言编写的源代码翻译成计算机硬件上可执行的机器代码。随着人工智能技术和大规模人工智能模型在数字世界中变得越来越普遍，其独特的特性给编译器带来了新的挑战。</p><p>随着人工智能模型从循环神经网络 (RNN) 和卷积神经网络 (CNN) 等早期版本发展到 Transformer 等最新版本，其基本架构也在不断发展。与此同时，底层硬件加速器，例如图形处理单元（GPU）和神经处理单元（NPU）也在快速迭代，一些设计颠覆了以前的架构。因此，人工智能编译器在帮助新的人工智能模型在新硬件上高效运行方面发挥着至关重要的作用。</p><p>对此，微软研究院的研究人员与学术界同事合作，进行了一系列研究，并发布了人工智能编译器的“重金属四重奏”： <a href="https://www.microsoft.com/en-us/research/publication/rammer-enabling-holistic-deep-learning-compiler-optimizations-with-rtasks/"><em>Rammer</em></a> <em>、</em> <a href="https://www.microsoft.com/en-us/research/publication/roller-fast-and-efficient-tensor-compilation-for-deep-learning/"><em>Roller</em></a> <em>、</em> <a href="https://www.microsoft.com/en-us/research/publication/welder-scheduling-deep-learning-memory-access-via-tile-graph/"><em>Welder</em></a><em>和</em><em> </em><a href="https://www.microsoft.com/en-us/research/publication/cocktailer-analyzing-and-optimizing-dynamic-control-flow-in-deep-learning/"><em>磨床</em></a><sup><a id="_ftnref1" href="#_ftn1">[1]</a></sup> .该四方为当前主流的人工智能模型和硬件编译提供了系统的、创新的解决方案。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" loading="lazy" width="978" height="367" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-1.png" alt="左图显示了以基于瓦片的中间表示（IR）为核心的统一编译器抽象。右图展示了AI编译的四大核心技术。" class="wp-image-963609" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-1.png 978w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-1-300x113.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-1-768x288.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-1-240x90.png 240w" sizes="(max-width: 978px) 100vw, 978px" /><figcaption class="wp-element-caption">图1：基于统一瓦片抽象的四大核心AI编译技术</figcaption></figure><div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div><div class="border-bottom border-top border-gray-300 mt-5 mt-md-4 mb-4 mb-md-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">焦点：微软研究通讯</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">微软研究院通讯</h2><p class="large">与 Microsoft 研究社区保持联系。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button is-style-fill-chevron"> <a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">立即订阅</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="ai-compilation-rammer-improves-hardware-parallel-utilization"> AI编译“Rammer”提高硬件并行利用率</h2><p>深度神经网络（DNN）广泛应用于图像分类、自然语言处理和许多其他智能任务。由于其重要性，许多计算设备（例如 CPU、GPU 和专门设计的 DNN 加速器）都被用来执行 DNN 计算。 DNN 计算效率的一个关键变量是调度，它决定了计算任务在硬件上执行的顺序。传统的 AI 编译器通常将 DNN 计算视为数据流图，其中每个节点代表一个 DNN 运算符。这些运算符作为不透明的库函数实现，并计划在加速器上单独运行。同时，这个过程还依赖于另一层调度程序（通常在硬件中实现），以利用运算符中可用的并行性。这种两级方法会产生大量的调度开销，并且通常不能充分利用硬件资源。</p><p>为了解决这个问题，研究人员提出了<em>一种新的 DNN 编译器 Rammer，它可以优化大规模并行加速器单元上 DNN 工作负载的执行。</em> Rammer 将 AI 编译的调度空间想象为一个二维平面，其中计算任务是可以分为不同形状和大小的“砖块”。 Rammer 中调度的目的是将这些砖块紧密地排列在二维平面的计算单元上（就像建造一堵墙一样）。安排不应留下任何间隙，否则会损害硬件利用率，从而降低执行速度。 Rammer 在这个二维空间中的工作方式就像一个压缩器：当 DNN 程序被翻译成砖块时，Rammer 可以将它们放置在加速器的不同计算单元上来压缩它们。 </p><figure class="wp-block-image aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-2-763x1024.png" alt="Rammer技术框架示意图。 Rammer 的输入是一个数据流图，其中节点是 rOperator。然后，Rammer 引入了 rTask 感知的 DFG 编译器来在一处管理运算符间和运算符内的调度。 rTask感知的DFG编译器将为运行时执行生成静态执行计划。 Rammer 将硬件加速器抽象为虚拟化并行设备（vDevice），其中包括多个虚拟化执行单元（vEU）。 vDevice提供了rTask级别的调度和同步能力，使得rProgram可以在编译时映射到相应的vEU。 vEU 与 vDevice 一起将在运行时映射到硬件。" class="wp-image-963612" width="427" height="573" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-2-763x1024.png 763w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-2-223x300.png 223w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-2-768x1031.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-2-134x180.png 134w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-2.png 1053w" sizes="(max-width: 427px) 100vw, 427px" /><figcaption class="wp-element-caption">图2：Rammer的技术框架</figcaption></figure><p>换句话说，Rammer 提前（在编译期间）为 DNN 生成高效的静态时空调度，从而最大限度地减少运行时调度开销。同时，通过新的独立于硬件的计算任务抽象和硬件加速器，Rammer 暴露了更大的调度空间，并提供了一种实现操作员内和操作员间协作调度的新颖方法。这使得Rammer能够找到更高效的调度，从而大大提高硬件利用率。</p><p>研究人员在多种设备上对 Rammer 进行了评估，包括 NVIDIA GPU、AMD GPU 和 Graphcore 智能处理单元 (IPU)。实验表明，Rammer 在 NVIDIA 和 AMD GPU 上的性能明显优于最先进的编译器，例如 XLA 和 TVM，实现了高达 20.1 倍的加速。与 NVIDIA 专有的 DNN 推理库 TensorRT 相比，Rammer 实现了高达 3.1 倍的加速。</p><h2 class="wp-block-heading" id="ai-compilation-roller-improves-compilation-efficiency"> AI编译“滚轮”提升编译效率</h2><p>加速器配备有并行计算单元和多层存储器层次结构。数据在计算之前需要从底层内存层逐层向上传递。在每一层，数据都被分成更小的块。最终，这些较小的砖块被交给顶层处理器进行计算。挑战在于如何对数据进行分区，并用大砖填充内存空间，从而更好地利用可用内存并提高效率。当前的方法涉及使用机器学习来确定更好的策略来划分这些砖块。然而，这通常需要数千个搜索步骤，每个搜索步骤都在加速器上进行评估，以便找到满意的解决方案。因此，编译完整的人工智能模型的过程可能需要几天甚至几周的时间。</p><p>考虑到每个存储层的计算逻辑和规范，提供了软件和硬件信息的整体视图，就可以制定划分块的最佳策略以及最佳的块尺寸。这使得编译速度更快，计算效率更高。这是<em>Roller</em>背后的关键思想。<em>就像压路机一样，系统将高维张量数据放置到二维存储器上，就像铺地板一样，根据存储器特性找到最佳的瓷砖尺寸。同时，它封装了符合底层加速器硬件特性的张量形状，通过限制形状的选择来实现高效编译。</em> <strong></strong></p><figure class="wp-block-image aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-3-1024x740.png" alt="Roller的技术框架示意图。 Roller 采用一个被描述为张量表达式的运算符。 Roller 从张量表达式中提取张量形状，并利用硬件规范来构造 rTiles。 Roller基于rTiles，提出了一种先扩展后扩展的递归构造算法，以生成描述数据处理管道的高效张量程序（名为rProgram）。在生成 rProgram 时，构造算法通过微性能模型评估构造的 rProgram 的性能来识别良好的 rTile 配置。它构建在通过硬件抽象层描述的设备之上，仅公开与 rTile 相关的接口：加载、计算和存储。构造好的rProgram最终通过代码生成器实现，发出对应特定设备的最终内核代码。" class="wp-image-963615" width="530" height="383" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-3-1024x740.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-3-300x217.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-3-768x555.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-3-1536x1110.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-3-240x173.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-3.png 1888w" sizes="(max-width: 530px) 100vw, 530px" /><figcaption class="wp-element-caption">图3：Roller的技术框架</figcaption></figure><p>对六种主流 DNN 模型和 119 个流行的 DNN 算子的评估表明，Roller 可以在几秒钟内生成高度优化的内核，特别是对于大型且昂贵的定制算子。与现有编译器相比，Roller 的编译时间缩短了三个数量级。 Roller 生成的内核的性能可与最先进的张量编译器（包括 DNN 库）相媲美，某些运算符的性能甚至更好。 Roller在内部也被用于定制DNN内核，这已经证明了其在开发敏捷性方面的真正提升。 </p><h2 class="wp-block-heading" id="ai-compilation-welder-optimizes-memory-access-and-improves-computing-efficiency"> AI编译“Welder”优化内存访问，提高计算效率</h2><p>随着处理更高保真度数据的需求不断增长，以及在更新的硬件加速器中使用更快的计算核心，现代 DNN 模型的内存消耗量越来越大。在各种流行的 DNN 模型中观察到未充分利用的计算核心和饱和内存带宽之间的差异。</p><p>例如，对最先进的 DNN 基准的分析显示，内存带宽利用率高达 96.7%，而计算核心的平均利用率仅为 51.6%。更严重的是，硬件和 DNN 模型的不断演进继续拉大了这一差距。现代人工智能模型倾向于处理高保真数据，例如更大的图像、更长的句子和更高分辨率的图形。此类数据在计算过程中需要更高的内存带宽。此外，更高效的专用计算核心（例如 NVIDIA Tensor Core 或 AMD Matrix Core）的引入进一步增加了内存压力。</p><p>为了解决这个问题，研究人员提出了<em>Welder深度学习编译器，它全面优化了端到端DNN模型的内存访问效率。</em>端到端 DNN 计算表示为数据流图，涉及多个阶段，其中输入数据被分为流经不同运算符的块。这些块被传输到处理器核心进行计算，然后传输回内存。由于跨内存层的数据移动，这会导致显着的开销。由于它包含多个阶段，所以整个过程可以想象为“工人”一层一层向上搬砖的场景。第一个工人拿起砖块，对其进行处理，然后将它们放回原来的位置。第二个工人再次把它们抬起来，雕刻它们，然后再次把它们放回去。这个过程继续进行，第三个工人、第四个工人等等，重复地移动砖块。然而，这会导致巨大的开销。是否可以让第一个worker完成部分子任务，然后直接交给最顶层的下一个worker？然后可以将这些任务“焊接”在一起，实现更高效率的流水线操作。焊机就起到了这样的焊接工具的作用。通过连接（焊接）不同的算子，以流水线的方式处理数据块，大大减少了底层内存层的内存访问流量。近年来，随着AI模型对内存效率的要求越来越高，Welder有助于显着提高计算效率。 </p><figure class="wp-block-image aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-4-1024x776.png" alt="Welder 技术框架示意图。 Welder 将完整的 DNN 模型作为输入，并将其转换为基于图块的计算任务的数据流图，称为图块图。然后，提出了一种两步调度算法，即图连接和子图调度，以递归地决定多个存储层的有效瓦片图执行计划，称为分层瓦片图。最后，使用硬件层中定义的四个抽象计算接口将该计划映射到特定硬件加速器的可执行代码。" class="wp-image-963621" width="768" height="582" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-4-1024x776.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-4-300x227.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-4-768x582.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-4-1536x1164.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-4-80x60.png 80w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-4-238x180.png 238w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-4.png 2022w" sizes="(max-width: 768px) 100vw, 768px" /><figcaption class="wp-element-caption">图4：Welder的技术框架</figcaption></figure><p>对10种主流DNN模型（包括视觉、自然语言处理、3D图形等各种任务的经典和最新的AI模型结构）的评估表明，Welder在两个方面都显着超过现有主流框架和编译器的性能NVIDIA 和 AMD GPU。例如，它的性能比 PyTorch、ONNXRuntime 和 Ansor 分别高出 21.4 倍、8.7 倍和 2.8 倍。 Welder 的自动优化甚至超越了 TensorRT 和 Faster Transformer（手工制作的库），分别实现了高达 3.0 倍和 1.7 倍的加速。此外，当在具有更快计算核心（例如 TensorCore）的硬件上运行这些模型时，性能会进一步提高，凸显了内存优化对于未来 AI 加速器的重要性。 </p><h2 class="wp-block-heading" id="ai-compilation-grinder-allows-efficient-control-flow-execution-on-accelerators"> AI编译“Grinder”允许在加速器上高效执行控制流</h2><p>在AI计算中，数据块的移动有时需要更复杂的控制逻辑，即控制流代码。例如，程序可以迭代地遍历句子中的每个单词，或者根据输入动态确定要执行程序的哪一部分。目前，大多数AI编译器侧重于解决数据流执行效率问题，并未对控制流提供有效支持。因此，控制流程较复杂的模型无法有效利用加速器性能。研究人员意识到，控制流和数据流可以被分段和重组，以便更有效地执行。他们的解决方案是<em>Grinder</em> ，它的作用就像便携式研磨和切割机。将数据流切割成不同大小的并行计算块后，再将控制流整合（研磨）到数据流中，使控制流也能在加速器上高效执行。 </p><figure class="wp-block-image aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-5-1024x774.png" alt="Grinder 技术框架示意图。示例循环结构被调度为映射到 3 级加速器上的 uProgram。 uProgram由4个loop-uTasks组成，分别对应4个L1-Unit，每个loop-uTask都映射到一个L1-Unit来执行。数据流运算符和循环都被调度到loop-uTasks中。" class="wp-image-963624" width="598" height="452" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-5-1024x774.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-5-300x227.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-5-768x580.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-5-1536x1161.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-5-80x60.png 80w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-5-238x180.png 238w, https://www.microsoft.com/en-us/research/uploads/prod/2023/08/ai-compilor-5.png 1712w" sizes="(max-width: 598px) 100vw, 598px" /><figcaption class="wp-element-caption">图5：Grinder的技术框架</figcaption></figure><p>Grinder可以通过uTask这种新的抽象，联合优化控制流和数据流在硬件加速器上的执行，并统一AI模型的表示，包括控制流和数据流。这允许 Grinder 公开整体调度空间，以便将控制流重新调度到较低级别的硬件并行性。 Grinder使用启发式策略来寻找有效的调度方案，并可以自动将控制流移动到设备内核中，从而实现跨控制流边界的优化。实验表明，Grinder 可以在控制流密集型 DNN 模型上实现高达 8.2 倍的加速，使其成为控制流 DNN 框架和编译器中最快的。</p><p>这四种AI编译器基于通用的编译器抽象和统一的中间表示（IR），<em>解决了</em><em>当前AI编译器中的多个基本问题，包括并行性、编译效率、内存和控制流。它们共同构成了一套全面的编译解决方案。</em>并在微软研究院新人工智能模型的定制和优化中发挥了重要作用。</p><p> MSR Asia 首席研究员薛继龙这样总结该项目：</p><blockquote class="wp-block-quote is-style-spectrum"><p> “一方面，人工智能编译器必须执行极端优化，例如针对硬件资源量身定制的运算符融合和内核专业化。另一方面，还必须为新的大规模硬件架构提供系统的编译支持，例如具有片上网络互连（NoC）或混合内存架构的AI芯片，甚至使用白盒编译技术指导硬件设计。我们开发的AI编译器在AI编译效率方面取得了显着提升，从而方便了AI模型的训练和部署。同时，大规模模型的演进也为下一代AI编译器带来了机遇。未来，这些大型模型本身可能本质上有助于实现优化和编译。”</p></blockquote><p>以下研究人员对此项目做出了贡献：</p><p> <em>（按字母顺序排列）</em><a href="https://www.microsoft.com/en-us/research/people/weicu/">崔伟</a>、 <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://yuxiaoguo.github.io/" target="_blank" rel="noreferrer noopener">郭雨晓</a>、<a href="https://www.microsoft.com/en-us/research/people/wenxh/">胡文祥</a>、<a href="https://www.microsoft.com/en-us/research/people/lingm/">马凌晓</a>、<a href="https://www.microsoft.com/en-us/research/people/yomia/">苗友山</a>、<a href="https://www.microsoft.com/en-us/research/people/zimiao/">苗子明</a>、<a href="https://www.microsoft.com/en-us/research/people/yuqxia/">夏雨晴</a>、<a href="https://www.microsoft.com/en-us/research/people/jxue/">薛继龙</a>、杨<a href="https://www.microsoft.com/en-us/research/people/fanyang/">范</a>、<a href="https://www.microsoft.com/en-us/research/people/maoyang/">杨毛</a>、<a href="https://www.microsoft.com/en-us/research/people/lidongz/">周立东</a></p><hr class="wp-block-separator has-alpha-channel-opacity"/><p><a id="_ftn1" href="#_ftnref1">[1]</a> Grinder是研究项目名称。然而，该系统在论文中被称为 Cocktailer。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/building-a-heavy-metal-quartet-of-ai-compilers/">构建人工智能编译器的“重金属四重奏”</a>一文首先出现在<a rel="nofollow" href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>;</content:encoded></item></channel></rss>