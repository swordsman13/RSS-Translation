<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2024 年 2 月 27 日星期二 20:43:36 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.4.3</generator><item><title>法学硕士的结构化知识提高了视觉语言模型的即时学习</title><link/>https://www.microsoft.com/en-us/research/blog/structed-knowledge-from-llms-improves-prompt-learning-for-visual-language-models/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Tue, 27 Feb 2024 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1009260 </guid><description><![CDATA[<p>使用法学硕士创建图像描述符的结构化图可以增强视觉语言模型生成的图像。了解结构化知识如何提高视觉和语言理解的及时调整。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/structured-knowledge-from-llms-improves-prompt-learning-for-visual-language-models/">法学硕士的结构化知识改善了视觉语言模型的即时学习一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center">这篇研究论文在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aaai.org/aaai-conference/" target="_blank" rel="noreferrer noopener"><strong><em>第 38 届 AAAI 人工智能年会</em></strong><span class="sr-only">（在新选项卡中打开）</span></a> (AAAI-24)<strong><em>上发表</em></strong><strong><em>，这是促进对智能及其在机器中的实现的理解的首要论坛。</em></strong> </p><figure class="wp-block-image aligncenter size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1.jpg" alt="第一页"Learning Hierarchical Prompt with Structured Linguistic Knowledge for Language Models" publication to the right of the AAAI conference on a blue and purple gradient background" class="wp-image-1009434" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AAAI-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>我们看到视觉语言模型在将文本描述转换为图像方面具有非凡的能力。然而，创建高质量的视觉效果需要制作精确的提示来捕获不同图像元素之间的关系，这是标准提示所缺乏的功能。在 AAAI-24 上发表的论文“使用<a href="https://www.microsoft.com/en-us/research/publication/learning-hierarchical-prompt-with-structured-linguistic-knowledge-for-vision-language-models/">语言模型的结构化语言知识学习分层提示</a>”中，我们介绍了一种使用大型语言模型 (LLM) 来增强视觉语言模型创建的图像的新颖方法。通过创建图像描述的详细图表，我们利用法学硕士的语言知识来生成更丰富的图像，从而扩展其在实际应用中的效用。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" width="3353" height="1315" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24.png" alt="VLM 中用于识别鸟类的三种类型提示的示例，即模板化提示（鸟的照片）、描述鸟类类别的基于自然语言的提示以及突出显示鸟类和鸟类的关键实体的树结构提示。相应的属性，例如喙、翅膀等。" class="wp-image-1009371" style="width:700px" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24.png 3353w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-300x118.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-1024x402.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-768x301.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-1536x602.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-2048x803.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure1_AAAI-24-240x94.png 240w" sizes="(max-width: 3353px) 100vw, 3353px" /><figcaption class="wp-element-caption">图 1. 结构化图提供了每个类名称的描述。</figcaption></figure><p>图 1 说明了我们构建包含每个类别或类的关键详细信息的结构化图的方法。这些图包含结构化信息，包括实体（对象、人员和概念）、属性（特征）以及它们之间的关系。例如，在定义“睡莲”时，我们包括“叶子”或“花朵”等实体及其属性“圆形”和“白色”，然后应用法学硕士的推理能力来识别这些术语之间的相互关系。如图 2 所示。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" width="2210" height="2316" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24.png" alt="使用 LLM 自动生成类别描述和知识图的管道和指令。我们首先指示LLM给出类别描述，然后要求它从非结构化描述中解析关键实体、属性及其关系。" class="wp-image-1009374" style="width:700px" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24.png 2210w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-286x300.png 286w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-977x1024.png 977w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-768x805.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-1466x1536.png 1466w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-1954x2048.png 1954w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure2_AAAI-24-172x180.png 172w" sizes="(max-width: 2210px) 100vw, 2210px" /><figcaption class="wp-element-caption">图 2. 通过将指令输入 LLM，我们可以接收与类别相关的描述以及相应的结构化图表。 </figcaption></figure><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="931956"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">焦点：点播视频</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" aria-label="AI Explainer: Foundation models ​and the next era of AI" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AIEx01_blog_hero_1400x788.png" alt="电脑屏幕截图 一名男子的屏幕截图" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4"> AI 解释者：基础模型和 AI 的下一个时代</h2><p class="large">探索 Transformer 架构、更大的模型和更多数据以及情境学习如何帮助推动人工智能从感知到创造。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Watch video" data-bi-cN="AI Explainer: Foundation models ​and the next era of AI" target="_blank">看视频</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="how-to-model-structural-knowledge"> 如何对结构知识进行建模</h2><p>在识别和构建生成的提示描述中的关系后，我们实现了分层提示调整（HTP），这是一种按层次结构组织内容的新提示调整框架。这种方法允许视觉语言模型辨别提示中不同级别的信息，从具体细节到更广泛的类别以及跨多个知识领域的总体主题，如图 3 所示。这有助于模型理解这些元素之间的联系，提高处理各种主题的复杂查询的能力。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1802" height="1655" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24.png" alt="建议分层提示调优的总体框架。带有类名的描述和关系引导图分别用作冻结文本编码器和分层提示文本编码器的输入。" class="wp-image-1009377" style="width:700px" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24.png 1802w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-300x276.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-1024x940.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-768x705.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-1536x1411.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure3_AAAI-24-196x180.png 196w" sizes="(max-width: 1802px) 100vw, 1802px" /><figcaption class="wp-element-caption">图 3.HPT 基于双路径非对称网络，接收图像和各种类型的文本输入。</figcaption></figure><p>该方法的核心是最先进的关系引导注意力模块，旨在帮助模型识别和分析图中元素之间的复杂互连。该模块还通过跨级自注意力机制来理解不同实体和属性之间的交互。自注意力使模型能够根据输入数据的各个部分（此处为图表）的相关性来评估并确定其优先级。 “跨级”自注意力将这种能力扩展到图中的各个语义层，允许模型检查多个抽象级别的关系。此功能有助于模型辨别各个级别的提示（或输入命令/问题）之间的相互关系，从而帮助其更深入地了解类别或概念。</p><p>我们的研究结果为更有效地导航和理解复杂语言数据、改进模型的知识发现和决策过程提供了宝贵的见解。基于这些进步，我们通过引入分层提示文本编码器改进了传统的文本编码方法，如图 4 所示。我们的目标是改进文本信息与视觉数据的对齐或关联方式，这是视觉语言模型的必要条件必须解释文本和视觉输入。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="2178" height="1921" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24.png" alt="分层提示文本编码器的框架，我们应用三种类型的提示，低级提示、高级提示和全局级提示进行分层调整，并设计一个关系引导的注意模块以更好地建模结构知识。" class="wp-image-1009380" style="width:700px" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24.png 2178w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-300x265.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-1024x903.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-768x677.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-1536x1355.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-2048x1806.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/figure4_AAAI-24-204x180.png 204w" sizes="(max-width: 2178px) 100vw, 2178px" /><figcaption class="wp-element-caption">图 4. 分层提示的文本编码器从多级提示中学习，并使用关系引导的注意力模块来建模结构知识。</figcaption></figure><h2 class="wp-block-heading" id="looking-ahead">展望未来</h2><p>通过将结构化知识纳入我们的模型训练框架，我们的研究为更复杂的应用奠定了基础。一个例子是增强的图像字幕，其中视觉语言模型能够以更高的准确性和深度描述照片、插图或任何视觉媒体的内容。这一改进可以显着有益于各种应用，例如帮助视力受损的用户。此外，我们设想文本到图像生成方面的进步，使视觉语言模型能够根据文本描述生成更精确、更详细且与上下文相关的视觉表示。</p><p>展望未来，我们希望我们的研究能够激发人们更广泛的兴趣，探索结构化知识在改善视觉和语言理解的即时调整方面的作用。这项探索预计会将这些模型的使用扩展到基本分类任务（模型对数据进行分类或标记数据）之外，从而实现人与人工智能系统之间更细致、更准确的交互。通过这样做，我们为人工智能系统更有效地解释人类语言的复杂性铺平了道路。</p><h2 class="wp-block-heading" id="acknowledgements">致谢</h2><p>感谢 Yubin Wang 在实现算法和执行实验方面做出的贡献。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/structured-knowledge-from-llms-improves-prompt-learning-for-visual-language-models/">法学硕士的结构化知识改善了视觉语言模型的即时学习一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title>研究重点：2024 年 2 月 19 日当周</title><link/>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-february-19-2024/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Wed, 21 Feb 2024 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=1008426 </guid><description><![CDATA[<p>本期：CaaSPER：垂直自动缩放算法动态保持最佳CPU利用率；改进的相机定位场景地标检测运行速度更快，占用的存储空间更少； ESUS 简化了技术产品和服务的可用性调查问卷。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-february-19-2024/">《研究焦点：2024 年 2 月 19 日一周》一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-pullquote"><blockquote><p><em class="">欢迎来到研究焦点，这是一系列博客文章，重点介绍 Microsoft 研究社区的著名出版物、活动、代码/数据集、新员工和其他里程碑。</em> </p></blockquote></figure><figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1.png" alt="2024 年 2 月 19 日研究焦点周" class="wp-image-1008450" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/02/RF35-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-a584a2137da4151ecbde93fba771f798" id="new-research">新研究</h3><h2 class="wp-block-heading" id="vertically-autoscaling-monolithic-applications-with-caasper-scalable-container-as-a-service-performance-enhanced-resizing-algorithm-for-the-cloud">使用 CaaSPER 垂直自动扩展单体应用程序：可扩展的容器即服务性能增强的云大小调整算法</h2><p>Kubernetes 是一个著名的开源平台，用于管理云应用程序，包括状态数据库，用于跟踪涉及底层数据的更改和事务。这些单体应用程序通常必须依赖垂直资源扩展而不是水平扩展，调整 CPU 内核以匹配负载波动。然而，对 Microsoft 数据库即服务 (DBaaS) 产品的分析显示，许多团队始终为峰值工作负载过度配置资源，而忽视了通过缩减规模来优化云资源消耗的机会。现有的垂直自动扩展工具缺乏最大限度地减少资源闲置和对限制及时响应的能力，从而导致成本增加并影响关键指标，例如吞吐量和可用性。</p><p>在最近的一篇论文：<a href="https://www.microsoft.com/en-us/research/publication/caasper-vertical-autoscaling/">使用 CaaSPER 垂直自动扩展单体应用程序：可扩展的容器即服务性能增强的云调整大小算法中</a>，微软的研究人员提出了 CaaSPER，这是一种垂直自动扩展算法，它融合了被动式和主动式策略来应对这一挑战。通过动态调整 CPU 资源，CaaSPER 最大限度地减少资源闲置、保持最佳 CPU 利用率并减少限制。重要的是，团队可以灵活地优先考虑节省成本或高性能。广泛的测试表明，CaaSPER 可以有效减少限制并将 CPU 利用率保持在目标水平内。 CaaSPER 的设计与应用程序和平台无关，有可能扩展到需要垂直自动扩展的其他应用程序和资源。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-layout-1 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/caasper-vertical-autoscaling/">阅读论文</a></div></div><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="970287"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-the-future-of-scale-with-ahmed-awadallah-and-ashley-llorens/" aria-label="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" data-bi-cN="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/09/Ahmed_AI_Frontiers_TW_LI_FB_1200x627_With_Name.png" alt="MSR 播客 |人工智能前沿 |艾哈迈德·阿瓦达拉" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">人工智能前沿：艾哈迈德·阿瓦达拉 (Ahmed Awadallah) 和阿什利·洛伦斯 (Ashley Llorens) 的规模化未来</h2><p class="large">本集的主角是高级首席研究经理<a href="https://www.microsoft.com/en-us/research/people/hassanam/" target="_blank" rel="noreferrer noopener">Ahmed H. Awadallah</a> ，他致力于提高大规模人工智能模型的效率，并努力帮助推动该领域从研究到实践的进步<strong> </strong>使他处于人工智能新时代的前沿。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-the-future-of-scale-with-ahmed-awadallah-and-ashley-llorens/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-21d8108ee594aad478409a8aa618b2ee" id="new-research-1">新研究</h3><h2 class="wp-block-heading" id="improved-scene-landmark-detection-for-camera-localization">改进了相机定位的场景地标检测</h2><p>相机定位是计算机视觉、机器人、增强现实和虚拟现实应用中常用的基本组件，用于估计场景中支持相机的设备的精确 3D 位置和方向。使用基于图像的检索、视觉特征匹配和基于 3D 结构的姿态估计的定位技术通常是准确的，但它们需要高存储空间，通常很慢，并且不保护隐私。来自微软和外部同事的研究人员最近提出了一种基于场景地标检测（SLD） <a href="https://www.microsoft.com/en-us/research/publication/learning-to-detect-scene-landmarks-for-camera-localization/">的替代学习定位方法</a>来解决这些限制。它涉及训练卷积神经网络来检测一些预先确定的、显着的、特定于场景的 3D 点或地标，并根据相关的 2D-3D 对应关系计算相机姿势。尽管 SLD 的性能优于现有的基于学习的方法，但它的准确度明显低于基于 3D 结构的方法。</p><p>在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/improved-scene-landmark-detection-for-camera-localization/">改进相机定位的场景地标检测</a>中，微软的研究人员表明，准确度差距是由于模型容量不足和训练期间的标签噪声造成的。为了缓解容量问题，他们建议将地标分成子组，并为每个子组训练一个单独的网络。为了生成更好的训练标签，他们建议使用密集重建来估计场景地标的准确可见性。最后，他们提出了一种紧凑的神经网络架构来提高记忆效率。这种方法与 INDOOR-6 数据集上最先进的基于结构的方法一样准确，但它的运行速度明显更快，并且使用的存储空间更少。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-layout-2 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/improved-scene-landmark-detection-for-camera-localization/">阅读论文</a></div><div class="wp-block-button is-style-fill-github"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/SceneLandmarkLocalization">代码和型号</a></div></div><hr class="wp-block-separator has-alpha-channel-opacity"/><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-73123c9697b9c6db2728fb2f179fa924" id="new-research-2">新研究</h3><h2 class="wp-block-heading" id="esus-aligning-and-simplifying-sus-for-enterprise-applications"> ESUS：针对企业应用程序调整和简化 SUS</h2><p>多年来，研究人员开发了标准调查问卷来评估可用性，并给出代表产品易用性总体水平的单一分数。这些评估对于研究人机交互（HCI）和用户体验（UX）的研究人员非常有价值。最著名的调查问卷之一是系统可用性量表（SUS）。然而，自 1986 年推出 SUS 以来，产品和服务在技术上取得了巨大进步，而 HCI 和 UX 研究实践也相当成熟。这些变化在企业环境中也是如此。</p><p>在最近的一篇文章： <a href="https://www.microsoft.com/en-us/research/publication/esus-aligning-and-simplifying-sus-for-enterprise-applications/">ESUS：调整和简化企业应用程序的 SUS 中</a>，Microsoft 的研究人员提供了初步证据，表明新的可用性调查问卷的有效性，与原来的 10 项 SUS 调查问卷相比，它对企业应用程序具有三个优势。企业系统可用性量表（ESUS）可以更好地衡量技术产品和服务的可用性；减少问卷项目；并与企业环境保持一致。结果表明，与 SUS 类似，ESUS 与用户满意度密切相关。 </p><div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-layout-3 wp-block-buttons-is-layout-flex"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/esus-aligning-and-simplifying-sus-for-enterprise-applications/">阅读论文</a></div></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-february-19-2024/">《研究焦点：2024 年 2 月 19 日一周》一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>