<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2023 年 12 月 6 日星期三 14:54:40 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.4.1</generator><item><title> LLMLingua：​​通过即时压缩创新法学硕士效率</title><link/>https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Thu, 07 Dec 2023 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/?p=987321 </guid><description><![CDATA[<p>法学硕士的高级提示技术可能会导致提示过长，从而引发问题。了解 LLMLingua 如何将提示压缩高达 20 倍、保持质量、减少延迟并支持改进的用户体验。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/">LLMLingua 文章：通过即时压缩创新 LLM 效率</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center"><strong><em>这篇研究论文发表在</em></strong><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://2023.emnlp.org/" target="_blank" rel="noreferrer noopener"><strong>2023 年自然语言处理经验方法会议</strong><span class="sr-only">（在新选项卡中打开）</span></a> <strong><em>(EMNLP 2023) 上，这是自然语言处理和人工智能的顶级会议。</em></strong> </p><figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1.png" alt="EMNLP 2023 徽标位于已接受论文的左侧"LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models" on a blue/green gradient background" class="wp-image-987333" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/EMNLP-2023-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>随着大型语言模型 (LLM) 模型的进步及其潜力变得越来越明显，人们逐渐认识到其输出的质量与给出的提示的性质直接相关。这导致了提示技术的兴起，例如思维链（CoT）和情境学习（ICL），它们有助于增加提示长度。在某些情况下，提示现在扩展到数万个标记或文本单元，甚至更多。虽然较长的提示具有相当大的潜力，但它们也带来了许多问题，例如需要超过聊天窗口的最大限制、保留上下文信息的能力降低以及 API 成本增加（无论是在货币方面还是在计算资源方面）。</p><p>为了应对这些挑战，我们在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://2023.emnlp.org/" target="_blank" rel="noreferrer noopener">EMNLP <span class="sr-only">2023</span></a>上发表的论文“ <a href="https://www.microsoft.com/en-us/research/publication/llmlingua-compressing-prompts-for-accelerated-inference-of-large-language-models/">LLMLingua：​​压缩大型语言模型加速推理的提示<span class="sr-only">（在新选项卡中打开）</span></a> ”中引入了一种提示压缩方法（在新选项卡中打开）。使用训练有素的小语言模型（例如 GPT2-small 或 LLaMA-7B），LLMLingua 可以识别并删除提示中不重要的标记。这种压缩技术使封闭的法学硕士能够从压缩的提示中进行推理。尽管令牌级压缩提示对于人类来说可能难以理解，但事实证明它们对于法学硕士来说非常有效。图 1 对此进行了说明。 </p><figure class="wp-block-image aligncenter size-full"><img decoding="async" width="3340" height="1938" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1.png" alt="这是 LLMLingua 框架的示例，该框架根据小型语言模型估计提示的重要标记。它由三个模块组成：预算控制器、迭代代币级提示压缩和分配对齐。该框架可以将 2,366 个令牌的复杂提示压缩到 117 个令牌，实现 20 倍的压缩，同时保持几乎不变的性能。" class="wp-image-988035" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1.png 3340w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-300x174.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-1024x594.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-768x446.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-1536x891.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-2048x1188.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-480x280.png 480w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure1-240x139.png 240w" sizes="(max-width: 3340px) 100vw, 3340px" /><figcaption class="wp-element-caption">图 1. LLMLingua 的框架</figcaption></figure><h2 class="wp-block-heading" id="llmlingua-s-method-and-evaluation">LLMLingua的方法和评估</h2><p>为了开发 LLMLingua 的框架，我们使用了预算控制器来平衡提示中不同模块的敏感性，从而保持语言的完整性。我们的两阶段过程涉及粗粒度的即时压缩。我们首先通过消除某些句子来简化提示，然后单独压缩剩余的标记。为了保持一致性，我们采用了迭代令牌级压缩方法，细化了令牌之间的个体关系。此外，我们对较小的模型进行了微调，通过将其与法学硕士生成数据中的模式对齐来捕获来自不同封闭法学硕士的分布信息。我们通过指令调整来做到这一点。</p><p>为了评估 LLMLingua 的性能，我们在 GSM8K、BBH、ShareGPT 和 Arxiv-March23 四个不同数据集上测试了压缩提示，包括 ICL、推理、总结和对话。我们的方法取得了令人印象深刻的结果，实现了高达 20 倍的压缩，同时保留了原始提示的功能，特别是在 ICL 和推理方面。 LLMLingua 还显着降低了系统延迟。</p><p>在我们的测试过程中，我们使用 LLaMA-7B 作为小语言模型，使用 OpenAI 的 LLM 之一 GPT-3.5-Turbo-0301 作为封闭的 LLM。结果表明，即使在最大压缩比为 20 倍的情况下，LLMLingua 仍保持了提示的原始推理、总结和对话能力，如表 1 和表 2 中的评估指标（EM）列所示。压缩方法未能保留提示中的关键语义信息，尤其是逻辑推理细节。有关这些结果的更深入讨论，请参阅<a href="https://www.microsoft.com/en-us/research/publication/llmlingua-compressing-prompts-for-accelerated-inference-of-large-language-models/" target="_blank" rel="noreferrer noopener">本文</a>第 5.2 节。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img decoding="async" width="1396" height="1480" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1.png" alt="这些是使用GPT-3.5-turbo在GSM8K和BBH上的实验结果，展示了基于不同方法和压缩约束的上下文学习和推理能力。结果表明，LLMLingua 可以实现高达 20 倍的压缩率，而性能仅损失 1.5 点。" class="wp-image-988044" style="width:554px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1.png 1396w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1-283x300.png 283w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1-966x1024.png 966w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1-768x814.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable1-170x180.png 170w" sizes="(max-width: 1396px) 100vw, 1396px" /><figcaption class="wp-element-caption">表 1. 不同方法在 GSM8K 和 BBH 数据集上不同目标压缩比的性能。 </figcaption></figure><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1706" height="468" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2.png" alt="这些是基于不同方法和压缩约束，使用 GPT-3.5-turbo 进行 ShareGPT（对话）和 Arxiv-March23（摘要）的实验结果。结果表明，LLMLingua 可以有效保留原始提示的语义信息，同时实现 3x-9x 的压缩率。" class="wp-image-988053" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2.png 1706w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-300x82.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-1024x281.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-768x211.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-1536x421.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable2-240x66.png 240w" sizes="(max-width: 1706px) 100vw, 1706px" /><figcaption class="wp-element-caption">表 2. 对话和摘要任务中不同方法在不同目标压缩比下的性能。 </figcaption></figure><h2 class="wp-block-heading" id="llmlingua-is-robust-cost-effective-efficient-and-recoverable"> LLMLingua 稳健、经济高效、高效且可恢复</h2><p>LLMLingua 还在各种小型语言模型和不同的封闭式法学硕士中显示了令人印象深刻的结果。当使用 GPT-2-small 时，LLMLingua 在 1/4 镜头约束下获得了 76.27 的强劲性能分数，接近 LLaMA-7B 的 77.33 的结果，并超过了标准提示结果 74.9。同样，即使没有调整 Claude-v1.3（后强大的 LLM 之一），LLMLingua 在 1/2 镜头约束下的得分为 82.61，优于标准提示结果 81.8。</p><p>事实证明，LLMLingua 在缩短响应长度方面也很有效，从而显着减少了 LLM 生成过程中的延迟，减少幅度在 20% 到 30% 之间，如图 2 所示。 </p><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="12000" height="8000" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2.png" alt="该图演示了压缩比与响应令牌数量之间的关系。在不同的任务中，随着压缩比的增加，响应长度都有不同程度的减少，最大减少20%-30%。" class="wp-image-988059" style="width:616px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2.png 12000w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-300x200.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-1024x683.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-768x512.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-1536x1024.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-2048x1365.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguafigure2-240x160.png 240w" sizes="(max-width: 12000px) 100vw, 12000px" /><figcaption class="wp-element-caption">图 2. 在不同压缩比下生成的令牌长度分布。</figcaption></figure><p> LLMLingua 更令人印象深刻的是它的可恢复功能。当我们使用 GPT-4 恢复压缩提示时，它成功地从完整的九步思想链 (CoT) 提示中恢复了所有关键推理信息，这使得法学硕士能够通过连续的中间步骤来解决问题。恢复的提示符与原来的几乎相同，并且其含义被保留。这如表 3 和表 4 所示。 </p><figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1780" height="2538" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3.png" alt="该图展示了原始提示、压缩提示以及使用 GPT-4 恢复压缩提示的结果。原始提示由9步思想链组成，压缩后的提示对于人类来说很难理解。然而，恢复的文本包含了思想链的所有 9 个步骤。" class="wp-image-988062" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3.png 1780w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-210x300.png 210w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-718x1024.png 718w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-768x1095.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-1077x1536.png 1077w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-1436x2048.png 1436w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable3-126x180.png 126w" sizes="(max-width: 1780px) 100vw, 1780px" /><figcaption class="wp-element-caption">表 3. GSM8K 上的延迟比较。 LLMLingua 可以将法学硕士的端到端推理加速 1.7-5.7 倍。 </figcaption></figure><figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1391" height="378" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4.png" alt="该图显示了使用 LLMLingua 时的端到端延迟、不使用 LLMLingua 以及压缩提示时的延迟。随着压缩率的增加，LLMLingua 和端到端延迟都会减少，以 10 倍的令牌压缩率实现高达 5.7 倍的加速。" class="wp-image-988068" style="width:584px;height:auto" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4.png 1391w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4-300x82.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4-1024x278.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4-768x209.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/LLMLinguatable4-240x65.png 240w" sizes="(max-width: 1391px) 100vw, 1391px" /><figcaption class="wp-element-caption">表 4. 使用 GPT-4 从 GSM8K 恢复压缩提示。 </figcaption></figure><h2 class="wp-block-heading" id="enhancing-the-user-experience-and-looking-ahead">增强用户体验并展望未来</h2><p>LLMLingua 已经通过实际应用证明了其价值。它已集成到<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/run-llama/llama_index/blob/main/llama_index/indices/postprocessor/longllmlingua.py" target="_blank" rel="noreferrer noopener">LlamaIndex <span class="sr-only">（在新选项卡中打开）</span></a>中，这是一种广泛采用的检索增强生成 (RAG) 框架。目前，我们正在与产品团队合作，减少 LLM 调用中所需的令牌数量，特别是对于多文档问答等任务。在这里，我们的目标是显着改善法学硕士的用户体验。</p><p>从长远来看，我们提出了<a href="https://www.microsoft.com/en-us/research/publication/longllmlingua-accelerating-and-enhancing-llms-in-long-context-scenarios-via-prompt-compression/">LongLLMLingua</a> ，这是一种专为长上下文场景而设计的提示压缩技术，例如聊天机器人等应用程序中的检索增强问答任务，当信息随时间动态变化时非常有用。它还适用于总结在线会议等任务。 LongLLMLingua 的主要目标是增强法学硕士感知关键信息的能力，使其适用于众多现实世界的应用程序，特别是基于信息的聊天机器人。我们希望这一创新能够为与法学硕士进行更复杂和用户友好的互动铺平道路。</p><p>在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://llmlingua.com/" target="_blank" rel="noreferrer noopener">LLMLingua <span class="sr-only">（在新选项卡中打开）</span></a>页面上了解有关我们工作的更多信息。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/">LLMLingua 文章：通过即时压缩创新 LLM 效率</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item><item><title> Microsoft 在 ESEC/FSE 2023 上：用于简化编码工作流程的人工智能技术</title><link/>https://www.microsoft.com/en-us/research/blog/microsoft-at-esec-fse-2023-ai-techniques-for-a-streamlined-coding-workflow/<dc:creator><![CDATA[Alyssa Hughes]]></dc:creator><pubDate> Wed, 06 Dec 2023 17:00:00 +0000</pubDate><category><![CDATA[Research Blog]]></category><guid ispermalink="false"> https://www.microsoft.com/en-us/research/blog/microsoft-at-esec-fse-2023-ai-techniques-for-a-streamlined-coding-workflow/ </guid><description><![CDATA[<p>探索旨在推进软件开发生命周期的最新人工智能创新。 AdaptivePaste 适应并优化 IDE 中粘贴的代码片段。 InferFix 可自动检测和修复错误。了解如何。</p><p> <a href="https://www.microsoft.com/en-us/research/blog/microsoft-at-esec-fse-2023-ai-techniques-for-a-streamlined-coding-workflow/">微软在 ESEC/FSE 2023 上的帖子：用于简化编码工作流程的人工智能技术</a>首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </description><content:encoded><![CDATA[
<p class="has-text-align-center">这些研究论文在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://2023.esec-fse.org/" target="_blank" rel="noreferrer noopener"><strong><em>ACM 欧洲软件工程联合会议和软件工程基础研讨会</em></strong><span class="sr-only">（在新选项卡中打开）</span></a> (ESEC/FSE 2023)<strong><em>上发表</em></strong><strong><em>，这是软件工程领域的重要会议。</em></strong> </p><figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1.jpg" alt="ESEC/FSE 2023 两篇关于蓝/绿渐变的论文：InterFix 和 AdaptivePaste" class="wp-image-988911" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/ESEC_FSE-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>软件开发的实践不可避免地涉及处理错误和各种编码违规行为的挑战。当开发人员采用从网络或其他同行项目复制和粘贴代码片段的常见做法时，这些问题可能会变得更加明显。虽然这种方法可能提供快速解决方案，但它可能会带来许多潜在的复杂性，包括编译问题、错误，甚至开发人员代码库中的安全漏洞。</p><p>为了解决这个问题，微软的研究人员一直致力于推进软件开发生命周期的不同方面，从代码适应到自动错误检测和修复。在<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://2023.esec-fse.org/" target="_blank" rel="noreferrer noopener">ESEC/FSE 2023 <span class="sr-only">（在新选项卡中打开）</span></a>上，我们介绍了两种旨在提高编码效率的技术。 AdaptivePaste 利用基于学习的方法在集成开发环境 (IDE) 中调整和完善粘贴的代码片段。 InferFix 是一个端到端程序修复框架，旨在自动检测和解决错误。该博客概述了这些技术。 </p><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="979236"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/whats-your-story-ranveer-chandra/" aria-label="What’s Your Story: Ranveer Chandra" data-bi-cN="What’s Your Story: Ranveer Chandra" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Ravneer_Hero_Feature_1400x788.png" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">你的故事是什么：兰维尔·钱德拉</h2><p class="large">您可能了解技术，但您对技术进步背后的人了解多少？ Ranveer Chandra 在#MSRPodcast“你的故事是什么”第一集中讲述了他在印度的成长经历、他在系统和网络方面的工作以及在工作中找到的乐趣。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/whats-your-story-ranveer-chandra/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="What’s Your Story: Ranveer Chandra" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h2 class="wp-block-heading" id="adaptivepaste-intelligent-copy-paste-in-ide"> AdaptivePaste：IDE 中的智能复制粘贴</h2><p>开发人员普遍的做法是将粘贴的代码片段适应特定的用例。然而，当前的代码分析和完成技术，例如屏蔽语言建模和 CodeT5，在识别和调整这些片段中的变量标识符以使它们与周围的代码对齐方面没有达到可接受的准确性水平。在论文“ <a href="https://www.microsoft.com/en-us/research/publication/adaptivepaste-intelligent-copy-paste-in-ide/">AdaptivePaste：IDE 中的智能复制粘贴</a>”中，我们提出了一种基于学习的源代码适应方法，旨在捕获变量使用模式的有意义的表示。首先，我们引入了一个专门的数据流感知去混淆预训练目标，用于粘贴代码片段的适应。接下来，我们介绍基于 Transformer 的模型的两个变体：传统的单解码器模型和具有绑定权重的并行解码器模型。</p><figure class="wp-block-image aligncenter size-full"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Figure1AdaptiveP.png"><img loading="lazy" decoding="async" width="1435" height="490" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Figure1AdaptiveP.png" alt="描述 AdaptivePaste 架构的图表。从带有粘贴代码片段的程序开始，AdaptivePaste 提取与学习任务最相关的语法层次结构并确定优先级，分析数据流，然后对粘贴的代码进行匿名化。生成的程序作为神经模型的输入。输出被序列化为令牌序列。" class="wp-image-988494" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Figure1AdaptiveP.png 1435w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Figure1AdaptiveP-300x102.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Figure1AdaptiveP-1024x350.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Figure1AdaptiveP-768x262.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Figure1AdaptiveP-240x82.png 240w" sizes="(max-width: 1435px) 100vw, 1435px" /></a><figcaption class="wp-element-caption">图 1. AdaptivePaste 架构。对于具有粘贴代码片段的程序，AdaptivePaste 会提取与学习任务最相关的语法层次结构并确定其优先级，分析数据流，并对粘贴代码片段中的变量标识符进行匿名化。生成的程序作为神经模型的输入。输出被序列化为令牌条目序列。</figcaption></figure><p> unidecoder 遵循标准的自回归解码器公式，将粘贴片段中的每个变量映射到上下文中的唯一符号或声明一个新变量。并行解码器为匿名粘贴片段中的每个匿名符号复制解码器，独立预测名称并对每个符号的输出分布进行因式分解。这可以通过显示高于指定阈值的模型预测并输出存在不确定性的“漏洞”来实现选择性代码片段适应。</p><p>为了建立用于粘贴代码片段适应的数据流感知的去混淆预训练目标，我们以整个代码标记的粒度将掩码符号分配给变量标识符。预先存在的代码上下文是未匿名的，允许模型关注范围中定义的现有标识符名称。</p><p>我们对 AdaptivePaste 的评估显示出有希望的结果。它成功改编了 Python 源代码片段，精确匹配精度为 67.8%。当我们分析置信度阈值对模型预测的影响时，我们观察到并行解码器转换器模型在选择性代码自适应设置中将精度提高到 85.9%。</p><h2 class="wp-block-heading" id="inferfix-end-to-end-program-repair-with-llms"> InferFix：利用法学硕士进行端到端程序修复</h2><p>解决软件缺陷占开发成本的很大一部分。为了解决这个问题，论文“ <a href="https://www.microsoft.com/en-us/research/publication/inferfix-end-to-end-program-repair-with-llms-over-retrieval-augmented-prompts/">InferFix：通过检索增强提示使用法学硕士进行端到端程序修复</a>”介绍了一个程序修复框架，该框架结合了最先进的静态分析器 Infer 的功能，Infer 是一个称为 Retriever 的语义检索器模型，以及称为 Generator 的基于转换器的模型，用于解决 Java 和 C# 中的关键安全性和性能错误。</p><p> Infer 静态分析器用于通过形式验证可靠地检测、分类和定位复杂系统中的关键错误。 Retriever 使用 Transformer 编码器模型在已知错误的大型数据集中搜索语义等效的错误和相应的修复。它使用对比学习目标进行训练，以擅长查找相同错误类型的相关示例。</p><p> Generator 采用 120 亿参数的 Codex 模型，根据监督的错误修复数据进行微调。为了增强其性能，提供给生成器的提示通过错误类型注释、错误上下文信息以及检索器从外部非参数存储器检索到的语义相似的修复进行了增强。生成器生成候选来修复错误。</p><figure class="wp-block-image aligncenter size-full"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Fig2_InterFixFlow.png"><img loading="lazy" decoding="async" width="1641" height="721" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Fig2_InterFixFlow.png" alt="描述 InferFix 方法工作流程的图表。从 Pull 请求开始，Infer Static Analyzer 进行错误检测、分类和本地化。随后，上下文提取收集错误和周围上下文的相关细节，然后检索器识别语义上相似的错误。该过程最后 LLM 生成器根据生成的提示提出修复建议。" class="wp-image-988497" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Fig2_InterFixFlow.png 1641w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Fig2_InterFixFlow-300x132.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Fig2_InterFixFlow-1024x450.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Fig2_InterFixFlow-768x337.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Fig2_InterFixFlow-1536x675.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/11/Fig2_InterFixFlow-240x105.png 240w" sizes="(max-width: 1641px) 100vw, 1641px" /></a><figcaption class="wp-element-caption">图 2：InferFix 工作流程。 Infer 静态分析器会检测到容易出错的代码修改，该分析器用于制作带有错误类型注释、位置信息、相关语法层次结构以及检索器识别的类似修复的提示。大型语言模型 (LLM) 生成器为开发人员提供了候选修复。</figcaption></figure><p>为了测试 InferFix，我们策划了一个名为<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/InferredBugs/" target="_blank" rel="noreferrer noopener">InferredBugs <span class="sr-only">（在新选项卡中打开）</span></a>的数据集，该数据集包含丰富的元数据，并包含通过在数千个 Java 和 C# 存储库上执行 Infer 静态分析器识别的错误。结果值得注意。 InferFix 的性能优于强大的 LLM 基线，在 InferredBugs 数据集上，C# 中的准确率达到了 65.6% 的 top-1，Java 中的准确率达到了令人印象深刻的 76.8%。</p><h2 class="wp-block-heading" id="looking-ahead">展望未来</h2><p>借助 AdaptivePaste 和 InferFix，我们希望显着简化编码流程，最大限度地减少错误并提高效率。这包括在添加代码片段时减少错误的引入，并提供自动错误检测、分类和补丁验证。我们相信这些工具有望增强软件开发工作流程，从而降低成本并全面提高项目效率。</p><p>展望未来，GPT-3.5 和 GPT-4 等法学硕士的快速发展激发了我们通过快速工程和其他方法探索如何利用其在错误管理方面的潜力的兴趣。我们的目标是通过简化错误检测和修复流程来增强开发人员的能力，促进更强大、更高效的开发环境。</p><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/microsoft-at-esec-fse-2023-ai-techniques-for-a-streamlined-coding-workflow/">微软在 ESEC/FSE 2023 上的帖子：用于简化编码工作流程的人工智能技术</a>首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>;</content:encoded></item></channel></rss>