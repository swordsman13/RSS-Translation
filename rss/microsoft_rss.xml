<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>微软研究院博客 - 微软研究院</title><atom:link href="https://www.microsoft.com/en-us/research/blog/feed/?from=https%3A%2F%2Fresearch.microsoft.com%2Frss%2Fnews.xml&amp;type=rss" rel="self" type="application/rss+xml"></atom:link><link/> https://www.microsoft.com/en-us/research/blog/<description></description><lastbuilddate> 2024 年 1 月 10 日星期三 16:36:06 +0000</lastbuilddate><language> en-US</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><generator> https://wordpress.org/?v=6.4.2</generator><item><title>提高透明度：负责任的人工智能研究的最新进展</title><link/>https://www.microsoft.com/en-us/research/blog/advancing-transparency-updates-on-responsible-ai-research/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Wed, 10 Jan 2024 17:00:00 +0000</pubDate> <category><![CDATA[Research Blog]]></category><guid ispermalink="false"></guid><description><![CDATA[<p>编者注：此处引用的所有论文都代表了整个 Microsoft 以及整个学术界和工业界的合作，其中包括为 Aether（微软内部人工智能道德和工程与研究影响咨询机构）做出贡献的作者。过去一年中生成式人工智能模型的激增引发了关于人工影响的大量讨论 […]</p><p>文章<a href="https://www.microsoft.com/en-us/research/blog/advancing-transparency-updates-on-responsible-ai-research/">《提高透明度：负责任的人工智能研究的更新》</a>首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>; </description><content:encoded><![CDATA[
<p><strong>编者注：</strong><em>此处引用的所有论文都代表了整个 Microsoft 以及整个学术界和工业界的合作，其中包括为 Aether（微软内部人工智能道德和工程与研究影响咨询机构）做出贡献的作者。</em> </p><figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1.jpg" alt="蓝绿色渐变，以灯泡图标为中心。它周围有六个图标：（从左到右）一群人、眼球、握手、一组秤、锁和盾牌。" class="wp-image-997476" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Responsible-AI-Research-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><p>去年，生成式人工智能模型的激增引发了关于人工智能对人类历史影响的大量讨论。人工智能的进步确实挑战了跨行业的思维，从考虑人们如何发挥创造性作用，到对教育、医学、制造等领域的影响。无论是探索 GPT-4 等大型语言模型 (LLM) 令人印象深刻的新功能，还是检查已经嵌入我们日常生活中的机器学习技术，研究人员都同意透明度的重要性。为了让社会从这项强大的技术中适当受益，必须为人们提供理解模型行为的方法。</p><p>透明度是负责任的、以人为本的人工智能的基本原则，也是问责制的基石。人工智能系统拥有广泛的利益相关者：人工智能从业者需要透明地评估数据和模型架构，以便他们能够识别、衡量和减轻潜在的故障；使用人工智能的人，无论是专家还是新手，都必须能够理解人工智能系统的功能和局限性； <a href="https://www.microsoft.com/en-us/research/publication/gam-coach-towards-interactive-and-user-centered-algorithmic-recourse/">受人工智能辅助决策影响的人们应该有必要时寻求补救的见解</a>；间接利益相关者，例如<a href="https://www.microsoft.com/en-us/research/publication/understanding-peoples-concerns-and-attitudes-toward-smart-cities/">使用智能技术的城市居民，需要明确人工智能部署可能对他们产生的影响</a>。</p><p>在使用极其复杂且通常是专有的模型时提供透明度必须采取不同的形式，以满足使用模型或用户界面的人员的需求。本文介绍了隶属于 Microsoft 人工智能道德和工程与研究影响咨询机构 Aether 的研究人员和工程师最近为提高透明度和负责任的人工智能 (RAI) 所做的一系列努力。这项工作包括<a href="https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4/">调查法学硕士的能力</a>，探索<a href="https://www.microsoft.com/en-us/research/publication/can-generalist-foundation-models-outcompete-special-purpose-tuning-case-study-in-medicine/">解锁这些强大模型的专业领域能力的策略</a><a href="https://www.microsoft.com/en-us/research/publication/ai-transparency-in-the-age-of-llms-a-human-centered-research-roadmap/">，同时敦促人工智能系统开发人员和使用这些系统的人员采取透明的方法</a>。研究人员还致力于改进人工智能危害的识别、测量和减轻，同时分享实用指南，例如<a href="https://www.microsoft.com/en-us/research/publication/un-handbook-on-privacy-preserving-computation-techniques/">红队法学硕士申请</a>和<a href="https://www.microsoft.com/en-us/research/publication/un-handbook-on-privacy-preserving-computation-techniques/" target="_blank" rel="noreferrer noopener">隐私保护计算。</a>这些努力的目标是从实证研究结果转向推进负责任的人工智能实践。 </p><div class="wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-style-border" style="grid-template-columns:60% auto" data-bi-an="media-text"><figure class="wp-block-media-text__media"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=ubacP34H9XE" target="_blank" rel="noreferrer noopener"><img decoding="async" width="1024" height="535" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/GAM_Coach_In_Blog_Asset_1200x627-1024x535.jpg" alt="演示视频"GAM Coach: Towards Interactive and User-centered Algorithmic Recourse"" class="wp-image-998040 size-full" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/GAM_Coach_In_Blog_Asset_1200x627-1024x535.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/GAM_Coach_In_Blog_Asset_1200x627-300x157.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/GAM_Coach_In_Blog_Asset_1200x627-768x401.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/GAM_Coach_In_Blog_Asset_1200x627-240x125.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/GAM_Coach_In_Blog_Asset_1200x627.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /></a> </figure><div class="wp-block-media-text__content" data-bi-an="media-text"><h4 class="wp-block-heading" id="toward-user-centered-algorithmic-recourse">走向以用户为中心的算法资源</h4><p>在 GAM Coach 的演示（AI 透明度方法的一个示例）中，交互式界面让贷款分配场景中的利益相关者了解模型如何基于其预测以及他们可以更改哪些因素来实现目标。</p><div class="wp-block-buttons"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.youtube.com/watch?v=ubacP34H9XE" target="_blank" rel="noreferrer noopener">观看演示</a></div></div></div></div><h4 class="wp-block-heading" id="related-papers">相关论文</h4><ul><li><a href="https://www.microsoft.com/en-us/research/publication/gam-coach-towards-interactive-and-user-centered-algorithmic-recourse/">GAM 教练：走向交互式和以用户为中心的算法资源</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/understanding-peoples-concerns-and-attitudes-toward-smart-cities/">了解人们对智慧城市的关注和态度</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4/">通用人工智能的火花：GPT-4 的早期实验</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/can-generalist-foundation-models-outcompete-special-purpose-tuning-case-study-in-medicine/">通用基础模型能否胜过专用调整？医学案例研究</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/ai-transparency-in-the-age-of-llms-a-human-centered-research-roadmap/">法学硕士时代的人工智能透明度：以人为本的研究路线图</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/un-handbook-on-privacy-preserving-computation-techniques/">联合国隐私保护计算技术手册</a></li></ul><div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div><h2 class="wp-block-heading" id="identifying-harms-in-llms-and-their-applications">识别法学硕士及其应用的危害</h2><p>随着产品团队争先恐后地将法学硕士的力量和吸引力整合到跨领域的对话代理和生产力工具中，人工智能的社会技术本质显而易见。与此同时，最近的报道，例如一名律师<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/" target="_blank" rel="noreferrer noopener">无意中向法庭提交了生成人工智能的虚构法律引文<span class="sr-only">（在新选项卡中打开）</span></a>或令人不安的深度伪造演示，揭示了误解这些模型能力的充足机会，更糟糕的是然而，故意滥用它们。</p><p>预见尚未部署的人工智能系统可能出现的问题是迈向负责任人工智能的第一步。为了应对这一挑战，研究人员推出了<a href="https://www.microsoft.com/en-us/research/publication/aha-facilitating-ai-impact-assessment-by-generating-examples-of-harms/">AHA！ （预测人工智能的危害），用于系统影响评估的人类与人工智能协作</a>。该框架使人们能够判断潜在部署对利益相关者的影响。它使用法学硕士生成小插曲或虚构场景，解释有问题的人工智能行为或危害的道德矩阵。在各种决策环境中对该框架进行评估发现，它所带来的潜在有害结果比人们或法学硕士所能想象的更广泛。 </p><div class="wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-style-border" data-bi-an="media-text"><figure class="wp-block-media-text__media"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming"><img decoding="async" width="1024" height="535" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Planning_Guide_Graphic_In_Blog_Asset_1200x627-1-1024x535.jpg" alt="红队法学硕士指南的插图，包括一个网络图标和一个项目符号列表图标。" class="wp-image-997821 size-full" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Planning_Guide_Graphic_In_Blog_Asset_1200x627-1-1024x535.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Planning_Guide_Graphic_In_Blog_Asset_1200x627-1-300x157.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Planning_Guide_Graphic_In_Blog_Asset_1200x627-1-768x401.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Planning_Guide_Graphic_In_Blog_Asset_1200x627-1-240x125.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Planning_Guide_Graphic_In_Blog_Asset_1200x627-1.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure><div class="wp-block-media-text__content" data-bi-an="media-text"><p> AI 从业者可以遵循此规划指南来帮助他们建立和管理大型语言模型 (LLM) 及其应用程序的红队。基于测试法学硕士以确定潜在有害输出并计划缓解策略的第一手经验，本指南提供了关于谁应该测试、测试什么以及如何测试的提示，以及记录红队数据的指导。</p><div class="wp-block-buttons"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming?source=recommendations" target="_blank" rel="noreferrer noopener">查看规划指南</a></div></div></div></div><p>负责任的人工智能红队，或探索模型及其应用程序来识别不良行为，是另一种危害识别方法。 Microsoft 分享了<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming?source=recommendations" target="_blank" rel="noreferrer noopener">LLM 及其应用程序的 RAI 红队实用指南</a>，并且 RAI 红队的自动化工具也开始出现。尽管法学硕士可以通过创造性的头脑风暴来促进影响评估和失败测试的重要任务，但研究人员强调，为了使人工智能以人为中心，此类工作永远不应该完全自动化。 <a href="https://www.microsoft.com/en-us/research/publication/supporting-human-ai-collaboration-in-auditing-llms-with-llms/" target="_blank" rel="noreferrer noopener">为了提高红队中人类与人工智能的互补性，AdaTest++ 基于现有工具构建</a>，该工具使用 LLM 在适应用户反馈时生成测试建议。重新设计为测试假设提供了更大的人类控制，能够编辑和探索反事实，并在广泛的多样性主题中进行深入测试。 </p><figure class="wp-block-image alignright size-full"><img loading="lazy" decoding="async" width="1200" height="627" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AdaTest_Graphic_In_Blog_Asset_1200x627-1.jpg" alt="替代文本：LLM 审核工具 AdaTest++ 的插图，包括悬停在交互式界面上的手指图标。包含 GitHub 徽标以表明该工具的开源可用性。" class="wp-image-997797" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AdaTest_Graphic_In_Blog_Asset_1200x627-1.jpg 1200w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AdaTest_Graphic_In_Blog_Asset_1200x627-1-300x157.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AdaTest_Graphic_In_Blog_Asset_1200x627-1-1024x535.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AdaTest_Graphic_In_Blog_Asset_1200x627-1-768x401.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AdaTest_Graphic_In_Blog_Asset_1200x627-1-240x125.jpg 240w" sizes="(max-width: 1200px) 100vw, 1200px" /><figcaption class="wp-element-caption">研究人员邀请致力于负责任人工智能的人工智能从业者使用<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/adatest/tree/AdaTest%2B%2B" target="_blank" rel="noreferrer noopener">AdaTest++ <span class="sr-only">（在新选项卡中打开）</span></a>并为其做出贡献，该项目利用人类与人工智能的互补性来审计法学硕士。 AdaTest++ 增强了现有工具，提供了提示模板，并引入了更大的人工控制来测试假设和探索反事实。</figcaption></figure><p>在人工智能隐私方面，研究人员演示了<a href="https://www.microsoft.com/en-us/research/publication/does-prompt-tuning-language-model-ensure-privacy/">如何使用即时调整来使用语言模型从电子邮件系统推断私人信息</a>以提供自动完成的回复。在分享他们的红队技术时，他们鼓励使用语言模型的应用程序加强隐私保护，并采取这样的立场：公开详细说明模型漏洞的透明度是实现对抗稳健性的重要一步。</p><p>识别和暴露安全漏洞是首要问题，尤其是当这些漏洞可能渗透到人工智能生成的代码中时。法学硕士与人工智能辅助编码的集成降低了新手程序员的进入门槛，并提高了资深程序员的生产力。但检查人工智能辅助编码的可靠性和安全性很重要。<em>尽管</em><a href="https://www.microsoft.com/en-us/research/publication/trojanpuzzle-covertly-poisoning-code-suggestion-models/">静态</a><em>分</em>基于变压器的模型替代令牌，为程序员提供看似无害但不安全的代码。研究人员揭露这些漏洞，呼吁采用新方法来训练代码建议模型，并制定流程以确保代码建议在程序员看到它们之前是安全的。</p><h4 class="wp-block-heading" id="related-papers-1">相关论文</h4><ul><li><a href="https://www.microsoft.com/en-us/research/publication/aha-facilitating-ai-impact-assessment-by-generating-examples-of-harms/">啊哈！通过生成危害示例促进人工智能影响评估</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/supporting-human-ai-collaboration-in-auditing-llms-with-llms/">与法学硕士一起支持人机协作审计法学硕士</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/does-prompt-tuning-language-model-ensure-privacy/">快速调整语言模型能否确保隐私？</a></li><li> <a href="https://www.microsoft.com/en-us/research/publication/trojanpuzzle-covertly-poisoning-code-suggestion-models/">TROJANPUZZLE：秘密中毒代码建议模型</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/increasing-diversity-while-maintaining-accuracy-text-data-generation-with-large-language-models-and-human-interventions/">在保持准确性的同时增加多样性：使用大型语言模型和人工干预生成文本数据</a></li></ul><div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div><h2 class="wp-block-heading" id="transparency-for-improving-measurement-and-its-validity">提高测量及其有效性的透明度</h2><p>如果不首先识别并衡量模型输出的潜在危害，透明地检查谁可能受益或可能不会受益，或者什么可能出错以及出错的程度，我们就无法开始减轻人工智能失败的可能性。</p><p> <a href="https://www.microsoft.com/en-us/research/publication/a-framework-for-automated-measurement-of-responsible-ai-harms-in-generative-ai-applications/">用于快速、大规模地自动测量危害的框架</a>有两个法学硕士，他们使用相关社会技术领域专家创建的资源来模拟产品或最终用户交互并评估潜在危害的输出。正如研究人员强调的那样，此类评估的有效性和可靠性严格依赖于这些资源的质量——模拟交互的模板和参数、危害的定义及其注释指南。换句话说，社会技术领域的专业知识是不可或缺的。</p><p>测量有效性——确保指标与测量目标保持一致——是负责任人工智能实践的核心。模型准确性本身并不是评估社会技术系统的充分指标：例如，在生产力应用的背景下， <a href="https://www.microsoft.com/en-us/research/publication/aligning-offline-metrics-and-human-judgments-of-value-for-code-generation-models/">还应该考虑捕获对使用人工智能系统的个人有价值的内容</a>。我们如何确定适合跨领域部署以服务于各种人群和目的的上下文相关模型的指标？团队需要方法来解决每个部署场景的测量和缓解问题。</p><p>语言模型说明了“上下文就是一切”这句格言。当涉及到衡量和减轻人工智能生成的文本中与上下文相关的公平相关危害时，数据集标签通常没有足够的粒度。将伤害归结为“有毒”或“仇恨言论”等笼统标签，并不能捕捉到衡量和减轻针对不同人群的特定伤害所需的细节。 <a href="https://www.microsoft.com/en-us/research/publication/fairprism-evaluating-fairness-related-harms-in-text-generation/">FairPrism 是一个用于检测性别和性行为相关伤害的新数据集</a>，它为人类注释提供了更精细的数据集文档和透明度，包括识别可能成为目标的人群。研究人员将 FairPrism 视为创建更详细的数据集以衡量和减轻人工智能危害的“良方”，并演示了新数据集的 5,000 个英文文本示例如何能够探测对特定群体的公平相关危害。 </p><figure class="wp-block-image alignright size-full"><img loading="lazy" decoding="async" width="1201" height="627" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/FairPrism_Graphic_In_Blog_Asset_1200x627-1.jpg" alt="替代文本：数据集 FairPrism 的插图，包括数据库堆栈的图标，其中箭头指向笔记本电脑的图标，上面覆盖着网络的图标。包含 GitHub 徽标以表明该数据集的开源可用性。" class="wp-image-997809" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/FairPrism_Graphic_In_Blog_Asset_1200x627-1.jpg 1201w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/FairPrism_Graphic_In_Blog_Asset_1200x627-1-300x157.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/FairPrism_Graphic_In_Blog_Asset_1200x627-1-1024x535.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/FairPrism_Graphic_In_Blog_Asset_1200x627-1-768x401.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/FairPrism_Graphic_In_Blog_Asset_1200x627-1-240x125.jpg 240w" sizes="(max-width: 1201px) 100vw, 1201px" /><figcaption class="wp-element-caption">人工智能从业者可以请求访问<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/FairPrism" target="_blank" rel="noreferrer noopener">FairPrism 数据集<span class="sr-only">（在新选项卡中打开）</span></a> ，以检测人工智能生成的文本中与性别和性相关的伤害。 FairPrism 为人工注释提供了更大的粒度。</figcaption></figure><p>同样，研究人员<a href="https://www.microsoft.com/en-us/research/publication/taxonomizing-and-measuring-representational-harms-a-look-at-image-tagging/">加深了围绕自动图像标记系统中代表性危害的讨论</a>，表示需要提高危害分类的透明度和特异性，以便更精确地测量和缓解。图像标记通常供人类使用，如替代文本或在线图像搜索，这与对象识别不同。图像标签可以归咎于具体化社会群体的与公平相关的危害以及刻板印象、贬低或删除。研究人员确定了这四种具体的代表性危害，并将它们映射到图像标记中的计算测量方法。他们指出了增加粒度的好处，但指出没有灵丹妙药：通过添加或删除特定标签来避免伤害的努力实际上可能会引入或加剧这些代表性伤害。</p><h4 class="wp-block-heading" id="related-papers-2">相关论文</h4><ul><li><a href="https://www.microsoft.com/en-us/research/publication/a-framework-for-automated-measurement-of-responsible-ai-harms-in-generative-ai-applications/">自动测量生成人工智能应用中负责任的人工智能危害的框架</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/aligning-offline-metrics-and-human-judgments-of-value-for-code-generation-models/">协调离线指标和人类对代码生成模型的价值判断</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/fairprism-evaluating-fairness-related-harms-in-text-generation/">FairPrism：评估文本生成中与公平相关的危害</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/taxonomizing-and-measuring-representational-harms-a-look-at-image-tagging/">分类和衡量代表性危害：看看图像标签</a></li></ul><div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div><h2 class="wp-block-heading" id="transparency-and-ux-based-mitigations-what-designers-need-and-end-users-want">透明度和基于用户体验的缓解措施：设计师需要什么和最终用户想要什么</h2><p>优先考虑人们的价值并为最佳用户体验 (UX) 进行设计是以人为本、负责任的人工智能的目标。不幸的是，用户体验设计经常被工程组织视为次要考虑因素。但由于人工智能是一门社会技术学科，技术解决方案必须与社会观点和社会科学理论相融合，人工智能不仅将用户体验专业知识带到前台，而且将设计师定位为潜在的创新者，能够通过用户体验干预来减轻一些危害和模型失败。为了实现这一点，用户体验设计师需要透明度——模型如何工作的可见性——这样他们就可以形成<a href="https://www.microsoft.com/en-us/research/publication/designerly-understanding-information-needs-for-model-transparency-to-support-design-ideation-for-ai-powered-user-experience/">“人工智能的设计师理解”</a> ，帮助他们有效地进行构思。一项针对 23 名完成实践设计任务的 UX 设计师的研究表明，他们需要更好的支持，包括更易于理解的模型文档和交互式工具，以帮助他们预测模型故障、设想缓解措施并探索人工智能的新用途。</p><p>具有不同水平的人工智能经验或主题专业知识的人们突然开始利用商业化的生成式人工智能副驾驶来提高生产力和跨领域的决策支持。但生成式人工智能可能会犯错误，而且这些失败的影响可能因用例的不同而有很大差异：例如，创意写作任务中的糟糕表现与医疗保健建议中的错误所产生的影响截然不同。随着风险的增加，减少这些失败的呼声也在增加：人们需要工具和机制来帮助他们审核人工智能的输出。用户体验干预措施非常适合减轻此类伤害。首先，研究人员提出了<a href="https://www.microsoft.com/en-us/research/publication/co-audit-tools-to-help-humans-double-check-ai-generated-content/">一种需求分类，联合审计系统在帮助人们仔细检查生成式人工智能模型响应时应满足这些需求</a>。基本考虑因素应包括个人检测错误的容易程度、他们的技能水平以及在给定情况下错误的成本或后果有多大。 <a href="https://www.microsoft.com/en-us/research/publication/coldeco-an-end-user-spreadsheet-inspection-tool-for-ai-generated-code/">原型 Excel 插件说明了这些注意事项</a>，帮助非程序员检查 LLM 生成的代码的准确性。</p><p>关注人们对透明度的需求和渴望可以带来生产力红利。人们在使用语言模型时遇到的一个核心问题是精心设计提示以产生有用的输出。研究人员在基于 LLM 的代码生成中提出了一个解决方案，展示了<a href="https://www.microsoft.com/en-us/research/publication/what-it-wants-me-to-say-bridging-the-abstraction-gap-between-end-user-programmers-and-code-generating-large-language-models/">一个界面，让人们可以了解模型如何将自然语言查询映射到系统操作</a>。这种透明的方法可以帮助人们调整代码生成器功能的心理模型并相应地修改他们的查询。用户研究（其中包括编码专业知识较低的参与者）的结果表明，这种透明方法增强了用户的信心和信任，同时促进了解释和调试。同样，以人为本的努力，例如<a href="https://www.microsoft.com/en-us/research/publication/when-to-show-a-suggestion-integrating-human-feedback-in-ai-assisted-programming/">对程序员认为接收代码建议最有价值的时间</a><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://erichorvitz.com/copilot_display_AAAI.pdf" target="_blank" rel="noreferrer noopener">进行建模</a>，强调了在解决生产力问题时最终用户需求的首要性。 </p><div class="wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-style-border" style="grid-template-columns:60% auto" data-bi-an="media-text"><figure class="wp-block-media-text__media"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=kNSjscPFpRk" target="_blank" rel="noreferrer noopener"><img loading="lazy" decoding="async" width="1024" height="535" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Demo_Video_In_Blog_Asset_1200x627-1024x535.jpg" alt="“它想让我说什么”演示视频" class="wp-image-998028 size-full" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Demo_Video_In_Blog_Asset_1200x627-1024x535.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Demo_Video_In_Blog_Asset_1200x627-300x157.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Demo_Video_In_Blog_Asset_1200x627-768x401.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Demo_Video_In_Blog_Asset_1200x627-240x125.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Demo_Video_In_Blog_Asset_1200x627.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /></a> </figure><div class="wp-block-media-text__content" data-bi-an="media-text"><h4 class="wp-block-heading" id="what-it-wants-me-to-say"> “它想让我说什么”</h4><p>这种透明方法为非专业程序员提供了一个界面，可以让他们了解语言模型如何将自然语言查询映射到系统操作，帮助他们调整心智模型并修改提示。</p><div class="wp-block-buttons"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.youtube.com/watch?v=kNSjscPFpRk" target="_blank" rel="noreferrer noopener">观看演示</a></div></div></div></div><p>对于经验丰富的编码人员来说，要充满信心并从人工智能辅助代码完成中受益，他们需要能够轻松发现并纠正错误和安全漏洞。在<a href="https://www.microsoft.com/en-us/research/publication/generation-probabilities-are-not-enough-exploring-the-effectiveness-of-uncertainty-highlighting-in-ai-powered-code-completions/">第一个关于标记突出显示对于传达人工智能预测的不确定性的有效性的实证研究</a>中，研究人员研究了一种以类似于拼写检查器的方式吸引程序员注意力的用户体验技术。突出显示预测编辑可能性最高的标记可以使程序员能够通过更有针对性的编辑更快地完成任务。参与者还希望解释形式更加透明，以帮助诊断不确定性，并提出交互设计建议，以提高效率并赋予他们控制权。</p><p>在每个部署环境中，以有意义的方式传达人工智能预测的不确定性都是一个设计挑战。如何通过解释提供透明度仍然是一个难题——研究表明，仅仅存在解释就会增加<a href="https://www.microsoft.com/en-us/research/publication/overreliance-on-ai-literature-review/">对人工智能的过度依赖</a>。设计可帮助人们自信地实现决策目标的用户体验需要了解他们对给定系统如何工作的看法。但实际上，人们对决策者在争论是依赖人工智能系统的输出还是依赖自己的直觉时所经历的过程知之甚少。 <a href="https://www.microsoft.com/en-us/research/publication/understanding-the-role-of-human-intuition-on-reliance-in-human-ai-decision-making-with-explanations/">为了深入了解人类直觉在人工智能辅助决策中依赖人工智能预测的作用，研究人员进行了一项有声思考研究，</a>确定了人们在决定推翻系统时使用的三种直觉。在人工智能支持下执行收入预测和传记分类的简单任务时，参与者表达了对决策结果的“直觉”；具体的数据特征或特征如何影响解释；以及人工智能系统的局限性。研究结果提出了作者所说的“直觉驱动途径”，以理解不同类型的解释对人们推翻人工智能的决定的影响。结果表明，基于示例的解释（即文本叙述）比基于特征的解释（通过条形图和其他视觉工具传达信息）更符合人们的直觉和对预测的推理。与此同时，参与者也表达了熟悉的愿望，希望帮助理解人工智能系统的局限性。建议包括更好地支持透明度和用户理解的界面设计，例如，交互式解释使人们能够更改属性以探索对模型预测的影响。</p><p>适应不同水平的用户专业知识是跨领域和应用程序的 AI UX 设计面临的日益严峻的挑战。例如，在商业中，人工智能或统计学知识有限的人必须越来越多地使用人工智能视觉分析系统来创建报告和提供建议。虽然研究旨在解决改善用户与人工智能交互的知识差距，但一些实用且以证据为依据的工具已经可用。 <a href="https://www.microsoft.com/en-us/research/publication/surfacing-ai-explainability-in-enterprise-product-visual-design-to-address-user-tech-proficiency-differences/">对不同人工智能熟练程度的商业专家进行的案例研究</a>证明了应用<a href="https://www.microsoft.com/en-us/haxtoolkit/ai-guidelines/">现有的人机交互准则</a>来获取透明度线索的有效性。视觉解释提高了参与者使用视觉人工智能系统提出建议的能力。与此同时，研究人员指出，无论参与者对人工智能的理解如何，对输出的信任度都很高，这说明了人工智能透明度对于适当信任的复杂性。</p><h4 class="wp-block-heading" id="related-papers-3">相关论文</h4><ul><li><a href="https://www.microsoft.com/en-us/research/publication/designerly-understanding-information-needs-for-model-transparency-to-support-design-ideation-for-ai-powered-user-experience/">设计性理解：模型透明度的信息需求，以支持人工智能驱动的用户体验的设计理念</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/co-audit-tools-to-help-humans-double-check-ai-generated-content/">联合审核：帮助人类仔细检查人工智能生成内容的工具</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/coldeco-an-end-user-spreadsheet-inspection-tool-for-ai-generated-code/">ColDeco：用于 AI 生成代码的最终用户电子表格检查工具</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/what-it-wants-me-to-say-bridging-the-abstraction-gap-between-end-user-programmers-and-code-generating-large-language-models/">“它想让我说什么”：弥合最终用户程序员和代码生成大型语言模型之间的抽象差距</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/when-to-show-a-suggestion-integrating-human-feedback-in-ai-assisted-programming/">何时显示建议？将人类反馈集成到人工智能辅助编程中</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/generation-probabilities-are-not-enough-exploring-the-effectiveness-of-uncertainty-highlighting-in-ai-powered-code-completions/">生成概率还不够：探索人工智能驱动的代码补全中不确定性突出显示的有效性</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/understanding-the-role-of-human-intuition-on-reliance-in-human-ai-decision-making-with-explanations/">通过解释理解人类直觉在人类人工智能决策中的依赖作用</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/surfacing-ai-explainability-in-enterprise-product-visual-design-to-address-user-tech-proficiency-differences/">在企业产品视觉设计中呈现人工智能可解释性，以解决用户技术熟练程度的差异</a></li></ul><div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div><figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1201" height="627" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AI_Transparency_Graphic_In_Blog_Asset_1200x627_3-1.png" alt="替代文本：大型语言模型时代透明度考虑因素的说明。标有“基线 LLM”的网络图标；标记为“改编的法学硕士”的网络和齿轮图标；一个包含键盘、网络和手指的图标，标记为“LLM 支持的应用程序”，代表了从技术角度考虑的因素。标有“利益相关者”的人物图标代表从利益相关者的角度考虑的问题。" class="wp-image-997791" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AI_Transparency_Graphic_In_Blog_Asset_1200x627_3-1.png 1201w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AI_Transparency_Graphic_In_Blog_Asset_1200x627_3-1-300x157.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AI_Transparency_Graphic_In_Blog_Asset_1200x627_3-1-1024x535.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AI_Transparency_Graphic_In_Blog_Asset_1200x627_3-1-768x401.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/AI_Transparency_Graphic_In_Blog_Asset_1200x627_3-1-240x125.png 240w" sizes="(max-width: 1201px) 100vw, 1201px" /><figcaption class="wp-element-caption">负责任的人工智能的透明度是一项社会技术努力。从技术角度来看，开发法学硕士及其应用程序的组织需要考虑如何适当地描述和传达能力、限制、绩效和风险，以及应在什么级别采取透明度方法。为了满足利益相关者（开发、部署、使用法学硕士或受其影响的广泛人员）的需求，透明度考虑因素应包括利益相关者的目标；改善人们对法学硕士的心智模式并支持适当的信任；并深入了解透明度如何有助于更好地控制法学硕士的机制。 （改编<a href="https://www.microsoft.com/en-us/research/publication/ai-transparency-in-the-age-of-llms-a-human-centered-research-roadmap/">自法学硕士时代的人工智能透明度：以人为本的研究路线图</a>） </figcaption></figure><div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div><h2 class="wp-block-heading" id="transparency-a-means-for-accountability-in-a-new-era-of-ai">透明度：人工智能新时代的问责手段</h2><p>该研究汇编强调，透明度是负责任人工智能多个组成部分的基础。除其他外，它需要对数据集及其组成以及模型行为、功能和限制的理解和交流。透明度还涉及负责任的人工智能危害缓解框架的各个方面：识别、测量、缓解。此外，这项研究确立了用户体验在减轻伤害方面的主要作用，因为人工智能融入了人们在个人和职业生活中每天依赖的应用程序。</p><p>正如<a href="https://www.microsoft.com/en-us/research/publication/ai-transparency-in-the-age-of-llms-a-human-centered-research-roadmap/">法学硕士时代透明度研究路线图</a>的作者所概述的那样，这些复杂模型的海量数据集、不确定性输出、适应性和快速的进化速度给负责任地部署人工智能带来了新的挑战。要提高高度依赖环境的人工智能系统的利益相关者的透明度，还有很多工作要做——从改进我们在模型报告方面发布评估目标和结果的方式，到提供适当的解释、传达模型不确定性，以及设计基于用户体验的模型。缓解措施。</p><p>在我们的人工智能系统设计中优先考虑透明度是为了承认人的首要地位，而技术的目的是服务于人。 <a href="https://www.microsoft.com/en-us/research/publication/the-rise-of-the-ai-co-pilot-lessons-for-design-from-aviation-and-beyond/">在人类与人工智能合作的新领域，透明度在尊重人类能动性和专业知识方面</a>发挥着至关重要的作用，并最终可以让我们对我们正在塑造的世界负责。 </p><div class="wp-block-media-text has-vertical-margin-small  has-vertical-padding-none  is-stacked-on-mobile is-style-border" style="grid-template-columns:60% auto" data-bi-an="media-text"><figure class="wp-block-media-text__media"> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://youtu.be/HDfxgKcnX9w?si=g1PYFuYCf25PK5EW" target="_blank" rel="noreferrer noopener"><img loading="lazy" decoding="async" width="1024" height="535" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Eric_Horvitz_Keynote_In_Blog_Asset_1200x627-1024x535.jpg" alt="Eric Horvitz 发表 KDD2023 主题演讲：人与机器：实现更深入的人类与人工智能协同的途径" class="wp-image-998025 size-full" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Eric_Horvitz_Keynote_In_Blog_Asset_1200x627-1024x535.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Eric_Horvitz_Keynote_In_Blog_Asset_1200x627-300x157.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Eric_Horvitz_Keynote_In_Blog_Asset_1200x627-768x401.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Eric_Horvitz_Keynote_In_Blog_Asset_1200x627-240x125.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/Eric_Horvitz_Keynote_In_Blog_Asset_1200x627.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /></a> </figure><div class="wp-block-media-text__content" data-bi-an="media-text"><h4 class="wp-block-heading" id="pathways-to-deeper-human-ai-synergy">人类与人工智能更深入协同的途径</h4><p>微软首席科学官埃里克·霍维茨 (Eric Horvitz) 在 KDD 2023 主题演讲中概述了法学硕士能力的力量以及丰富人类与人工智能互补性的潜力。</p><div class="wp-block-buttons"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://youtu.be/HDfxgKcnX9w?si=g1PYFuYCf25PK5EW" target="_blank" rel="noreferrer noopener">观看主题演讲</a></div></div></div></div><h4 class="wp-block-heading" id="related-papers-4">相关论文</h4><ul><li><a href="https://www.microsoft.com/en-us/research/publication/ai-transparency-in-the-age-of-llms-a-human-centered-research-roadmap/">法学硕士时代的人工智能透明度：以人为本的研究路线图</a></li><li><a href="https://www.microsoft.com/en-us/research/publication/the-rise-of-the-ai-co-pilot-lessons-for-design-from-aviation-and-beyond/">人工智能副驾驶的崛起：航空及其他领域的设计经验教训</a></li></ul><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="956148"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究院播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-models-and-systems-with-ece-kamar/" aria-label="AI Frontiers: Models and Systems with Ece Kamar" data-bi-cN="AI Frontiers: Models and Systems with Ece Kamar" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/04/ece-podcast-_Topic_podcast-2023Mmm_hero_1400x788_16-9.jpg" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">人工智能前沿：Ece Kamar 的模型和系统</h2><p class="large">Ece Kamar 探索短期缓解技术，使这些模型成为人工智能系统的可行组成部分，赋予它们目的，并分享有助于最大化其价值的长期研究问题。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-models-and-systems-with-ece-kamar/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="AI Frontiers: Models and Systems with Ece Kamar" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p>文章<a href="https://www.microsoft.com/en-us/research/blog/advancing-transparency-updates-on-responsible-ai-research/">《提高透明度：负责任的人工智能研究的更新》</a>首先出现在<a href="https://www.microsoft.com/en-us/research">微软研究院</a>。</p> ]]>;</content:encoded></item><item><title>研究重点：2024 年 1 月 8 日当周</title><link/>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-january-8-2024/<dc:creator><![CDATA[Brenda Potts]]></dc:creator><pubDate> Wed, 10 Jan 2024 16:30:00 +0000</pubDate> <category><![CDATA[Research Blog]]></category><guid ispermalink="false"></guid><description><![CDATA[<p>用于长期时间序列预测的线性专家混合；具有真正零样本能力的弱监督流式多语言语音模型； KBFormer：结构化实体完成的扩散模型；识别人工智能介导的数据访问的风险：</p><p> <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-january-8-2024/">《研究焦点：2024 年 1 月 8 日一周》一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<figure class="wp-block-pullquote"><blockquote><p><em class="">欢迎来到研究焦点，这是一系列博客文章，重点介绍 Microsoft 研究社区的著名出版物、活动、代码/数据集、新员工和其他里程碑。</em> </p></blockquote></figure><figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1.png" alt="研究焦点 - 2024 年 1 月 8 日当周" class="wp-image-997752" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1.png 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2024/01/RF32-BlogHeroFeature-1400x788-1-1280x720.png 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-a584a2137da4151ecbde93fba771f798" id="new-research">新研究</h3><h2 class="wp-block-heading" id="mixture-of-linear-experts-for-long-term-time-series-forecasting">用于长期时间序列预测的混合线性专家</h2><p>长期时间序列预测（LTSF）旨在预测给定过去值的序列的未来值，是机器学习社区中的一个重要问题。它在天气建模、交通流量预测和财务预测等领域非常有用。</p><p>在某些情况下，LTSF 的当前技术水平是通过以线性为中心的模型实现的。然而，现实世界的时间序列通常是非平稳的。例如，流量模式在一周中的不同日子会发生变化。以线性为中心的模型固有的简单性使得它们无法捕获这些模式。在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/mixture-of-linear-experts-for-long-term-time-series-forecasting/" target="_blank" rel="noreferrer noopener">长期时间序列预测的线性专家混合中</a>，来自 Microsoft 的研究人员和外部同事提出了线性专家混合 (MoLE) 来解决这个问题。 MoLE 不是训练单个模型，而是训练多个以线性为中心的模型（即专家）和一个加权并混合其输出的路由器模型。虽然整个框架是端到端训练的，但每个专家都学习专门研究特定的时间模式，并且路由器模型学习自适应地组成专家。实验表明，MoLE 显着降低了以线性为中心的模型的预测误差，并且 MoLE 在 68% 的设置中优于最先进的基于变压器的方法。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/mixture-of-linear-experts-for-long-term-time-series-forecasting/" target="_blank" rel="noreferrer noopener">阅读论文</a></div></div><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-a584a2137da4151ecbde93fba771f798" id="new-research">新研究</h3><h2 class="wp-block-heading" id="a-weakly-supervised-streaming-multilingual-speech-model-with-truly-zero-shot-capability">具有真正零样本能力的弱监督流式多语言语音模型</h2><p>端到端（E2E）模型是自动语音识别（ASR）和语音翻译（ST）中的主导模型结构。这导致了为多语言 ASR 和多语言 ST 任务开发统一的 E2E 模型的努力。过去，流式 ASR 和 ST 任务广泛使用了神经传感器。</p><p>在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/a-weakly-supervised-streaming-multilingual-speech-model-with-truly-zero-shot-capability/" target="_blank" rel="noreferrer noopener">具有真正零射击能力的弱监督流式多语言语音模型中</a>，微软的研究人员提出了一种流式多语言语音模型 - $SM^2$ - 它采用单个神经换能器模型来将多种语言转录或翻译成目标语言。 $SM^2$ 使用弱监督数据进行训练，该数据是通过机器翻译模型转换语音识别转录而创建的。 $SM^2$ 利用来自 25 种语言的 351,000 小时的语音训练数据，实现了令人印象深刻的 ST 性能。值得注意的是，训练期间没有使用人类标记的 ST 数据。它是纯粹的弱监督 ST 数据，是通过使用基于文本的机器翻译服务转换 25 种语言的 351,000 小时匿名 ASR 数据而生成的。</p><p>研究人员还展示了 $SM^2$ 在扩展到新的目标语言时真正的零样本能力，为 \{源语音，目标文本\} 对生成高质量的零样本 ST 翻译，这些在训练。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/a-weakly-supervised-streaming-multilingual-speech-model-with-truly-zero-shot-capability/" target="_blank" rel="noreferrer noopener">阅读论文</a></div></div><div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="956154"><p class="msr-promo__label text-gray-800 text-center text-uppercase"><span class="px-4 bg-white display-inline-block font-weight-semibold small">微软研究院播客</span></p><div class="row pt-3 pb-4 align-items-center"><div class="msr-promo__media col-12 col-md-5"> <a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/collaborators-gov4git-with-kasia-sitkiewicz-and-petar-maymounkov/" aria-label="Collaborators: Gov4git with Petar Maymounkov and Kasia Sitkiewicz" data-bi-cN="Collaborators: Gov4git with Petar Maymounkov and Kasia Sitkiewicz" target="_blank"><img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/04/collaborators_hero_1400x788.jpg" alt="GitHub 产品经理 Kasia Sitkiewicz 和协议实验室研究科学家 Petar Maymounkov 在 Microsoft Research 播客上讨论了他们在 Gov4git 上的合作" /></a></div><div class="msr-promo__content p-3 px-5 col-12 col-md"><h2 class="h4">合作者：Gov4git、Petar Maymounkov 和 Kasia Sitkiewicz</h2><p class="large"> Gov4git 是一种用于去中心化、开源合作的治理工具，有助于为未来奠定基础，让每个人都可以更高效、透明、轻松地进行协作，并以满足各自社区独特愿望和需求的方式进行协作。 </p><div class="wp-block-buttons justify-content-center justify-content-md-start"><div class="wp-block-button"> <a href="https://www.microsoft.com/en-us/research/podcast/collaborators-gov4git-with-kasia-sitkiewicz-and-petar-maymounkov/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Collaborators: Gov4git with Petar Maymounkov and Kasia Sitkiewicz" target="_blank">现在听</a></div></div></div><!--/.msr-promo__content--></div><!--/.msr-promo__inner-wrap--><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span></div><!--/.msr-promo--><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-a584a2137da4151ecbde93fba771f798" id="new-research">新研究</h3><h2 class="wp-block-heading" id="kbformer-a-diffusion-model-for-structured-entity-completion"> KBFormer：结构化实体完成的扩散模型</h2><p>深度生成模型包括文本的大型语言模型 (LLM)，以及其他模式的模型，例如视觉和音频模型。在最近的一篇论文： <a href="https://www.microsoft.com/en-us/research/publication/kbformer-a-diffusion-model-for-structured-entity-completion/" target="_blank" rel="noreferrer noopener">KBFormer：结构化实体完成的扩散模型</a>中，来自 Microsoft 的研究人员和外部同事探索了具有异构属性（例如数字、分类、字符串和复合）的结构化实体的生成建模。这包括丰富的知识库 (KB) 中的条目、产品目录或科学目录中的项目以及元素周期表和同位素的各种属性等本体。</p><p>他们的方法通过属性上的混合连续离散扩散过程来处理此类异构数据，使用可以对具有任意分层属性的实体进行建模的灵活框架。使用这种方法，研究人员在 15 个数据集的大多数案例中获得了最先进的性能。此外，使用设备知识库和核物理数据集进行的实验证明了该模型能够学习对不同设置中的实体完成有用的表示。这有许多下游用例，包括高精度建模数值属性——这对于科学应用至关重要，这也受益于模型固有的概率性质。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/kbformer-a-diffusion-model-for-structured-entity-completion/" target="_blank" rel="noreferrer noopener">阅读论文</a></div></div><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-a584a2137da4151ecbde93fba771f798" id="new-research">新研究</h3><h2 class="wp-block-heading" id="a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers">探索人工智能介导的企业知识获取的后果并识别工人风险的框架</h2><p>人们越来越多地与工作场所中人工智能系统的部署进行互动并受到其影响。对于系统设计者、政策制定者和工人本身来说，这是一个紧迫的问题，微软的研究人员在最近的一篇论文中解决了这个问题： <a href="https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/" target="_blank" rel="noreferrer noopener">探索人工智能介导的企业知识访问的后果和识别工人风险的框架</a>。</p><p>组织产生大量信息，这给组织知识的维护、传播和发现带来了挑战。人工智能的最新发展，尤其是大型语言模型 (LLM)，呈现出该领域可能发生的转变。最近的进展可以实现更广泛的挖掘、知识合成以及与知识相关的自然语言交互。</p><p>研究人员提出了后果-机制-风险框架，以识别与部署人工智能介导的企业知识访问系统相关的工人风险。目标是支持参与此类系统设计和/或部署的人员识别它们引入的风险、引入这些风险的特定系统机制以及降低这些风险的可行杠杆。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/" target="_blank" rel="noreferrer noopener">阅读论文</a></div></div><h3 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-a584a2137da4151ecbde93fba771f798" id="new-research">新研究</h3><h2 class="wp-block-heading" id="large-search-model-redefining-search-stack-in-the-era-of-llms">大搜索模型：法学硕士时代重新定义搜索堆栈</h2><p>现代搜索引擎建立在一堆不同的组件之上，包括查询理解、检索、多阶段排名和问题回答等。这些组件通常是独立优化和部署的。在最近的一篇论文《 <a href="https://www.microsoft.com/en-us/research/publication/large-search-model-redefining-search-stack-in-the-era-of-llms/" target="_blank" rel="noreferrer noopener">大型搜索模型：重新定义 LLM 时代的搜索堆栈》</a>中，微软的研究人员介绍了一种称为大型搜索模型的新颖概念框架，该框架通过将搜索任务与一个大型语言模型 (LLM) 统一来重新定义传统搜索堆栈。所有任务都被表述为自回归文本生成问题，允许通过使用自然语言提示来定制任务。该框架利用了法学硕士强大的语言理解和推理能力，提供了提高搜索结果质量的潜力，同时简化了繁琐的搜索堆栈。为了证实该框架的可行性，研究人员提出了一系列概念验证实验，并讨论了在现实世界的搜索系统中实施该方法所面临的潜在挑战。 </p><div class="wp-block-buttons is-content-justification-center"><div class="wp-block-button is-style-outline"> <a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/large-search-model-redefining-search-stack-in-the-era-of-llms/" target="_blank" rel="noreferrer noopener">阅读论文</a></div></div><span id="label-external-link" class="sr-only" aria-hidden="true">在新选项卡中打开</span><p><a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-january-8-2024/">《研究焦点：2024 年 1 月 8 日一周》一文</a>首先出现在<a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>上。</p> ]]>;</content:encoded></item></channel></rss>