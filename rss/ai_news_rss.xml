<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/"><channel><title>人工智能新闻</title><atom:link href="https://www.artificialintelligence-news.com/feed/" rel="self" type="application/rss+xml"></atom:link><link/> https://www.artificialintelligence-news.com/<description>人工智能新闻</description><lastbuilddate>2024 年 12 月 10 日星期二 17:18:29 +0000</lastbuilddate><language> en-GB</language><sy:updateperiod>每小时</sy:updateperiod><sy:updatefrequency>1</sy:updatefrequency><image/><url> https://www.artificialintelligence-news.com/wp-content/uploads/2020/09/ai-icon-60x60.png</url><title>人工智能新闻</title><link/>https://www.artificialintelligence-news.com/<width> 32</width><height> 32</height><item><title>机器遗忘：研究人员让人工智能模型“忘记”数据</title><link/>https://www.artificialintelligence-news.com/news/machine-unlearning-researchers-ai-models-forget-data/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=machine-unlearning-researchers-ai-models-forget-data<comments> https://www.artificialintelligence-news.com/news/machine-unlearning-researchers-ai-models-forget-data/#respond</comments><dc:creator><![CDATA[Ryan Daws]]></dc:creator><pubDate> Tue, 10 Dec 2024 17:18:26 +0000</pubDate> <category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[Ethics & Society]]></category><category><![CDATA[Machine Learning]]></category><category><![CDATA[Privacy]]></category><category><![CDATA[ai]]></category><category><![CDATA[artificial intelligence]]></category><category><![CDATA[ethics]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[privacy]]></category><guid ispermalink="false"> https://www.artificialintelligence-news.com/?p=16680 </guid><description><![CDATA[<p>东京理科大学 (TUS) 的研究人员开发了一种方法，使大规模人工智能模型能够选择性地“忘记”特定类别的数据。人工智能的进步提供了能够彻底改变从医疗保健到自动驾驶等各个领域的工具。然而，随着技术的进步，其复杂性和道德考虑也在不断提高。  大规模的范例<a class="excerpt-read-more" href="https://www.artificialintelligence-news.com/news/machine-unlearning-researchers-ai-models-forget-data/" title="ReadMachine 忘却：研究人员让人工智能模型“忘记”数据">......阅读更多 ”</a></p><p> <a href="https://www.artificialintelligence-news.com/news/machine-unlearning-researchers-ai-models-forget-data/">机器遗忘：研究人员让人工智能模型“忘记”数据</a>一文首先出现在<a href="https://www.artificialintelligence-news.com">《人工智能新闻》</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p><a href="https://www.tus.ac.jp/en/">东京理科大学</a>(TUS) 的研究人员开发了一种方法，使大规模人工智能模型能够选择性地“忘记”特定类别的数据。</p><p>人工智能的进步提供了能够彻底改变从医疗保健到自动驾驶等各个领域的工具。然而，随着技术的进步，其复杂性和道德考虑也在不断提高。</p><p>大规模预训练人工智能系统的范式，例如 OpenAI 的 ChatGPT 和<a href="https://openai.com/index/clip/">CLIP</a> （对比语言-图像预训练），重塑了人们对机器的期望。这些高度通用的模型能够以一致的精度处理大量任务，已广泛应用于专业和个人用途。</p><p>然而，这种多功能性的代价高昂。训练和运行这些模型需要大量的精力和时间，引发了可持续性问题，并且需要比标准计算机昂贵得多的尖端硬件。使这些问题更加复杂的是，当人工智能模型应用于特定任务时，通才倾向可能会阻碍人工智能模型的效率。</p><p>例如，“在实际应用中，很少需要对各种对象类别进行分类，”领导这项研究的 Go Irie 副教授解释道。 “例如，在自动驾驶系统中，识别有限类别的物体（例如汽车、行人和交通标志）就足够了。</p><p> “我们不需要识别食物、家具或动物物种。保留不需要识别的类可能会降低整体分类精度，并导致计算资源浪费和信息泄漏风险等操作缺陷。”</p><p>一个潜在的解决方案在于训练模型来“忘记”冗余或不必要的信息——简化流程，仅关注所需的信息。虽然一些现有方法已经满足了这种需求，但它们倾向于采用“白盒”方法，用户可以访问模型的内部架构和参数。然而，用户通常无法获得这样的可见性。</p><p>由于商业和道德限制，“黑匣子”人工智能系统更为常见，它隐藏了其内部机制，使得传统的遗忘技术变得不切实际。为了解决这一差距，研究团队转向了无衍生优化——这种方法可以避免对模型难以访问的内部运作的依赖。</p><h3 class="wp-block-heading">在遗忘中进步</h3><p>该研究将于 2024 年在神经信息处理系统 (NeurIPS) 会议上发表，引入了一种被称为“黑盒遗忘”的方法。</p><p>该过程在迭代中修改输入提示（输入模型的文本指令），使人工智能逐渐“忘记”某些类别。入江副教授与合著者 Yusuke Kuwana 和 Yuta Goto（均来自启蒙大学）以及<a href="https://www.nec.com/">NEC 公司</a>的 Takashi Shibata 博士合作完成了这项工作。</p><p>在他们的实验中，研究人员以 CLIP 为目标，这是一种具有图像分类能力的视觉语言模型。他们开发的方法基于协方差矩阵适应进化策略（CMA-ES），这是一种旨在逐步优化解决方案的进化算法。在这项研究中，CMA-ES 用于评估和完善向 CLIP 提供的提示，最终抑制其对特定图像类别进行分类的能力。</p><p>随着项目的进展，挑战也随之而来。现有的优化技术难以扩大规模以适应更大数量的目标类别，因此团队设计了一种称为“潜在上下文共享”的新颖参数化策略。</p><p>这种方法将潜在上下文（提示生成的信息的表示）分解为更小、更易于管理的部分。通过将某些元素分配给单个标记（单词或字符），同时在多个标记中重用其他元素，它们大大降低了问题的复杂性。至关重要的是，这使得该过程在计算上易于处理，即使对于广泛的遗忘应用也是如此。</p><p>通过对多个图像分类数据集进行基准测试，研究人员验证了黑盒遗忘的功效——实现了让 CLIP 在不直接访问 AI 模型内部架构的情况下“忘记”约 40% 目标类别的目标。</p><p>这项研究标志着在黑盒视觉语言模型中诱导选择性遗忘的首次成功尝试，展示了有希望的结果。</p><h3 class="wp-block-heading">帮助人工智能模型忘记数据的好处</h3><p>除了其技术独创性之外，这项创新对于特定任务的精度至关重要的实际应用具有巨大的潜力。</p><p>简化专门任务的模型可以使它们更快、更节省资源，并且能够在功能较弱的设备上运行，从而加速人工智能在以前被认为不可行的领域的采用。</p><p>另一个关键用途在于图像生成，其中忘记视觉上下文的整个类别可以防止模型无意中创建不良或有害的内容，无论是攻击性材料还是错误信息。</p><p>也许最重要的是，这种方法解决了人工智能最大的道德困境之一：<a href="https://www.artificialintelligence-news.com/categories/privacy/">隐私</a>。</p><p>人工智能模型，尤其是大型模型，通常是在海量数据集上进行训练的，这些数据集可能会无意中包含敏感或过时的信息。删除此类数据的请求——尤其是考虑到倡导“被遗忘权”的法律——构成了重大挑战。</p><p>重新训练整个模型以排除有问题的数据成本高昂且耗时，但不解决这一问题的风险可能会产生深远的后果。</p><p> “重新训练大型模型会消耗大量能量，”入江副教授指出。 “‘选择性遗忘’，或者所谓的机器遗忘，可能会为这个问题提供有效的解决方案。”</p><p>这些注重隐私的应用程序在<a href="https://www.artificialintelligence-news.com/categories/ai-industries/healthcare/">医疗保健</a>和<a href="https://www.artificialintelligence-news.com/news/large-language-models-could-revolutionsise-the-finance-sector-within-two-years/">金融</a>等高风险行业尤其相关，这些行业敏感数据是运营的核心。</p><p>随着全球人工智能竞赛的加速，东京理科大学的黑盒遗忘方法开辟了一条重要的前进道路——不仅使技术更具适应性和效率，而且还为用户增加了重要的保障措施。</p><p>尽管滥用的可能性仍然存在，但选择性遗忘等方法表明研究人员正在积极应对道德和实际挑战。</p><p><strong>另请参阅：</strong> <a href="https://www.artificialintelligence-news.com/news/why-qwq-32b-preview-is-the-reasoning-ai-to-watch/"><strong>为什么 QwQ-32B-Preview 是值得关注的推理 AI</strong></a></p><figure class="wp-block-image size-full is-resized"> <a href="https://www.ai-expo.net/"><img fetchpriority="high" decoding="async" width="728" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" alt="" class="wp-image-11874" style="width:769px;height:auto" srcset="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png 728w, https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01-300x37.png 300w, https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01-380x47.png 380w, https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01-350x43.png 350w, https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01-100x12.png 100w, https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01-60x7.png 60w" sizes="(max-width: 728px) 100vw, 728px" /></a></figure><p><strong>想向行业领导者了解更多关于人工智能和大数据的知识吗？</strong>查看在阿姆斯特丹、加利福尼亚州和伦敦举办的<a href="https://www.ai-expo.net/">人工智能与大数据博览会</a>。该综合活动与其他领先活动同期举行，包括<a href="https://intelligentautomation-conference.com/northamerica/">智能自动化大会</a>、 <a href="https://www.blockchain-expo.com/">BlockX</a> 、<a href="https://digitaltransformation-week.com/">数字化转型周</a>以及<a href="https://www.cybersecuritycloudexpo.com/">网络安全与云博览会</a>。</p><p><a href="https://techforge.pub/events/">在此</a>探索由 TechForge 提供支持的其他即将举行的企业技术活动和网络研讨会。</p><p> <a href="https://www.artificialintelligence-news.com/news/machine-unlearning-researchers-ai-models-forget-data/">机器遗忘：研究人员让人工智能模型“忘记”数据</a>一文首先出现在<a href="https://www.artificialintelligence-news.com">《人工智能新闻》</a>上。</p> ]]>;</content:encoded><wfw:commentrss> https://www.artificialintelligence-news.com/news/machine-unlearning-researchers-ai-models-forget-data/feed/</wfw:commentrss><slash:comments> 0</slash:comments></item><item><title>缩小更广泛采用人工智能的信心差距</title><link/>https://www.artificialintelligence-news.com/news/narrowing-the-confidence-gap-for-wider-ai-adoption/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=narrowing-the-confidence-gap-for-wider-ai -采用<comments>https://www.artificialintelligence-news.com/news/narrowing-the-confidence-gap-for-wider-ai-adoption/#respond</comments><dc:creator><![CDATA[AI News]]></dc:creator><pubDate> Mon, 09 Dec 2024 09:41:12 +0000</pubDate><category><![CDATA[Artificial Intelligence]]></category><guid ispermalink="false"> https://www.artificialintelligence-news.com/?p=16671 </guid><description><![CDATA[<p>人工智能轰动一时地进入市场，引发了广泛的关注和采用。但现在步伐却步履蹒跚。商界领袖仍然谈论拥抱人工智能，因为他们想要好处——麦肯锡估计 GenAI 可以在一系列业务中为公司节省高达 2.6 万亿美元。然而，他们并没有走<a class="excerpt-read-more" href="https://www.artificialintelligence-news.com/news/narrowing-the-confidence-gap-for-wider-ai-adoption/" title="阅读缩小人工智能更广泛采用的信心差距">……阅读更多 ”</a></p><p> <a href="https://www.artificialintelligence-news.com/news/narrowing-the-confidence-gap-for-wider-ai-adoption/">《缩小对更广泛人工智能采用的信心差距》</a>一文首先出现在<a href="https://www.artificialintelligence-news.com">《人工智能新闻》</a>上。</p> ]]>; </description><content:encoded><![CDATA[
<p>人工智能轰动一时地进入市场，引发了广泛的关注和采用。但现在步伐却步履蹒跚。</p><p>商界领袖仍然谈论拥抱人工智能，因为他们想要好处——麦肯锡估计 GenAI 可以在一系列业务<a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier">中为公司节省高达 2.6 万亿美元</a>。然而，他们并没有付诸行动。根据对高级分析和 IT 领导者的一项调查，目前<a href="https://blog.dataiku.com/bridge-the-modern-analytics-gap">只有 20% 的 GenAI 应用程序</a>投入生产。</p><p>为什么兴趣与现实差距如此之大？</p><p>答案是多方面的。对安全和数据隐私、合规风险和数据管理的担忧备受关注，但人们也对人工智能缺乏透明度以及投资回报率、成本和技能差距感到担忧。在本文中，我们将探讨采用人工智能的障碍，并分享企业领导者可以采取的一些克服这些障碍的措施。</p><h3 class="wp-block-heading">掌握数据</h3><p>SolarWinds 副总裁兼解决方案工程全球主管<a href="https://www.artificialintelligence-news.com/news/solarwinds-it-professionals-stronger-ai-regulation/">Rob Johnson 表示</a>：“高质量数据是准确可靠的 AI 模型的基石，进而推动更好的决策和结果。”他补充道，“值得信赖的数据增强了人们对 AI 的信心IT 专业人员之间的合作，加速人工智能技术的更广泛采用和集成。”</p><p>如今，只有 43% 的 IT 专业人员表示他们对自己满足人工智能数据需求的能力充满信心。鉴于数据对于人工智能的成功至关重要，数据挑战成为人工智能采用缓慢的一个经常被提及的因素也就不足为奇了。</p><p>克服这一障碍的最佳方法是回到数据基础知识。组织需要从头开始构建强大的数据治理策略，并通过严格的控制来加强数据质量和完整性。</p><h3 class="wp-block-heading">认真对待道德和治理</h3><p>随着法规的不断涌现，合规性已经成为许多组织头疼的问题。人工智能只会增加新的风险领域、更多的监管以及更多的道德治理问题，让企业​​领导者需要担心，以至于安全和合规风险<a href="https://www.cloudera.com/campaign/the-state-of-enterprise-ai-and-modern-data-architecture.html?internalkeyplay=cross&amp;internalcampaign=Thought-Leadership-Reports%E2%80%93FY25-Q2-GLOBAL-ME-Sponsored-GenAI-MDA-Survey-PR&amp;cid=701Hr000001ffvDIAQ&amp;internallink=blog-body-content">是 Cloudera 的《企业人工智能和现代数据架构现状》报告中提到最多的问题</a>。</p><p>虽然人工智能法规的兴起乍一看似乎令人担忧，但高管们应该接受这些框架提供的支持，因为它们可以为组织提供一个结构，围绕该结构建立自己的风险控制和道德护栏。</p><p>制定合规政策、任命人工智能治理团队以及确保人类保留对人工智能决策的权威，都是创建全面的人工智能道德和治理体系的重要步骤。</p><h3 class="wp-block-heading">加强对安全和隐私的控制</h3><p>安全和数据隐私问题对每个企业来说都很重要，这是有充分理由的。思科 2024 年数据隐私基准研究显示， <a href="https://investor.cisco.com/news/news-details/2024/More-than-1-in-4-Organisations-Banned-Use-of-GenAI-Over-Privacy-and-Data-Security-Risks%E2%80%93New-Cisco-Study/default.aspx">48% 的员工</a>承认将非公开公司信息输入到 GenAI 工具中（还有未知数量的人这样做了但不会承认），这导致 27% 的组织禁止使用此类工具工具。</p><p>降低风险的最佳方法是限制对敏感数据的访问。这涉及加倍加强访问控制和特权蔓延，并使数据远离公共托管的法学硕士。 Pyramid Analytics 的首席技术官 Avi Perez 解释说，他的商业智能软件的 AI 基础设施是特意构建的，目的是<a href="https://www.pyramidanalytics.com/blog/what-is-generative-bi/">让数据远离 LLM</a> ，仅共享描述问题的元数据，并与 LLM 交互作为本地托管引擎运行的最佳方式分析。</p><p> “那里存在很多问题。这不仅涉及隐私，还涉及误导性结果。因此，在我看来，在这个框架中，数据隐私和与之相关的问题是巨大的。他们是一个令人惊叹的人，”<a href="https://happyfutureai.com/revolutionising-business-intelligence-with-llm-chatbots/">佩雷斯说</a>。然而，通过金字塔的设置，“法学硕士生成了配方，但它在没有获得数据的情况下就完成了它，也没有进行数学运算。 [……]就数据隐私风险而言，这消除了 95% 的问题。”</p><h3 class="wp-block-heading">提高透明度和可解释性</h3><p>人工智能采用的另一个严重障碍是对其结果缺乏信任。亚马逊人工智能招聘工具歧视女性的臭名昭著的故事已经成为一个警示故事，让许多人远离人工智能。消除这种恐惧的最佳方法是提高可解释性和透明度。</p><p> UST 首席人工智能架构师兼微软区域总监<a href="https://www.techtarget.com/searchcio/tip/AI-transparency-What-is-it-and-why-do-we-need-it">Adnan Masood 表示</a>：“人工智能透明度就是要清楚地解释输出背后的原因，使决策过程易于理解。” “归根结底，这是为了消除人工智能的黑匣子之谜，并提供对人工智能决策的方式和原因的洞察。” 不幸的是，许多高管忽视了透明度的重要性。 IBM 最近的一项研究报告称， <a href="https://www.bigdatawire.com/this-just-in/ibm-study-c-suite-executives-voice-growing-doubts-about-it-amid-genai-investments/">只有 45% 的首席执行官</a>表示他们正在提供开放性能力。人工智能拥护者需要优先制定严格的人工智能治理政策，以防止黑匣子的出现，并投资于 SHapley Additive exPlanations (SHAP) 等可解释性工具、谷歌公平指标等公平工具包以及内部审计师协会人工智能等自动合规性检查审计框架。</p><h3 class="wp-block-heading">定义明确的商业价值</h3><p>一如既往，成本是人工智能的障碍之一。 Cloudera 调查发现，26% 的受访者表示 AI 工具太贵，Gartner 将“商业价值不明确”列为 AI 项目失败的一个因素。然而同一份 Gartner 报告指出，GenAI 的用户平均收入增长和成本节省超过 15%，这证明人工智能如果实施得当，可以推动财务增长。</p><p>这就是为什么像对待其他商业项目一样对待人工智能至关重要——确定能够提供快速投资回报的领域，定义您期望看到的收益，并设置具体的 KPI，以便您可以证明价值。” UiPath 产品营销总监<a href="https://www.uipath.com/blog/ai/overcome-ai-adoption-barriers">Michael Robinson 表示</a>：“制定人工智能战略和路线图时，关键的第一步是确定要关注的最有价值和最具变革性的人工智能用例。”</p><h3 class="wp-block-heading">制定有效的培训计划</h3><p>技能差距仍然是人工智能采用的一个重大障碍，但似乎没有采取什么措施来解决这个问题。 Worklife 的一份报告表明，人工智能采用的最初热潮来自于早期采用者。现在，这取决于落后者，他们本质上持怀疑态度，而且普遍对人工智能和任何新技术缺乏信心。</p><p>这使得培训至关重要。然而，根据 Asana 的工作中人工智能现状研究， <a href="https://asana.com/resources/state-of-ai-work-2024-thank-you">82% 的参与者表示，</a>他们的组织尚未提供有关使用生成式人工智能的培训。没有迹象表明培训不起作用；相反，它没有按应有的方式发生。</p><p>明确的要点是提供质量提示和其他相关技能的全面培训。令人鼓舞的是，同一项研究表明，即使在未经培训的情况下使用人工智能也能提高人们的技能和信心。因此，最好开始使用低代码和无代码工具，让不熟悉人工智能的员工在工作中学习。</p><h3 class="wp-block-heading">采用人工智能的障碍并非不可克服</h3><p>尽管人工智能的采用速度已经放缓，但没有迹象表明它从长远来看处于危险之中。阻碍公司推出人工智能工具的许多障碍可以轻松克服。无论是否考虑人工智能，都应该采取许多步骤，例如加强数据质量和道德治理，而采取的其他步骤将通过人工智能带来的收入增加和生产力提升而得到回报。</p><p> <a href="https://www.artificialintelligence-news.com/news/narrowing-the-confidence-gap-for-wider-ai-adoption/">《缩小对更广泛人工智能采用的信心差距》</a>一文首先出现在<a href="https://www.artificialintelligence-news.com">《人工智能新闻》</a>上。</p> ]]>;</content:encoded><wfw:commentrss> https://www.artificialintelligence-news.com/news/narrowing-the-confidence-gap-for-wider-ai-adoption/feed/</wfw:commentrss><slash:comments> 0</slash:comments></item></channel></rss>