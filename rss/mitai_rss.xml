<rss version="2.0" xml:base="https://news.mit.edu" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>麻省理工学院新闻 - 计算机科学与人工智能实验室 (CSAIL)</title><link/> https://news.mit.edu/topic/mitcomputers-rss.xml <atom:link href="https://news.mit.edu/topic/mitcomputers-rss.xml" rel="self" type="application/rss+xml"></atom:link><description>麻省理工学院新闻提要：计算机科学与人工智能实验室 (CSAIL)</description><language> zh</language><lastbuilddate> 2023 年 7 月 31 日星期一 00:00:00 -0400</lastbuilddate><item><title>使用 AI 防止 AI 图像操纵</title><link/>https://news.mit.edu/2023/using-ai-protect-against-ai-image-manipulation-0731<description>由麻省理工学院 CSAIL 研究人员开发的“PhotoGuard”可防止未经授权的图像处理，从而在先进生成模型时代保障真实性。</description><pubDate> Mon, 31 Jul 2023 00:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2023/using-ai-protect-against-ai-image-manipulation-0731</guid><dc:creator>雷切尔戈登|麻省理工学院计算机科学与人工智能实验室</dc:creator><content:encoded>&lt;p>;当我们进入一个新时代时，人工智能驱动的技术可以精确地制作和操纵图像，从而模糊了现实与虚构之间的界限，滥用的幽灵变得越来越大。最近，DALL-E 和 Midjourney 等先进的生成模型以其令人印象深刻的精度和用户友好的界面而闻名，使超现实图像的制作变得相对轻松。随着进入门槛的降低，即使是没有经验的用户也可以通过简单的文本描述生成和操作高质量的图像——从无害的图像更改到恶意的更改。像水印这样的技术是一个很有前途的解决方案。 ，但滥用需要先发制人（而不是事后）措施。&lt;/p>; &lt;p>;为了创建这样一种新措施，麻省理工学院计算机科学和人工智能实验室 (CSAIL) 的研究人员开发了“&lt; a href=&quot;https://arxiv.org/abs/2302.06588&quot; target=&quot;_blank&quot;>;PhotoGuard&lt;/a>;”，一种使用扰动的技术——人眼看不见但计算机模型可以检测到的像素值的微小变化— 这有效地破坏了模型操纵图像的能力。&lt;/p>; &lt;p>;PhotoGuard 使用两种不同的“攻击”方法来生成这些扰动。更直接的“编码器”攻击针对人工智能模型中图像的潜在表示，导致模型将图像感知为随机实体。更复杂的“扩散”定义目标图像并优化扰动，使最终图像尽可能接近目标。&lt;/p>; &lt;p>;“考虑虚假灾难性事件（例如爆炸）的欺诈性传播的可能性在一个重要的里程碑。这种欺骗可以操纵市场趋势和公众情绪，但风险不仅仅限于公共领域。个人图像可能会被不当更改并用于勒索，大规模执行时会导致重大的财务影响，”麻省理工学院电气工程和计算机科学 (EECS) 研究生、麻省理工学院 CSAIL 附属机构、主要作者 Hadi Salman 说道。 &lt;a href=&quot;https://arxiv.org/abs/2302.06588&quot; target=&quot;_blank&quot;>;关于 PhotoGuard 的新论文&lt;/a>;。&lt;/p>; &lt;p>;“在更极端的情况下，这些模型可以模拟声音和图像来实施虚假犯罪，造成心理困扰和经济损失。这些行动的迅速性使问题变得更加复杂。即使欺骗最终被揭穿，损害——无论是声誉、情感还是财务——往往已经发生。这对于各个层面的受害者来说都是现实，从在学校被欺负的个人到全社会范围内的操纵。”&lt;/p>; &lt;p>;&lt;strong>;实践中的 PhotoGuard&lt;/strong>;&lt;/p>; &lt;p>;人工智能模型认为图像与人类的不同。它将图像视为一组复杂的数学数据点，描述每个像素的颜色和位置——这是图像的潜在表示。编码器攻击会对这种数学表示形式进行细微调整，导致 AI 模型将图像感知为随机实体。因此，任何使用模型来操纵图像的尝试都几乎不可能。引入的变化是如此微小，以至于人眼看不见，从而在确保其受到保护的同时保留了图像的视觉完整性。&lt;/p>; &lt;p>;第二种显然更复杂的“扩散”攻击战略性地针对整个扩散模型端-到结束。这涉及确定所需的目标图像，然后启动优化过程，旨在将生成的图像与预先选择的目标紧密对齐。&lt;/p>; &lt;p>;在实现过程中，团队在原始图像的输入空间内创建了扰动。然后，这些扰动将在推理阶段使用，并应用于图像，从而为未经授权的操纵提供强有力的防御。&lt;/p>; &lt;p>;“我们所见证的人工智能进步确实令人惊叹，但它也使得有益和恶意成为可能。麻省理工学院 EECS 教授和 CSAIL 首席研究员 Aleksander Madry 说道，他也是该论文的作者之一。 “因此，我们迫切需要努力识别和减轻后者。我认为 PhotoGuard 是我们对这项重要工作的微小贡献。”&lt;/p>; &lt;p>;扩散攻击比其更简单的同类攻击计算量更大，并且需要大量 GPU 内存。该团队表示，用更少的步骤来近似扩散过程可以缓解这个问题，从而使该技术更加实用。&lt;/p>; &lt;p>;为了更好地说明攻击，请考虑一个艺术项目。原始图像是一幅图画，而目标图像是另一幅完全不同的图画。扩散攻击就像对第一张图进行微小的、看不见的改变，这样，对于人工智能模型来说，它开始类似于第二张图。然而，对于人眼来说，原始绘图保持不变。&lt;/p>; &lt;p>;通过这样做，任何试图修改原始图像的 AI 模型现在都会像处理目标图像一样无意中进行更改，从而保护原始图像来自有意操纵的图像。结果是一张对于人类观察者来说在视觉上保持不变的图片，但可以防止人工智能模型进行未经授权的编辑。&lt;/p>; &lt;p>;就 PhotoGuard 的真实示例而言，请考虑具有多个面孔的图像。您可以遮盖任何不想修改的面孔，然后提示“两个男人参加婚礼”。提交后，系统将相应地调整图像，创建两个参加婚礼的男子的合理描述。&lt;/p>; &lt;p>;现在，考虑保护图像不被编辑；在上传之前向图像添加扰动可以使其免受修改。在这种情况下，与原始的未免疫图像相比，最终输出将缺乏真实感。&lt;/p>; &lt;p>;&lt;strong>;全体人员齐心协力&lt;/strong>;&lt;/p>; &lt;p>;战斗中的关键盟友该团队表示，反对图像处理的是图像编辑模型的创建者。为了使 PhotoGuard 发挥作用，所有利益相关者必须做出综合反应。 “政策制定者应考虑实施法规，要求公司保护用户数据免遭此类操纵。这些人工智能模型的开发人员可以设计 API，自动向用户的图像添加扰动，从而提供额外的保护层，防止未经授权的编辑。&lt;/p>; &lt;p>;尽管 PhotoGuard 做出了承诺，但它并不是万能的。一旦图像上线，怀有恶意的个人可能会尝试通过应用噪声、裁剪或旋转图像来对保护措施进行逆向工程。然而，对抗性示例文献中有大量先前的工作，可以在这里用来实现抵抗常见图像操作的鲁棒扰动。&lt;/p>; &lt;p>;“一种涉及模型开发人员、社交媒体平台和政策制定者的协作方法提出了针对未经授权的图像操纵的强大防御。解决这一紧迫问题在今天至关重要。”萨尔曼说。 “虽然我很高兴为这一解决方案做出贡献，但要使这种保护切实可行还需要做很多工作。开发这些模型的公司需要投资设计强大的免疫系统，以应对这些人工智能工具可能带来的威胁。当我们踏入这个生成模型的新时代时，让我们以平等的方式争取潜力和保护。”&lt;/p>; &lt;p>;“利用对机器学习的攻击来保护我们免遭滥用这项技术的前景是非常引人注目的”，苏黎世联邦理工学院助理教授 Florian Tramèr 说道。 “该论文有一个很好的见解，即生成式人工智能模型的开发者有强烈的动机为其用户提供此类免疫保护，这甚至可能成为未来的法律要求。然而，设计有效抵御规避企图的图像保护是一个具有挑战性的问题：一旦生成人工智能公司致力于免疫机制并且人们开始将其应用到他们的在线图像上，我们需要确保这种保护能够对抗有动机的对手，他们甚至可能使用在不久的将来开发的更好的生成人工智能模型。设计如此强大的保护措施是一个难以解决的问题，本文提出了一个令人信服的案例，表明生成式人工智能公司应该致力于解决这个问题。”&lt;/p>; &lt;p>;Salman 与其他主要作者 Alaa Khaddaj 和 Guillaume Leclerc MS 一起撰写了这篇论文。 &#39;18，以及安德鲁·伊利亚斯 &#39;18、MEng &#39;18；这三人都是 EECS 研究生和 MIT CSAIL 附属机构。该团队的部分工作是在麻省理工学院超级云计算集群上完成的，得到美国国家科学基金会拨款和开放慈善事业的支持，并以美国国防高级研究计划局支持的工作为基础。它已在今年 7 月的国际机器学习会议上发布。&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202307/malicious-ai-image-cov.jpg?itok=fJltJR8b" width="390"><media:description type="plain">在此示例中，攻击者试图修改在线找到的图像。对手通过文本提示描述所需的更改，然后使用扩散模型生成与提示匹配的逼真图像。通过在对手访问原始图像之前对其进行免疫，PhotoGuard 系统破坏了成功执行此类编辑的能力。</media:description><media:credit>图片由研究人员提供。</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/technology-society">技术与社会</category><category domain="https://news.mit.edu/topic/algorithms">算法</category><category domain="https://news.mit.edu/topic/technology-and-policy">技术与政策</category><category domain="https://news.mit.edu/topic/computer-vision">计算机视觉</category><category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">电气工程与计算机科学（eecs） </category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL）</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/nsf">美国国家科学基金会 (NSF)</category><category domain="https://news.mit.edu/topic/darpa">国防高级研究计划局 (DARPA)</category></item><item><title>更快地教导机器人的方法</title><link/>https://news.mit.edu/2023/faster-way-teach-robot-technique-0718<description>一项新技术可以帮助非技术用户了解机器人失败的原因，然后以最小的努力对其进行微调以有效地执行任务。</description><pubDate> Tue, 18 Jul 2023 00:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2023/faster-way-teach-robot-technique-0718</guid><dc:creator>亚当·泽威 |麻省理工学院新闻办公室</dc:creator><content:encoded>&lt;p>;想象一下购买一个机器人来执行家务。该机器人是在工厂中制造并接受特定任务训练的，并且从未见过您家中的物品。当你要求它从厨房桌子上拿起一个杯子时，它可能无法识别你的杯子（可能是因为这个杯子上画着一个不寻常的图像，比如麻省理工学院的吉祥物海狸蒂姆）。所以，机器人失败了。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;“现在，我们训练这些机器人的方式，当它们失败时，我们真的不知道为什么。所以你会举起双手说：“好吧，我想我们必须重新开始。”该系统缺少的一个关键组件是让机器人能够展示其失败的原因，以便用户可以向其提供反馈。”麻省理工学院电气工程和计算机科学 (EECS) 研究生 Andi Peng 说道。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;Peng 和她在麻省理工学院、纽约大学和加州大学伯克利分校的合作者创建了一个 &lt;a href=&quot;https://arxiv.org/pdf/2307.06333.pdf&quot; 目标=&quot;_blank&quot;>;框架&lt;/a>;使人类能够以最少的努力快速教会机器人他们想要它做什么。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;当机器人出现故障时，系统使用算法​​生成反事实解释，描述机器人需要做出哪些改变才能成功。例如，如果杯子是某种颜色，机器人也许能够拿起杯子。它向人类展示这些反事实，并要求反馈机器人失败的原因。然后，系统利用这种反馈和反事实解释来生成新数据，用于微调机器人。&lt;/p>; &lt;p>;微调涉及调整已经被训练来执行一项任务的机器学习模型，因此它可以执行第二个类似的任务。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;研究人员在模拟中测试了这项技术，发现它可以比其他方法更有效地教导机器人。使用该框架训练的机器人表现更好，而训练过程消耗的人类时间更少。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;该框架可以帮助机器人在新环境中更快地学习，而无需用户具备技术知识。从长远来看，这可能是朝着使通用机器人能够在各种环境中为老年人或残疾人有效执行日常任务迈出的一步。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;Peng，主要作者、联合作者、EECS 研究生阿维夫·内塔尼亚胡 (Aviv Netanyahu)； Mark Ho，史蒂文斯理工学院助理教授；舒天民，麻省理工学院博士后； Andreea Bobu，加州大学伯克利分校研究生；资深作者 Julie Shah（麻省理工学院航空航天学教授、计算机科学与人工智能实验室 (CSAIL) 交互式机器人小组主任）和 Pulkit Agrawal（EECS 教授兼 CSAIL 附属机构）。&lt;strong>; &lt;/strong >;该研究将在国际机器学习会议上公布。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;&lt;strong>;在职培训&lt;/strong>;&lt;/p>; &lt;p>;&lt;/ p>; &lt;p>;机器人经常因分布转移而失败——机器人会看到它在训练期间没有看到的物体和空间，并且它不知道在这个新环境中要做什么。&lt;/p>; &lt;p>;&lt; /p>; &lt;p>;重新训练机器人执行特定任务的一种方法是模仿学习。用户可以演示正确的任务来教机器人做什么。如果用户尝试教机器人拿起杯子，但用白色杯子进行演示，则机器人可以了解到所有杯子都是白色的。然后它可能无法拿起红色、蓝色或“Tim-the-Beaver-brown”杯子。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;训练机器人识别杯子是杯子，无论其颜色如何，都可能需要数千次演示。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;“我不想用 30,000 个杯子进行演示。我想只用一个杯子来演示。但接下来我需要教机器人，让它认识到它可以拿起任何颜色的杯子。”Peng 说。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;为了实现这一目标，研究人员的系统决定了什么用户关心的特定对象（杯子）以及哪些元素对任务不重要（也许杯子的颜色并不重要）。它利用这些信息通过改变这些“不重要”的视觉概念来生成新的合成数据。此过程称为数据增强。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;该框架分为三个步骤。首先，它显示导致机器人失败的任务。然后，它从用户那里收集所需操作的演示，并通过搜索空间中的所有特征来生成反事实，这些特征显示机器人成功需要进行哪些更改。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;系统向用户显示这些反事实，并请求反馈以确定哪些视觉概念不会影响所需的操作。然后，它使用这种人类反馈来生成许多新的增强演示。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;通过这种方式，用户可以演示拿起一个杯子，但系统会生成显示所需操作的演示通过改变颜色来制作数千种不同的杯子。它使用这些数据来微调机器人。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;Peng 说，创建反事实解释并征求用户反馈对于该技术的成功至关重要。&lt;/p>; &lt; p>;&lt;/p>; &lt;p>;&lt;strong>;从人类推理到机器人推理&lt;/strong>;&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;因为他们的工作旨在将人类置于训练循环中，研究人员对人类用户测试了他们的技术。他们首先进行了一项研究，询问人们反事实解释是否可以帮助他们识别可以在不影响任务的情况下进行更改的元素。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;“情况一目了然。人类非常擅长这种反事实推理。这个反事实的步骤使得人类推理能够以一种有意义的方式转化为机器人推理。”她说。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;然后他们将他们的框架应用到机器人的三个模拟中我们的任务是：导航到目标对象，拿起钥匙并打开门，然后拿起所需的对象然后将其放在桌面上。在每种情况下，他们的方法都使机器人能够比其他技术更快地学习，同时需要更少的用户演示。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;展望未来，研究人员希望在实际中测试这个框架机器人。他们还希望专注于减少系统使用生成机器学习模型创建新数据所需的时间。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;“我们希望机器人做人类做的事情，我们希望他们以语义上有意义的方式做到这一点。人类倾向于在这个抽象的空间中进行操作，他们不会考虑图像中的每一个属性。归根结底，这实际上是为了让机器人能够在抽象层面上学习良好的、类似人类的表示。”Peng 说。&lt;/p>; &lt;p>;&lt;/p>; &lt;p>;这项研究得到了支持部分由美国国家科学基金会研究生研究奖学金、开放慈善机构、Apple AI/ML 奖学金、现代汽车公司、麻省理工学院-IBM 沃森人工智能实验室以及美国国家科学基金会人工智能和基础交互研究所提供。&lt;/ p>; &lt;p>;&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202307/MIT-HumanLoop-01-press.jpg?itok=HrvrBxf6" width="390"><media:description type="plain">麻省理工学院和其他地方的研究人员开发了一种技术，使人类能够有效地微调未能完成所需任务（例如拿起一个独特的杯子）的机器人，而人类只需付出很少的努力。</media:description><media:credit>图片：Jose-Luis Olivares/MIT，图片来自 iStock 和 The Coop</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/robots">机器人</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/algorithms">算法</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/data">数据</category><category domain="https://news.mit.edu/topic/human-computer-interaction">人机交互</category><category domain="https://news.mit.edu/topic/robotics">机器人技术</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL） </category><category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">电气工程与计算机科学（eecs）</category><category domain="https://news.mit.edu/topic/aeronautics">航空航天工程</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category><category domain="https://news.mit.edu/topic/nsf">美国国家科学基金会 (NSF)</category></item></channel></rss>