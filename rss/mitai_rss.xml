<rss version="2.0" xml:base="https://news.mit.edu" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>麻省理工学院新闻 - 计算机科学与人工智能实验室 (CSAIL)</title><link/> https://news.mit.edu/topic/mitcomputers-rss.xml <atom:link href="https://news.mit.edu/topic/mitcomputers-rss.xml" rel="self" type="application/rss+xml"></atom:link><description>麻省理工学院新闻提要：计算机科学与人工智能实验室 (CSAIL)</description><language> zh</language><lastbuilddate> 2024 年 3 月 25 日星期一 04:00:00 +0000</lastbuilddate><item><title>工程家用机器人要有一点常识</title><link/>https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325<description>在大型语言模型的帮助下，麻省理工学院的工程师使机器人能够在失误后自我纠正并继续做家务。</description><pubDate> Mon, 25 Mar 2024 00:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325</guid><dc:creator>朱珍妮|麻省理工学院新闻</dc:creator><content:encoded>&lt;p>;从擦拭溢出物到提供食物，机器人正在被教导执行日益复杂的家务劳动。许多这样的家庭机器人学员都是通过模仿来学习的。它们被编程为模仿人类物理引导它们完成的动作。&lt;/p>; &lt;p>;事实证明，机器人是出色的模仿者。但是，除非工程师也对它们进行编程，以适应每一种可能的碰撞和推动，否则机器人不一定知道如何处理这些情况，除非从头开始执行任务。&lt;/p>; &lt;p>;现在，麻省理工学院的工程师们的目标是提供当机器人面临偏离训练轨迹的情况时，它们会掌握一些常识。他们开发了一种方法，将机器人运动数据与大型语言模型（LLM）的“常识知识”联系起来。&lt;/p>; &lt;p>;他们的方法使机器人能够逻辑地将许多给定的家庭任务解析为子任务，并对子任务中的中断进行物理调整，以便机器人可以继续前进，而无需返回并从头开始任务，并且工程师无需针对整个过程中的每个可能的故障明确地进行编程修复。 &amp;nbsp;&amp;nbsp;&lt;/p>; &lt;img alt=&quot;一只机械手试图舀起红色弹珠并将它们放入另一个碗中，而研究人员的手经常扰乱它。机器人最终成功了。&quot; data-align=&quot;center&quot; data-caption=&quot;图片由研究人员提供。&quot; data-entity-type=&quot;文件&quot; data-entity-uuid=&quot;49962673-31d2-4023-893e-cb462fed9a91&quot; src=&quot;/sites/default/files/images/inline/ComonsenseBots-ani_1.gif&quot; />; &lt;p >;“模仿学习是实现家用机器人的主流方法。但如果机器人盲目地模仿人类的运动轨迹，微小的错误就会累积起来，最终使其余的执行脱轨。”麻省理工学院电气工程与计算机科学系 (EECS) 的研究生 Yanwei Wang 说道。 “通过我们的方法，机器人可以自我纠正执行错误并提高整体任务的成功率。”&lt;/p>; &lt;p>;Wang 和他的同事在 &lt;a href=&quot;https://openreview.net/ 中详细介绍了他们的新方法。 forum?id=qoHeuRAcSl&quot; target=&quot;_blank&quot;>;研究&lt;/a>; 他们将在 5 月份的国际学习表征会议 (ICLR) 上进行展示。该研究的共同作者包括 EECS 研究生 Tsun-Hsuan Wang 和 Jiayuan Mao、麻省理工学院航空航天系 (AeroAstro) 博士后 Michael Hagenow 以及麻省理工学院航空航天系 HN Slater 教授 Julie Shah。&lt;/ p>; &lt;p>;&lt;strong>;语言任务&lt;/strong>;&lt;/p>; &lt;p>;研究人员通过一项简单的工作来说明他们的新方法：从一个碗中舀出弹珠并将其倒入另一个碗中。为了完成这项任务，工程师通常会移动机器人进行舀取和倾倒的动作——所有这些都在一个流体轨迹中进行。他们可能会多次这样做，以便让机器人进行多次人类演示来模仿。&lt;/p>; &lt;p>;“但人类演示是一条长而连续的轨迹，”王说。&lt;/p>; &lt;p>;团队意识到，虽然人类可能一次性演示一项任务，但该任务取决于一系列子任务或轨迹。例如，机器人必须先把手伸进碗里，然后才能舀起，并且必须先舀起弹珠，然后才能移动到空碗，等等。如果机器人在任何这些子任务中被推动或轻推而犯错误，它唯一的办法就是停止并从头开始，除非工程师明确标记每个子任务和程序或收集新的演示，以便机器人从错误中恢复。王说：“这种程度的规划非常乏味。”&lt;/p>; &lt;p>;相反，他和他的同事发现了一些这项工作的一部分可以由法学硕士自动完成。这些深度学习模型处理巨大的文本库，用于在单词、句子和段落之间建立联系。通过这些联系，法学硕士可以根据其对可能跟在最后一个单词后面的单词类型的了解来生成新句子。&lt;/p>; &lt;p>;就他们而言，研究人员发现，除了句子和段落，可以提示法学硕士生成给定任务中涉及的子任务的逻辑列表。例如，如果要求列出将弹珠从一个碗舀到另一个碗中所涉及的动作，法学硕士可能会产生一系列动词，例如“到达”、“舀”、“运输”和“倒”。&lt;/p>; &lt;p>;“法学硕士有办法用自然语言告诉您如何完成任务的每一步。人类的持续演示就是这些步骤在物理空间中的体现，”王说。 “我们希望将两者连接起来，以便机器人能够自动知道它处于任务的哪个阶段，并且能够自行重新计划和恢复。”&lt;/p>; &lt;p>;&lt;strong>;绘制弹珠图&lt;/对于他们的新方法，该团队开发了一种算法，可以自动将特定子任务的法学硕士自然语言标签与机器人在物理空间中的位置或对机器人状态进行编码的图像连接起来。将机器人的物理坐标或机器人状态的图像映射到自然语言标签称为“接地”。该团队的新算法旨在学习一个基础“分类器”，这意味着它能够学习根据机器人的物理坐标或图像视图自动识别机器人所处的语义子任务，例如“到达”与“舀取”。&lt; /p>; &lt;p>;“接地分类器促进了机器人在物理空间中所做的事情与法学硕士对子任务的了解以及每个子任务中必须注意的约束之间的对话，”Wang 解释道。&lt; /p>; &lt;p>;该团队在实验中展示了该方法，并用机械臂进行了铲大理石任务的训练。实验人员通过物理引导机器人完成以下任务来训练机器人：首先将手伸入碗中，舀起弹珠，将它们运送到空碗上，然后将其倒入。经过几次演示后，团队随后使用经过预训练的法学硕士并询问模型列出将弹珠从一个碗舀到另一个碗所涉及的步骤。然后，研究人员使用他们的新算法将法学硕士定义的子任务与机器人的运动轨迹数据连接起来。该算法自动学习将机器人在轨迹中的物理坐标和相应的图像视图映射到给定的子任务。&lt;/p>; &lt;p>;然后，团队让机器人利用新学习的接地自行执行舀取任务分类器。当机器人完成任务的步骤时，实验者将机器人推离其路径，并在不同的点上将勺子上的弹子碰掉。机器人不会停下来重新从头开始，也不会盲目地继续下去，而是能够自我纠正，完成每个子任务，然后再继续下一个。 （例如，它会确保它在将弹珠运送到空碗之前成功舀起弹珠。）&lt;/p>; &lt;p>;“通过我们的方法，当机器人犯错误时，我们不需要要求人类计划或提供额外的演示如何从失败中恢复，”王说。 “这非常令人兴奋，因为现在人们在利用远程操作系统收集的数据来训练家用机器人方面付出了巨大的努力。我们的算法现在可以将训练数据转换为强大的机器人行为，尽管存在外部扰动，但仍可以执行复杂的任务。”&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/CommonSense-01-press.jpg?itok=VbWDKM8h" width="390"><media:description type="plain">在这张拼贴图像中，一只机械手试图舀起红色弹珠并将其放入另一个碗中，而研究人员的手经常破坏它。机器人最终成功了。</media:description><media:credit>图片：Jose-Luis Olivares，麻省理工学院。剧照由研究人员提供</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/aeronautics">航空航天工程</category><category domain="https://news.mit.edu/topic/algorithms">算法</category><category domain="https://news.mit.edu/topic/assistive-technology">辅助技术</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/computer-modeling">计算机建模</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL）</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/human-computer-interaction">人机交互</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/robotics">机器人技术</category><category domain="https://news.mit.edu/topic/robots">机器人</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category></item><item><title>大型语言模型使用一种非常简单的机制来检索一些存储的知识</title><link/>https://news.mit.edu/2024/large-language-models-use-surprisingly-simple-mechanism-retrieve-stored-knowledge-0325<description>研究人员展示了一种可用于探测模型以了解其对新主题的了解的技术。</description><pubDate> Mon, 25 Mar 2024 00:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2024/large-language-models-use-surprisingly-simple-mechanism-retrieve-stored-knowledge-0325</guid><dc:creator>亚当·泽威 |麻省理工学院新闻</dc:creator><content:encoded>&lt;p>;大型语言模型（例如为 ChatGPT 等流行人工智能聊天机器人提供支持的语言模型）非常复杂。尽管这些模型在许多领域被用作工具，例如客户支持、代码生成和语言翻译，但科学家们仍然没有完全掌握它们的工作原理。&lt;/p>; &lt;p>;为了更好地理解什么麻省理工学院和其他地方的研究人员研究了这些巨大的机器学习模型检索存储的知识时的工作机制。&lt;/p>; &lt;p>;他们发现了一个令人惊讶的结果：大型语言模型（LLM）经常使用非常简单的线性函数来恢复和解码存储的事实。此外，该模型对相似类型的事实使用相同的解码函数。线性函数，即只有两个变量且没有指数的方程，捕捉两个变量之间直接的直线关系。&lt;/p>; &lt;p>;研究人员表明，通过识别不同事实的线性函数，他们可以探索模型了解它对新主题的了解，以及知识存储在模型中的何处。&lt;/p>; &lt;p>;使用他们开发的技术来估计这些简单的函数，研究人员发现，即使模型错误地回答了提示，它也会通常存储了正确的信息。将来，科学家可以使用这种方法来查找并纠正模型内部的错误，这可以减少模型有时给出错误或无意义答案的倾向。&lt;/p>; &lt;p>;“尽管这些模型确实很复杂，但非线性经过大量数据训练并且很难理解的函数，有时它们内部的工作机制非常简单。这就是一个例子，”电气工程和计算机科学 (EECS) 研究生、&lt;a href=&quot;https://arxiv.org/pdf/2308.09124.pdf&quot; 目标的共同主要作者 Evan Hernandez 说道=&quot;_blank&quot;>;论文详细介绍了这些发现&lt;/a>;。&lt;/p>; &lt;p>;Hernandez 与共同主要作者 Arnab Sharma 共同撰写了这篇论文，Arnab Sharma 是东北大学计算机科学研究生；他的导师 Jacob Andreas，EECS 副教授，计算机科学与人工智能实验室 (CSAIL) 成员；资深作者 David Bau，东北大学计算机科学助理教授；以及麻省理工学院、哈佛大学和以色列理工学院的其他人。该研究将在学习表示国际会议上公布。&lt;/p>; &lt;p>;&lt;strong>;寻找事实&lt;/strong>;&lt;/p>; &lt;p>;大多数大型语言模型，也称为转换器模型，是&lt;a href =&quot;https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414&quot; target=&quot;_blank&quot;>;神经网络&lt;/a>;。神经网络松散地基于人脑，包含数十亿个互连的节点或神经元，它们分为许多层，并对数据进行编码和处理。&lt;/p>; &lt;p>;存储在变压器中的大部分知识都可以表示作为连接主体和客体的关系。例如，“迈尔斯·戴维斯演奏小号”是一种将主语迈尔斯·戴维斯与客体小号联系起来的关系。&lt;/p>; &lt;p>;随着变压器获得更多知识，它会存储有关某个主题的更多事实。多层。如果用户询问该主题，模型必须解码最相关的事实以响应查询。&lt;/p>; &lt;p>;如果有人通过说“迈尔斯·戴维斯演奏。”来提示变压器。 。 ”。模型应该用“喇叭”而不是“伊利诺伊州”（迈尔斯·戴维斯出生的州）来响应。&lt;/p>; &lt;p>;“在网络计算的某个地方，必须有一种机制来寻找事实迈尔斯·戴维斯（Miles Davis）吹小号，然后提取该信息并帮助生成下一个单词。我们想了解这种机制是什么。”Hernandez 说道。&lt;/p>; &lt;p>;研究人员进行了一系列实验来探索 LLM，结果发现，尽管它们极其复杂，但模型仍使用简单的线性函数。每个函数都特定于正在检索的事实类型。&lt;/p>; &lt;p>;例如，变压器在每次想要输出一个人演奏的乐器时都会使用一个解码函数，而每次想要输出时都会使用不同的函数一个人出生的州。&lt;/p>; &lt;p>;研究人员开发了一种方法来估计这些简单的函数，然后计算 47 种不同关系的函数，例如“一个国家的首都”和“一个国家的主唱”虽然可能存在无限数量的可能关系，但研究人员选择研究这个特定的子集，因为它们代表了可以用这种方式编写的事实类型。&lt;/p>; &lt;p>;他们通过改变主题来测试每个函数，看看它是否可以恢复正确的对象信息。例如，如果主题是挪威，则“某个国家的首都”的函数应检索奥斯陆；如果主题是英格兰，则应检索伦敦。&lt;/p>; &lt;p>;函数在超过 60% 的时间内检索到正确信息，显示变压器中的某些信息是以这种方式编码和检索的。&lt;/p>; &lt;p>;“但并非所有内容都是线性编码的。对于某些事实，即使模型知道它们并且会预测与这些事实一致的文本，我们也无法找到它们的线性函数。这表明模型正在做一些更复杂的事情来存储这些信息。”他说。&lt;/p>; &lt;p>;&lt;strong>;可视化模型的知识&lt;/strong>;&lt;/p>; &lt;p>;他们还使用这些函数来确定模型对不同主题的看法是正确的。&lt;/p>; &lt;p>;在一项实验中，他们从提示“Bill Bradley was a”开始，并使用“参加体育运动”和“上过大学”的解码功能来了解如果模型知道参议员布拉德利是一名就读于普林斯顿大学的篮球运动员。&lt;/p>; &lt;p>;“我们可以证明，即使模型在生成文本时可能选择关注不同的信息，但它确实对所有这些信息进行了编码埃尔南德斯说。&lt;/p>; &lt;p>;他们使用这种探测技术来产生所谓的“属性透镜”，这是一个网格，可以可视化有关特定关系的特定信息存储在变压器的许多层中的位置。&lt;/p>; &lt;p>; p>; &lt;p>;属性透镜可以自动生成，提供了一种简化的方法来帮助研究人员更多地了解模型。这种可视化工具可以帮助科学家和工程师纠正存储的知识，并帮助防止人工智能聊天机器人提供虚假信息。&lt;/p>; &lt;p>;未来，埃尔南德斯和他的合作者希望更好地了解在事实不符的情况下会发生什么。线性存储。他们还想用更大的模型进行实验，并研究线性解码函数的精度。&lt;/p>; &lt;p>;“这是一项令人兴奋的工作，它揭示了我们对大型语言模型如何回忆事实的理解中缺失的部分。推理过程中的知识。先前的工作表明，法学硕士为给定主题构建信息丰富的表示，在推理过程中从中提取特定属性。这项工作表明，用于属性提取的法学硕士的复杂非线性计算可以用一个简单的线性函数很好地逼近。”特拉维夫大学计算机科学学院助理教授 Mor Geva Pipek 说道，他没有参与这项工作。 &lt;/p>; &lt;p>;这项研究部分得到了开放慈善事业、以色列科学基金会和阿兹里利基金会早期职业教师奖学金的支持。&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/MIT-Transformer-Relations-01.jpg?itok=4bPBl75Y" width="390"><media:description type="plain">麻省理工学院和其他地方的研究人员发现，复杂的大语言机器学习模型在响应用户提示时使用一种简单的机制来检索存储的知识。研究人员可以利用这些简单的机制来了解模型对不同主题的了解，并可能纠正其存储的错误信息。</media:description><media:credit>图片：iStock</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/algorithms">算法</category><category domain="https://news.mit.edu/topic/human-computer-interaction">人机交互</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL） </category><category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">电气工程与计算机科学（eecs）</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category></item></channel></rss>