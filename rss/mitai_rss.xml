<rss version="2.0" xml:base="https://news.mit.edu" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>麻省理工学院新闻 - 计算机科学与人工智能实验室 (CSAIL)</title><link/> https://news.mit.edu/topic/mitcomputers-rss.xml <atom:link href="https://news.mit.edu/topic/mitcomputers-rss.xml" rel="self" type="application/rss+xml"></atom:link><description>麻省理工学院新闻提要：计算机科学与人工智能实验室 (CSAIL)</description><language> zh</language><lastbuilddate> 2024 年 6 月 12 日，星期三 00:00:00 -0400</lastbuilddate><item><title>研究人员使用大型语言模型来帮助机器人导航</title><link/>https://news.mit.edu/2024/researchers-use-large-language-models-to-help-robots-navigate-0612<description>该方法使用基于语言的输入而不是昂贵的视觉数据来指导机器人完成多步导航任务。</description><pubDate> Wed, 12 Jun 2024 00:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2024/researchers-use-large-language-models-to-help-robots-navigate-0612</guid><dc:creator>亚当·泽威 |麻省理工学院新闻</dc:creator><content:encoded>&lt;p>;有一天，您可能希望您的家庭机器人将一堆脏衣服搬到楼下，并将它们存放在地下室最左角的洗衣机中。机器人需要将您的指令与其视觉观察相结合，以确定完成此任务应采取的步骤。&lt;/p>;&lt;p>;对于人工智能代理来说，这说起来容易做起来难。当前的方法通常利用多个手工制作的机器学习模型来处理任务的不同部分，这需要大量的人力和专业知识来构建。这些方法使用视觉表示直接做出导航决策，需要大量视觉数据进行训练，而这些数据通常很难获得。&lt;/p>;&lt;p>;为了克服这些挑战，来自 MIT 和 MIT-IBM 的研究人员Watson AI 实验室设计了一种导航方法，可将视觉表示转换为语言片段，然后将其输入到一个大型语言模型中，以实现多步骤导航任务的所有部分。&lt;/p>;&lt;p>;而不是从图像中编码视觉特征将机器人的周围环境作为视觉表示，这是计算密集型的，他们的方法创建描述机器人观点的文本标题。大型语言模型使用字幕来预测机器人应采取的动作来完成用户基于语言的指令。&lt;/p>;&lt;p>;由于他们的方法利用纯粹基于语言的表示，因此他们可以使用大型语言模型来有效地执行任务。生成大量的合成训练数据。&lt;/p>;&lt;p>;虽然这种方法的性能并不优于使用视觉特征的技术，但它在缺乏足够的视觉数据进行训练的情况下表现良好。研究人员发现，将基于语言的输入与视觉信号相结合可以带来更好的导航性能。&lt;/p>;&lt;p>;“通过纯粹使用语言作为感知表示，我们的方法更加简单。由于所有输入都可以编码为语言，因此我们可以生成人类可以理解的轨迹。”电气工程和计算机科学 (EECS) 研究生、&lt;a href=&quot;https://arxiv&quot; 的主要作者 Bowen Pan 说道.org/pdf/2310.07889&quot; target=&quot;_blank&quot;>;关于这种方法的论文&lt;/a>;。&lt;/p>;&lt;p>;Pan 的合著者包括他的顾问、麻省理工学院苏世民学院战略行业参与主任 Aude Oliva计算机系博士，MIT-IBM Watson AI 实验室主任，计算机科学与人工智能实验室 (CSAIL) 高级研究科学家； Philip Isola，EECS 副教授，CSAIL 会员；资深作者 Yoon Kim，EECS 助理教授，CSAIL 成员；以及 MIT-IBM Watson AI 实验室和达特茅斯学院的其他人员。该研究将在计算语言学协会北美分会的会议上公布。&lt;/p>;&lt;p>;&lt;strong>;用语言解决视觉问题&lt;/strong>;&lt;/p>;&lt;p>;由于大语言潘说，模型是现有的最强大的机器学习模型，研究人员试图将它们合并到称为视觉和语言导航的复杂任务中。&lt;/p>;&lt;p>;但此类模型采用基于文本的输入，并且可以不处理来自机器人相机的视觉数据。因此，该团队需要找到一种使用语言的方法。&lt;/p>;&lt;p>;他们的技术利用简单的字幕模型来获取机器人视觉观察的文本描述。这些说明文字与基于语言的指令相结合，并输入到大型语言模型中，该模型决定机器人下一步应采取的导航步骤。&lt;/p>;&lt;p>;大型语言模型输出机器人应该看到的场景的说明文字完成该步骤。这用于更新轨迹历史记录，以便机器人可以跟踪其去过的位置。&lt;/p>;&lt;p>;模型重复这些过程以生成一条轨迹，引导机器人一次一步地到达目标。 &lt;/p>;&lt;p>;为了简化流程，研究人员设计了模板，以便观察信息以标准形式呈现给模型 - 作为机器人可以根据周围环境做出的一系列选择。&lt;/p>;&lt;p>;例如，标题可能会说“在你的左边 30 度处有一扇门，旁边有一盆植物，在你的后面是一间小办公室，里面有一张桌子和一台电脑”等等。该模型会选择机器人是否应该朝门或办公室。&lt;/p>;&lt;p>;“最大的挑战之一是弄清楚如何以适当的方式将此类信息编码为语言，以使代理了解任务是什么以及他们应该如何响应， ”潘说。&lt;/p>;&lt;p>;&lt;strong>;语言的优点&lt;/strong>;&lt;/p>;&lt;p>;当他们测试这种方法时，虽然它无法超越基于视觉的技术，但他们发现它提供了一些优点。&lt;/p>;&lt;p>;首先，由于合成文本所需的计算资源比复杂的图像数据少，因此他们的方法可用于快速生成合成训练数据。在一项测试中，他们根据 10 个真实世界的视觉轨迹生成了 10,000 个合成轨迹。&lt;/p>;&lt;p>;该技术还可以弥补在模拟环境中训练的智能体在现实世界中表现不佳的差距。 。这种差距经常发生，因为由于光照或颜色等因素，计算机生成的图像可能与现实世界的场景有很大不同。但潘说，描述合成图像和真实图像的语言很难区分。&lt;/p>;&lt;p>;此外，他们的模型使用的表示更容易被人类理解，因为它们是用自然语言编写的&lt;/p>;&lt;p>;“如果智能体未能达到其目标，我们可以更轻松地确定它失败的位置以及失败的原因。也许历史信息不够清晰，或者观察忽略了一些重要的细节。”潘说。&lt;/p>;&lt;p>;此外，他们的方法可以更容易地应用于不同的任务和环境，因为它只使用一种类型的输入。只要数据可以编码为语言，他们就可以使用相同的模型而不进行任何修改。&lt;/p>;&lt;p>;但一个缺点是他们的方法自然会丢失一些基于视觉的模型可以捕获的信息，例如作为深度信息。&lt;/p>;&lt;p>;然而，研究人员惊讶地发现，将基于语言的表示与基于视觉的方法相结合可以提高代理的导航能力。&lt;/p>;&lt;p>;“也许这意味着语言可以捕获一些比纯视觉特征无法捕获的更高级别的信息。”他说。&lt;/p>;&lt;p>;这是研究人员想要继续探索的一个领域。他们还希望开发一种面向导航的字幕生成器，以提高该方法的性能。此外，他们希望探索大型语言模型展示空间意识的能力，并了解这如何帮助基于语言的导航。&lt;/p>;&lt;p>;这项研究部分由 MIT-IBM Watson AI 资助实验室。&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202406/MIT_LangNAV-01.jpg?itok=vdy3n1UY" width="390"><media:description type="plain">一种新的导航方法使用基于语言的输入来指导机器人完成多步骤的导航任务，例如洗衣服。</media:description><media:credit>图片来源：iStock</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/computer-vision">计算机视觉</category><category domain="https://news.mit.edu/topic/robotics">机器人技术</category><category domain="https://news.mit.edu/topic/robots">机器人</category><category domain="https://news.mit.edu/topic/data">数据</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL） </category><category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">电气工程与计算机科学（eecs）</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category><category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">麻省理工学院-IBM沃森人工智能实验室</category></item><item><title>新算法仅通过观看视频即可发现语言</title><link/>https://news.mit.edu/2024/denseav-algorithm-discovers-language-just-watching-videos-0611<description> DenseAV 由麻省理工学院开发，仅通过观看人们说话的视频来学习解析和理解语言的含义，在多媒体搜索、语言学习和机器人技术方面具有潜在的应用。</description><pubDate> Tue, 11 Jun 2024 14:10:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2024/denseav-algorithm-discovers-language-just-watching-videos-0611</guid><dc:creator>雷切尔戈登|麻省理工学院CSAIL</dc:creator><content:encoded> &lt;p dir=&quot;ltr&quot;>;马克·汉密尔顿 (Mark Hamilton) 是麻省理工学院电气工程和计算机科学专业的博士生，也是麻省理工学院计算机科学和人工智能实验室 (CSAIL) 的附属机构，他希望利用机器来了解动物如何交流。为此，他首先着手创建一个可以“从头开始”学习人类语言的系统。&lt;/p>;&lt;p dir=&quot;ltr&quot;>;“有趣的是，灵感的关键时刻来自电影《三月》企鹅。&#39;有一个场景，一只企鹅在穿过冰面时摔倒，然后在爬起来时发出轻微的痛苦呻吟。当你观看它时，几乎很明显这个呻吟声代表了一个四个字母的单词。就在那时，我们想到，也许我们需要使用音频和视频来学习语言，”汉密尔顿说。 “有没有办法让算法整天看电视，并从中弄清楚我们在说什么？”&lt;/p>;&lt;p dir=&quot;ltr&quot;>;“我们的模型‘DenseAV’旨在学习通过根据所听到的内容预测所看到的内容来学习语言，反之亦然。例如，如果您听到有人说“以 350 度烘烤蛋糕”，那么您可能看到的是蛋糕或烤箱。为了在数百万个视频中成功完成这款音频视频匹配游戏，该模型必须了解人们在谈论什么。”Hamilton 说道。&lt;/p>;&lt;p dir=&quot;ltr&quot;>;一旦他们在这款匹配游戏上训练了 DenseAV，汉密尔顿和他的同事们研究了模型在听到声音时会寻找哪些像素。例如，当有人说“狗”时，算法立即开始在视频流中寻找狗。通过查看算法选择了哪些像素，人们可以发现算法认为一个词的含义。&lt;/p>;&lt;p dir=&quot;ltr&quot;>;有趣的是，当 DenseAV 听狗叫时，会发生类似的搜索过程：它会搜索对于视频流中的狗。 “这引起了我们的兴趣。我们想看看算法是否知道“狗”这个词和狗的叫声之间的区别，”汉密尔顿说。该团队通过为 DenseAV 提供“双面大脑”来探索这一问题。有趣的是，他们发现 DenseAV 大脑的一侧自然地专注于语言，例如“狗”这个词，而另一侧则专注于诸如吠叫之类的声音。这表明 DenseAV 不仅学习了单词的含义和声音的位置，还学会了区分这些类型的跨模式连接，所有这些都无需人工干预或任何书面语言知识。&lt;/p>;&lt;p dir= “ltr&quot;>;应用程序的一个分支是从每天发布到互联网上的大量视频中学习：“我们希望系统能够从大量视频内容中学习，例如教学视频，”汉密尔顿说。 “另一个令人兴奋的应用是理解新语言，例如海豚或鲸鱼的交流，它们没有书面的交流形式。我们希望 DenseAV 能够帮助我们理解这些从一开始就逃避人工翻译工作的语言。最后，我们希望这种方法可以用于发现其他信号对之间的模式，例如地球发出的地震声音及其地质情况。”&lt;/p>;&lt;p dir=&quot;ltr&quot;>;摆在面前的是艰巨的挑战团队简介：无需任何文本输入即可学习语言。他们的目标是从空白中重新发现语言的含义，避免使用预先训练的语言模型。这种方法的灵感来自于儿童如何通过观察和聆听环境来理解语言。&lt;/p>;&lt;p dir=&quot;ltr&quot;>;为了实现这一壮举，DenseAV 使用两个主要组件来分别处理音频和视觉数据。这种分离使得算法不可能通过让视觉侧查看音频来作弊，反之亦然。它迫使算法识别物体，并为音频和视觉信号创建详细且有意义的特征。 DenseAV 通过比较音频和视觉信号对来学习，以找出哪些信号匹配、哪些信号不匹配。这种方法称为对比学习，不需要标记示例，并且允许 DenseAV 找出语言本身的重要预测模式。&lt;/p>;&lt;p dir=&quot;ltr&quot;>;DenseAV 与以前的算法之间的一个主要区别是之前的作品集中于声音和图像之间相似性的单一概念。匹配了一个完整的音频片段，例如有人说“狗坐在草地上”到狗的完整图像。这不允许以前的方法发现细粒度的细节，比如“草”这个词和狗下面的草之间的联系。该团队的算法搜索并聚合音频剪辑和图像像素之间所有可能的匹配。这不仅提高了性能，而且使团队能够以以前的算法无法做到的方式精确定位声音。 “传统方法使用单一类别标记，但我们的方法会比较每个像素和每一秒的声音。这种细粒度的方法让 DenseAV 能够建立更详细的连接，以实现更好的本地化。”Hamilton 说道。&lt;/p>;&lt;p dir=&quot;ltr&quot;>;研究人员在包含 200 万个 YouTube 视频的 AudioSet 上训练 DenseAV。他们还创建了新的数据集来测试模型链接声音和图像的能力。在这些测试中，DenseAV 在从名称和声音识别物体等任务中表现优于其他顶级模型，证明了其有效性。 “以前的数据集仅支持粗略评估，因此我们使用语义分割数据集创建了一个数据集。这有助于进行像素完美的注释，以精确评估我们的模型的性能。我们可以用特定的声音或图像提示算法，并获得详细的定位。”Hamilton 说道。&lt;/p>;&lt;p dir=&quot;ltr&quot;>;由于涉及大量数据，该项目大约花了一年时间才完成。该团队表示，过渡到大型变压器架构带来了挑战，因为这些模型很容易忽略细粒度的细节。鼓励模型关注这些细节是一个重大障碍。&lt;/p>;&lt;p dir=&quot;ltr&quot;>;展望未来，该团队的目标是创建可以从大量视频或纯音频数据中学习的系统。这对于有很多两种模式但不同时存在的新领域至关重要。他们还旨在使用更大的骨干网来扩展此功能，并可能整合语言模型中的知识以提高性能。&lt;/p>;&lt;p dir=&quot;ltr&quot;>;“识别和分割图像中的视觉对象以及环境声音和口语单词在音频录音中，每个问题本身都是困难的。历史上，研究人员依赖昂贵的、人工提供的注释来训练机器学习模型来完成这些任务，”德克萨斯大学奥斯汀分校计算机科学助理教授 David Harwath 说，他没有参与这项工作。 “DenseAV 在开发方法方面取得了重大进展，这些方法可以通过简单地通过视觉和声音观察世界来学习同时解决这些任务 - 基于我们看到并与之互动的事物经常发出声音的洞察力，并且我们还使用口语进行交谈关于他们。该模型也不对正在使用的特定语言做出任何假设，因此原则上可以从任何语言的数据中学习。看到 DenseAV 可以通过将其扩展到数千或数百万小时的多种语言视频数据来学习什么，这将是令人兴奋的。”&lt;/p>;&lt;p dir=&quot;ltr&quot;>;&lt;a href= “https://arxiv.org/abs/2406.05629&quot;>;描述这项工作的论文&lt;/a>;是牛津大学计算机视觉工程教授 Andrew Zisserman； John R. Hershey，谷歌人工智能感知研究员； William T. Freeman，麻省理工学院电气工程和计算机科学教授兼 CSAIL 首席研究员。他们的研究部分得到了美国国家科学基金会、英国皇家学会研究教授职位和 EPSRC 视觉人工智能项目资助的支持。这项工作将于本月在 IEEE/CVF 计算机视觉和模式识别会议上展示。&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202406/DenseAV.jpg?itok=VoJbcJhL" width="390"><media:description type="plain"> DenseAV 算法仅通过关联音频和视频信号来学习语言的含义</media:description><media:credit>图片：马克·汉密尔顿</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/imaging">影像学</category><category domain="https://news.mit.edu/topic/computer-vision">计算机视觉</category><category domain="https://news.mit.edu/topic/algorithms">算法</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL）</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category></item></channel></rss>