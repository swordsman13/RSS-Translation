<rss version="2.0" xml:base="https://news.mit.edu" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>麻省理工学院新闻 - 计算机科学与人工智能实验室 (CSAIL)</title><link/> https://news.mit.edu/topic/mitcomputers-rss.xml <atom:link href="https://news.mit.edu/topic/mitcomputers-rss.xml" rel="self" type="application/rss+xml"></atom:link><description>麻省理工学院新闻提要：计算机科学与人工智能实验室 (CSAIL)</description><language> zh</language><lastbuilddate> 2023 年 9 月 13 日，星期三 00:00:00 -0400</lastbuilddate><item><title>帮助计算机视觉和语言模型理解他们所看到的内容</title><link/>https://news.mit.edu/2023/helping-computer-vision-language-models-see-0913<description>研究人员使用合成数据来提高模型掌握概念信息的能力，这可以增强自动字幕和问答系统。</description><pubDate> Wed, 13 Sep 2023 00:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2023/helping-computer-vision-language-models-see-0913</guid><dc:creator>亚当·泽威 |麻省理工学院新闻</dc:creator><content:encoded>&lt;p>;强大的机器学习算法（称为视觉和语言模型）可以学习将文本与图像进行匹配，当被要求生成字幕或总结视频时，它显示出了显着的结果。&lt;/p>; &lt;p>;虽然这些模型擅长识别对象，他们经常难以理解概念，例如对象属性或场景中项目的排列。例如，视觉和语言模型可能会识别图像中的杯子和桌子，但无法识别杯子位于桌子上。&lt;/p>; &lt;p>;来自 MIT、MIT-IBM Watson AI 实验室的研究人员，和其他地方已经展示了一种新技术，利用计算机生成的数据来帮助视觉和语言模型克服这一缺点。&lt;/p>; &lt;p>;研究人员创建了一个图像合成数据集，描绘了各种场景、物体排列和人类的行为，加上详细的文字描述。他们使用这个带注释的数据集来“修复”视觉和语言模型，以便他们可以更有效地学习概念。他们的技术确保这些模型在看到真实图像时仍然可以做出准确的预测。&lt;/p>; &lt;p>;当他们测试模型的概念理解时，研究人员发现他们的技术将准确性提高了 10%。这可以改进自动为视频添加字幕的系统，或增强为图像问题提供自然语言答案的模型，并应用于电子商务或医疗保健等领域。&lt;/p>; &lt;p>;“通过这项工作，我们将超越名词从某种意义上说，我们正在超越对象的名称，更多地涉及对象及其周围一切的语义概念。我们的想法是，当机器学习模型看到许多不同排列的对象时，它将更好地了解排列在场景中的重要性。”电气工程和计算机科学系的研究生 Khaled Shehada 说道。关于该技术的&lt;a href=&quot;http://olivalab.mit.edu/Papers/going_beyond_nouns.pdf&quot; target=&quot;_blank&quot;>;论文&lt;/a>;的合著者。&lt;/p>; &lt;p>;Shehada 写道该论文的主要作者是莱斯大学计算机科学研究生 Paola Cascante-Bonilla； Aude Oliva，麻省理工学院施瓦茨曼计算学院战略行业参与主任、麻省理工学院-IBM沃森人工智能实验室主任、计算机科学与人工智能实验室（CSAIL）高级研究科学家；资深作者 Leonid Karlinsky，MIT-IBM Watson AI 实验室的研究人员；以及麻省理工学院、麻省理工学院-IBM Watson AI 实验室、佐治亚理工学院、莱斯大学、巴黎桥大学、魏茨曼科学研究所和 IBM 研究院的其他人员。该论文将在国际计算机视觉会议上发表。&lt;/p>; &lt;p>;&lt;strong>;关注对象&lt;/strong>;&lt;/p>; &lt;p>;视觉和语言模型通常学习识别场景中的对象，并且最终可能会忽略对象属性，例如颜色和大小，或位置关系，例如哪个对象位于另一个对象之上。&lt;/p>; &lt;p>;这是由于这些模型经常训练的方法造成的，称为对比学习。这种训练方法涉及强制模型预测图像和文本之间的对应关系。在比较自然图像时，每个场景中的物体往往会产生最显着的差异。 （也许一张图片显示了田野中的一匹马，而第二张图片显示了水面上的一艘帆船。）&lt;/p>; &lt;p>;“每张图像都可以由图像中的对象唯一地定义。所以，当你进行对比学习时，只关注名词和宾语就可以解决问题。为什么模型会做出不同的事情？”卡林斯基说。&lt;/p>; &lt;p>;研究人员试图通过使用合成数据来微调视觉和语言模型来缓解这个问题。微调过程涉及调整已经过训练的模型，以提高其在特定任务上的性能。&lt;/p>; &lt;p>;他们使用计算机自动创建具有不同 3D 环境和物体（例如家具和物体）的合成视频。他们使用这些视频的各个帧生成了近 800,000 个逼真的图像，然后将每个图像与详细的标题配对。研究人员开发了一种方法来注释图像的各个方面，以在密集的字幕中清晰一致地捕获对象属性、位置关系和人与对象的交互。&lt;/p>; &lt;p>;由于研究人员创建了图像，因此他们可以控制物体的外观和位置，以及人类化身的性别、服装、姿势和动作。&lt;/p>; &lt;p>;“合成数据允许很大的多样性。使用真实图像，一个房间里可能不会有很多大象，但通过合成数据，如果你愿意，你实际上可以在一个房间里有一头粉红色的大象和一个人，”Cascante-Bonilla 说。&lt;/p>; &lt;合成数据还有其他优点。它们的生成成本比真实数据更便宜，但图像却非常逼真。他们还保护隐私，因为图像中没有显示真人。而且，由于数据是由计算机自动生成的，因此可以快速大量生成。&lt;/p>; &lt;p>;通过使用不同的相机视点，或稍微改变对象的位置或属性，研究人员创建了一个数据集，其中包含比在自然数据集中发现的场景要广泛得多。&lt;/p>; &lt;p>;&lt;strong>;微调，但不要忘记&lt;/strong>;&lt;/p>; &lt;p>;但是，当一个微调时 -使用合成数据调整模型，存在模型可能“忘记”最初使用真实数据训练时学到的内容的风险。&lt;/p>; &lt;p>;研究人员采用了一些技术来防止此问题，例如调整合成数据使颜色、光照和阴影更接近自然图像中的颜色、光照和阴影。他们还在微调后对模型的内部工作原理进行了调整，以进一步减少遗忘。&lt;/p>; &lt;p>;他们的合成数据集和微调策略将流行的视觉和语言模型准确识别概念的能力提高了高达至 10%。同时，模型并没有忘记他们已经学到的东西。&lt;/p>; &lt;p>;既然他们已经展示了如何使用合成数据来解决这个问题，研究人员希望找出提高视觉质量的方法这些数据的多样性，以及使合成场景看起来逼真的基础物理原理。此外，他们计划测试可扩展性的极限，并调查模型改进是否在更大、更多样化的合成数据集上开始趋于稳定。&lt;/p>; &lt;p>;这项研究部分由美国国防高级研究项目资助该机构、美国国家科学基金会和 MIT-IBM Watson AI 实验室。&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202309/MIT-Beyond-Nouns-01-press.jpg?itok=RiJ1w3pt" width="390"><media:description type="plain">麻省理工学院的研究人员创建了一个新的带注释的合成图像数据集，描绘了各种场景，可用于帮助机器学习模型理解场景中的概念。</media:description><media:credit>由研究人员提供</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/data">数据</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/computer-vision">计算机视觉</category><category domain="https://news.mit.edu/topic/privacy">隐私</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL） </category><category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">电气工程与计算机科学（eecs）</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category><category domain="https://news.mit.edu/topic/nsf">美国国家科学基金会 (NSF)</category><category domain="https://news.mit.edu/topic/darpa">国防高级研究计划局 (DARPA)</category></item><item><title>系统结合光和电子来解锁更快、更环保的计算</title><link/>https://news.mit.edu/2023/system-combines-light-electrons-unlock-faster-greener-computing-0911<description> “闪电”系统使用新颖的抽象将光子连接到计算机的电子组件，创建了第一个光子计算原型来服务实时机器学习推理请求。</description><pubDate> Mon, 11 Sep 2023 09:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2023/system-combines-light-electrons-unlock-faster-greener-computing-0911</guid><dc:creator>亚历克斯·希普斯 |麻省理工学院计算机科学与人工智能实验室</dc:creator><content:encoded>&lt;p>;计算正处于拐点。摩尔定律预测电子芯片上的晶体管数量每年将增加一倍，但由于在经济实惠的微芯片上安装更多晶体管的物理限制，摩尔定律的速度正在放缓。随着对能够支持日益复杂的人工智能模型的高性能计算机的需求增长，计算机能力的增长正在放缓。这种不便促使工程师探索扩展机器计算能力的新方法，但解决方案仍不清楚。&lt;/p>; &lt;p>;光子计算是解决机器学习模型不断增长的计算需求的一种潜在补救措施。这些系统不使用晶体管和电线，而是利用光子（微观光粒子）在模拟域中执行计算操作。激光产生这些小能量束，它们以光速移动，就像科幻电影中以超速飞行的宇宙飞船一样。当光子计算核心添加到网络接口卡（NIC 及其增强版 SmartNIC）等可编程加速器中时，可以插入所得硬件来为标准计算机提供涡轮增压。&lt;/p>; &lt;p>;麻省理工学院的研究人员现已利用光子学通过展示其机器学习能力来加速现代计算的潜力。他们的光子-电子可重构 SmartNIC 被称为“Lightning”，可帮助深度神经网络（模仿大脑如何处理信息的机器学习模型）完成推理任务，例如 ChatGPT 等聊天机器人中的图像识别和语言生成。该原型的新颖设计实现了令人印象深刻的速度，创建了第一个光子计算系统来服务实时机器学习推理请求。&lt;/p>; &lt;p>;尽管它具有潜力，但实现光子计算设备的一个主要挑战是它们被动，这意味着它们缺乏控制数据流的内存或指令，与电子产品不同。以前的光子计算系统面临这个瓶颈，但闪电消除了这个障碍，确保电子和光子组件之间的数据移动顺利运行。&lt;/p>; &lt;p>;“光子计算在加速矩阵乘法等庞大的线性计算任务方面显示出显着的优势，同时它需要电子设备来处理其余的事情：内存访问、非线性计算和条件逻辑。这会产生大量数据在光子学和电子学之间交换，以完成现实世界的计算任务，例如机器学习推理请求。”麻省理工学院计算机科学系副教授 Manya Ghobadi 团队的博士后钟志珍说道和人工智能实验室（CSAIL）。 “控制光子学和电子学之间的数据流是过去最先进的光子计算工作的致命弱点。即使您拥有超快的光子计算机，您也需要足够的数据来为其提供动力而不会出现停顿。否则，你的超级计算机就会闲置而不会进行任何合理的计算。”&lt;/p>; &lt;p>;Ghobadi，麻省理工学院电气工程与计算机科学 (EECS) 系副教授、CSAIL 成员和她的团队同事们是第一个发现并解决这个问题的人。为了完成这一壮举，他们将光子学的速度和电子计算机的数据流控制能力结合起来。&lt;/p>; &lt;p>;在闪电网络之前，光子和电子计算方案独立运行，使用不同的语言。该团队的混合系统使用可重新配置的计数动作抽象来跟踪数据路径上所需的计算操作，该抽象将光子学与计算机的电子组件连接起来。这种编程抽象充当两者之间的统一语言，控制对经过的数据流的访问。电子携带的信息被转化为光子形式的光，光子以光速工作以协助完成推理任务。然后，光子被转换回电子，将信息传递给计算机。&lt;/p>; &lt;p>;通过将光子学与电子学无缝连接，新颖的计数动作抽象使闪电的快速实时计算频率成为可能。之前的尝试使用了走走停停的方法，这意味着数据将受到慢得多的控制软件的阻碍，该软件做出有关其运动的所有决定。 “在没有计数动作编程抽象的情况下构建光子计算系统就像在不知道如何驾驶的情况下试图驾驶兰博基尼一样，”该论文的高级作者戈巴迪说。 “你会怎么办？你可能一只手拿着一本驾驶手册，然后踩下离合器，然后检查手册，然后松开刹车，然后检查手册，等等。这是一个走走停停的操作，因为对于每一个决定，你都必须咨询一些更高级别的实体来告诉你该怎么做。但这不是我们开车的方式；我们是这样开车的。我们学习如何驾驶，然后使用肌肉记忆，而无需检查方向盘后面的手册或驾驶规则。我们的计数动作编程抽象充当闪电中的肌肉记忆。它在运行时无缝地驱动系统中的电子和光子。”&lt;/p>; &lt;p>;&lt;strong>;环保解决方案&lt;/strong>;&lt;/p>; &lt;p>;机器学习服务完成基于推理的任务，像 ChatGPT 和 BERT 一样，目前需要大量的计算资源。它们不仅价格昂贵，一些估计显示 ChatGPT 需要 &lt;a href=&quot;https://indianexpress.com/article/technology/tech-news-technology/chatgpt-interesting-things-to-know-8334991/&quot;>;&lt;每月运行费用为 300 万美元&lt;/u>;&lt;/a>;，但它们对环境也有害，可能排放超过&lt;a href=&quot;https://earth.org/environmental-impact-chatgpt/#: ~:text=根据%20to%20估计%2C%20ChatGPT%20排放量，是%204%20吨%20每%20年。&quot;>;双倍&lt;/a>;普通人的二氧化碳。闪电使用比电子在电线中移动更快的光子，同时产生&lt;a href=&quot;https://www.bbvaopenmind.com/en/technology/future/optical-computing-solving-problems-at-the-speed-of -light/&quot;>;&lt;u>;更少的热量&lt;/u>;&lt;/a>;，使其能够以更快的频率进行计算，同时更加节能。&lt;/p>; &lt;p>;为了衡量这一点，Ghobadi 小组比较了他们的产品通过合成闪电芯片，将设备连接到标准图形处理单元、数据处理单元、SmartNIC 和其他加速器。该团队观察到，闪电网络在完成推理请求时更加节能。 “我们的综合和模拟研究表明，与最先进的加速器相比，闪电网络将机器学习推理功耗降低了几个数量级，”Ghobadi 实验室的研究生、该论文的合著者 Mingran Yang 说道。作为一种更具成本效益、更快速的选择，闪电网络为数据中心提供了潜在的升级，以减少其机器学习模型的碳足迹，同时加快用户的推理响应时间。&lt;/p>; &lt;p>;该论文的其他作者是麻省理工学院CSAIL 博士后 Homa Esfahanizadeh 和本科生 Liam Kronman，以及 MIT EECS 副教授 Dirk Englund 和该系的三名应届毕业生：Jay Lang &#39;22、MEng &#39;23；克里斯蒂安·威廉姆斯 &#39;22，工程硕士 &#39;23；和Alexander Sludds &#39;18，工程硕士&#39;19，博士&#39;23。他们的研究部分得到了 DARPA FastNICs 计划、ARPA-E ENLITENED 计划、DAF-MIT 人工智能加速器、美国陆军研究办公室通过士兵纳米技术研究所、国家科学基金会 (NSF) 拨款、 NSF 量子网络中心和斯隆奖学金。&lt;/p>; &lt;p>;该小组将于本月在计算机协会数据通信特别兴趣小组 (SIGCOMM) 上展示他们的研究结果。&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202309/MIT-Lightning-cov.png?itok=VUCRP5rv" width="390"><media:description type="plain">麻省理工学院的研究人员推出了 Lightning，这是一种可重新配置的光子电子智能网卡，能够以 100 Gbps 的速度满足实时深度神经网络推理请求。</media:description><media:credit> Alex Shipps/麻省理工学院 CSAIL，来自 Midjourney</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/photonics">光子学</category><category domain="https://news.mit.edu/topic/electronics">电子产品</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/invention">发明</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL） </category><category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">电气工程与计算机科学（eecs）</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category><category domain="https://news.mit.edu/topic/darpa">国防高级研究计划局 (DARPA)</category><category domain="https://news.mit.edu/topic/nsf">美国国家科学基金会 (NSF)</category></item></channel></rss>