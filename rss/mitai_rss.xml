<rss version="2.0" xml:base="https://news.mit.edu" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>麻省理工学院新闻 - 计算机科学与人工智能实验室 (CSAIL)</title><link/> https://news.mit.edu/topic/mitcomputers-rss.xml <atom:link href="https://news.mit.edu/topic/mitcomputers-rss.xml" rel="self" type="application/rss+xml"></atom:link><description>麻省理工学院新闻提要：计算机科学与人工智能实验室 (CSAIL)</description><language> zh</language><lastbuilddate> 2024 年 3 月 27 日星期三 04:00:00 +0000</lastbuilddate><item><title>新软件使盲人和弱视用户能够创建交互式、可访问的图表</title><link/>https://news.mit.edu/2024/umwelt-enables-interactive-accessible-charts-creation-blind-low-vision-users-0327<description>屏幕阅读器用户可以上传数据集并创建结合了可视化、文本描述和可听化功能的自定义数据表示形式。</description><pubDate> Wed, 27 Mar 2024 00:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2024/umwelt-enables-interactive-accessible-charts-creation-blind-low-vision-users-0327</guid><dc:creator>亚当·泽威 |麻省理工学院新闻</dc:creator><content:encoded>&lt;p>;越来越多的工具使用户能够制作在线数据表示（例如图表），供盲人或弱视人士使用。然而，大多数工具都需要现有的可视化图表，然后可以将其转换为可访问的格式。&lt;/p>; &lt;p>;这会造成障碍，阻止盲人和低视力用户构建自己的自定义数据表示形式，并可能限制他们的探索和分析重要信息的能力。&lt;/p>; &lt;p>;来自麻省理工学院和伦敦大学学院 (UCL) 的研究团队希望改变人们对可访问数据表示的看法。&lt;/p>; &lt;p>;他们创建了一个名为&lt;a href=&quot;https://arxiv.org/pdf/2403.00106.pdf&quot; target=&quot;_blank&quot;>;Umwelt&lt;/a>;（德语中“环境”的意思）的软件系统，可以帮助盲人和低视力人士用户无需初始可视化图表即可构建自定义的多模式数据表示。&lt;/p>; &lt;p>;Umwelt 是一个专为屏幕阅读器用户设计的创作环境，它包含一个编辑器，允许用户上传数据集并创建自定义表示，例如散点图，它可以包括三种模式：可视化、文本描述和可听化。可听化涉及将数据转换为非语音音频。&lt;/p>; &lt;p>;该系统可以表示各种数据类型，包括一个查看器，使盲人或弱视用户能够交互式地探索数据表示，并在每种数据表示之间无缝切换。研究人员对五位屏幕阅读器专家用户进行了一项研究，他们发现 Umwelt 非常有用且易于学习。除了提供一个界面，使他们能够创建数据表示（他们说这是非常缺乏的）之外，用户还表示 Umwelt 可以促进依赖不同感官的人们之间的交流。&lt;/p>; &lt;p>;“我们必须记住，盲目的低视力人群并不孤立。他们存在于这些想要与其他人谈论数据的环境中，”电气工程和计算机科学 (EECS) 研究生兼 &lt;a href=&quot;https://arxiv.org/ 的主要作者pdf/2403.00106.pdf&quot; target=&quot;_blank&quot;>;介绍环境世界的论文&lt;/a>;。 “我希望 Umwelt 能够帮助改变研究人员对可访问数据分析的思考方式。让盲人和低视力人士充分参与数据分析，需要将可视化视为这个更大的多感官难题的一小部分。”&lt;/p>; &lt;p>;与 Zong 一起撰写论文的还有 EECS 研究生 Isabella Pedraza Pineros 和陈梦珠“凯蒂”； Daniel Hajas，伦敦大学学院研究员，在全球残疾人创新中心工作；资深作者 Arvind Satyanarayan，麻省理工学院计算机科学副教授，领导计算机科学和人工智能实验室可视化小组。该论文将在 ACM 计算中的人为因素会议上发表。&lt;/p>; &lt;p>;&lt;strong>;去中心可视化&lt;/strong>;&lt;/p>; &lt;p>;研究人员之前开发了交互式界面，可提供更丰富的交互界面屏幕阅读器用户&lt;a href=&quot;https://news.mit.edu/2022/data-visualization-accessible-blind-0602&quot; target=&quot;_blank&quot;>;探索可访问的数据表示&lt;/a>;时的体验。通过这项工作，他们意识到创建此类表示的大多数工具都涉及转换现有的视觉图表。&lt;/p>; &lt;p>;为了在数据分析中分散视觉表示，Zong 和 16 岁时失明的 Hajas 开始共同设计一年多前的环境世界。&lt;/p>; &lt;p>;一开始，他们意识到需要重新思考如何使用视觉、听觉和文本形式来表示相同的数据。&lt;/p>; &lt;p>;“我们有使三种模式背后有一个共同点。通过创建这种新的表示语言，并使输出和输入易于访问，整体大于各部分之和。”Hajas 说。&lt;/p>; &lt;p>;为了构建 Umwelt，他们首先考虑了环境的独特之处。人们使用每种感觉的方式。&lt;/p>; &lt;p>;例如，有视力的用户可以看到散点图的整体模式，同时移动眼睛以关注不同的数据点。但对于聆听可听化的人来说，体验是线性的，因为数据被转换成必须一次播放一个的音调。&lt;/p>; &lt;p>;“如果您只想直接将视觉特征转化为非视觉特征，那么你就会错过每种模式的独特优点和缺点。”Zong 补充道。&lt;/p>; &lt;p>;他们设计 Umwelt 是为了提供灵活性，使用户能够在更适合特定任务的模式之间轻松切换。时间。&lt;/p>; &lt;p>;要使用编辑器，需要将数据集上传到 Umwelt，Umwelt 采用启发式方法自动创建每种模式的默认表示。&lt;/p>; &lt;p>;如果数据集包含公司的股票价格，Umwelt可能会生成多系列折线图、按股票代码和日期对数据进行分组的文本结构，以及使用音调长度表示每个日期价格（按股票代码排列）的可听化。&lt;/p>; &lt;p>;默认启发式为旨在帮助用户入门。&lt;/p>; &lt;p>;“在任何类型的创意工具中，你都会遇到一片空白的效果，很难知道如何开始。这是在多模式工具中复合的，因为您必须以三种不同的表示形式指定事物。”Zong 说。&lt;/p>; &lt;p>;编辑器链接跨模式的交互，因此，如果用户更改文本描述，该信息会在相应的可听化。有人可以利用编辑器构建多模式表示，切换到查看器进行初步探索，然后返回编辑器进行调整。&lt;/p>; &lt;p>;&lt;strong>;帮助用户交流数据&lt;/strong>;&lt;/为了测试 Umwelt，他们创建了一组不同的多模式表示形式，从散点图到多视图图表，以确保系统能够有效地表示不同的数据类型。然后，他们将该工具交给了五位屏幕阅读器专家用户。&lt;/p>; &lt;p>;研究参与者大多发现 Umwelt 对于创建、探索和讨论数据表示很有用。一位用户表示 Umwelt 就像一个“推动者”，减少了他们分析数据所需的时间。用户一致认为，Umwelt 可以帮助他们更轻松地与视力正常的同事交流数据。&lt;/p>; &lt;p>;“Umwelt 的突出之处在于其核心理念，即不强调视觉，而是支持平衡的多感官数据体验。通常，非视觉数据表示被归为次要考虑因素，仅仅是视觉对应物的附加组件。然而，可视化只是数据表示的一方面。我很欣赏他们为改变这种看法并采用更具包容性的数据科学方法所做的努力，”伊利诺伊大学厄巴纳-香槟分校信息科学学院助理教授 JooYoung Seo 说道，他没有参与这项工作。 &lt;/p>; &lt;p>;展望未来，研究人员计划创建一个开源版本的 Umwelt，供其他人使用。他们还希望将触觉传感作为一种附加方式集成到软件系统中，从而能够使用可刷新触觉图形显示器等工具。&lt;/p>; &lt;p>;“除了对最终用户的影响之外，我希望 Umwelt 能够成为一个询问人们如何使用和感知多模态表示的科学问题的平台，以及我们如何在最初的步骤之外改进设计。”Zong 说。&lt;/p>; &lt;p>;这项工作部分得到了国家实验室的支持科学基金会和麻省理工学院晨兴设计学院奖学金。&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/MIT-MultiModal-01-press.jpg?itok=95DNajE-" width="390"><media:description type="plain"> Umwelt 是一个新系统，使盲人和弱视用户能够以三种方式创作可访问的交互式图表来表示数据：可视化、文本描述和可听化。</media:description><media:credit>图片：麻省理工学院新闻；股票</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/assistive-technology">辅助技术</category><category domain="https://news.mit.edu/topic/design">设计</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/software">软件</category><category domain="https://news.mit.edu/topic/data">数据</category><category domain="https://news.mit.edu/topic/human-computer-interaction">人机交互</category><category domain="https://news.mit.edu/topic/technology-society">技术与社会</category><category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">电气工程与计算机科学（eecs） </category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL）</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category><category domain="https://news.mit.edu/topic/mit-morningside-academy-design">麻省理工学院晨兴设计学院</category><category domain="https://news.mit.edu/topic/nsf">美国国家科学基金会 (NSF)</category></item><item><title>工程家用机器人要有一点常识</title><link/>https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325<description>在大型语言模型的帮助下，麻省理工学院的工程师使机器人能够在失误后自我纠正并继续做家务。</description><pubDate> Mon, 25 Mar 2024 00:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325</guid><dc:creator>朱珍妮|麻省理工学院新闻</dc:creator><content:encoded>&lt;p>;从擦拭溢出物到提供食物，机器人正在被教导执行日益复杂的家务劳动。许多这样的家庭机器人学员都是通过模仿来学习的。它们被编程为模仿人类物理引导它们完成的动作。&lt;/p>; &lt;p>;事实证明，机器人是出色的模仿者。但是，除非工程师也对它们进行编程，以适应每一种可能的碰撞和推动，否则机器人不一定知道如何处理这些情况，除非从头开始执行任务。&lt;/p>; &lt;p>;现在，麻省理工学院的工程师们的目标是提供当机器人面临偏离训练轨迹的情况时，它们会掌握一些常识。他们开发了一种方法，将机器人运动数据与大型语言模型（LLM）的“常识知识”联系起来。&lt;/p>; &lt;p>;他们的方法使机器人能够逻辑地将许多给定的家庭任务解析为子任务，并对子任务中的中断进行物理调整，以便机器人可以继续前进，而无需返回并从头开始任务，并且工程师无需针对整个过程中的每个可能的故障明确地进行编程修复。 &amp;nbsp;&amp;nbsp;&lt;/p>; &lt;img alt=&quot;一只机械手试图舀起红色弹珠并将它们放入另一个碗中，而研究人员的手经常扰乱它。机器人最终成功了。&quot; data-align=&quot;center&quot; data-caption=&quot;图片由研究人员提供。&quot; data-entity-type=&quot;文件&quot; data-entity-uuid=&quot;49962673-31d2-4023-893e-cb462fed9a91&quot; src=&quot;/sites/default/files/images/inline/ComonsenseBots-ani_1.gif&quot; />; &lt;p >;“模仿学习是实现家用机器人的主流方法。但如果机器人盲目地模仿人类的运动轨迹，微小的错误就会累积起来，最终使其余的执行脱轨。”麻省理工学院电气工程与计算机科学系 (EECS) 的研究生 Yanwei Wang 说道。 “通过我们的方法，机器人可以自我纠正执行错误并提高整体任务的成功率。”&lt;/p>; &lt;p>;Wang 和他的同事在 &lt;a href=&quot;https://openreview.net/ 中详细介绍了他们的新方法。 forum?id=qoHeuRAcSl&quot; target=&quot;_blank&quot;>;研究&lt;/a>; 他们将在 5 月份的国际学习表征会议 (ICLR) 上进行展示。该研究的共同作者包括 EECS 研究生 Tsun-Hsuan Wang 和 Jiayuan Mao、麻省理工学院航空航天系 (AeroAstro) 博士后 Michael Hagenow 以及麻省理工学院航空航天系 HN Slater 教授 Julie Shah。&lt;/ p>; &lt;p>;&lt;strong>;语言任务&lt;/strong>;&lt;/p>; &lt;p>;研究人员通过一项简单的工作来说明他们的新方法：从一个碗中舀出弹珠并将其倒入另一个碗中。为了完成这项任务，工程师通常会移动机器人进行舀取和倾倒的动作——所有这些都在一个流体轨迹中进行。他们可能会多次这样做，以便让机器人进行多次人类演示来模仿。&lt;/p>; &lt;p>;“但人类演示是一条长而连续的轨迹，”王说。&lt;/p>; &lt;p>;团队意识到，虽然人类可能一次性演示一项任务，但该任务取决于一系列子任务或轨迹。例如，机器人必须先把手伸进碗里，然后才能舀起，并且必须先舀起弹珠，然后才能移动到空碗，等等。如果机器人在任何这些子任务中被推动或轻推而犯错误，它唯一的办法就是停止并从头开始，除非工程师明确标记每个子任务和程序或收集新的演示，以便机器人从错误中恢复。王说：“这种程度的规划非常乏味。”&lt;/p>; &lt;p>;相反，他和他的同事发现了一些这项工作的一部分可以由法学硕士自动完成。这些深度学习模型处理巨大的文本库，用于在单词、句子和段落之间建立联系。通过这些联系，法学硕士可以根据其对可能跟在最后一个单词后面的单词类型的了解来生成新句子。&lt;/p>; &lt;p>;就他们而言，研究人员发现，除了句子和段落，可以提示法学硕士生成给定任务中涉及的子任务的逻辑列表。例如，如果要求列出将弹珠从一个碗舀到另一个碗中所涉及的动作，法学硕士可能会产生一系列动词，例如“到达”、“舀”、“运输”和“倒”。&lt;/p>; &lt;p>;“法学硕士有办法用自然语言告诉您如何完成任务的每一步。人类的持续演示就是这些步骤在物理空间中的体现，”王说。 “我们希望将两者连接起来，以便机器人能够自动知道它处于任务的哪个阶段，并且能够自行重新计划和恢复。”&lt;/p>; &lt;p>;&lt;strong>;绘制弹珠图&lt;/对于他们的新方法，该团队开发了一种算法，可以自动将特定子任务的法学硕士自然语言标签与机器人在物理空间中的位置或对机器人状态进行编码的图像连接起来。将机器人的物理坐标或机器人状态的图像映射到自然语言标签称为“接地”。该团队的新算法旨在学习一个基础“分类器”，这意味着它能够学习根据机器人的物理坐标或图像视图自动识别机器人所处的语义子任务，例如“到达”与“舀取”。&lt; /p>; &lt;p>;“接地分类器促进了机器人在物理空间中所做的事情与法学硕士对子任务的了解以及每个子任务中必须注意的约束之间的对话，”Wang 解释道。&lt; /p>; &lt;p>;该团队在实验中展示了该方法，并用机械臂进行了铲大理石任务的训练。实验人员通过物理引导机器人完成以下任务来训练机器人：首先将手伸入碗中，舀起弹珠，将它们运送到空碗上，然后将其倒入。经过几次演示后，团队随后使用经过预训练的法学硕士并询问模型列出将弹珠从一个碗舀到另一个碗所涉及的步骤。然后，研究人员使用他们的新算法将法学硕士定义的子任务与机器人的运动轨迹数据连接起来。该算法自动学习将机器人在轨迹中的物理坐标和相应的图像视图映射到给定的子任务。&lt;/p>; &lt;p>;然后，团队让机器人利用新学习的接地自行执行舀取任务分类器。当机器人完成任务的步骤时，实验者将机器人推离其路径，并在不同的点将勺子上的弹珠碰掉。机器人不会停下来重新从头开始，或者在勺子上没有弹珠的情况下盲目地继续，而是能够自我纠正，并在继续下一个子任务之前完成每个子任务。 （例如，它会确保它在将弹珠运送到空碗之前成功舀起弹珠。）&lt;/p>; &lt;p>;“通过我们的方法，当机器人犯错误时，我们不需要要求人类计划或提供额外的演示如何从失败中恢复，”王说。 “这非常令人兴奋，因为现在人们在利用远程操作系统收集的数据来训练家用机器人方面付出了巨大的努力。我们的算法现在可以将训练数据转换为强大的机器人行为，尽管存在外部扰动，但仍可以执行复杂的任务。”&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/CommonSense-01-press.jpg?itok=VbWDKM8h" width="390"><media:description type="plain">在这张拼贴图像中，一只机械手试图舀起红色弹珠并将其放入另一个碗中，而研究人员的手经常破坏它。机器人最终成功了。</media:description><media:credit>图片：Jose-Luis Olivares，麻省理工学院。剧照由研究人员提供</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/aeronautics">航空航天工程</category><category domain="https://news.mit.edu/topic/algorithms">算法</category><category domain="https://news.mit.edu/topic/assistive-technology">辅助技术</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/computer-modeling">计算机建模</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL）</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/human-computer-interaction">人机交互</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/robotics">机器人技术</category><category domain="https://news.mit.edu/topic/robots">机器人</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category></item></channel></rss>