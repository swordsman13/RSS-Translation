<rss version="2.0" xml:base="https://news.mit.edu" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>麻省理工学院新闻 - 计算机科学与人工智能实验室 (CSAIL)</title><link/> https://news.mit.edu/topic/mitcomputers-rss.xml <atom:link href="https://news.mit.edu/topic/mitcomputers-rss.xml" rel="self" type="application/rss+xml"></atom:link><description>麻省理工学院新闻提要：计算机科学与人工智能实验室 (CSAIL)</description><language> zh</language><lastbuilddate> 2023 年 6 月 30 日星期五 00:00:00 -0400</lastbuilddate><item><title>研究人员教人工智能编写更好的图表标题</title><link/>https://news.mit.edu/2023/researchers-chart-captions-ai-vistext-0630<description>新的数据集可以帮助科学家开发自动系统，为在线图表生成更丰富、更具描述性的标题。</description><pubDate> Fri, 30 Jun 2023 00:00:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2023/researchers-chart-captions-ai-vistext-0630</guid><dc:creator>亚当·泽威 |麻省理工学院新闻办公室</dc:creator><content:encoded>&lt;p>;解释复杂趋势和模式的图表标题对于提高读者理解和保留所呈现数据的能力非常重要。对于视力障碍人士来说，标题中的信息通常是他们理解图表的唯一手段。&lt;/p>; &lt;p>;但是编写有效、详细的标题是一个劳动密集型过程。虽然自动字幕技术可以减轻这种负担，但它们通常很难描述提供额外上下文的认知特征。&lt;/p>; &lt;p>;为了帮助人们创作高质量的图表字幕，麻省理工学院的研究人员开发了一个数据集来改进自动字幕系统。使用此工具，研究人员可以教授机器学习模型，以根据用户的需求改变图表标题中包含的内容类型和复杂程度。&lt;/p>; &lt;p>;麻省理工学院的研究人员发现，机器学习模型经过自动字幕训练，他们的数据集始终生成精确、语义丰富的字幕，并描述了数据趋势和复杂模式。定量和定性分析表明，他们的模型比其他自动字幕系统更有效地为图表添加字幕。 &lt;/p>; &lt;p>;该团队的目标是提供名为 VisText 的数据集，作为研究人员在解决图表自动字幕这一棘手问题时可以使用的工具。麻省理工学院电气工程和计算机科学专业的研究生、计算机科学和计算机科学可视化小组成员、共同主要作者 Angie Boggust 表示，这些自动系统可以帮助为无字幕的在线图表提供字幕，并提高视力障碍人士的可访问性。人工智能实验室（CSAIL）。&lt;/p>; &lt;p>;“我们尝试将许多人类价值观嵌入到我们的数据集中，这样当我们和其他研究人员构建自动图表标题系统时，我们就不会最终陷入困境。其模型并不是人们想要或需要的，”她说。&lt;/p>; &lt;p>;博古斯特加入了 &lt;a href=&quot;https://vis.mit.edu/pubs/vistext.pdf&quot; 目标=&quot;_blank&quot;>;论文&lt;/a>;由共同主要作者兼研究生 Benny J. Tang 和高级作者 Arvind Satyanarayan（麻省理工学院计算机科学副教授，领导 CSAIL 可视化小组）撰写。该研究将在计算语言学协会年会上公布。&lt;/p>; &lt;p>;&lt;strong>;以人为中心的分析&lt;/strong>;&lt;/p>; &lt;p>;研究人员受到启发，开发了 VisText可视化小组的 href=&quot;https://news.mit.edu/2021/data-visualizations-accessible-blind-1012&quot; target=&quot;_blank&quot;>;之前的工作&lt;/a>;，探讨了如何制作良好的图表标题。在这项研究中，研究人员发现，视力正常的用户和盲人或低视力用户对字幕中语义内容的复杂性有不同的偏好。&lt;/p>; &lt;p>;该小组希望将以人为本的分析引入自动字幕中研究。为此，他们开发了 VisText，这是一个图表和相关字幕的数据集，可用于训练机器学习模型以生成准确、语义丰富、可自定义的字幕。&lt;/p>; &lt;p>;开发有效的自动字幕系统并非易事。现有的机器学习方法通​​常尝试以图像的方式为图表添加标题，但人和模型对自然图像的解释与我们阅读图表的方式不同。其他技术完全跳过视觉内容，并使用其基础数据表为图表添加标题。然而，图表发布后往往无法获得此类数据表。&lt;/p>; &lt;p>;鉴于使用图像和数据表的不足，VisText 还将图表表示为场景图。场景图可以从图表图像中提取，包含所有图表数据，但还包括其他图像上下文。&lt;/p>; &lt;p>;“场景图就像两全其美 - 它包含几乎所有存在的信息在图像中，同时比数据表更容易从图像中提取。由于它也是文本，我们可以利用现代大型语言模型的进步来制作字幕。”Tang 解释道。&lt;/p>; &lt;p>;他们编译了一个包含 12,000 多个图表的数据集，每个图表都表示为数据表、图像和场景图表——以及相关的标题。每个图表都有两个单独的标题：一个描述图表结构（如轴范围）的低级标题和一个描述统计数据、数据关系和复杂趋势的高级标题。&lt;/p>; &lt;p>;研究人员使用自动化系统生成低级字幕，并从人类工作者那里众包更高级别的字幕。&lt;/p>; &lt;p>;“我们的字幕基于先前研究的两项关键内容：&lt;a href=&quot;https: //dl.acm.org/doi/10.1145/2764916&quot; target=&quot;_blank&quot;>;视觉媒体的可访问描述&lt;/a>;以及我们小组的&lt;a href=&quot;https://news.mit. edu/2021/data-visualizations-accessible-blind-1012&quot; target=&quot;_blank&quot;>;对语义内容进行分类&lt;/a>;。这确保了我们的字幕能够为视力障碍读者提供重要的低级图表元素，例如轴、比例和单位，同时保留字幕编写方式的人类差异。”Tang 说道。&lt;/p>; &lt;p>;&lt;strong>;翻译图表&lt;/strong>;&lt;/p>; &lt;p>;收集图表图像和标题后，研究人员使用 VisText 训练五种用于自动字幕的机器学习模型。他们想了解每种表示形式（图像、数据表和场景图）以及表示形式的组合如何影响字幕的质量。&lt;/p>; &lt;p>;“您可以将图表字幕模型视为语言模型翻译。但我们不是说将这段德语文本翻译成英语，而是说将这种“图表语言”翻译成英语。”Boggust 说。&lt;/p>; &lt;p>;他们的结果表明，使用场景图训练的模型表现得与使用场景图训练的模型一样好甚至更好。那些使用数据表进行训练的人。由于场景图更容易从现有图表中提取，研究人员认为它们可能是一种更有用的表示形式。&lt;/p>; &lt;p>;他们还分别使用低级和高级标题训练模型。这种技术被称为语义前缀调整，使他们能够教导模型改变字幕内容的复杂性。&lt;/p>; &lt;p>;此外，他们对由性能最佳的方法生成的字幕进行了定性检查，并进行了分类六种常见错误。例如，如果模型显示趋势正在减少，而实际上趋势在增加，则就会出现方向性错误。&lt;/p>; &lt;p>;这种细粒度、稳健的定性评估对于理解模型如何犯错非常重要。例如，使用定量方法，方向错误可能会招致与重复错误相同的惩罚，其中模型重复相同的单词或短语。但方向错误可能比重复错误更容易误导用户。博格斯特说，定性分析帮助他们理解这些类型的微妙之处。&lt;/p>; &lt;p>;她补充说，这些类型的错误也暴露了当前模型的局限性，并提出了研究人员在开发自动字幕系统时必须考虑的伦理考虑。 &lt;/p>; &lt;p>;生成机器学习模型（例如为 ChatGPT 提供支持的模型）已被证明会产生幻觉或提供可能具有误导性的错误信息。虽然使用这些模型为现有图表自动添加标题有明显的好处，但如果图表的标题不正确，可能会导致错误信息的传播。&lt;/p>; &lt;p>;“也许这意味着我们不仅仅为看到的所有内容添加标题与人工智能。相反，也许我们提供这些自动字幕系统作为作者工具供人们编辑。在整个研究过程中考虑这些伦理影响非常重要，而不仅仅是在我们有要部署的模型时才考虑，”她说。&lt;/p>; &lt;p>;Boggust、Tang 和他们的同事希望继续优化模型以减少一些常见错误。他们还希望扩展 VisText 数据集以包含更多图表和更复杂的图表，例如具有堆叠条形图或多条线的图表。他们还希望深入了解这些自动字幕模型实际上正在了解图表数据的哪些内容。&lt;/p>; &lt;p>;这项研究部分得到了 Google 研究学者奖、国家科学基金会、MLA@ 的支持CSAIL 计划和美国空军研究实验室。&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202306/MIT-vistext-01-press.jpg?itok=er2jZAiv" width="390"><media:description type="plain">一种新工具可帮助科学家开发机器学习模型，为图表生成更丰富、更详细的说明文字，并根据用户的需求改变说明文字的复杂程度。这可以帮助为无字幕的在线图表提供字幕，并提高视障人士的可访问性。</media:description><media:credit>图片来源：Jose-Luis Olivares/麻省理工学院</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/assistive-technology">辅助技术</category><category domain="https://news.mit.edu/topic/data">数据</category><category domain="https://news.mit.edu/topic/science-communications">科学传播</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL） </category><category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">电气工程与计算机科学（eecs）</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category><category domain="https://news.mit.edu/topic/nsf">美国国家科学基金会 (NSF)</category></item><item><title>计算机视觉系统将图像识别和生成结合在一起</title><link/>https://news.mit.edu/2023/computer-vision-system-marries-image-recognition- Generation-0628<description> MAGE 将图像生成和识别这两个关键任务（通常单独训练）合并到一个系统中。</description><pubDate> Wed, 28 Jun 2023 15:50:00 -0400</pubDate><guid ispermalink="true"> https://news.mit.edu/2023/computer-vision-system-marries-image-recognition- Generation-0628</guid><dc:creator>雷切尔戈登|麻省理工学院计算机科学与人工智能实验室</dc:creator><content:encoded>&lt;p>;计算机在图像方面拥有两种非凡的能力：它们既可以识别图像，也可以重新生成图像。从历史上看，这些功能是分开的，就像善于创造菜肴的厨师（一代）和善于品尝菜肴的鉴赏家（识别）的不同行为一样。&lt;/p>; &lt;p>;然而，人们可以我们不禁想知道：如何才能在这两种独特的能力之间协调一致？厨师和鉴赏家对食物的味道有着共同的理解。同样，统一的视觉系统需要对视觉世界有深入的了解。&lt;/p>; &lt;p>;现在，麻省理工学院计算机科学与人工智能实验室 (CSAIL) 的研究人员已经训练了一个系统来推断图像中缺失的部分，即需要深入理解图像内容的任务。通过成功填补空白，该系统（称为&lt;a href=&quot;https://arxiv.org/abs/2211.09117&quot; target=&quot;_blank&quot;>;掩码生成编码器&lt;/a>;（MAGE））实现了两个目标同时：准确识别图像并创建与现实惊人相似的新图像。&lt;/p>; &lt;p>;这个双用途系统可实现无数潜在的应用，例如图像中的对象识别和分类、从最小的示例中快速学习、在文本或类等特定条件下创建图像，以及增强现有图像。&lt;/p>; &lt;p>;与其他技术不同，MAGE 不适用于原始像素。相反，它将图像转换为所谓的“语义标记”，这是图像部分的紧凑但抽象的版本。将这些标记视为迷你拼图块，每个块代表原始图像的 16x16 块。正如单词形成句子一样，这些标记创建了图像的抽象版本，可用于复杂的处理任务，同时保留原始图像中的信息。这样的标记化步骤可以在自监督框架内进行训练，使其能够在没有标签的大型图像数据集上进行预训练。&lt;/p>; &lt;p>;现在，当 MAGE 使用“屏蔽标记建模”时，魔法就开始了。它随机隐藏其中一些标记，创建一个不完整的谜题，然后训练神经网络来填补空白。通过这种方式，它既可以学习理解图像中的模式（图像识别），又可以生成新的模式（图像生成）。&lt;/p>; &lt;p>;“MAGE 的一个显着部分是预训练期间的可变掩蔽策略，允许麻省理工学院电气工程和计算机科学博士生、CSAIL 附属机构、&lt;a href=&quot;https 文章的主要作者://arxiv.org/abs/2211.09117&quot; target=&quot;_blank&quot;>;有关研究的论文&lt;/a>;。 “MAGE 能够在‘标记空间’而不是‘像素空间’中工作，从而生成清晰、详细和高质量的图像，以及语义丰富的图像表示。这有望为先进的集成计算机视觉模型铺平道路。”&lt;/p>; &lt;p>;除了从头开始生成逼真图像的能力之外，MAGE 还允许生成条件图像。用户可以为他们想要 MAGE 生成的图像指定某些标准，该工具将生成适当的图像。它还能够执行图像编辑任务，例如从图像中删除元素，同时保持逼真的外观。&lt;/p>; &lt;p>;识别任务是 MAGE 的另一个强项。凭借其对大型未标记数据集进行预训练的能力，它可以仅使用学习到的表示来对图像进行分类。此外，它在少样本学习方面表现出色，只需少量标记示例即可在 ImageNet 等大型图像数据集上取得令人印象深刻的结果。&lt;/p>; &lt;p>;MAGE 性能的验证令人印象深刻。一方面，它在生成新图像方面创造了新记录，比之前的模型有了显着的改进。另一方面，MAGE 在识别任务中名列前茅，在线性探测中实现了 80.9% 的准确率，在 ImageNet 上实现了 71.9% 的 10-shot 准确率（这意味着它在 71.9% 的情况下正确识别了图像，其中每个图像只有 10 个标记示例）类）。&lt;/p>; &lt;p>;尽管 MAGE 具有优势，但研究团队承认它是一项正在进行中的工作。将图像转换为令牌的过程不可避免地会导致一些信息丢失。他们热衷于探索在未来的工作中压缩图像而不丢失重要细节的方法。该团队还打算在更大的数据集上测试 MAGE。未来的探索可能包括在更大的未标记数据集上训练 MAGE，这可能会带来更好的性能。&lt;/p>; &lt;p>;“在一个系统中实现图像生成和图像识别一直是一个长期的梦想。 MAGE 是一项开创性的研究，它成功地利用了这两项任务的协同作用，并在一个系统中实现了最先进的技术。”研究和机器智能领域人类与交互高级软件工程师 Huisheng Wang 说道谷歌部门，他没有参与这项工作。 “这个创新系统具有广泛的应用，并且有潜力激发计算机视觉领域的许多未来工作。”&amp;nbsp;&lt;/p>; &lt;p>;Li 与 Dina Katabi、Thuan 和 Nicole Pham 一起撰写了这篇论文麻省理工学院电气工程与计算机科学系教授、CSAIL 首席研究员； Huiwen Chang，谷歌高级研究科学家； Shlok Kumar Mishra，马里兰大学博士生、Google 研究实习生；张涵，谷歌高级研究科学家；以及谷歌研究科学家迪利普·克里希南 (Dilip Krishnan)。计算资源由 Google Cloud Platform 和 MIT-IBM Watson AI 实验室提供。该团队的研究成果已在 2023 年计算机视觉和模式识别会议上发表。&lt;/p>; </content:encoded><media:content height="260" medium="image" type="image/jpeg" url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202306/MIT-MAGE-cov.jpg?itok=hpWyepGb" width="390"><media:description type="plain">麻省理工学院和谷歌的研究人员开发了一种称为 Msked Generative Encoder (MAGE) 的统一视觉系统，它可以用于许多事情，例如查找和分类图像中的对象、从几个示例中学习、生成具有特定条件的图像，例如作为文本或类、编辑现有图像等等。</media:description><media:credit>图片来源：Alex Shipps/MIT CSAIL，来自 Midjourney</media:credit></media:content><category domain="https://news.mit.edu/topic/research">研究</category><category domain="https://news.mit.edu/topic/computer-vision">计算机视觉</category><category domain="https://news.mit.edu/topic/artificial-intelligence2">人工智能</category><category domain="https://news.mit.edu/topic/computers">计算机科学与技术</category><category domain="https://news.mit.edu/topic/machine-learning">机器学习</category><category domain="https://news.mit.edu/topic/arts">艺术</category><category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">电气工程与计算机科学（eecs）</category><category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">麻省理工学院-IBM沃森人工智能实验室</category><category domain="https://news.mit.edu/topic/school-science">理学院</category><category domain="https://news.mit.edu/topic/school-engineering">工程学院</category><category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">计算机科学与人工智能实验室（CSAIL）</category><category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">麻省理工学院施瓦茨曼计算学院</category></item></channel></rss>